{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaEecmENUYRE"
   },
   "source": [
    "# Machine Learning Term Project\n",
    "\n",
    "## Sihwa Park\n",
    "\n",
    "## 0. Enviroment.\n",
    "I used google colab ipython notebook with Python3.\n",
    "\n",
    "## 1. Task Definition\n",
    "I found the data from Kaggle(https://www.kaggle.com/ramamet4/app-store-apple-data-set-10k-apps). This data is from iOS Appstore which contains information about 7200+ apps. The task I want to do is predicting the app's current rate. The data has general information about app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xuJk_I8NdhDg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fD8lwA6-yAB3",
    "outputId": "33ae3cb7-2299-45dd-ae3a-97ad41e05111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZzZfBoRyfjl"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/AppleStore.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmzud1Q2Xe_Q"
   },
   "source": [
    "## 2. Dataset Description\n",
    "In this data set there are 16 attributes and information about 7197 apps.\n",
    "\n",
    "1. \"id\" : App ID\n",
    "\n",
    "2. \"track_name\": App Name\n",
    "\n",
    "3. \"size_bytes\": Size (in Bytes)\n",
    "\n",
    "4. \"currency\": 100% USD\n",
    "\n",
    "5. \"price\": App Price\n",
    "\n",
    "6. \"rating_count_tot\": rating count for all versions of the app\n",
    "\n",
    "7. \"rating_count_ver\": rating count for current version of the app\n",
    "\n",
    "8. \"user_rating\" : average rating for all versions of the app\n",
    "\n",
    "**9. \"user_rating_ver\": average rating for current version of the app - what we want to predict**\n",
    "\n",
    "10. \"ver\" : newest version (ex. 10.4.1)\n",
    "\n",
    "11. \"cont_rating\": restricted age\n",
    "\n",
    "12. \"prime_genre\": genre\n",
    "\n",
    "13. \"sup_devices.num\": supporting device number\n",
    "\n",
    "14. \"ipadSc_urls.num\": the number of submitted screenshot images about the app (max 5)\n",
    "\n",
    "15. \"lang.num\": supporting language\n",
    "\n",
    "16. \"vpp_lic\": vpp license enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "qy0qSn0gTMT4",
    "outputId": "d9f703da-6720-4f10-92dd-48fafe9b02e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7197 entries, 0 to 7196\n",
      "Data columns (total 17 columns):\n",
      "Unnamed: 0          7197 non-null int64\n",
      "id                  7197 non-null int64\n",
      "track_name          7197 non-null object\n",
      "size_bytes          7197 non-null int64\n",
      "currency            7197 non-null object\n",
      "price               7197 non-null float64\n",
      "rating_count_tot    7197 non-null int64\n",
      "rating_count_ver    7197 non-null int64\n",
      "user_rating         7197 non-null float64\n",
      "user_rating_ver     7197 non-null float64\n",
      "ver                 7197 non-null object\n",
      "cont_rating         7197 non-null object\n",
      "prime_genre         7197 non-null object\n",
      "sup_devices.num     7197 non-null int64\n",
      "ipadSc_urls.num     7197 non-null int64\n",
      "lang.num            7197 non-null int64\n",
      "vpp_lic             7197 non-null int64\n",
      "dtypes: float64(3), int64(9), object(5)\n",
      "memory usage: 955.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "colab_type": "code",
    "id": "cDZ8pM7nXwhX",
    "outputId": "a41934ca-69a4-4bd9-e87f-86d5e878d226"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>currency</th>\n",
       "      <th>price</th>\n",
       "      <th>rating_count_tot</th>\n",
       "      <th>rating_count_ver</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>user_rating_ver</th>\n",
       "      <th>ver</th>\n",
       "      <th>cont_rating</th>\n",
       "      <th>prime_genre</th>\n",
       "      <th>sup_devices.num</th>\n",
       "      <th>ipadSc_urls.num</th>\n",
       "      <th>lang.num</th>\n",
       "      <th>vpp_lic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>281656475</td>\n",
       "      <td>PAC-MAN Premium</td>\n",
       "      <td>100788224</td>\n",
       "      <td>USD</td>\n",
       "      <td>3.99</td>\n",
       "      <td>21292</td>\n",
       "      <td>26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.3.5</td>\n",
       "      <td>4+</td>\n",
       "      <td>Games</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>281796108</td>\n",
       "      <td>Evernote - stay organized</td>\n",
       "      <td>158578688</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>161065</td>\n",
       "      <td>26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.2.2</td>\n",
       "      <td>4+</td>\n",
       "      <td>Productivity</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>281940292</td>\n",
       "      <td>WeatherBug - Local Weather, Radar, Maps, Alerts</td>\n",
       "      <td>100524032</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>188583</td>\n",
       "      <td>2822</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0.0</td>\n",
       "      <td>4+</td>\n",
       "      <td>Weather</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>282614216</td>\n",
       "      <td>eBay: Best App to Buy, Sell, Save! Online Shop...</td>\n",
       "      <td>128512000</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>262241</td>\n",
       "      <td>649</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.10.0</td>\n",
       "      <td>12+</td>\n",
       "      <td>Shopping</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>282935706</td>\n",
       "      <td>Bible</td>\n",
       "      <td>92774400</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>985920</td>\n",
       "      <td>5320</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.5.1</td>\n",
       "      <td>4+</td>\n",
       "      <td>Reference</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         id                                         track_name  \\\n",
       "0           1  281656475                                    PAC-MAN Premium   \n",
       "1           2  281796108                          Evernote - stay organized   \n",
       "2           3  281940292    WeatherBug - Local Weather, Radar, Maps, Alerts   \n",
       "3           4  282614216  eBay: Best App to Buy, Sell, Save! Online Shop...   \n",
       "4           5  282935706                                              Bible   \n",
       "\n",
       "   size_bytes currency  price  rating_count_tot  rating_count_ver  \\\n",
       "0   100788224      USD   3.99             21292                26   \n",
       "1   158578688      USD   0.00            161065                26   \n",
       "2   100524032      USD   0.00            188583              2822   \n",
       "3   128512000      USD   0.00            262241               649   \n",
       "4    92774400      USD   0.00            985920              5320   \n",
       "\n",
       "   user_rating  user_rating_ver     ver cont_rating   prime_genre  \\\n",
       "0          4.0              4.5   6.3.5          4+         Games   \n",
       "1          4.0              3.5   8.2.2          4+  Productivity   \n",
       "2          3.5              4.5   5.0.0          4+       Weather   \n",
       "3          4.0              4.5  5.10.0         12+      Shopping   \n",
       "4          4.5              5.0   7.5.1          4+     Reference   \n",
       "\n",
       "   sup_devices.num  ipadSc_urls.num  lang.num  vpp_lic  \n",
       "0               38                5        10        1  \n",
       "1               37                5        23        1  \n",
       "2               37                5         3        1  \n",
       "3               37                5         9        1  \n",
       "4               37                5        45        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTSe-fLclqYF"
   },
   "source": [
    "##2. Preprocessing\n",
    "###1) row number가 들어가 있는 칼럼, id 칼럼(앱 id, 앱 이름, 최신 version) 을 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cn0N75nYnsLh"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0', 'track_name', 'id', 'ver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "W6vvzSeoiqKk",
    "outputId": "6c17b579-2734-422b-fe8f-fbb3a7e3fc4c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>price</th>\n",
       "      <th>rating_count_tot</th>\n",
       "      <th>rating_count_ver</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>user_rating_ver</th>\n",
       "      <th>sup_devices.num</th>\n",
       "      <th>ipadSc_urls.num</th>\n",
       "      <th>lang.num</th>\n",
       "      <th>vpp_lic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.197000e+03</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7.197000e+03</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.991345e+08</td>\n",
       "      <td>1.726218</td>\n",
       "      <td>1.289291e+04</td>\n",
       "      <td>460.373906</td>\n",
       "      <td>3.526956</td>\n",
       "      <td>3.253578</td>\n",
       "      <td>37.361817</td>\n",
       "      <td>3.707100</td>\n",
       "      <td>5.434903</td>\n",
       "      <td>0.993053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.592069e+08</td>\n",
       "      <td>5.833006</td>\n",
       "      <td>7.573941e+04</td>\n",
       "      <td>3920.455183</td>\n",
       "      <td>1.517948</td>\n",
       "      <td>1.809363</td>\n",
       "      <td>3.737715</td>\n",
       "      <td>1.986005</td>\n",
       "      <td>7.919593</td>\n",
       "      <td>0.083066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.898240e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.692275e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.715302e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.819249e+08</td>\n",
       "      <td>1.990000</td>\n",
       "      <td>2.793000e+03</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.025970e+09</td>\n",
       "      <td>299.990000</td>\n",
       "      <td>2.974676e+06</td>\n",
       "      <td>177050.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         size_bytes        price  rating_count_tot  rating_count_ver  \\\n",
       "count  7.197000e+03  7197.000000      7.197000e+03       7197.000000   \n",
       "mean   1.991345e+08     1.726218      1.289291e+04        460.373906   \n",
       "std    3.592069e+08     5.833006      7.573941e+04       3920.455183   \n",
       "min    5.898240e+05     0.000000      0.000000e+00          0.000000   \n",
       "25%    4.692275e+07     0.000000      2.800000e+01          1.000000   \n",
       "50%    9.715302e+07     0.000000      3.000000e+02         23.000000   \n",
       "75%    1.819249e+08     1.990000      2.793000e+03        140.000000   \n",
       "max    4.025970e+09   299.990000      2.974676e+06     177050.000000   \n",
       "\n",
       "       user_rating  user_rating_ver  sup_devices.num  ipadSc_urls.num  \\\n",
       "count  7197.000000      7197.000000      7197.000000      7197.000000   \n",
       "mean      3.526956         3.253578        37.361817         3.707100   \n",
       "std       1.517948         1.809363         3.737715         1.986005   \n",
       "min       0.000000         0.000000         9.000000         0.000000   \n",
       "25%       3.500000         2.500000        37.000000         3.000000   \n",
       "50%       4.000000         4.000000        37.000000         5.000000   \n",
       "75%       4.500000         4.500000        38.000000         5.000000   \n",
       "max       5.000000         5.000000        47.000000         5.000000   \n",
       "\n",
       "          lang.num      vpp_lic  \n",
       "count  7197.000000  7197.000000  \n",
       "mean      5.434903     0.993053  \n",
       "std       7.919593     0.083066  \n",
       "min       0.000000     0.000000  \n",
       "25%       1.000000     1.000000  \n",
       "50%       1.000000     1.000000  \n",
       "75%       8.000000     1.000000  \n",
       "max      75.000000     1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_V3mrIfOoC6R"
   },
   "source": [
    "### 2) Delete columns with almost one value while looking at the distribution by column.\n",
    "For price column the graph looks like one value because differences between values is very small and for rating_count_tot and rating_count_ver, the x interval is quite big so it looks like one value. In fact if we do df.describe() their standard deviation is quite high. By looking at this histogram we will erase columns currency and vpp_lic which has almost same value for all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "colab_type": "code",
    "id": "_1SMfhnOhX4K",
    "outputId": "76f244e0-21c3-45be-c43d-d7e3a58563b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f785afaf588>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f785af827f0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f785af398d0>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f785afcbdd8>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f785aea34e0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f785aea3518>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f785ae11710>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f785adca780>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f785adfc400>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f785adb34e0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f785ad6d550>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f785ad826a0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAJNCAYAAAAGSrD3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f785afa83c8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kB5gO7v0jEi7"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['currency', 'vpp_lic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3krp68nUm8Y"
   },
   "source": [
    "### 3) normalize & delete outliers\n",
    "Normalize columns with continuous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "RzSRkm3FUtc9",
    "outputId": "6e9f2881-5c49-4d9f-9c6e-fbf548523d21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "x = df[['size_bytes']].values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df[['size_bytes']] = pd.DataFrame(x_scaled)\n",
    "x = df[['price']].values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df[['price']] = pd.DataFrame(x_scaled)\n",
    "x = df[['rating_count_tot']].values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df[['rating_count_tot']] = pd.DataFrame(x_scaled)\n",
    "x = df[['rating_count_ver']].values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df[['rating_count_ver']] = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "8AZDpQQAHV58",
    "outputId": "39efbd78-7f48-4263-9f6a-30c1d9c04a4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x = df[['sup_devices.num']].values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df[['sup_devices.num']] = pd.DataFrame(x_scaled)\n",
    "x = df[['ipadSc_urls.num']].values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df[['ipadSc_urls.num']] = pd.DataFrame(x_scaled)\n",
    "x = df[['lang.num']].values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df[['lang.num']] = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "-O-IgPkaEIVx",
    "outputId": "e0f01b9d-e09f-45b3-9caa-0110266e7ede"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>price</th>\n",
       "      <th>rating_count_tot</th>\n",
       "      <th>rating_count_ver</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>user_rating_ver</th>\n",
       "      <th>sup_devices.num</th>\n",
       "      <th>ipadSc_urls.num</th>\n",
       "      <th>lang.num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "      <td>7197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.049323</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.004334</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>3.526956</td>\n",
       "      <td>3.253578</td>\n",
       "      <td>0.746364</td>\n",
       "      <td>0.741420</td>\n",
       "      <td>0.072465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.089236</td>\n",
       "      <td>0.019444</td>\n",
       "      <td>0.025461</td>\n",
       "      <td>0.022143</td>\n",
       "      <td>1.517948</td>\n",
       "      <td>1.809363</td>\n",
       "      <td>0.098361</td>\n",
       "      <td>0.397201</td>\n",
       "      <td>0.105595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.011510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.023989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.045048</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.106667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        size_bytes        price  rating_count_tot  rating_count_ver  \\\n",
       "count  7197.000000  7197.000000       7197.000000       7197.000000   \n",
       "mean      0.049323     0.005754          0.004334          0.002600   \n",
       "std       0.089236     0.019444          0.025461          0.022143   \n",
       "min       0.000000     0.000000          0.000000          0.000000   \n",
       "25%       0.011510     0.000000          0.000009          0.000006   \n",
       "50%       0.023989     0.000000          0.000101          0.000130   \n",
       "75%       0.045048     0.006634          0.000939          0.000791   \n",
       "max       1.000000     1.000000          1.000000          1.000000   \n",
       "\n",
       "       user_rating  user_rating_ver  sup_devices.num  ipadSc_urls.num  \\\n",
       "count  7197.000000      7197.000000      7197.000000      7197.000000   \n",
       "mean      3.526956         3.253578         0.746364         0.741420   \n",
       "std       1.517948         1.809363         0.098361         0.397201   \n",
       "min       0.000000         0.000000         0.000000         0.000000   \n",
       "25%       3.500000         2.500000         0.736842         0.600000   \n",
       "50%       4.000000         4.000000         0.736842         1.000000   \n",
       "75%       4.500000         4.500000         0.763158         1.000000   \n",
       "max       5.000000         5.000000         1.000000         1.000000   \n",
       "\n",
       "          lang.num  \n",
       "count  7197.000000  \n",
       "mean      0.072465  \n",
       "std       0.105595  \n",
       "min       0.000000  \n",
       "25%       0.013333  \n",
       "50%       0.013333  \n",
       "75%       0.106667  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJ9Lh_3nJumR"
   },
   "source": [
    "Consider value bigger than the average +-2*sigma  as outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEol2r6rE4hv"
   },
   "outputs": [],
   "source": [
    "df_new = df.copy()\n",
    "df_new = df_new.drop(df_new[(df_new[\"size_bytes\"]>0.049323+2*0.089236) | (df_new[\"size_bytes\"]<0.049323-2*0.089236)].index)\n",
    "df_new = df_new.drop(df_new[(df_new[\"price\"]>0.005754+2*0.019444) | (df_new[\"price\"]<0.005754-2*0.019444)].index)\n",
    "df_new = df_new.drop(df_new[(df_new[\"rating_count_tot\"]>0.004334+2*0.025461) | (df_new[\"rating_count_tot\"]<0.004334-2*0.025461)].index)\n",
    "df_new = df_new.drop(df_new[(df_new[\"rating_count_ver\"]>0.002600+2*0.022143) | (df_new[\"rating_count_ver\"]<0.002600-2*0.022143)].index)\n",
    "df_new = df_new.drop(df_new[(df_new[\"sup_devices.num\"]>0.746364+2*0.098361) | (df_new[\"sup_devices.num\"]<0.746364-2*0.098361)].index)\n",
    "df_new = df_new.drop(df_new[(df_new[\"ipadSc_urls.num\"]>0.741420+2*0.397201) | (df_new[\"ipadSc_urls.num\"]<0.741420-2*0.397201)].index)\n",
    "df_new = df_new.drop(df_new[(df_new[\"lang.num\"]>0.072465+2*0.105595) | (df_new[\"lang.num\"]<0.072465-2*0.105595)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GNcgiCQEJkv2",
    "outputId": "a7526b27-fb75-40cb-dc54-b48d9681a0ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6014, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_old = df.copy()\n",
    "df = df_new.copy()\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SkTCaHimmJMz"
   },
   "source": [
    "After deleting outliers, remaining instances are 6014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gctGa5R4_qXk"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g5BRKGj5AOVk"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "id": "LgUVgJHfASdl",
    "outputId": "da61b5b8-384e-4cef-cb4d-ff4ec7c197da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>price</th>\n",
       "      <th>rating_count_tot</th>\n",
       "      <th>rating_count_ver</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>user_rating_ver</th>\n",
       "      <th>cont_rating</th>\n",
       "      <th>prime_genre</th>\n",
       "      <th>sup_devices.num</th>\n",
       "      <th>ipadSc_urls.num</th>\n",
       "      <th>lang.num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024892</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.007158</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4+</td>\n",
       "      <td>Games</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.056443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040168</td>\n",
       "      <td>0.004965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4+</td>\n",
       "      <td>Finance</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012088</td>\n",
       "      <td>0.033301</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4+</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017249</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4+</td>\n",
       "      <td>Games</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012180</td>\n",
       "      <td>0.016634</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4+</td>\n",
       "      <td>Games</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.146667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size_bytes     price  rating_count_tot  rating_count_ver  user_rating  \\\n",
       "0    0.024892  0.013300          0.007158          0.000147          4.0   \n",
       "1    0.056443  0.000000          0.040168          0.004965          4.0   \n",
       "2    0.012088  0.033301          0.000376          0.000023          4.5   \n",
       "3    0.017249  0.013300          0.002651          0.000226          4.0   \n",
       "4    0.012180  0.016634          0.025791          0.022689          4.5   \n",
       "\n",
       "   user_rating_ver cont_rating prime_genre  sup_devices.num  ipadSc_urls.num  \\\n",
       "0              4.5          4+       Games         0.763158              1.0   \n",
       "1              4.5          4+     Finance         0.736842              0.0   \n",
       "2              5.0          4+   Utilities         0.736842              1.0   \n",
       "3              4.0          4+       Games         0.763158              0.0   \n",
       "4              4.5          4+       Games         0.763158              0.8   \n",
       "\n",
       "   lang.num  \n",
       "0  0.133333  \n",
       "1  0.253333  \n",
       "2  0.013333  \n",
       "3  0.133333  \n",
       "4  0.146667  "
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "laBRr8GyjUCu"
   },
   "source": [
    "### 3) Encoding\n",
    "We will encode cont_rating and prime_genre column. For cont_rating it goes up like 4+, 17+ by criteria of violent and sexually suggestive, I will convert it to an integer from 0 with interval 1. For prime_gengre, it is difficult to convert every genre in one value, I will just change it to one hot vector. For out target user_rating_ver, now it is float datatype but in fact it has values with 0 to 5 with 0.5 interval. So I will convert it to one hot vector. The task is to predict a value with 0.5 interval I consider this task as classification not regression. Hence in the end I thought the model choose one rate using softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhDLQ7VDnSIl"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "XFpbBNKFKPG3",
    "outputId": "0fd4156c-5e94-4477-86a6-d777568a94b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6014 entries, 0 to 6013\n",
      "Data columns (total 11 columns):\n",
      "size_bytes          6014 non-null float64\n",
      "price               6014 non-null float64\n",
      "rating_count_tot    6014 non-null float64\n",
      "rating_count_ver    6014 non-null float64\n",
      "user_rating         6014 non-null float64\n",
      "user_rating_ver     6014 non-null float64\n",
      "cont_rating         6014 non-null object\n",
      "prime_genre         6014 non-null object\n",
      "sup_devices.num     6014 non-null float64\n",
      "ipadSc_urls.num     6014 non-null float64\n",
      "lang.num            6014 non-null float64\n",
      "dtypes: float64(9), object(2)\n",
      "memory usage: 516.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9PkZUipwwfGC",
    "outputId": "218b8a02-69fd-4d32-e50e-73304c64797c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4+'"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnS_fOvnt0Vo"
   },
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "  df.iloc[index, 6] = int(df.iloc[index, 6][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1071
    },
    "colab_type": "code",
    "id": "s5-ZBUe2AjiU",
    "outputId": "a9701de1-32eb-4720-ae4b-f85a506b2220"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4\n",
       "1        4\n",
       "2        4\n",
       "3        4\n",
       "4        4\n",
       "5        4\n",
       "6        4\n",
       "7        4\n",
       "8        4\n",
       "9        4\n",
       "10       4\n",
       "11       4\n",
       "12       4\n",
       "13       4\n",
       "14       9\n",
       "15       4\n",
       "16       4\n",
       "17      12\n",
       "18       4\n",
       "19       4\n",
       "20       9\n",
       "21       4\n",
       "22      17\n",
       "23       9\n",
       "24       4\n",
       "25      12\n",
       "26       4\n",
       "27       4\n",
       "28       4\n",
       "29       9\n",
       "        ..\n",
       "5984     4\n",
       "5985     4\n",
       "5986     4\n",
       "5987     4\n",
       "5988     9\n",
       "5989     4\n",
       "5990     4\n",
       "5991     4\n",
       "5992     4\n",
       "5993     4\n",
       "5994     4\n",
       "5995     4\n",
       "5996     4\n",
       "5997     9\n",
       "5998     9\n",
       "5999     4\n",
       "6000     9\n",
       "6001     9\n",
       "6002     9\n",
       "6003     4\n",
       "6004     4\n",
       "6005    12\n",
       "6006     9\n",
       "6007     4\n",
       "6008    17\n",
       "6009     4\n",
       "6010     4\n",
       "6011     9\n",
       "6012    12\n",
       "6013     4\n",
       "Name: cont_rating, Length: 6014, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cont_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NPVGAEKln0Pv",
    "outputId": "883c9ebc-b738-4cf5-c26e-f766c2579700"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 4,  9, 12, 17])]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "ord_enc.fit(df[['cont_rating']])\n",
    "ord_enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1071
    },
    "colab_type": "code",
    "id": "7NHjkyUcxcda",
    "outputId": "2a86b859-e38a-418d-92a6-78df671c287e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "5       0.0\n",
       "6       0.0\n",
       "7       0.0\n",
       "8       0.0\n",
       "9       0.0\n",
       "10      0.0\n",
       "11      0.0\n",
       "12      0.0\n",
       "13      0.0\n",
       "14      1.0\n",
       "15      0.0\n",
       "16      0.0\n",
       "17      2.0\n",
       "18      0.0\n",
       "19      0.0\n",
       "20      1.0\n",
       "21      0.0\n",
       "22      3.0\n",
       "23      1.0\n",
       "24      0.0\n",
       "25      2.0\n",
       "26      0.0\n",
       "27      0.0\n",
       "28      0.0\n",
       "29      1.0\n",
       "       ... \n",
       "5984    0.0\n",
       "5985    0.0\n",
       "5986    0.0\n",
       "5987    0.0\n",
       "5988    1.0\n",
       "5989    0.0\n",
       "5990    0.0\n",
       "5991    0.0\n",
       "5992    0.0\n",
       "5993    0.0\n",
       "5994    0.0\n",
       "5995    0.0\n",
       "5996    0.0\n",
       "5997    1.0\n",
       "5998    1.0\n",
       "5999    0.0\n",
       "6000    1.0\n",
       "6001    1.0\n",
       "6002    1.0\n",
       "6003    0.0\n",
       "6004    0.0\n",
       "6005    2.0\n",
       "6006    1.0\n",
       "6007    0.0\n",
       "6008    3.0\n",
       "6009    0.0\n",
       "6010    0.0\n",
       "6011    1.0\n",
       "6012    2.0\n",
       "6013    0.0\n",
       "Name: cont_rating, Length: 6014, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cont_rating'] = ord_enc.transform(df[['cont_rating']])\n",
    "df.cont_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "deiV8Y6tV9oe",
    "outputId": "8a90c927-ea91-46fd-f872-cf13e1ed4382"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Games                3238\n",
       "Entertainment         468\n",
       "Education             365\n",
       "Photo & Video         290\n",
       "Utilities             207\n",
       "Health & Fitness      153\n",
       "Lifestyle             131\n",
       "Productivity          129\n",
       "Social Networking     129\n",
       "Shopping              110\n",
       "Book                  106\n",
       "Sports                102\n",
       "Music                  97\n",
       "Finance                95\n",
       "News                   68\n",
       "Travel                 67\n",
       "Weather                58\n",
       "Food & Drink           56\n",
       "Reference              43\n",
       "Business               41\n",
       "Navigation             36\n",
       "Medical                15\n",
       "Catalogs               10\n",
       "Name: prime_genre, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prime_genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b72sdHCmmaf0"
   },
   "source": [
    "총 22개의 장르"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGaZlhbsINNT"
   },
   "outputs": [],
   "source": [
    "onehot_enc = OneHotEncoder()\n",
    "onehot_enc.fit(df[['prime_genre']])\n",
    "prime_df = pd.DataFrame(data = onehot_enc.fit_transform(df[['prime_genre']]).toarray(), columns=[\"genre_\"+str(int(i)) for i in range(0, 23)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4EMBTvcTWks"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((df, prime_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "ttZinYDLTqL7",
    "outputId": "4519c3eb-fabc-48a1-de1d-fbf8827b40ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6014 entries, 0 to 6013\n",
      "Data columns (total 34 columns):\n",
      "size_bytes          6014 non-null float64\n",
      "price               6014 non-null float64\n",
      "rating_count_tot    6014 non-null float64\n",
      "rating_count_ver    6014 non-null float64\n",
      "user_rating         6014 non-null float64\n",
      "user_rating_ver     6014 non-null float64\n",
      "cont_rating         6014 non-null float64\n",
      "prime_genre         6014 non-null object\n",
      "sup_devices.num     6014 non-null float64\n",
      "ipadSc_urls.num     6014 non-null float64\n",
      "lang.num            6014 non-null float64\n",
      "genre_0             6014 non-null float64\n",
      "genre_1             6014 non-null float64\n",
      "genre_2             6014 non-null float64\n",
      "genre_3             6014 non-null float64\n",
      "genre_4             6014 non-null float64\n",
      "genre_5             6014 non-null float64\n",
      "genre_6             6014 non-null float64\n",
      "genre_7             6014 non-null float64\n",
      "genre_8             6014 non-null float64\n",
      "genre_9             6014 non-null float64\n",
      "genre_10            6014 non-null float64\n",
      "genre_11            6014 non-null float64\n",
      "genre_12            6014 non-null float64\n",
      "genre_13            6014 non-null float64\n",
      "genre_14            6014 non-null float64\n",
      "genre_15            6014 non-null float64\n",
      "genre_16            6014 non-null float64\n",
      "genre_17            6014 non-null float64\n",
      "genre_18            6014 non-null float64\n",
      "genre_19            6014 non-null float64\n",
      "genre_20            6014 non-null float64\n",
      "genre_21            6014 non-null float64\n",
      "genre_22            6014 non-null float64\n",
      "dtypes: float64(33), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHU9SDxbVZWD"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['prime_genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVM-wS5YULBx"
   },
   "source": [
    "First of all, change rate to integer using labelEncoder and then make it to one hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "ecxttQcLUC3B",
    "outputId": "4e7ea6cf-c959-4cbb-ef2f-80366cffab65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0. , 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ])"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_enc = LabelEncoder()\n",
    "label_enc.fit(df[['user_rating_ver']])\n",
    "label_enc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1125
    },
    "colab_type": "code",
    "id": "ciTZvFAyY2Qw",
    "outputId": "d17ad478-673e-410b-ce2a-dbc01751e45e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       8\n",
       "1       8\n",
       "2       9\n",
       "3       7\n",
       "4       8\n",
       "5       4\n",
       "6       8\n",
       "7       8\n",
       "8       8\n",
       "9       9\n",
       "10      8\n",
       "11      0\n",
       "12      7\n",
       "13      7\n",
       "14      6\n",
       "15      6\n",
       "16      8\n",
       "17      9\n",
       "18      0\n",
       "19      9\n",
       "20      4\n",
       "21      7\n",
       "22      0\n",
       "23      5\n",
       "24      9\n",
       "25      7\n",
       "26      0\n",
       "27      8\n",
       "28      0\n",
       "29      7\n",
       "       ..\n",
       "5984    0\n",
       "5985    9\n",
       "5986    1\n",
       "5987    7\n",
       "5988    9\n",
       "5989    9\n",
       "5990    8\n",
       "5991    0\n",
       "5992    6\n",
       "5993    0\n",
       "5994    7\n",
       "5995    0\n",
       "5996    5\n",
       "5997    9\n",
       "5998    0\n",
       "5999    0\n",
       "6000    7\n",
       "6001    0\n",
       "6002    0\n",
       "6003    3\n",
       "6004    8\n",
       "6005    7\n",
       "6006    0\n",
       "6007    5\n",
       "6008    0\n",
       "6009    8\n",
       "6010    8\n",
       "6011    0\n",
       "6012    8\n",
       "6013    9\n",
       "Name: user_rating_ver_new, Length: 6014, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['user_rating_ver_new'] = label_enc.transform(df[['user_rating_ver']])\n",
    "df.user_rating_ver_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "colab_type": "code",
    "id": "0jqX5l9xgRfv",
    "outputId": "3197944e-cf61-439b-8f64-f37e234de204"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>price</th>\n",
       "      <th>rating_count_tot</th>\n",
       "      <th>rating_count_ver</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>user_rating_ver</th>\n",
       "      <th>cont_rating</th>\n",
       "      <th>sup_devices.num</th>\n",
       "      <th>ipadSc_urls.num</th>\n",
       "      <th>lang.num</th>\n",
       "      <th>...</th>\n",
       "      <th>genre_14</th>\n",
       "      <th>genre_15</th>\n",
       "      <th>genre_16</th>\n",
       "      <th>genre_17</th>\n",
       "      <th>genre_18</th>\n",
       "      <th>genre_19</th>\n",
       "      <th>genre_20</th>\n",
       "      <th>genre_21</th>\n",
       "      <th>genre_22</th>\n",
       "      <th>user_rating_ver_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024892</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.007158</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.056443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040168</td>\n",
       "      <td>0.004965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012088</td>\n",
       "      <td>0.033301</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017249</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012180</td>\n",
       "      <td>0.016634</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   size_bytes     price  rating_count_tot  rating_count_ver  user_rating  \\\n",
       "0    0.024892  0.013300          0.007158          0.000147          4.0   \n",
       "1    0.056443  0.000000          0.040168          0.004965          4.0   \n",
       "2    0.012088  0.033301          0.000376          0.000023          4.5   \n",
       "3    0.017249  0.013300          0.002651          0.000226          4.0   \n",
       "4    0.012180  0.016634          0.025791          0.022689          4.5   \n",
       "\n",
       "   user_rating_ver  cont_rating  sup_devices.num  ipadSc_urls.num  lang.num  \\\n",
       "0              4.5          0.0         0.763158              1.0  0.133333   \n",
       "1              4.5          0.0         0.736842              0.0  0.253333   \n",
       "2              5.0          0.0         0.736842              1.0  0.013333   \n",
       "3              4.0          0.0         0.763158              0.0  0.133333   \n",
       "4              4.5          0.0         0.763158              0.8  0.146667   \n",
       "\n",
       "          ...           genre_14  genre_15  genre_16  genre_17  genre_18  \\\n",
       "0         ...                0.0       0.0       0.0       0.0       0.0   \n",
       "1         ...                0.0       0.0       0.0       0.0       0.0   \n",
       "2         ...                0.0       0.0       0.0       0.0       0.0   \n",
       "3         ...                0.0       0.0       0.0       0.0       0.0   \n",
       "4         ...                0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   genre_19  genre_20  genre_21  genre_22  user_rating_ver_new  \n",
       "0       0.0       0.0       0.0       0.0                    8  \n",
       "1       0.0       0.0       0.0       0.0                    8  \n",
       "2       0.0       0.0       1.0       0.0                    9  \n",
       "3       0.0       0.0       0.0       0.0                    7  \n",
       "4       0.0       0.0       0.0       0.0                    8  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NGvddLYewXz"
   },
   "source": [
    "For later, I stored target classified as ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHiE0vSRZ5ic"
   },
   "outputs": [],
   "source": [
    "train = df.sample(frac=0.6)\n",
    "valid_old = df.drop(index=train.index)\n",
    "valid = valid_old.sample(frac=0.5)\n",
    "test = valid_old.drop(index=valid.index)\n",
    "train_x = train.drop(columns=['user_rating_ver', 'user_rating_ver_new'])\n",
    "train_y = train[[ 'user_rating_ver_new']]\n",
    "train_ordinal_y = train[[ 'user_rating_ver_new']]\n",
    "valid_x = valid.drop(columns=['user_rating_ver', 'user_rating_ver_new'])\n",
    "valid_y = valid[[ 'user_rating_ver_new']]\n",
    "valid_ordinal_y = valid[[ 'user_rating_ver_new']]\n",
    "test_x = test.drop(columns=['user_rating_ver', 'user_rating_ver_new'])\n",
    "test_y = test[[ 'user_rating_ver_new']]\n",
    "test_ordinal_y = test[[ 'user_rating_ver_new']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "OfglbLzzIcVW",
    "outputId": "6c22c7b2-3b50-47d4-f579-02b47cf8cb05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "onehot_enc = OneHotEncoder()\n",
    "onehot_enc.fit(df[['user_rating_ver_new']])\n",
    "user_rating_ver_df = pd.DataFrame(data = onehot_enc.fit_transform(train_y[['user_rating_ver_new']]).toarray(), columns=[\"user_rating_ver_new_\"+str(int(i)) for i in range(0, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8UNSlKNMIhrr"
   },
   "outputs": [],
   "source": [
    "train_y = user_rating_ver_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "wKorrI4Tx1OR",
    "outputId": "83b9e5b0-2709-4056-f4fd-3c0c420c6343"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "user_rating_ver_df = pd.DataFrame(data = onehot_enc.fit_transform(test_y[['user_rating_ver_new']]).toarray(), columns=[\"user_rating_ver_new_\"+str(int(i)) for i in range(0, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48h2YQR5x2RS"
   },
   "outputs": [],
   "source": [
    "test_y = user_rating_ver_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "NnfqpEgQyKcz",
    "outputId": "5584b65e-69ab-4860-d2dc-a6733dfd6d27"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "user_rating_ver_df = pd.DataFrame(data = onehot_enc.fit_transform(valid_y[['user_rating_ver_new']]).toarray(), columns=[\"user_rating_ver_new_\"+str(int(i)) for i in range(0, 10)])\n",
    "valid_y = user_rating_ver_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7vYV8TJJy_W-"
   },
   "source": [
    "## 3. First Experiment\n",
    "Except baseline model, I will use four models.(Linear Regression, Decision Tree, Random Forest, MLP(DL))\n",
    "\n",
    "In sklearn, supporting multilabel models are following 8 models.\n",
    "\n",
    "sklearn.tree.DecisionTreeClassifier\n",
    "\n",
    "sklearn.tree.ExtraTreeClassifier\n",
    "\n",
    "sklearn.ensemble.ExtraTreesClassifier\n",
    "\n",
    "sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "sklearn.neural_network.MLPClassifier\n",
    "\n",
    "sklearn.neighbors.RadiusNeighborsClassifier\n",
    "\n",
    "sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "sklearn.linear_model.RidgeClassifierCV\n",
    "\n",
    "I chose 4 models out of 8.\n",
    "\n",
    "### 1) Baseline\n",
    "I made oneR algorithm for baseline model.\n",
    "Each result of experiment of one model will be stored in result dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gGHC1Wk4Jphq"
   },
   "outputs": [],
   "source": [
    "from  sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "result = pd.DataFrame(columns=['model', 'accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "TCir3bf4WIhr",
    "outputId": "2bd37df5-979e-4df6-c4bc-a476678e85c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_rating_ver_new_0     812.0\n",
       "user_rating_ver_new_1      77.0\n",
       "user_rating_ver_new_2      39.0\n",
       "user_rating_ver_new_3      79.0\n",
       "user_rating_ver_new_4      91.0\n",
       "user_rating_ver_new_5     154.0\n",
       "user_rating_ver_new_6     234.0\n",
       "user_rating_ver_new_7     564.0\n",
       "user_rating_ver_new_8    1048.0\n",
       "user_rating_ver_new_9     510.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4F_CLJnI1g-3"
   },
   "source": [
    "Assume user_rating_ver_new_8 is the answer because it has the largest amount of intances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "cOvkykmUdQ6t",
    "outputId": "4f399bd5-2886-4ee8-e27d-3d6b2326593c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_y = pd.DataFrame([list('0000000010')], index=valid_y.index, columns=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "pred_y = pred_y.astype('int')\n",
    "precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "result = result.append({'model':'oneR', 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZT8wJVHFAW4"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(train_x, train_y)\n",
    "pred_y = dt.predict(valid_x)\n",
    "precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "result = result.append({'model':'decision tree', 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeH7qyIRpGp8"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_x, train_y)\n",
    "pred_y = knn.predict(valid_x)\n",
    "precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "result = result.append({'model':'knn', 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "Tb0E2O3YlzsO",
    "outputId": "a4866584-7da4-4f9e-a7ea-cf818fdbc6e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_x, train_y)\n",
    "pred_y = rf.predict(valid_x)\n",
    "precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "result = result.append({'model':'random forest', 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJjAWiP6m0lD"
   },
   "source": [
    "Start with a simple model with just one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17408
    },
    "colab_type": "code",
    "id": "ZHepNP80NDTP",
    "outputId": "eff748e4-bb14-4a65-eeba-a27244c35580"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 1,754\n",
      "Trainable params: 1,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3608 samples, validate on 1203 samples\n",
      "Epoch 1/500\n",
      "3608/3608 [==============================] - 2s 421us/step - loss: 0.0791 - acc: 0.4072 - val_loss: 0.0674 - val_acc: 0.4655\n",
      "Epoch 2/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0677 - acc: 0.4443 - val_loss: 0.0658 - val_acc: 0.4655\n",
      "Epoch 3/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0669 - acc: 0.4435 - val_loss: 0.0647 - val_acc: 0.4663\n",
      "Epoch 4/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0662 - acc: 0.4440 - val_loss: 0.0646 - val_acc: 0.4738\n",
      "Epoch 5/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0657 - acc: 0.4482 - val_loss: 0.0640 - val_acc: 0.4713\n",
      "Epoch 6/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0651 - acc: 0.4521 - val_loss: 0.0636 - val_acc: 0.4705\n",
      "Epoch 7/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0646 - acc: 0.4523 - val_loss: 0.0630 - val_acc: 0.4730\n",
      "Epoch 8/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0640 - acc: 0.4559 - val_loss: 0.0630 - val_acc: 0.4705\n",
      "Epoch 9/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0635 - acc: 0.4662 - val_loss: 0.0625 - val_acc: 0.4780\n",
      "Epoch 10/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0628 - acc: 0.4737 - val_loss: 0.0618 - val_acc: 0.4738\n",
      "Epoch 11/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0624 - acc: 0.4770 - val_loss: 0.0612 - val_acc: 0.4821\n",
      "Epoch 12/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0620 - acc: 0.4839 - val_loss: 0.0609 - val_acc: 0.4788\n",
      "Epoch 13/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0615 - acc: 0.4920 - val_loss: 0.0609 - val_acc: 0.5037\n",
      "Epoch 14/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0612 - acc: 0.4914 - val_loss: 0.0603 - val_acc: 0.4813\n",
      "Epoch 15/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0607 - acc: 0.5036 - val_loss: 0.0608 - val_acc: 0.4863\n",
      "Epoch 16/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0604 - acc: 0.5094 - val_loss: 0.0596 - val_acc: 0.4938\n",
      "Epoch 17/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0602 - acc: 0.5119 - val_loss: 0.0595 - val_acc: 0.4879\n",
      "Epoch 18/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0598 - acc: 0.5194 - val_loss: 0.0595 - val_acc: 0.5295\n",
      "Epoch 19/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0596 - acc: 0.5249 - val_loss: 0.0590 - val_acc: 0.5362\n",
      "Epoch 20/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0593 - acc: 0.5322 - val_loss: 0.0590 - val_acc: 0.5195\n",
      "Epoch 21/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0591 - acc: 0.5424 - val_loss: 0.0585 - val_acc: 0.5229\n",
      "Epoch 22/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0589 - acc: 0.5382 - val_loss: 0.0584 - val_acc: 0.5345\n",
      "Epoch 23/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0586 - acc: 0.5435 - val_loss: 0.0582 - val_acc: 0.5337\n",
      "Epoch 24/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0583 - acc: 0.5474 - val_loss: 0.0584 - val_acc: 0.5470\n",
      "Epoch 25/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0581 - acc: 0.5516 - val_loss: 0.0582 - val_acc: 0.5536\n",
      "Epoch 26/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0580 - acc: 0.5596 - val_loss: 0.0579 - val_acc: 0.5470\n",
      "Epoch 27/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0579 - acc: 0.5546 - val_loss: 0.0577 - val_acc: 0.5528\n",
      "Epoch 28/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0577 - acc: 0.5618 - val_loss: 0.0578 - val_acc: 0.5453\n",
      "Epoch 29/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0578 - acc: 0.5554 - val_loss: 0.0576 - val_acc: 0.5461\n",
      "Epoch 30/500\n",
      "3608/3608 [==============================] - 1s 185us/step - loss: 0.0573 - acc: 0.5657 - val_loss: 0.0573 - val_acc: 0.5486\n",
      "Epoch 31/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0572 - acc: 0.5665 - val_loss: 0.0574 - val_acc: 0.5411\n",
      "Epoch 32/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0570 - acc: 0.5674 - val_loss: 0.0571 - val_acc: 0.5520\n",
      "Epoch 33/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0569 - acc: 0.5729 - val_loss: 0.0582 - val_acc: 0.5345\n",
      "Epoch 34/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0569 - acc: 0.5723 - val_loss: 0.0568 - val_acc: 0.5569\n",
      "Epoch 35/500\n",
      "3608/3608 [==============================] - 1s 185us/step - loss: 0.0567 - acc: 0.5690 - val_loss: 0.0567 - val_acc: 0.5653\n",
      "Epoch 36/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0568 - acc: 0.5701 - val_loss: 0.0570 - val_acc: 0.5520\n",
      "Epoch 37/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0565 - acc: 0.5748 - val_loss: 0.0572 - val_acc: 0.5628\n",
      "Epoch 38/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0567 - acc: 0.5710 - val_loss: 0.0580 - val_acc: 0.5411\n",
      "Epoch 39/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0564 - acc: 0.5748 - val_loss: 0.0567 - val_acc: 0.5669\n",
      "Epoch 40/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0563 - acc: 0.5743 - val_loss: 0.0568 - val_acc: 0.5669\n",
      "Epoch 41/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0563 - acc: 0.5773 - val_loss: 0.0567 - val_acc: 0.5578\n",
      "Epoch 42/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0562 - acc: 0.5759 - val_loss: 0.0568 - val_acc: 0.5686\n",
      "Epoch 43/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0560 - acc: 0.5798 - val_loss: 0.0567 - val_acc: 0.5694\n",
      "Epoch 44/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0560 - acc: 0.5818 - val_loss: 0.0568 - val_acc: 0.5727\n",
      "Epoch 45/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0561 - acc: 0.5779 - val_loss: 0.0569 - val_acc: 0.5661\n",
      "Epoch 46/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0559 - acc: 0.5815 - val_loss: 0.0572 - val_acc: 0.5694\n",
      "Epoch 47/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0557 - acc: 0.5843 - val_loss: 0.0573 - val_acc: 0.5486\n",
      "Epoch 48/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0558 - acc: 0.5862 - val_loss: 0.0565 - val_acc: 0.5669\n",
      "Epoch 49/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0557 - acc: 0.5851 - val_loss: 0.0563 - val_acc: 0.5702\n",
      "Epoch 50/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0556 - acc: 0.5851 - val_loss: 0.0569 - val_acc: 0.5661\n",
      "Epoch 51/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0555 - acc: 0.5881 - val_loss: 0.0564 - val_acc: 0.5611\n",
      "Epoch 52/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0555 - acc: 0.5840 - val_loss: 0.0566 - val_acc: 0.5702\n",
      "Epoch 53/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0555 - acc: 0.5845 - val_loss: 0.0568 - val_acc: 0.5677\n",
      "Epoch 54/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0555 - acc: 0.5884 - val_loss: 0.0567 - val_acc: 0.5727\n",
      "Epoch 55/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0553 - acc: 0.5879 - val_loss: 0.0569 - val_acc: 0.5636\n",
      "Epoch 56/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0554 - acc: 0.5909 - val_loss: 0.0565 - val_acc: 0.5661\n",
      "Epoch 57/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0555 - acc: 0.5848 - val_loss: 0.0570 - val_acc: 0.5569\n",
      "Epoch 58/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0552 - acc: 0.5906 - val_loss: 0.0566 - val_acc: 0.5578\n",
      "Epoch 59/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0551 - acc: 0.5909 - val_loss: 0.0567 - val_acc: 0.5611\n",
      "Epoch 60/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0553 - acc: 0.5895 - val_loss: 0.0582 - val_acc: 0.5470\n",
      "Epoch 61/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0551 - acc: 0.5873 - val_loss: 0.0564 - val_acc: 0.5578\n",
      "Epoch 62/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0552 - acc: 0.5909 - val_loss: 0.0564 - val_acc: 0.5736\n",
      "Epoch 63/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0552 - acc: 0.5912 - val_loss: 0.0566 - val_acc: 0.5677\n",
      "Epoch 64/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0549 - acc: 0.5884 - val_loss: 0.0567 - val_acc: 0.5702\n",
      "Epoch 65/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0550 - acc: 0.5887 - val_loss: 0.0565 - val_acc: 0.5644\n",
      "Epoch 66/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0550 - acc: 0.5898 - val_loss: 0.0563 - val_acc: 0.5694\n",
      "Epoch 67/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0548 - acc: 0.5934 - val_loss: 0.0563 - val_acc: 0.5653\n",
      "Epoch 68/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0549 - acc: 0.5923 - val_loss: 0.0568 - val_acc: 0.5636\n",
      "Epoch 69/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0550 - acc: 0.5915 - val_loss: 0.0563 - val_acc: 0.5669\n",
      "Epoch 70/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0548 - acc: 0.5926 - val_loss: 0.0574 - val_acc: 0.5628\n",
      "Epoch 71/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0549 - acc: 0.5909 - val_loss: 0.0562 - val_acc: 0.5636\n",
      "Epoch 72/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0547 - acc: 0.5948 - val_loss: 0.0564 - val_acc: 0.5744\n",
      "Epoch 73/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0548 - acc: 0.5942 - val_loss: 0.0562 - val_acc: 0.5636\n",
      "Epoch 74/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0547 - acc: 0.5945 - val_loss: 0.0568 - val_acc: 0.5536\n",
      "Epoch 75/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0548 - acc: 0.5931 - val_loss: 0.0567 - val_acc: 0.5661\n",
      "Epoch 76/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0545 - acc: 0.5970 - val_loss: 0.0584 - val_acc: 0.5411\n",
      "Epoch 77/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0547 - acc: 0.5937 - val_loss: 0.0567 - val_acc: 0.5644\n",
      "Epoch 78/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0547 - acc: 0.5959 - val_loss: 0.0563 - val_acc: 0.5603\n",
      "Epoch 79/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0546 - acc: 0.5940 - val_loss: 0.0564 - val_acc: 0.5661\n",
      "Epoch 80/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0546 - acc: 0.5948 - val_loss: 0.0563 - val_acc: 0.5711\n",
      "Epoch 81/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0546 - acc: 0.5934 - val_loss: 0.0565 - val_acc: 0.5686\n",
      "Epoch 82/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0545 - acc: 0.5978 - val_loss: 0.0564 - val_acc: 0.5619\n",
      "Epoch 83/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0546 - acc: 0.5953 - val_loss: 0.0564 - val_acc: 0.5644\n",
      "Epoch 84/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0544 - acc: 0.5940 - val_loss: 0.0563 - val_acc: 0.5661\n",
      "Epoch 85/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0544 - acc: 0.5959 - val_loss: 0.0567 - val_acc: 0.5702\n",
      "Epoch 86/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0545 - acc: 0.5923 - val_loss: 0.0567 - val_acc: 0.5628\n",
      "Epoch 87/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0543 - acc: 0.5973 - val_loss: 0.0563 - val_acc: 0.5611\n",
      "Epoch 88/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0545 - acc: 0.5951 - val_loss: 0.0565 - val_acc: 0.5677\n",
      "Epoch 89/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0544 - acc: 0.6009 - val_loss: 0.0563 - val_acc: 0.5661\n",
      "Epoch 90/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0543 - acc: 0.5981 - val_loss: 0.0574 - val_acc: 0.5569\n",
      "Epoch 91/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0542 - acc: 0.5962 - val_loss: 0.0565 - val_acc: 0.5702\n",
      "Epoch 92/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0543 - acc: 0.5998 - val_loss: 0.0572 - val_acc: 0.5636\n",
      "Epoch 93/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0544 - acc: 0.5973 - val_loss: 0.0563 - val_acc: 0.5736\n",
      "Epoch 94/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0542 - acc: 0.5934 - val_loss: 0.0564 - val_acc: 0.5686\n",
      "Epoch 95/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0542 - acc: 0.5984 - val_loss: 0.0566 - val_acc: 0.5644\n",
      "Epoch 96/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0542 - acc: 0.6003 - val_loss: 0.0566 - val_acc: 0.5669\n",
      "Epoch 97/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0542 - acc: 0.5995 - val_loss: 0.0565 - val_acc: 0.5669\n",
      "Epoch 98/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0542 - acc: 0.5998 - val_loss: 0.0569 - val_acc: 0.5644\n",
      "Epoch 99/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0541 - acc: 0.6012 - val_loss: 0.0565 - val_acc: 0.5669\n",
      "Epoch 100/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0541 - acc: 0.6014 - val_loss: 0.0574 - val_acc: 0.5486\n",
      "Epoch 101/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0541 - acc: 0.5998 - val_loss: 0.0565 - val_acc: 0.5644\n",
      "Epoch 102/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0541 - acc: 0.5926 - val_loss: 0.0564 - val_acc: 0.5694\n",
      "Epoch 103/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0542 - acc: 0.5987 - val_loss: 0.0571 - val_acc: 0.5619\n",
      "Epoch 104/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0540 - acc: 0.5987 - val_loss: 0.0566 - val_acc: 0.5661\n",
      "Epoch 105/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0539 - acc: 0.5967 - val_loss: 0.0567 - val_acc: 0.5669\n",
      "Epoch 106/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0541 - acc: 0.5987 - val_loss: 0.0567 - val_acc: 0.5677\n",
      "Epoch 107/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0542 - acc: 0.5995 - val_loss: 0.0567 - val_acc: 0.5719\n",
      "Epoch 108/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0539 - acc: 0.5998 - val_loss: 0.0565 - val_acc: 0.5711\n",
      "Epoch 109/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0538 - acc: 0.6070 - val_loss: 0.0566 - val_acc: 0.5653\n",
      "Epoch 110/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0538 - acc: 0.6023 - val_loss: 0.0570 - val_acc: 0.5653\n",
      "Epoch 111/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0538 - acc: 0.6009 - val_loss: 0.0564 - val_acc: 0.5694\n",
      "Epoch 112/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0541 - acc: 0.5956 - val_loss: 0.0572 - val_acc: 0.5586\n",
      "Epoch 113/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0539 - acc: 0.5981 - val_loss: 0.0564 - val_acc: 0.5653\n",
      "Epoch 114/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0537 - acc: 0.6023 - val_loss: 0.0571 - val_acc: 0.5636\n",
      "Epoch 115/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0538 - acc: 0.6042 - val_loss: 0.0565 - val_acc: 0.5744\n",
      "Epoch 116/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0537 - acc: 0.6003 - val_loss: 0.0568 - val_acc: 0.5677\n",
      "Epoch 117/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0538 - acc: 0.6009 - val_loss: 0.0565 - val_acc: 0.5636\n",
      "Epoch 118/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0537 - acc: 0.6006 - val_loss: 0.0566 - val_acc: 0.5636\n",
      "Epoch 119/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0536 - acc: 0.6009 - val_loss: 0.0566 - val_acc: 0.5727\n",
      "Epoch 120/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0536 - acc: 0.6023 - val_loss: 0.0568 - val_acc: 0.5636\n",
      "Epoch 121/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0537 - acc: 0.6039 - val_loss: 0.0566 - val_acc: 0.5677\n",
      "Epoch 122/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0536 - acc: 0.6048 - val_loss: 0.0573 - val_acc: 0.5636\n",
      "Epoch 123/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0536 - acc: 0.6006 - val_loss: 0.0572 - val_acc: 0.5644\n",
      "Epoch 124/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0536 - acc: 0.6031 - val_loss: 0.0571 - val_acc: 0.5619\n",
      "Epoch 125/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0536 - acc: 0.6037 - val_loss: 0.0572 - val_acc: 0.5644\n",
      "Epoch 126/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0536 - acc: 0.6025 - val_loss: 0.0581 - val_acc: 0.5586\n",
      "Epoch 127/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0534 - acc: 0.6048 - val_loss: 0.0569 - val_acc: 0.5644\n",
      "Epoch 128/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0534 - acc: 0.6084 - val_loss: 0.0570 - val_acc: 0.5636\n",
      "Epoch 129/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0535 - acc: 0.6014 - val_loss: 0.0571 - val_acc: 0.5619\n",
      "Epoch 130/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0534 - acc: 0.6020 - val_loss: 0.0569 - val_acc: 0.5686\n",
      "Epoch 131/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0535 - acc: 0.6034 - val_loss: 0.0579 - val_acc: 0.5569\n",
      "Epoch 132/500\n",
      "3608/3608 [==============================] - 1s 185us/step - loss: 0.0535 - acc: 0.6045 - val_loss: 0.0567 - val_acc: 0.5744\n",
      "Epoch 133/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0532 - acc: 0.6070 - val_loss: 0.0569 - val_acc: 0.5719\n",
      "Epoch 134/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0532 - acc: 0.6034 - val_loss: 0.0571 - val_acc: 0.5653\n",
      "Epoch 135/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0533 - acc: 0.6034 - val_loss: 0.0568 - val_acc: 0.5661\n",
      "Epoch 136/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0533 - acc: 0.6020 - val_loss: 0.0565 - val_acc: 0.5736\n",
      "Epoch 137/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0532 - acc: 0.6050 - val_loss: 0.0569 - val_acc: 0.5661\n",
      "Epoch 138/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0532 - acc: 0.6073 - val_loss: 0.0568 - val_acc: 0.5694\n",
      "Epoch 139/500\n",
      "3608/3608 [==============================] - 1s 188us/step - loss: 0.0532 - acc: 0.6092 - val_loss: 0.0570 - val_acc: 0.5669\n",
      "Epoch 140/500\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0532 - acc: 0.6070 - val_loss: 0.0576 - val_acc: 0.5578\n",
      "Epoch 141/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0533 - acc: 0.6070 - val_loss: 0.0571 - val_acc: 0.5661\n",
      "Epoch 142/500\n",
      "3608/3608 [==============================] - 1s 185us/step - loss: 0.0531 - acc: 0.6070 - val_loss: 0.0569 - val_acc: 0.5711\n",
      "Epoch 143/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0531 - acc: 0.6081 - val_loss: 0.0569 - val_acc: 0.5661\n",
      "Epoch 144/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0531 - acc: 0.6086 - val_loss: 0.0565 - val_acc: 0.5727\n",
      "Epoch 145/500\n",
      "3608/3608 [==============================] - 1s 189us/step - loss: 0.0531 - acc: 0.6109 - val_loss: 0.0587 - val_acc: 0.5470\n",
      "Epoch 146/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0533 - acc: 0.6056 - val_loss: 0.0574 - val_acc: 0.5578\n",
      "Epoch 147/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0531 - acc: 0.6059 - val_loss: 0.0568 - val_acc: 0.5677\n",
      "Epoch 148/500\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0531 - acc: 0.6092 - val_loss: 0.0569 - val_acc: 0.5711\n",
      "Epoch 149/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0533 - acc: 0.6103 - val_loss: 0.0570 - val_acc: 0.5603\n",
      "Epoch 150/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0531 - acc: 0.6092 - val_loss: 0.0569 - val_acc: 0.5686\n",
      "Epoch 151/500\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0530 - acc: 0.6078 - val_loss: 0.0565 - val_acc: 0.5761\n",
      "Epoch 152/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0530 - acc: 0.6109 - val_loss: 0.0568 - val_acc: 0.5661\n",
      "Epoch 153/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0531 - acc: 0.6073 - val_loss: 0.0573 - val_acc: 0.5653\n",
      "Epoch 154/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0530 - acc: 0.6070 - val_loss: 0.0571 - val_acc: 0.5677\n",
      "Epoch 155/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0531 - acc: 0.6086 - val_loss: 0.0572 - val_acc: 0.5653\n",
      "Epoch 156/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0530 - acc: 0.6095 - val_loss: 0.0577 - val_acc: 0.5520\n",
      "Epoch 157/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0530 - acc: 0.6098 - val_loss: 0.0573 - val_acc: 0.5636\n",
      "Epoch 158/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0529 - acc: 0.6073 - val_loss: 0.0569 - val_acc: 0.5677\n",
      "Epoch 159/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0527 - acc: 0.6081 - val_loss: 0.0567 - val_acc: 0.5711\n",
      "Epoch 160/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0529 - acc: 0.6064 - val_loss: 0.0567 - val_acc: 0.5736\n",
      "Epoch 161/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0528 - acc: 0.6111 - val_loss: 0.0571 - val_acc: 0.5719\n",
      "Epoch 162/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0529 - acc: 0.6092 - val_loss: 0.0590 - val_acc: 0.5378\n",
      "Epoch 163/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0529 - acc: 0.6095 - val_loss: 0.0568 - val_acc: 0.5686\n",
      "Epoch 164/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0527 - acc: 0.6109 - val_loss: 0.0572 - val_acc: 0.5619\n",
      "Epoch 165/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0528 - acc: 0.6089 - val_loss: 0.0568 - val_acc: 0.5694\n",
      "Epoch 166/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0528 - acc: 0.6075 - val_loss: 0.0570 - val_acc: 0.5611\n",
      "Epoch 167/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0528 - acc: 0.6098 - val_loss: 0.0571 - val_acc: 0.5711\n",
      "Epoch 168/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0528 - acc: 0.6106 - val_loss: 0.0573 - val_acc: 0.5677\n",
      "Epoch 169/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0528 - acc: 0.6109 - val_loss: 0.0567 - val_acc: 0.5711\n",
      "Epoch 170/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0527 - acc: 0.6147 - val_loss: 0.0568 - val_acc: 0.5736\n",
      "Epoch 171/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0527 - acc: 0.6120 - val_loss: 0.0576 - val_acc: 0.5653\n",
      "Epoch 172/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0527 - acc: 0.6134 - val_loss: 0.0568 - val_acc: 0.5711\n",
      "Epoch 173/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0526 - acc: 0.6092 - val_loss: 0.0572 - val_acc: 0.5661\n",
      "Epoch 174/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0527 - acc: 0.6120 - val_loss: 0.0573 - val_acc: 0.5686\n",
      "Epoch 175/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0527 - acc: 0.6081 - val_loss: 0.0571 - val_acc: 0.5694\n",
      "Epoch 176/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0527 - acc: 0.6139 - val_loss: 0.0574 - val_acc: 0.5628\n",
      "Epoch 177/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0526 - acc: 0.6103 - val_loss: 0.0574 - val_acc: 0.5661\n",
      "Epoch 178/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0526 - acc: 0.6103 - val_loss: 0.0569 - val_acc: 0.5702\n",
      "Epoch 179/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0526 - acc: 0.6098 - val_loss: 0.0573 - val_acc: 0.5677\n",
      "Epoch 180/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0526 - acc: 0.6103 - val_loss: 0.0576 - val_acc: 0.5644\n",
      "Epoch 181/500\n",
      "3608/3608 [==============================] - 1s 193us/step - loss: 0.0526 - acc: 0.6092 - val_loss: 0.0572 - val_acc: 0.5653\n",
      "Epoch 182/500\n",
      "3608/3608 [==============================] - 1s 208us/step - loss: 0.0524 - acc: 0.6103 - val_loss: 0.0574 - val_acc: 0.5644\n",
      "Epoch 183/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0525 - acc: 0.6103 - val_loss: 0.0569 - val_acc: 0.5677\n",
      "Epoch 184/500\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0525 - acc: 0.6109 - val_loss: 0.0570 - val_acc: 0.5694\n",
      "Epoch 185/500\n",
      "3608/3608 [==============================] - 1s 208us/step - loss: 0.0524 - acc: 0.6103 - val_loss: 0.0571 - val_acc: 0.5644\n",
      "Epoch 186/500\n",
      "3608/3608 [==============================] - 1s 207us/step - loss: 0.0525 - acc: 0.6114 - val_loss: 0.0573 - val_acc: 0.5694\n",
      "Epoch 187/500\n",
      "3608/3608 [==============================] - 1s 208us/step - loss: 0.0524 - acc: 0.6111 - val_loss: 0.0573 - val_acc: 0.5669\n",
      "Epoch 188/500\n",
      "3608/3608 [==============================] - 1s 193us/step - loss: 0.0526 - acc: 0.6075 - val_loss: 0.0572 - val_acc: 0.5694\n",
      "Epoch 189/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0524 - acc: 0.6084 - val_loss: 0.0570 - val_acc: 0.5694\n",
      "Epoch 190/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0525 - acc: 0.6075 - val_loss: 0.0569 - val_acc: 0.5736\n",
      "Epoch 191/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0523 - acc: 0.6092 - val_loss: 0.0579 - val_acc: 0.5619\n",
      "Epoch 192/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0524 - acc: 0.6103 - val_loss: 0.0570 - val_acc: 0.5694\n",
      "Epoch 193/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0524 - acc: 0.6123 - val_loss: 0.0576 - val_acc: 0.5644\n",
      "Epoch 194/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0523 - acc: 0.6117 - val_loss: 0.0570 - val_acc: 0.5694\n",
      "Epoch 195/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0523 - acc: 0.6164 - val_loss: 0.0601 - val_acc: 0.5345\n",
      "Epoch 196/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0525 - acc: 0.6145 - val_loss: 0.0578 - val_acc: 0.5470\n",
      "Epoch 197/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0522 - acc: 0.6153 - val_loss: 0.0574 - val_acc: 0.5644\n",
      "Epoch 198/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0522 - acc: 0.6111 - val_loss: 0.0573 - val_acc: 0.5686\n",
      "Epoch 199/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0523 - acc: 0.6150 - val_loss: 0.0571 - val_acc: 0.5686\n",
      "Epoch 200/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0523 - acc: 0.6125 - val_loss: 0.0569 - val_acc: 0.5794\n",
      "Epoch 201/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0522 - acc: 0.6134 - val_loss: 0.0570 - val_acc: 0.5744\n",
      "Epoch 202/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0523 - acc: 0.6111 - val_loss: 0.0574 - val_acc: 0.5694\n",
      "Epoch 203/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0524 - acc: 0.6081 - val_loss: 0.0573 - val_acc: 0.5702\n",
      "Epoch 204/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0521 - acc: 0.6134 - val_loss: 0.0579 - val_acc: 0.5661\n",
      "Epoch 205/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0525 - acc: 0.6120 - val_loss: 0.0572 - val_acc: 0.5686\n",
      "Epoch 206/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0522 - acc: 0.6114 - val_loss: 0.0577 - val_acc: 0.5644\n",
      "Epoch 207/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0522 - acc: 0.6136 - val_loss: 0.0573 - val_acc: 0.5677\n",
      "Epoch 208/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0522 - acc: 0.6161 - val_loss: 0.0575 - val_acc: 0.5644\n",
      "Epoch 209/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0522 - acc: 0.6159 - val_loss: 0.0582 - val_acc: 0.5586\n",
      "Epoch 210/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0522 - acc: 0.6114 - val_loss: 0.0579 - val_acc: 0.5636\n",
      "Epoch 211/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0522 - acc: 0.6147 - val_loss: 0.0575 - val_acc: 0.5686\n",
      "Epoch 212/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0523 - acc: 0.6178 - val_loss: 0.0584 - val_acc: 0.5611\n",
      "Epoch 213/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0522 - acc: 0.6156 - val_loss: 0.0576 - val_acc: 0.5669\n",
      "Epoch 214/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0522 - acc: 0.6114 - val_loss: 0.0585 - val_acc: 0.5611\n",
      "Epoch 215/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0522 - acc: 0.6178 - val_loss: 0.0576 - val_acc: 0.5669\n",
      "Epoch 216/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0521 - acc: 0.6175 - val_loss: 0.0578 - val_acc: 0.5603\n",
      "Epoch 217/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0520 - acc: 0.6134 - val_loss: 0.0575 - val_acc: 0.5719\n",
      "Epoch 218/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0520 - acc: 0.6164 - val_loss: 0.0574 - val_acc: 0.5653\n",
      "Epoch 219/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0519 - acc: 0.6178 - val_loss: 0.0574 - val_acc: 0.5702\n",
      "Epoch 220/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0520 - acc: 0.6139 - val_loss: 0.0578 - val_acc: 0.5619\n",
      "Epoch 221/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0522 - acc: 0.6150 - val_loss: 0.0573 - val_acc: 0.5719\n",
      "Epoch 222/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0520 - acc: 0.6156 - val_loss: 0.0575 - val_acc: 0.5702\n",
      "Epoch 223/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0521 - acc: 0.6134 - val_loss: 0.0574 - val_acc: 0.5694\n",
      "Epoch 224/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0521 - acc: 0.6150 - val_loss: 0.0581 - val_acc: 0.5628\n",
      "Epoch 225/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0522 - acc: 0.6131 - val_loss: 0.0576 - val_acc: 0.5686\n",
      "Epoch 226/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0519 - acc: 0.6192 - val_loss: 0.0580 - val_acc: 0.5669\n",
      "Epoch 227/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0520 - acc: 0.6153 - val_loss: 0.0586 - val_acc: 0.5611\n",
      "Epoch 228/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0520 - acc: 0.6147 - val_loss: 0.0579 - val_acc: 0.5603\n",
      "Epoch 229/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0520 - acc: 0.6192 - val_loss: 0.0574 - val_acc: 0.5694\n",
      "Epoch 230/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0519 - acc: 0.6203 - val_loss: 0.0580 - val_acc: 0.5644\n",
      "Epoch 231/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0522 - acc: 0.6181 - val_loss: 0.0587 - val_acc: 0.5561\n",
      "Epoch 232/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0519 - acc: 0.6161 - val_loss: 0.0576 - val_acc: 0.5669\n",
      "Epoch 233/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0520 - acc: 0.6170 - val_loss: 0.0579 - val_acc: 0.5677\n",
      "Epoch 234/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0518 - acc: 0.6167 - val_loss: 0.0585 - val_acc: 0.5628\n",
      "Epoch 235/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0522 - acc: 0.6111 - val_loss: 0.0575 - val_acc: 0.5711\n",
      "Epoch 236/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0518 - acc: 0.6172 - val_loss: 0.0583 - val_acc: 0.5603\n",
      "Epoch 237/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0518 - acc: 0.6214 - val_loss: 0.0577 - val_acc: 0.5686\n",
      "Epoch 238/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0519 - acc: 0.6206 - val_loss: 0.0577 - val_acc: 0.5711\n",
      "Epoch 239/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0518 - acc: 0.6178 - val_loss: 0.0579 - val_acc: 0.5669\n",
      "Epoch 240/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0518 - acc: 0.6192 - val_loss: 0.0578 - val_acc: 0.5686\n",
      "Epoch 241/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0520 - acc: 0.6214 - val_loss: 0.0580 - val_acc: 0.5653\n",
      "Epoch 242/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0519 - acc: 0.6175 - val_loss: 0.0587 - val_acc: 0.5636\n",
      "Epoch 243/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0517 - acc: 0.6172 - val_loss: 0.0580 - val_acc: 0.5711\n",
      "Epoch 244/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0520 - acc: 0.6142 - val_loss: 0.0584 - val_acc: 0.5628\n",
      "Epoch 245/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0519 - acc: 0.6192 - val_loss: 0.0579 - val_acc: 0.5661\n",
      "Epoch 246/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0520 - acc: 0.6167 - val_loss: 0.0579 - val_acc: 0.5653\n",
      "Epoch 247/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0517 - acc: 0.6175 - val_loss: 0.0582 - val_acc: 0.5653\n",
      "Epoch 248/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0517 - acc: 0.6183 - val_loss: 0.0587 - val_acc: 0.5603\n",
      "Epoch 249/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0517 - acc: 0.6220 - val_loss: 0.0580 - val_acc: 0.5669\n",
      "Epoch 250/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0518 - acc: 0.6164 - val_loss: 0.0586 - val_acc: 0.5603\n",
      "Epoch 251/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0517 - acc: 0.6167 - val_loss: 0.0581 - val_acc: 0.5669\n",
      "Epoch 252/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0517 - acc: 0.6203 - val_loss: 0.0589 - val_acc: 0.5619\n",
      "Epoch 253/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0519 - acc: 0.6164 - val_loss: 0.0579 - val_acc: 0.5636\n",
      "Epoch 254/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0517 - acc: 0.6242 - val_loss: 0.0579 - val_acc: 0.5653\n",
      "Epoch 255/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0516 - acc: 0.6200 - val_loss: 0.0575 - val_acc: 0.5711\n",
      "Epoch 256/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0518 - acc: 0.6203 - val_loss: 0.0577 - val_acc: 0.5686\n",
      "Epoch 257/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0517 - acc: 0.6189 - val_loss: 0.0584 - val_acc: 0.5569\n",
      "Epoch 258/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0516 - acc: 0.6203 - val_loss: 0.0577 - val_acc: 0.5677\n",
      "Epoch 259/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0516 - acc: 0.6200 - val_loss: 0.0578 - val_acc: 0.5686\n",
      "Epoch 260/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0517 - acc: 0.6181 - val_loss: 0.0578 - val_acc: 0.5719\n",
      "Epoch 261/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0517 - acc: 0.6208 - val_loss: 0.0581 - val_acc: 0.5644\n",
      "Epoch 262/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0517 - acc: 0.6233 - val_loss: 0.0576 - val_acc: 0.5677\n",
      "Epoch 263/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0515 - acc: 0.6222 - val_loss: 0.0578 - val_acc: 0.5711\n",
      "Epoch 264/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0515 - acc: 0.6192 - val_loss: 0.0577 - val_acc: 0.5727\n",
      "Epoch 265/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0516 - acc: 0.6161 - val_loss: 0.0587 - val_acc: 0.5594\n",
      "Epoch 266/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0518 - acc: 0.6214 - val_loss: 0.0577 - val_acc: 0.5727\n",
      "Epoch 267/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0516 - acc: 0.6211 - val_loss: 0.0578 - val_acc: 0.5603\n",
      "Epoch 268/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0516 - acc: 0.6214 - val_loss: 0.0579 - val_acc: 0.5694\n",
      "Epoch 269/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0516 - acc: 0.6197 - val_loss: 0.0581 - val_acc: 0.5719\n",
      "Epoch 270/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0516 - acc: 0.6206 - val_loss: 0.0578 - val_acc: 0.5677\n",
      "Epoch 271/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0515 - acc: 0.6222 - val_loss: 0.0577 - val_acc: 0.5686\n",
      "Epoch 272/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0515 - acc: 0.6206 - val_loss: 0.0578 - val_acc: 0.5719\n",
      "Epoch 273/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0516 - acc: 0.6211 - val_loss: 0.0581 - val_acc: 0.5719\n",
      "Epoch 274/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0515 - acc: 0.6203 - val_loss: 0.0578 - val_acc: 0.5661\n",
      "Epoch 275/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0514 - acc: 0.6203 - val_loss: 0.0582 - val_acc: 0.5636\n",
      "Epoch 276/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0514 - acc: 0.6167 - val_loss: 0.0578 - val_acc: 0.5702\n",
      "Epoch 277/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0516 - acc: 0.6195 - val_loss: 0.0575 - val_acc: 0.5694\n",
      "Epoch 278/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0513 - acc: 0.6242 - val_loss: 0.0587 - val_acc: 0.5644\n",
      "Epoch 279/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0514 - acc: 0.6200 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 280/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0517 - acc: 0.6175 - val_loss: 0.0580 - val_acc: 0.5677\n",
      "Epoch 281/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0514 - acc: 0.6206 - val_loss: 0.0584 - val_acc: 0.5628\n",
      "Epoch 282/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0514 - acc: 0.6244 - val_loss: 0.0583 - val_acc: 0.5644\n",
      "Epoch 283/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0513 - acc: 0.6206 - val_loss: 0.0581 - val_acc: 0.5677\n",
      "Epoch 284/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0515 - acc: 0.6236 - val_loss: 0.0579 - val_acc: 0.5694\n",
      "Epoch 285/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0513 - acc: 0.6220 - val_loss: 0.0590 - val_acc: 0.5611\n",
      "Epoch 286/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0515 - acc: 0.6222 - val_loss: 0.0582 - val_acc: 0.5686\n",
      "Epoch 287/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0514 - acc: 0.6233 - val_loss: 0.0579 - val_acc: 0.5727\n",
      "Epoch 288/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0515 - acc: 0.6189 - val_loss: 0.0584 - val_acc: 0.5661\n",
      "Epoch 289/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0514 - acc: 0.6217 - val_loss: 0.0598 - val_acc: 0.5486\n",
      "Epoch 290/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0514 - acc: 0.6258 - val_loss: 0.0580 - val_acc: 0.5661\n",
      "Epoch 291/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0512 - acc: 0.6233 - val_loss: 0.0588 - val_acc: 0.5586\n",
      "Epoch 292/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0515 - acc: 0.6222 - val_loss: 0.0589 - val_acc: 0.5636\n",
      "Epoch 293/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0512 - acc: 0.6233 - val_loss: 0.0597 - val_acc: 0.5520\n",
      "Epoch 294/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0514 - acc: 0.6214 - val_loss: 0.0582 - val_acc: 0.5661\n",
      "Epoch 295/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0515 - acc: 0.6250 - val_loss: 0.0581 - val_acc: 0.5669\n",
      "Epoch 296/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0514 - acc: 0.6197 - val_loss: 0.0581 - val_acc: 0.5711\n",
      "Epoch 297/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0516 - acc: 0.6222 - val_loss: 0.0582 - val_acc: 0.5694\n",
      "Epoch 298/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0513 - acc: 0.6242 - val_loss: 0.0591 - val_acc: 0.5619\n",
      "Epoch 299/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0511 - acc: 0.6225 - val_loss: 0.0583 - val_acc: 0.5686\n",
      "Epoch 300/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0513 - acc: 0.6256 - val_loss: 0.0582 - val_acc: 0.5636\n",
      "Epoch 301/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0512 - acc: 0.6275 - val_loss: 0.0587 - val_acc: 0.5653\n",
      "Epoch 302/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0514 - acc: 0.6220 - val_loss: 0.0581 - val_acc: 0.5702\n",
      "Epoch 303/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0511 - acc: 0.6253 - val_loss: 0.0585 - val_acc: 0.5686\n",
      "Epoch 304/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0512 - acc: 0.6244 - val_loss: 0.0581 - val_acc: 0.5669\n",
      "Epoch 305/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0513 - acc: 0.6261 - val_loss: 0.0585 - val_acc: 0.5661\n",
      "Epoch 306/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0511 - acc: 0.6253 - val_loss: 0.0579 - val_acc: 0.5694\n",
      "Epoch 307/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0513 - acc: 0.6247 - val_loss: 0.0579 - val_acc: 0.5711\n",
      "Epoch 308/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0513 - acc: 0.6233 - val_loss: 0.0579 - val_acc: 0.5677\n",
      "Epoch 309/500\n",
      "3608/3608 [==============================] - 1s 203us/step - loss: 0.0511 - acc: 0.6261 - val_loss: 0.0581 - val_acc: 0.5702\n",
      "Epoch 310/500\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0513 - acc: 0.6197 - val_loss: 0.0585 - val_acc: 0.5677\n",
      "Epoch 311/500\n",
      "3608/3608 [==============================] - 1s 203us/step - loss: 0.0511 - acc: 0.6269 - val_loss: 0.0589 - val_acc: 0.5636\n",
      "Epoch 312/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0512 - acc: 0.6244 - val_loss: 0.0582 - val_acc: 0.5694\n",
      "Epoch 313/500\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0511 - acc: 0.6236 - val_loss: 0.0588 - val_acc: 0.5661\n",
      "Epoch 314/500\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0511 - acc: 0.6308 - val_loss: 0.0591 - val_acc: 0.5594\n",
      "Epoch 315/500\n",
      "3608/3608 [==============================] - 1s 205us/step - loss: 0.0510 - acc: 0.6258 - val_loss: 0.0584 - val_acc: 0.5619\n",
      "Epoch 316/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0512 - acc: 0.6258 - val_loss: 0.0582 - val_acc: 0.5677\n",
      "Epoch 317/500\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0510 - acc: 0.6286 - val_loss: 0.0580 - val_acc: 0.5694\n",
      "Epoch 318/500\n",
      "3608/3608 [==============================] - 1s 205us/step - loss: 0.0511 - acc: 0.6308 - val_loss: 0.0583 - val_acc: 0.5619\n",
      "Epoch 319/500\n",
      "3608/3608 [==============================] - 1s 206us/step - loss: 0.0512 - acc: 0.6280 - val_loss: 0.0584 - val_acc: 0.5661\n",
      "Epoch 320/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0511 - acc: 0.6256 - val_loss: 0.0580 - val_acc: 0.5669\n",
      "Epoch 321/500\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0511 - acc: 0.6258 - val_loss: 0.0581 - val_acc: 0.5736\n",
      "Epoch 322/500\n",
      "3608/3608 [==============================] - 1s 198us/step - loss: 0.0509 - acc: 0.6247 - val_loss: 0.0583 - val_acc: 0.5727\n",
      "Epoch 323/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0512 - acc: 0.6256 - val_loss: 0.0589 - val_acc: 0.5603\n",
      "Epoch 324/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0512 - acc: 0.6278 - val_loss: 0.0580 - val_acc: 0.5661\n",
      "Epoch 325/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0510 - acc: 0.6267 - val_loss: 0.0581 - val_acc: 0.5719\n",
      "Epoch 326/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0511 - acc: 0.6233 - val_loss: 0.0581 - val_acc: 0.5669\n",
      "Epoch 327/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0510 - acc: 0.6283 - val_loss: 0.0583 - val_acc: 0.5686\n",
      "Epoch 328/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0512 - acc: 0.6258 - val_loss: 0.0588 - val_acc: 0.5644\n",
      "Epoch 329/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0510 - acc: 0.6292 - val_loss: 0.0583 - val_acc: 0.5661\n",
      "Epoch 330/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0509 - acc: 0.6269 - val_loss: 0.0585 - val_acc: 0.5711\n",
      "Epoch 331/500\n",
      "3608/3608 [==============================] - 1s 187us/step - loss: 0.0511 - acc: 0.6247 - val_loss: 0.0587 - val_acc: 0.5636\n",
      "Epoch 332/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0509 - acc: 0.6272 - val_loss: 0.0588 - val_acc: 0.5594\n",
      "Epoch 333/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0510 - acc: 0.6280 - val_loss: 0.0586 - val_acc: 0.5661\n",
      "Epoch 334/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0509 - acc: 0.6303 - val_loss: 0.0584 - val_acc: 0.5686\n",
      "Epoch 335/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0508 - acc: 0.6247 - val_loss: 0.0583 - val_acc: 0.5677\n",
      "Epoch 336/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0509 - acc: 0.6311 - val_loss: 0.0586 - val_acc: 0.5661\n",
      "Epoch 337/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0509 - acc: 0.6256 - val_loss: 0.0593 - val_acc: 0.5536\n",
      "Epoch 338/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0509 - acc: 0.6317 - val_loss: 0.0584 - val_acc: 0.5661\n",
      "Epoch 339/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0510 - acc: 0.6258 - val_loss: 0.0587 - val_acc: 0.5644\n",
      "Epoch 340/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0508 - acc: 0.6294 - val_loss: 0.0586 - val_acc: 0.5636\n",
      "Epoch 341/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0510 - acc: 0.6278 - val_loss: 0.0591 - val_acc: 0.5603\n",
      "Epoch 342/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0509 - acc: 0.6278 - val_loss: 0.0586 - val_acc: 0.5661\n",
      "Epoch 343/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0511 - acc: 0.6269 - val_loss: 0.0594 - val_acc: 0.5569\n",
      "Epoch 344/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0510 - acc: 0.6275 - val_loss: 0.0590 - val_acc: 0.5644\n",
      "Epoch 345/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0508 - acc: 0.6289 - val_loss: 0.0584 - val_acc: 0.5694\n",
      "Epoch 346/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0511 - acc: 0.6280 - val_loss: 0.0586 - val_acc: 0.5653\n",
      "Epoch 347/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0509 - acc: 0.6272 - val_loss: 0.0582 - val_acc: 0.5694\n",
      "Epoch 348/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0508 - acc: 0.6322 - val_loss: 0.0591 - val_acc: 0.5636\n",
      "Epoch 349/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0508 - acc: 0.6292 - val_loss: 0.0590 - val_acc: 0.5661\n",
      "Epoch 350/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0508 - acc: 0.6353 - val_loss: 0.0588 - val_acc: 0.5677\n",
      "Epoch 351/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0510 - acc: 0.6261 - val_loss: 0.0589 - val_acc: 0.5611\n",
      "Epoch 352/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0507 - acc: 0.6294 - val_loss: 0.0591 - val_acc: 0.5594\n",
      "Epoch 353/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0509 - acc: 0.6283 - val_loss: 0.0586 - val_acc: 0.5661\n",
      "Epoch 354/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0508 - acc: 0.6308 - val_loss: 0.0585 - val_acc: 0.5644\n",
      "Epoch 355/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0508 - acc: 0.6308 - val_loss: 0.0586 - val_acc: 0.5669\n",
      "Epoch 356/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0508 - acc: 0.6275 - val_loss: 0.0592 - val_acc: 0.5520\n",
      "Epoch 357/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0508 - acc: 0.6261 - val_loss: 0.0597 - val_acc: 0.5520\n",
      "Epoch 358/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0507 - acc: 0.6256 - val_loss: 0.0588 - val_acc: 0.5677\n",
      "Epoch 359/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0509 - acc: 0.6308 - val_loss: 0.0590 - val_acc: 0.5619\n",
      "Epoch 360/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0507 - acc: 0.6292 - val_loss: 0.0585 - val_acc: 0.5719\n",
      "Epoch 361/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0509 - acc: 0.6300 - val_loss: 0.0589 - val_acc: 0.5669\n",
      "Epoch 362/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0510 - acc: 0.6275 - val_loss: 0.0586 - val_acc: 0.5669\n",
      "Epoch 363/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0506 - acc: 0.6317 - val_loss: 0.0601 - val_acc: 0.5470\n",
      "Epoch 364/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0506 - acc: 0.6305 - val_loss: 0.0586 - val_acc: 0.5719\n",
      "Epoch 365/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0508 - acc: 0.6267 - val_loss: 0.0589 - val_acc: 0.5578\n",
      "Epoch 366/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0507 - acc: 0.6300 - val_loss: 0.0595 - val_acc: 0.5486\n",
      "Epoch 367/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0506 - acc: 0.6267 - val_loss: 0.0589 - val_acc: 0.5653\n",
      "Epoch 368/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0509 - acc: 0.6278 - val_loss: 0.0587 - val_acc: 0.5644\n",
      "Epoch 369/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0508 - acc: 0.6325 - val_loss: 0.0589 - val_acc: 0.5653\n",
      "Epoch 370/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0506 - acc: 0.6353 - val_loss: 0.0588 - val_acc: 0.5653\n",
      "Epoch 371/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0508 - acc: 0.6289 - val_loss: 0.0587 - val_acc: 0.5644\n",
      "Epoch 372/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0506 - acc: 0.6292 - val_loss: 0.0587 - val_acc: 0.5686\n",
      "Epoch 373/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0507 - acc: 0.6283 - val_loss: 0.0589 - val_acc: 0.5619\n",
      "Epoch 374/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0506 - acc: 0.6303 - val_loss: 0.0593 - val_acc: 0.5561\n",
      "Epoch 375/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0509 - acc: 0.6305 - val_loss: 0.0589 - val_acc: 0.5628\n",
      "Epoch 376/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0507 - acc: 0.6333 - val_loss: 0.0589 - val_acc: 0.5644\n",
      "Epoch 377/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0507 - acc: 0.6336 - val_loss: 0.0590 - val_acc: 0.5653\n",
      "Epoch 378/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0507 - acc: 0.6258 - val_loss: 0.0589 - val_acc: 0.5661\n",
      "Epoch 379/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0510 - acc: 0.6258 - val_loss: 0.0595 - val_acc: 0.5594\n",
      "Epoch 380/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0506 - acc: 0.6294 - val_loss: 0.0594 - val_acc: 0.5528\n",
      "Epoch 381/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0510 - acc: 0.6272 - val_loss: 0.0598 - val_acc: 0.5445\n",
      "Epoch 382/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0505 - acc: 0.6319 - val_loss: 0.0586 - val_acc: 0.5669\n",
      "Epoch 383/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0507 - acc: 0.6328 - val_loss: 0.0588 - val_acc: 0.5653\n",
      "Epoch 384/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0505 - acc: 0.6303 - val_loss: 0.0593 - val_acc: 0.5594\n",
      "Epoch 385/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0506 - acc: 0.6347 - val_loss: 0.0594 - val_acc: 0.5628\n",
      "Epoch 386/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0506 - acc: 0.6292 - val_loss: 0.0592 - val_acc: 0.5628\n",
      "Epoch 387/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0506 - acc: 0.6308 - val_loss: 0.0592 - val_acc: 0.5603\n",
      "Epoch 388/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0506 - acc: 0.6305 - val_loss: 0.0589 - val_acc: 0.5653\n",
      "Epoch 389/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0505 - acc: 0.6355 - val_loss: 0.0598 - val_acc: 0.5528\n",
      "Epoch 390/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0506 - acc: 0.6292 - val_loss: 0.0589 - val_acc: 0.5644\n",
      "Epoch 391/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0506 - acc: 0.6333 - val_loss: 0.0591 - val_acc: 0.5644\n",
      "Epoch 392/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0506 - acc: 0.6292 - val_loss: 0.0591 - val_acc: 0.5653\n",
      "Epoch 393/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0505 - acc: 0.6322 - val_loss: 0.0590 - val_acc: 0.5653\n",
      "Epoch 394/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0503 - acc: 0.6305 - val_loss: 0.0590 - val_acc: 0.5653\n",
      "Epoch 395/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0504 - acc: 0.6317 - val_loss: 0.0588 - val_acc: 0.5644\n",
      "Epoch 396/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0505 - acc: 0.6300 - val_loss: 0.0588 - val_acc: 0.5677\n",
      "Epoch 397/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0505 - acc: 0.6292 - val_loss: 0.0590 - val_acc: 0.5603\n",
      "Epoch 398/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0505 - acc: 0.6355 - val_loss: 0.0592 - val_acc: 0.5619\n",
      "Epoch 399/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0504 - acc: 0.6328 - val_loss: 0.0595 - val_acc: 0.5611\n",
      "Epoch 400/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0503 - acc: 0.6336 - val_loss: 0.0602 - val_acc: 0.5528\n",
      "Epoch 401/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0506 - acc: 0.6317 - val_loss: 0.0590 - val_acc: 0.5669\n",
      "Epoch 402/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0507 - acc: 0.6336 - val_loss: 0.0593 - val_acc: 0.5486\n",
      "Epoch 403/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0505 - acc: 0.6347 - val_loss: 0.0595 - val_acc: 0.5603\n",
      "Epoch 404/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0505 - acc: 0.6330 - val_loss: 0.0591 - val_acc: 0.5611\n",
      "Epoch 405/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0506 - acc: 0.6280 - val_loss: 0.0588 - val_acc: 0.5661\n",
      "Epoch 406/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0504 - acc: 0.6330 - val_loss: 0.0589 - val_acc: 0.5644\n",
      "Epoch 407/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0505 - acc: 0.6325 - val_loss: 0.0595 - val_acc: 0.5528\n",
      "Epoch 408/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0505 - acc: 0.6294 - val_loss: 0.0589 - val_acc: 0.5702\n",
      "Epoch 409/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0504 - acc: 0.6350 - val_loss: 0.0589 - val_acc: 0.5694\n",
      "Epoch 410/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0505 - acc: 0.6339 - val_loss: 0.0598 - val_acc: 0.5536\n",
      "Epoch 411/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0505 - acc: 0.6325 - val_loss: 0.0589 - val_acc: 0.5686\n",
      "Epoch 412/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0503 - acc: 0.6358 - val_loss: 0.0596 - val_acc: 0.5561\n",
      "Epoch 413/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0504 - acc: 0.6314 - val_loss: 0.0599 - val_acc: 0.5503\n",
      "Epoch 414/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0504 - acc: 0.6353 - val_loss: 0.0593 - val_acc: 0.5586\n",
      "Epoch 415/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0504 - acc: 0.6358 - val_loss: 0.0589 - val_acc: 0.5677\n",
      "Epoch 416/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0503 - acc: 0.6347 - val_loss: 0.0593 - val_acc: 0.5653\n",
      "Epoch 417/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0503 - acc: 0.6355 - val_loss: 0.0591 - val_acc: 0.5594\n",
      "Epoch 418/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0502 - acc: 0.6361 - val_loss: 0.0593 - val_acc: 0.5669\n",
      "Epoch 419/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0504 - acc: 0.6330 - val_loss: 0.0591 - val_acc: 0.5636\n",
      "Epoch 420/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0503 - acc: 0.6344 - val_loss: 0.0587 - val_acc: 0.5702\n",
      "Epoch 421/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0503 - acc: 0.6355 - val_loss: 0.0596 - val_acc: 0.5611\n",
      "Epoch 422/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0503 - acc: 0.6333 - val_loss: 0.0590 - val_acc: 0.5677\n",
      "Epoch 423/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0503 - acc: 0.6350 - val_loss: 0.0590 - val_acc: 0.5644\n",
      "Epoch 424/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0503 - acc: 0.6311 - val_loss: 0.0590 - val_acc: 0.5686\n",
      "Epoch 425/500\n",
      "3608/3608 [==============================] - 1s 187us/step - loss: 0.0504 - acc: 0.6308 - val_loss: 0.0603 - val_acc: 0.5528\n",
      "Epoch 426/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0503 - acc: 0.6308 - val_loss: 0.0589 - val_acc: 0.5727\n",
      "Epoch 427/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0503 - acc: 0.6308 - val_loss: 0.0589 - val_acc: 0.5686\n",
      "Epoch 428/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0505 - acc: 0.6300 - val_loss: 0.0602 - val_acc: 0.5520\n",
      "Epoch 429/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0503 - acc: 0.6383 - val_loss: 0.0590 - val_acc: 0.5702\n",
      "Epoch 430/500\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0502 - acc: 0.6344 - val_loss: 0.0592 - val_acc: 0.5619\n",
      "Epoch 431/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0503 - acc: 0.6317 - val_loss: 0.0595 - val_acc: 0.5611\n",
      "Epoch 432/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0502 - acc: 0.6341 - val_loss: 0.0588 - val_acc: 0.5653\n",
      "Epoch 433/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0501 - acc: 0.6375 - val_loss: 0.0589 - val_acc: 0.5669\n",
      "Epoch 434/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0504 - acc: 0.6303 - val_loss: 0.0588 - val_acc: 0.5702\n",
      "Epoch 435/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0504 - acc: 0.6256 - val_loss: 0.0604 - val_acc: 0.5536\n",
      "Epoch 436/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0506 - acc: 0.6264 - val_loss: 0.0593 - val_acc: 0.5669\n",
      "Epoch 437/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0501 - acc: 0.6375 - val_loss: 0.0598 - val_acc: 0.5553\n",
      "Epoch 438/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0503 - acc: 0.6319 - val_loss: 0.0593 - val_acc: 0.5653\n",
      "Epoch 439/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0502 - acc: 0.6369 - val_loss: 0.0599 - val_acc: 0.5495\n",
      "Epoch 440/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0502 - acc: 0.6353 - val_loss: 0.0591 - val_acc: 0.5661\n",
      "Epoch 441/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0502 - acc: 0.6347 - val_loss: 0.0598 - val_acc: 0.5503\n",
      "Epoch 442/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0500 - acc: 0.6383 - val_loss: 0.0594 - val_acc: 0.5636\n",
      "Epoch 443/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0502 - acc: 0.6344 - val_loss: 0.0605 - val_acc: 0.5495\n",
      "Epoch 444/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0501 - acc: 0.6341 - val_loss: 0.0593 - val_acc: 0.5653\n",
      "Epoch 445/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0502 - acc: 0.6339 - val_loss: 0.0592 - val_acc: 0.5611\n",
      "Epoch 446/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0501 - acc: 0.6317 - val_loss: 0.0599 - val_acc: 0.5653\n",
      "Epoch 447/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0502 - acc: 0.6311 - val_loss: 0.0598 - val_acc: 0.5561\n",
      "Epoch 448/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0501 - acc: 0.6350 - val_loss: 0.0589 - val_acc: 0.5686\n",
      "Epoch 449/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0502 - acc: 0.6336 - val_loss: 0.0592 - val_acc: 0.5677\n",
      "Epoch 450/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0501 - acc: 0.6355 - val_loss: 0.0597 - val_acc: 0.5594\n",
      "Epoch 451/500\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0502 - acc: 0.6358 - val_loss: 0.0594 - val_acc: 0.5669\n",
      "Epoch 452/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0501 - acc: 0.6361 - val_loss: 0.0597 - val_acc: 0.5603\n",
      "Epoch 453/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0501 - acc: 0.6297 - val_loss: 0.0589 - val_acc: 0.5694\n",
      "Epoch 454/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0501 - acc: 0.6369 - val_loss: 0.0591 - val_acc: 0.5619\n",
      "Epoch 455/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0500 - acc: 0.6339 - val_loss: 0.0595 - val_acc: 0.5628\n",
      "Epoch 456/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0502 - acc: 0.6341 - val_loss: 0.0603 - val_acc: 0.5520\n",
      "Epoch 457/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0499 - acc: 0.6355 - val_loss: 0.0602 - val_acc: 0.5520\n",
      "Epoch 458/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0504 - acc: 0.6361 - val_loss: 0.0599 - val_acc: 0.5461\n",
      "Epoch 459/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0501 - acc: 0.6355 - val_loss: 0.0595 - val_acc: 0.5578\n",
      "Epoch 460/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0500 - acc: 0.6394 - val_loss: 0.0594 - val_acc: 0.5644\n",
      "Epoch 461/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0501 - acc: 0.6364 - val_loss: 0.0599 - val_acc: 0.5544\n",
      "Epoch 462/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0500 - acc: 0.6416 - val_loss: 0.0608 - val_acc: 0.5445\n",
      "Epoch 463/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0501 - acc: 0.6383 - val_loss: 0.0590 - val_acc: 0.5677\n",
      "Epoch 464/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0501 - acc: 0.6366 - val_loss: 0.0614 - val_acc: 0.5320\n",
      "Epoch 465/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0500 - acc: 0.6344 - val_loss: 0.0593 - val_acc: 0.5669\n",
      "Epoch 466/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0502 - acc: 0.6344 - val_loss: 0.0596 - val_acc: 0.5628\n",
      "Epoch 467/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0501 - acc: 0.6366 - val_loss: 0.0605 - val_acc: 0.5503\n",
      "Epoch 468/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0501 - acc: 0.6377 - val_loss: 0.0592 - val_acc: 0.5653\n",
      "Epoch 469/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0501 - acc: 0.6375 - val_loss: 0.0596 - val_acc: 0.5653\n",
      "Epoch 470/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0499 - acc: 0.6364 - val_loss: 0.0594 - val_acc: 0.5586\n",
      "Epoch 471/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0498 - acc: 0.6377 - val_loss: 0.0599 - val_acc: 0.5569\n",
      "Epoch 472/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0503 - acc: 0.6350 - val_loss: 0.0593 - val_acc: 0.5669\n",
      "Epoch 473/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0500 - acc: 0.6369 - val_loss: 0.0598 - val_acc: 0.5586\n",
      "Epoch 474/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0500 - acc: 0.6347 - val_loss: 0.0593 - val_acc: 0.5628\n",
      "Epoch 475/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0503 - acc: 0.6341 - val_loss: 0.0594 - val_acc: 0.5653\n",
      "Epoch 476/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0500 - acc: 0.6358 - val_loss: 0.0592 - val_acc: 0.5661\n",
      "Epoch 477/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0498 - acc: 0.6366 - val_loss: 0.0598 - val_acc: 0.5653\n",
      "Epoch 478/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0500 - acc: 0.6394 - val_loss: 0.0596 - val_acc: 0.5603\n",
      "Epoch 479/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0498 - acc: 0.6377 - val_loss: 0.0592 - val_acc: 0.5694\n",
      "Epoch 480/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0500 - acc: 0.6375 - val_loss: 0.0594 - val_acc: 0.5686\n",
      "Epoch 481/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0499 - acc: 0.6328 - val_loss: 0.0598 - val_acc: 0.5603\n",
      "Epoch 482/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0500 - acc: 0.6366 - val_loss: 0.0593 - val_acc: 0.5669\n",
      "Epoch 483/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0501 - acc: 0.6416 - val_loss: 0.0595 - val_acc: 0.5619\n",
      "Epoch 484/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0498 - acc: 0.6383 - val_loss: 0.0596 - val_acc: 0.5636\n",
      "Epoch 485/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0500 - acc: 0.6339 - val_loss: 0.0594 - val_acc: 0.5694\n",
      "Epoch 486/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0500 - acc: 0.6397 - val_loss: 0.0595 - val_acc: 0.5677\n",
      "Epoch 487/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0500 - acc: 0.6377 - val_loss: 0.0593 - val_acc: 0.5628\n",
      "Epoch 488/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0498 - acc: 0.6394 - val_loss: 0.0603 - val_acc: 0.5520\n",
      "Epoch 489/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0500 - acc: 0.6380 - val_loss: 0.0599 - val_acc: 0.5628\n",
      "Epoch 490/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0499 - acc: 0.6355 - val_loss: 0.0599 - val_acc: 0.5628\n",
      "Epoch 491/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0500 - acc: 0.6317 - val_loss: 0.0594 - val_acc: 0.5653\n",
      "Epoch 492/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0498 - acc: 0.6377 - val_loss: 0.0597 - val_acc: 0.5603\n",
      "Epoch 493/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0500 - acc: 0.6355 - val_loss: 0.0603 - val_acc: 0.5561\n",
      "Epoch 494/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0499 - acc: 0.6377 - val_loss: 0.0596 - val_acc: 0.5603\n",
      "Epoch 495/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0502 - acc: 0.6341 - val_loss: 0.0599 - val_acc: 0.5578\n",
      "Epoch 496/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0498 - acc: 0.6397 - val_loss: 0.0616 - val_acc: 0.5278\n",
      "Epoch 497/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0499 - acc: 0.6402 - val_loss: 0.0600 - val_acc: 0.5603\n",
      "Epoch 498/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0497 - acc: 0.6391 - val_loss: 0.0598 - val_acc: 0.5603\n",
      "Epoch 499/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0498 - acc: 0.6452 - val_loss: 0.0593 - val_acc: 0.5727\n",
      "Epoch 500/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0498 - acc: 0.6391 - val_loss: 0.0601 - val_acc: 0.5586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78165fe518>"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "nn_model = Sequential()\n",
    "from keras.layers import InputLayer, Dense\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.summary()\n",
    "nn_model.add(Dense(units=32, activation='relu'))\n",
    "nn_model.add(Dense(units=16, activation='relu'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "nn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "nn_model.fit(x=train_x, y=train_y, epochs=500, validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7PqUM2AMZVr"
   },
   "outputs": [],
   "source": [
    "pred_y = nn_model.predict(valid_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result = result.append({'model':'nn', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "AQSYP7TAMrlU",
    "outputId": "577fba5c-e158-45c8-d576-c473b95e175e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oneR</td>\n",
       "      <td>0.307564</td>\n",
       "      <td>0.030756</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.047044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision tree</td>\n",
       "      <td>0.571904</td>\n",
       "      <td>0.376798</td>\n",
       "      <td>0.385949</td>\n",
       "      <td>0.380384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.425603</td>\n",
       "      <td>0.453355</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.283087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.487116</td>\n",
       "      <td>0.488223</td>\n",
       "      <td>0.230310</td>\n",
       "      <td>0.271195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nn</td>\n",
       "      <td>0.558603</td>\n",
       "      <td>0.414678</td>\n",
       "      <td>0.380780</td>\n",
       "      <td>0.393183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  accuracy  precision    recall        f1\n",
       "0           oneR  0.307564   0.030756  0.100000  0.047044\n",
       "1  decision tree  0.571904   0.376798  0.385949  0.380384\n",
       "2            knn  0.425603   0.453355  0.225434  0.283087\n",
       "3  random forest  0.487116   0.488223  0.230310  0.271195\n",
       "4             nn  0.558603   0.414678  0.380780  0.393183"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQYEP2Wsryo_"
   },
   "source": [
    "## 4. Underfitting\n",
    "Accruacy is quite low so I will deal with this issue with changing parameters and cross-validation.\n",
    "\n",
    "### 1) Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-M4Lz4fT2lCq"
   },
   "outputs": [],
   "source": [
    "result_dt = pd.DataFrame(columns=['detail', 'accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJrwzmJmsKej"
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(train_x, train_y)\n",
    "pred_y = dt.predict(valid_x)\n",
    "precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "result_dt = result_dt.append({'detail':'default', 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSjQuru33v-p"
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(train_x, train_y)\n",
    "pred_y = dt.predict(valid_x)\n",
    "precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "result_dt = result_dt.append({'detail':'criterion=entropy', 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "H6qJvrDq32mh",
    "outputId": "4b35d0ab-3bd8-436b-d1b3-692c409a2cda"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detail</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>0.571904</td>\n",
       "      <td>0.366029</td>\n",
       "      <td>0.371762</td>\n",
       "      <td>0.368385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>criterion=entropy</td>\n",
       "      <td>0.573566</td>\n",
       "      <td>0.393742</td>\n",
       "      <td>0.379571</td>\n",
       "      <td>0.383320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              detail  accuracy  precision    recall        f1\n",
       "0            default  0.571904   0.366029  0.371762  0.368385\n",
       "1  criterion=entropy  0.573566   0.393742  0.379571  0.383320"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKEf-3zY33n8"
   },
   "outputs": [],
   "source": [
    "for i in [10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]:\n",
    "  dt = DecisionTreeClassifier(max_depth=int(i))\n",
    "  dt.fit(train_x, train_y)\n",
    "  pred_y = dt.predict(valid_x)\n",
    "  precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "  result_dt = result_dt.append({'detail':'max_depth='+str(i), 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "RAAgf8wU4PgY",
    "outputId": "54bd98c4-b3d4-4adf-a818-735c82d21dac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detail</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>0.571904</td>\n",
       "      <td>0.366029</td>\n",
       "      <td>0.371762</td>\n",
       "      <td>0.368385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>criterion=entropy</td>\n",
       "      <td>0.573566</td>\n",
       "      <td>0.393742</td>\n",
       "      <td>0.379571</td>\n",
       "      <td>0.383320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max_depth=10</td>\n",
       "      <td>0.555278</td>\n",
       "      <td>0.469068</td>\n",
       "      <td>0.312841</td>\n",
       "      <td>0.353938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max_depth=15</td>\n",
       "      <td>0.572735</td>\n",
       "      <td>0.407025</td>\n",
       "      <td>0.361352</td>\n",
       "      <td>0.378836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max_depth=20</td>\n",
       "      <td>0.581047</td>\n",
       "      <td>0.347149</td>\n",
       "      <td>0.351601</td>\n",
       "      <td>0.348119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max_depth=25</td>\n",
       "      <td>0.573566</td>\n",
       "      <td>0.358643</td>\n",
       "      <td>0.358009</td>\n",
       "      <td>0.356983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>max_depth=30</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.375521</td>\n",
       "      <td>0.381024</td>\n",
       "      <td>0.377941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max_depth=35</td>\n",
       "      <td>0.574397</td>\n",
       "      <td>0.374589</td>\n",
       "      <td>0.374087</td>\n",
       "      <td>0.373342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max_depth=40</td>\n",
       "      <td>0.581879</td>\n",
       "      <td>0.383046</td>\n",
       "      <td>0.381011</td>\n",
       "      <td>0.380938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max_depth=45</td>\n",
       "      <td>0.570241</td>\n",
       "      <td>0.358905</td>\n",
       "      <td>0.357110</td>\n",
       "      <td>0.357533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>max_depth=50</td>\n",
       "      <td>0.584372</td>\n",
       "      <td>0.379257</td>\n",
       "      <td>0.386373</td>\n",
       "      <td>0.381327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>max_depth=55</td>\n",
       "      <td>0.579385</td>\n",
       "      <td>0.385263</td>\n",
       "      <td>0.399234</td>\n",
       "      <td>0.391176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>max_depth=60</td>\n",
       "      <td>0.568579</td>\n",
       "      <td>0.368803</td>\n",
       "      <td>0.373395</td>\n",
       "      <td>0.370644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               detail  accuracy  precision    recall        f1\n",
       "0             default  0.571904   0.366029  0.371762  0.368385\n",
       "1   criterion=entropy  0.573566   0.393742  0.379571  0.383320\n",
       "2        max_depth=10  0.555278   0.469068  0.312841  0.353938\n",
       "3        max_depth=15  0.572735   0.407025  0.361352  0.378836\n",
       "4        max_depth=20  0.581047   0.347149  0.351601  0.348119\n",
       "5        max_depth=25  0.573566   0.358643  0.358009  0.356983\n",
       "6        max_depth=30  0.583541   0.375521  0.381024  0.377941\n",
       "7        max_depth=35  0.574397   0.374589  0.374087  0.373342\n",
       "8        max_depth=40  0.581879   0.383046  0.381011  0.380938\n",
       "9        max_depth=45  0.570241   0.358905  0.357110  0.357533\n",
       "10       max_depth=50  0.584372   0.379257  0.386373  0.381327\n",
       "11       max_depth=55  0.579385   0.385263  0.399234  0.391176\n",
       "12       max_depth=60  0.568579   0.368803  0.373395  0.370644"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W51IOAeh4UPw"
   },
   "outputs": [],
   "source": [
    "for i in [2, 4, 6, 8]:\n",
    "  dt = DecisionTreeClassifier(min_samples_split=int(i))\n",
    "  dt.fit(train_x, train_y)\n",
    "  pred_y = dt.predict(valid_x)\n",
    "  precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "  result_dt = result_dt.append({'detail':'min_samples_split='+str(i), 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9feUmUXH6ffP"
   },
   "outputs": [],
   "source": [
    "for i in [10, 15, 20, 25]:\n",
    "  dt = DecisionTreeClassifier(max_features=int(i))\n",
    "  dt.fit(train_x, train_y)\n",
    "  pred_y = dt.predict(valid_x)\n",
    "  precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "  result_dt = result_dt.append({'detail':'max_features='+str(i), 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "L3uw8L8c6gQk",
    "outputId": "8ae36b33-8997-4ad9-ed67-9ff93df95946"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detail</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>0.571904</td>\n",
       "      <td>0.366029</td>\n",
       "      <td>0.371762</td>\n",
       "      <td>0.368385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>criterion=entropy</td>\n",
       "      <td>0.573566</td>\n",
       "      <td>0.393742</td>\n",
       "      <td>0.379571</td>\n",
       "      <td>0.383320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max_depth=10</td>\n",
       "      <td>0.555278</td>\n",
       "      <td>0.469068</td>\n",
       "      <td>0.312841</td>\n",
       "      <td>0.353938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max_depth=15</td>\n",
       "      <td>0.572735</td>\n",
       "      <td>0.407025</td>\n",
       "      <td>0.361352</td>\n",
       "      <td>0.378836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max_depth=20</td>\n",
       "      <td>0.581047</td>\n",
       "      <td>0.347149</td>\n",
       "      <td>0.351601</td>\n",
       "      <td>0.348119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max_depth=25</td>\n",
       "      <td>0.573566</td>\n",
       "      <td>0.358643</td>\n",
       "      <td>0.358009</td>\n",
       "      <td>0.356983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>max_depth=30</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.375521</td>\n",
       "      <td>0.381024</td>\n",
       "      <td>0.377941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max_depth=35</td>\n",
       "      <td>0.574397</td>\n",
       "      <td>0.374589</td>\n",
       "      <td>0.374087</td>\n",
       "      <td>0.373342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max_depth=40</td>\n",
       "      <td>0.581879</td>\n",
       "      <td>0.383046</td>\n",
       "      <td>0.381011</td>\n",
       "      <td>0.380938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max_depth=45</td>\n",
       "      <td>0.570241</td>\n",
       "      <td>0.358905</td>\n",
       "      <td>0.357110</td>\n",
       "      <td>0.357533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>max_depth=50</td>\n",
       "      <td>0.584372</td>\n",
       "      <td>0.379257</td>\n",
       "      <td>0.386373</td>\n",
       "      <td>0.381327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>max_depth=55</td>\n",
       "      <td>0.579385</td>\n",
       "      <td>0.385263</td>\n",
       "      <td>0.399234</td>\n",
       "      <td>0.391176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>max_depth=60</td>\n",
       "      <td>0.568579</td>\n",
       "      <td>0.368803</td>\n",
       "      <td>0.373395</td>\n",
       "      <td>0.370644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>min_samples_split=2</td>\n",
       "      <td>0.577722</td>\n",
       "      <td>0.370909</td>\n",
       "      <td>0.376728</td>\n",
       "      <td>0.371905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min_samples_split=4</td>\n",
       "      <td>0.560266</td>\n",
       "      <td>0.420089</td>\n",
       "      <td>0.369339</td>\n",
       "      <td>0.391041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>min_samples_split=6</td>\n",
       "      <td>0.557772</td>\n",
       "      <td>0.432835</td>\n",
       "      <td>0.355230</td>\n",
       "      <td>0.385879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>min_samples_split=8</td>\n",
       "      <td>0.551122</td>\n",
       "      <td>0.407586</td>\n",
       "      <td>0.330108</td>\n",
       "      <td>0.359695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max_features=10</td>\n",
       "      <td>0.555278</td>\n",
       "      <td>0.353629</td>\n",
       "      <td>0.346265</td>\n",
       "      <td>0.347946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>max_features=15</td>\n",
       "      <td>0.582710</td>\n",
       "      <td>0.377008</td>\n",
       "      <td>0.373372</td>\n",
       "      <td>0.374561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>max_features=20</td>\n",
       "      <td>0.565254</td>\n",
       "      <td>0.378053</td>\n",
       "      <td>0.369309</td>\n",
       "      <td>0.371760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>max_features=25</td>\n",
       "      <td>0.578554</td>\n",
       "      <td>0.377843</td>\n",
       "      <td>0.381823</td>\n",
       "      <td>0.379126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 detail  accuracy  precision    recall        f1\n",
       "0               default  0.571904   0.366029  0.371762  0.368385\n",
       "1     criterion=entropy  0.573566   0.393742  0.379571  0.383320\n",
       "2          max_depth=10  0.555278   0.469068  0.312841  0.353938\n",
       "3          max_depth=15  0.572735   0.407025  0.361352  0.378836\n",
       "4          max_depth=20  0.581047   0.347149  0.351601  0.348119\n",
       "5          max_depth=25  0.573566   0.358643  0.358009  0.356983\n",
       "6          max_depth=30  0.583541   0.375521  0.381024  0.377941\n",
       "7          max_depth=35  0.574397   0.374589  0.374087  0.373342\n",
       "8          max_depth=40  0.581879   0.383046  0.381011  0.380938\n",
       "9          max_depth=45  0.570241   0.358905  0.357110  0.357533\n",
       "10         max_depth=50  0.584372   0.379257  0.386373  0.381327\n",
       "11         max_depth=55  0.579385   0.385263  0.399234  0.391176\n",
       "12         max_depth=60  0.568579   0.368803  0.373395  0.370644\n",
       "13  min_samples_split=2  0.577722   0.370909  0.376728  0.371905\n",
       "14  min_samples_split=4  0.560266   0.420089  0.369339  0.391041\n",
       "15  min_samples_split=6  0.557772   0.432835  0.355230  0.385879\n",
       "16  min_samples_split=8  0.551122   0.407586  0.330108  0.359695\n",
       "17      max_features=10  0.555278   0.353629  0.346265  0.347946\n",
       "18      max_features=15  0.582710   0.377008  0.373372  0.374561\n",
       "19      max_features=20  0.565254   0.378053  0.369309  0.371760\n",
       "20      max_features=25  0.578554   0.377843  0.381823  0.379126"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnlBASao7Tb3"
   },
   "source": [
    "### 2) KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vF5N2Jxu68Bp"
   },
   "outputs": [],
   "source": [
    "result_knn = pd.DataFrame(columns=['detail', 'accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rYGprWwxFCvE"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_x, train_y)\n",
    "pred_y = knn.predict(valid_x)\n",
    "precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "result_knn = result_knn.append({'detail':'default', 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "C_Im3kUFFQ2S",
    "outputId": "f44aed92-8cd8-46b7-afbf-a9ada88ee069"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "  from sklearn.neighbors import KNeighborsClassifier\n",
    "  knn = KNeighborsClassifier(n_neighbors=i)\n",
    "  knn.fit(train_x, train_y)\n",
    "  pred_y = knn.predict(valid_x)\n",
    "  precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "  result_knn = result_knn.append({'detail':'n_neighbors='+str(i), 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGRHisfRFzAJ"
   },
   "source": [
    "It seems like default n=5 is our best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "fJMkyhCIFmjU",
    "outputId": "78449cd2-0cf4-4ca9-bade-2db4ad351f8d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detail</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>0.425603</td>\n",
       "      <td>0.453355</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.283087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n_neighbors=1</td>\n",
       "      <td>0.458853</td>\n",
       "      <td>0.275551</td>\n",
       "      <td>0.280919</td>\n",
       "      <td>0.277043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n_neighbors=2</td>\n",
       "      <td>0.319202</td>\n",
       "      <td>0.496562</td>\n",
       "      <td>0.180109</td>\n",
       "      <td>0.247873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n_neighbors=3</td>\n",
       "      <td>0.439734</td>\n",
       "      <td>0.450200</td>\n",
       "      <td>0.251372</td>\n",
       "      <td>0.306323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n_neighbors=4</td>\n",
       "      <td>0.351621</td>\n",
       "      <td>0.503248</td>\n",
       "      <td>0.180158</td>\n",
       "      <td>0.243145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n_neighbors=5</td>\n",
       "      <td>0.425603</td>\n",
       "      <td>0.453355</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.283087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n_neighbors=6</td>\n",
       "      <td>0.359102</td>\n",
       "      <td>0.488255</td>\n",
       "      <td>0.182193</td>\n",
       "      <td>0.244967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n_neighbors=7</td>\n",
       "      <td>0.411471</td>\n",
       "      <td>0.406193</td>\n",
       "      <td>0.205719</td>\n",
       "      <td>0.257544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n_neighbors=8</td>\n",
       "      <td>0.369077</td>\n",
       "      <td>0.505631</td>\n",
       "      <td>0.184362</td>\n",
       "      <td>0.243879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>n_neighbors=9</td>\n",
       "      <td>0.408146</td>\n",
       "      <td>0.448421</td>\n",
       "      <td>0.204857</td>\n",
       "      <td>0.259001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>n_neighbors=10</td>\n",
       "      <td>0.374065</td>\n",
       "      <td>0.515145</td>\n",
       "      <td>0.186929</td>\n",
       "      <td>0.247026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            detail  accuracy  precision    recall        f1\n",
       "0          default  0.425603   0.453355  0.225434  0.283087\n",
       "1    n_neighbors=1  0.458853   0.275551  0.280919  0.277043\n",
       "2    n_neighbors=2  0.319202   0.496562  0.180109  0.247873\n",
       "3    n_neighbors=3  0.439734   0.450200  0.251372  0.306323\n",
       "4    n_neighbors=4  0.351621   0.503248  0.180158  0.243145\n",
       "5    n_neighbors=5  0.425603   0.453355  0.225434  0.283087\n",
       "6    n_neighbors=6  0.359102   0.488255  0.182193  0.244967\n",
       "7    n_neighbors=7  0.411471   0.406193  0.205719  0.257544\n",
       "8    n_neighbors=8  0.369077   0.505631  0.184362  0.243879\n",
       "9    n_neighbors=9  0.408146   0.448421  0.204857  0.259001\n",
       "10  n_neighbors=10  0.374065   0.515145  0.186929  0.247026"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aC8V5o_7Xir"
   },
   "source": [
    "### 3) Random Forest 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4wg9fGd7YRl"
   },
   "outputs": [],
   "source": [
    "result_rf = pd.DataFrame(columns=['detail', 'accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "91_rZ-lUG3y0",
    "outputId": "f7ec637f-3d9f-4d43-b9f5-d5a18f2e57a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_x, train_y)\n",
    "pred_y = rf.predict(valid_x)\n",
    "precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "result_rf = result_rf.append({'detail':'default', 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "id": "j-yWG4GlG8vw",
    "outputId": "c959f532-2471-484d-e0bd-b0669e0482ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 21):\n",
    "  rf = RandomForestClassifier(n_estimators = i*10)\n",
    "  rf.fit(train_x, train_y)\n",
    "  pred_y = rf.predict(valid_x)\n",
    "  precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "  result_rf = result_rf.append({'detail':'n_estimators='+str(i*10), 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "2NzxkRfWHRlV",
    "outputId": "b0f0735e-7a0f-4197-da96-2063843da08a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detail</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>0.456359</td>\n",
       "      <td>0.577750</td>\n",
       "      <td>0.218550</td>\n",
       "      <td>0.268241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n_estimators=10</td>\n",
       "      <td>0.458022</td>\n",
       "      <td>0.471849</td>\n",
       "      <td>0.217438</td>\n",
       "      <td>0.262553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n_estimators=20</td>\n",
       "      <td>0.463009</td>\n",
       "      <td>0.375412</td>\n",
       "      <td>0.216553</td>\n",
       "      <td>0.254532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n_estimators=30</td>\n",
       "      <td>0.482959</td>\n",
       "      <td>0.529079</td>\n",
       "      <td>0.229114</td>\n",
       "      <td>0.275424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n_estimators=40</td>\n",
       "      <td>0.493766</td>\n",
       "      <td>0.486675</td>\n",
       "      <td>0.232756</td>\n",
       "      <td>0.276628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n_estimators=50</td>\n",
       "      <td>0.488778</td>\n",
       "      <td>0.508282</td>\n",
       "      <td>0.224289</td>\n",
       "      <td>0.265610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n_estimators=60</td>\n",
       "      <td>0.502078</td>\n",
       "      <td>0.561996</td>\n",
       "      <td>0.241398</td>\n",
       "      <td>0.289091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n_estimators=70</td>\n",
       "      <td>0.488778</td>\n",
       "      <td>0.447299</td>\n",
       "      <td>0.224541</td>\n",
       "      <td>0.260468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n_estimators=80</td>\n",
       "      <td>0.495428</td>\n",
       "      <td>0.560699</td>\n",
       "      <td>0.230032</td>\n",
       "      <td>0.272665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>n_estimators=90</td>\n",
       "      <td>0.489609</td>\n",
       "      <td>0.496133</td>\n",
       "      <td>0.226090</td>\n",
       "      <td>0.265411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>n_estimators=100</td>\n",
       "      <td>0.488778</td>\n",
       "      <td>0.378352</td>\n",
       "      <td>0.220016</td>\n",
       "      <td>0.255637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>n_estimators=110</td>\n",
       "      <td>0.483791</td>\n",
       "      <td>0.420394</td>\n",
       "      <td>0.216950</td>\n",
       "      <td>0.252186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>n_estimators=120</td>\n",
       "      <td>0.492934</td>\n",
       "      <td>0.424127</td>\n",
       "      <td>0.230128</td>\n",
       "      <td>0.270039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>n_estimators=130</td>\n",
       "      <td>0.494597</td>\n",
       "      <td>0.413909</td>\n",
       "      <td>0.229607</td>\n",
       "      <td>0.267964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>n_estimators=140</td>\n",
       "      <td>0.488778</td>\n",
       "      <td>0.469523</td>\n",
       "      <td>0.219229</td>\n",
       "      <td>0.254968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>n_estimators=150</td>\n",
       "      <td>0.487116</td>\n",
       "      <td>0.456290</td>\n",
       "      <td>0.219698</td>\n",
       "      <td>0.255559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>n_estimators=160</td>\n",
       "      <td>0.486284</td>\n",
       "      <td>0.375978</td>\n",
       "      <td>0.218385</td>\n",
       "      <td>0.253614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>n_estimators=170</td>\n",
       "      <td>0.493766</td>\n",
       "      <td>0.424177</td>\n",
       "      <td>0.227892</td>\n",
       "      <td>0.267311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>n_estimators=180</td>\n",
       "      <td>0.489609</td>\n",
       "      <td>0.384403</td>\n",
       "      <td>0.226749</td>\n",
       "      <td>0.264523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>n_estimators=190</td>\n",
       "      <td>0.496259</td>\n",
       "      <td>0.425069</td>\n",
       "      <td>0.229972</td>\n",
       "      <td>0.269167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>n_estimators=200</td>\n",
       "      <td>0.494597</td>\n",
       "      <td>0.374531</td>\n",
       "      <td>0.223138</td>\n",
       "      <td>0.257447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              detail  accuracy  precision    recall        f1\n",
       "0            default  0.456359   0.577750  0.218550  0.268241\n",
       "1    n_estimators=10  0.458022   0.471849  0.217438  0.262553\n",
       "2    n_estimators=20  0.463009   0.375412  0.216553  0.254532\n",
       "3    n_estimators=30  0.482959   0.529079  0.229114  0.275424\n",
       "4    n_estimators=40  0.493766   0.486675  0.232756  0.276628\n",
       "5    n_estimators=50  0.488778   0.508282  0.224289  0.265610\n",
       "6    n_estimators=60  0.502078   0.561996  0.241398  0.289091\n",
       "7    n_estimators=70  0.488778   0.447299  0.224541  0.260468\n",
       "8    n_estimators=80  0.495428   0.560699  0.230032  0.272665\n",
       "9    n_estimators=90  0.489609   0.496133  0.226090  0.265411\n",
       "10  n_estimators=100  0.488778   0.378352  0.220016  0.255637\n",
       "11  n_estimators=110  0.483791   0.420394  0.216950  0.252186\n",
       "12  n_estimators=120  0.492934   0.424127  0.230128  0.270039\n",
       "13  n_estimators=130  0.494597   0.413909  0.229607  0.267964\n",
       "14  n_estimators=140  0.488778   0.469523  0.219229  0.254968\n",
       "15  n_estimators=150  0.487116   0.456290  0.219698  0.255559\n",
       "16  n_estimators=160  0.486284   0.375978  0.218385  0.253614\n",
       "17  n_estimators=170  0.493766   0.424177  0.227892  0.267311\n",
       "18  n_estimators=180  0.489609   0.384403  0.226749  0.264523\n",
       "19  n_estimators=190  0.496259   0.425069  0.229972  0.269167\n",
       "20  n_estimators=200  0.494597   0.374531  0.223138  0.257447"
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "z-9iAVZPHvzy",
    "outputId": "b9c92c12-0fe0-4c48-dfae-ea031a51c071"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(warm_start=True)\n",
    "rf.fit(train_x, train_y)\n",
    "pred_y = rf.predict(valid_x)\n",
    "precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "result_rf = result_rf.append({'detail':'warm_start=true', 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "cl_sl5HqIFT6",
    "outputId": "e26e967b-7aec-42ac-a3b8-bfd6c1c717dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detail</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>0.456359</td>\n",
       "      <td>0.577750</td>\n",
       "      <td>0.218550</td>\n",
       "      <td>0.268241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n_estimators=10</td>\n",
       "      <td>0.458022</td>\n",
       "      <td>0.471849</td>\n",
       "      <td>0.217438</td>\n",
       "      <td>0.262553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n_estimators=20</td>\n",
       "      <td>0.463009</td>\n",
       "      <td>0.375412</td>\n",
       "      <td>0.216553</td>\n",
       "      <td>0.254532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n_estimators=30</td>\n",
       "      <td>0.482959</td>\n",
       "      <td>0.529079</td>\n",
       "      <td>0.229114</td>\n",
       "      <td>0.275424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n_estimators=40</td>\n",
       "      <td>0.493766</td>\n",
       "      <td>0.486675</td>\n",
       "      <td>0.232756</td>\n",
       "      <td>0.276628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n_estimators=50</td>\n",
       "      <td>0.488778</td>\n",
       "      <td>0.508282</td>\n",
       "      <td>0.224289</td>\n",
       "      <td>0.265610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n_estimators=60</td>\n",
       "      <td>0.502078</td>\n",
       "      <td>0.561996</td>\n",
       "      <td>0.241398</td>\n",
       "      <td>0.289091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n_estimators=70</td>\n",
       "      <td>0.488778</td>\n",
       "      <td>0.447299</td>\n",
       "      <td>0.224541</td>\n",
       "      <td>0.260468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n_estimators=80</td>\n",
       "      <td>0.495428</td>\n",
       "      <td>0.560699</td>\n",
       "      <td>0.230032</td>\n",
       "      <td>0.272665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>n_estimators=90</td>\n",
       "      <td>0.489609</td>\n",
       "      <td>0.496133</td>\n",
       "      <td>0.226090</td>\n",
       "      <td>0.265411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>n_estimators=100</td>\n",
       "      <td>0.488778</td>\n",
       "      <td>0.378352</td>\n",
       "      <td>0.220016</td>\n",
       "      <td>0.255637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>n_estimators=110</td>\n",
       "      <td>0.483791</td>\n",
       "      <td>0.420394</td>\n",
       "      <td>0.216950</td>\n",
       "      <td>0.252186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>n_estimators=120</td>\n",
       "      <td>0.492934</td>\n",
       "      <td>0.424127</td>\n",
       "      <td>0.230128</td>\n",
       "      <td>0.270039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>n_estimators=130</td>\n",
       "      <td>0.494597</td>\n",
       "      <td>0.413909</td>\n",
       "      <td>0.229607</td>\n",
       "      <td>0.267964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>n_estimators=140</td>\n",
       "      <td>0.488778</td>\n",
       "      <td>0.469523</td>\n",
       "      <td>0.219229</td>\n",
       "      <td>0.254968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>n_estimators=150</td>\n",
       "      <td>0.487116</td>\n",
       "      <td>0.456290</td>\n",
       "      <td>0.219698</td>\n",
       "      <td>0.255559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>n_estimators=160</td>\n",
       "      <td>0.486284</td>\n",
       "      <td>0.375978</td>\n",
       "      <td>0.218385</td>\n",
       "      <td>0.253614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>n_estimators=170</td>\n",
       "      <td>0.493766</td>\n",
       "      <td>0.424177</td>\n",
       "      <td>0.227892</td>\n",
       "      <td>0.267311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>n_estimators=180</td>\n",
       "      <td>0.489609</td>\n",
       "      <td>0.384403</td>\n",
       "      <td>0.226749</td>\n",
       "      <td>0.264523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>n_estimators=190</td>\n",
       "      <td>0.496259</td>\n",
       "      <td>0.425069</td>\n",
       "      <td>0.229972</td>\n",
       "      <td>0.269167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>n_estimators=200</td>\n",
       "      <td>0.494597</td>\n",
       "      <td>0.374531</td>\n",
       "      <td>0.223138</td>\n",
       "      <td>0.257447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>warm_start=true</td>\n",
       "      <td>0.452203</td>\n",
       "      <td>0.510121</td>\n",
       "      <td>0.225476</td>\n",
       "      <td>0.272592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              detail  accuracy  precision    recall        f1\n",
       "0            default  0.456359   0.577750  0.218550  0.268241\n",
       "1    n_estimators=10  0.458022   0.471849  0.217438  0.262553\n",
       "2    n_estimators=20  0.463009   0.375412  0.216553  0.254532\n",
       "3    n_estimators=30  0.482959   0.529079  0.229114  0.275424\n",
       "4    n_estimators=40  0.493766   0.486675  0.232756  0.276628\n",
       "5    n_estimators=50  0.488778   0.508282  0.224289  0.265610\n",
       "6    n_estimators=60  0.502078   0.561996  0.241398  0.289091\n",
       "7    n_estimators=70  0.488778   0.447299  0.224541  0.260468\n",
       "8    n_estimators=80  0.495428   0.560699  0.230032  0.272665\n",
       "9    n_estimators=90  0.489609   0.496133  0.226090  0.265411\n",
       "10  n_estimators=100  0.488778   0.378352  0.220016  0.255637\n",
       "11  n_estimators=110  0.483791   0.420394  0.216950  0.252186\n",
       "12  n_estimators=120  0.492934   0.424127  0.230128  0.270039\n",
       "13  n_estimators=130  0.494597   0.413909  0.229607  0.267964\n",
       "14  n_estimators=140  0.488778   0.469523  0.219229  0.254968\n",
       "15  n_estimators=150  0.487116   0.456290  0.219698  0.255559\n",
       "16  n_estimators=160  0.486284   0.375978  0.218385  0.253614\n",
       "17  n_estimators=170  0.493766   0.424177  0.227892  0.267311\n",
       "18  n_estimators=180  0.489609   0.384403  0.226749  0.264523\n",
       "19  n_estimators=190  0.496259   0.425069  0.229972  0.269167\n",
       "20  n_estimators=200  0.494597   0.374531  0.223138  0.257447\n",
       "21   warm_start=true  0.452203   0.510121  0.225476  0.272592"
      ]
     },
     "execution_count": 117,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTPENb_7nWB0"
   },
   "source": [
    "n_estimators=60 으로 성능을 5% 정도 올릴 수 있었다. warm_start의 경우 별 효과가 없었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImxT27VHoXL7"
   },
   "outputs": [],
   "source": [
    "result_rf = pd.DataFrame(columns=['detail', 'accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "D2ifOXLJpCjU",
    "outputId": "47d8bfaf-b0a3-44de-d02e-2d36a24bdc2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(criterion='entropy', n_estimators=60)\n",
    "rf.fit(train_x, train_y)\n",
    "pred_y = rf.predict(valid_x)\n",
    "precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "result_rf = result_rf.append({'detail':'criterion=entropy', 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "RkPrYeRfpbaz",
    "outputId": "cadaceda-68a1-43f0-f59e-04704d34e541"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "  rf = RandomForestClassifier(n_estimators = 60, max_depth=i*10)\n",
    "  rf.fit(train_x, train_y)\n",
    "  pred_y = rf.predict(valid_x)\n",
    "  precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "  result_rf = result_rf.append({'detail':'max_depth='+str(i*10), 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "kqfc2H42pODq",
    "outputId": "4aece291-faa0-4af6-e2ea-035bdf55a974"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detail</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>criterion=entropy</td>\n",
       "      <td>0.487947</td>\n",
       "      <td>0.489727</td>\n",
       "      <td>0.223285</td>\n",
       "      <td>0.262005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max_depth=10</td>\n",
       "      <td>0.438903</td>\n",
       "      <td>0.356899</td>\n",
       "      <td>0.181856</td>\n",
       "      <td>0.203746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max_depth=20</td>\n",
       "      <td>0.492103</td>\n",
       "      <td>0.417603</td>\n",
       "      <td>0.226739</td>\n",
       "      <td>0.265051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max_depth=30</td>\n",
       "      <td>0.488778</td>\n",
       "      <td>0.417227</td>\n",
       "      <td>0.227553</td>\n",
       "      <td>0.268549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max_depth=40</td>\n",
       "      <td>0.486284</td>\n",
       "      <td>0.426887</td>\n",
       "      <td>0.218789</td>\n",
       "      <td>0.254902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max_depth=50</td>\n",
       "      <td>0.479634</td>\n",
       "      <td>0.381862</td>\n",
       "      <td>0.221033</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>max_depth=60</td>\n",
       "      <td>0.491272</td>\n",
       "      <td>0.513011</td>\n",
       "      <td>0.232465</td>\n",
       "      <td>0.274712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max_depth=70</td>\n",
       "      <td>0.484622</td>\n",
       "      <td>0.559400</td>\n",
       "      <td>0.235750</td>\n",
       "      <td>0.284327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max_depth=80</td>\n",
       "      <td>0.486284</td>\n",
       "      <td>0.370197</td>\n",
       "      <td>0.218190</td>\n",
       "      <td>0.253105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max_depth=90</td>\n",
       "      <td>0.484622</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.226269</td>\n",
       "      <td>0.269088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>max_depth=100</td>\n",
       "      <td>0.491272</td>\n",
       "      <td>0.517826</td>\n",
       "      <td>0.224736</td>\n",
       "      <td>0.263559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               detail  accuracy  precision    recall        f1\n",
       "0   criterion=entropy  0.487947   0.489727  0.223285  0.262005\n",
       "1        max_depth=10  0.438903   0.356899  0.181856  0.203746\n",
       "2        max_depth=20  0.492103   0.417603  0.226739  0.265051\n",
       "3        max_depth=30  0.488778   0.417227  0.227553  0.268549\n",
       "4        max_depth=40  0.486284   0.426887  0.218789  0.254902\n",
       "5        max_depth=50  0.479634   0.381862  0.221033  0.258400\n",
       "6        max_depth=60  0.491272   0.513011  0.232465  0.274712\n",
       "7        max_depth=70  0.484622   0.559400  0.235750  0.284327\n",
       "8        max_depth=80  0.486284   0.370197  0.218190  0.253105\n",
       "9        max_depth=90  0.484622   0.484838  0.226269  0.269088\n",
       "10      max_depth=100  0.491272   0.517826  0.224736  0.263559"
      ]
     },
     "execution_count": 123,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Pi_2qIB7azo"
   },
   "source": [
    "### 4) Neural Network 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euVNa-9mIVXt"
   },
   "source": [
    "I tried encoder decoder type neural network models but the result is not that different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17561
    },
    "colab_type": "code",
    "id": "bPtcA58_7dIk",
    "outputId": "bc7aaa64-4618-40e8-a8ef-384850f50091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                90        \n",
      "=================================================================\n",
      "Total params: 1,908\n",
      "Trainable params: 1,908\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3608 samples, validate on 1203 samples\n",
      "Epoch 1/500\n",
      "3608/3608 [==============================] - 1s 404us/step - loss: 0.0891 - acc: 0.1599 - val_loss: 0.0876 - val_acc: 0.1729\n",
      "Epoch 2/500\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0859 - acc: 0.1563 - val_loss: 0.0830 - val_acc: 0.1737\n",
      "Epoch 3/500\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0807 - acc: 0.2009 - val_loss: 0.0725 - val_acc: 0.2269\n",
      "Epoch 4/500\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0721 - acc: 0.3631 - val_loss: 0.0700 - val_acc: 0.4555\n",
      "Epoch 5/500\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0703 - acc: 0.4365 - val_loss: 0.0685 - val_acc: 0.4547\n",
      "Epoch 6/500\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0691 - acc: 0.4354 - val_loss: 0.0675 - val_acc: 0.4547\n",
      "Epoch 7/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0682 - acc: 0.4354 - val_loss: 0.0662 - val_acc: 0.4547\n",
      "Epoch 8/500\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0670 - acc: 0.4354 - val_loss: 0.0651 - val_acc: 0.4547\n",
      "Epoch 9/500\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0663 - acc: 0.4354 - val_loss: 0.0648 - val_acc: 0.4547\n",
      "Epoch 10/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0658 - acc: 0.4357 - val_loss: 0.0642 - val_acc: 0.4547\n",
      "Epoch 11/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0654 - acc: 0.4357 - val_loss: 0.0639 - val_acc: 0.4547\n",
      "Epoch 12/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0652 - acc: 0.4360 - val_loss: 0.0636 - val_acc: 0.4547\n",
      "Epoch 13/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0649 - acc: 0.4360 - val_loss: 0.0634 - val_acc: 0.4547\n",
      "Epoch 14/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0646 - acc: 0.4360 - val_loss: 0.0631 - val_acc: 0.4547\n",
      "Epoch 15/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0644 - acc: 0.4360 - val_loss: 0.0631 - val_acc: 0.4547\n",
      "Epoch 16/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0643 - acc: 0.4368 - val_loss: 0.0637 - val_acc: 0.4547\n",
      "Epoch 17/500\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0640 - acc: 0.4371 - val_loss: 0.0622 - val_acc: 0.4539\n",
      "Epoch 18/500\n",
      "3608/3608 [==============================] - 1s 306us/step - loss: 0.0634 - acc: 0.4415 - val_loss: 0.0626 - val_acc: 0.4572\n",
      "Epoch 19/500\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0628 - acc: 0.4454 - val_loss: 0.0612 - val_acc: 0.4589\n",
      "Epoch 20/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0622 - acc: 0.4593 - val_loss: 0.0608 - val_acc: 0.4655\n",
      "Epoch 21/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0617 - acc: 0.4609 - val_loss: 0.0602 - val_acc: 0.4838\n",
      "Epoch 22/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0610 - acc: 0.4787 - val_loss: 0.0609 - val_acc: 0.4896\n",
      "Epoch 23/500\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0611 - acc: 0.4820 - val_loss: 0.0593 - val_acc: 0.4988\n",
      "Epoch 24/500\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0604 - acc: 0.4903 - val_loss: 0.0586 - val_acc: 0.5145\n",
      "Epoch 25/500\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0601 - acc: 0.4942 - val_loss: 0.0582 - val_acc: 0.5087\n",
      "Epoch 26/500\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0598 - acc: 0.5025 - val_loss: 0.0580 - val_acc: 0.5129\n",
      "Epoch 27/500\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0596 - acc: 0.5091 - val_loss: 0.0590 - val_acc: 0.5312\n",
      "Epoch 28/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0597 - acc: 0.5122 - val_loss: 0.0577 - val_acc: 0.5278\n",
      "Epoch 29/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0592 - acc: 0.5164 - val_loss: 0.0575 - val_acc: 0.5420\n",
      "Epoch 30/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0594 - acc: 0.5197 - val_loss: 0.0592 - val_acc: 0.5254\n",
      "Epoch 31/500\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0591 - acc: 0.5277 - val_loss: 0.0577 - val_acc: 0.5387\n",
      "Epoch 32/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0588 - acc: 0.5241 - val_loss: 0.0588 - val_acc: 0.5353\n",
      "Epoch 33/500\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0596 - acc: 0.5194 - val_loss: 0.0591 - val_acc: 0.5212\n",
      "Epoch 34/500\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0589 - acc: 0.5230 - val_loss: 0.0574 - val_acc: 0.5544\n",
      "Epoch 35/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0586 - acc: 0.5297 - val_loss: 0.0574 - val_acc: 0.5528\n",
      "Epoch 36/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0587 - acc: 0.5258 - val_loss: 0.0584 - val_acc: 0.5370\n",
      "Epoch 37/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0588 - acc: 0.5261 - val_loss: 0.0574 - val_acc: 0.5395\n",
      "Epoch 38/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0585 - acc: 0.5288 - val_loss: 0.0576 - val_acc: 0.5478\n",
      "Epoch 39/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0582 - acc: 0.5305 - val_loss: 0.0579 - val_acc: 0.5353\n",
      "Epoch 40/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0583 - acc: 0.5308 - val_loss: 0.0576 - val_acc: 0.5337\n",
      "Epoch 41/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0583 - acc: 0.5310 - val_loss: 0.0573 - val_acc: 0.5544\n",
      "Epoch 42/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0584 - acc: 0.5283 - val_loss: 0.0581 - val_acc: 0.5403\n",
      "Epoch 43/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0583 - acc: 0.5316 - val_loss: 0.0570 - val_acc: 0.5520\n",
      "Epoch 44/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0581 - acc: 0.5346 - val_loss: 0.0577 - val_acc: 0.5436\n",
      "Epoch 45/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0580 - acc: 0.5322 - val_loss: 0.0570 - val_acc: 0.5520\n",
      "Epoch 46/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0580 - acc: 0.5341 - val_loss: 0.0573 - val_acc: 0.5536\n",
      "Epoch 47/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0581 - acc: 0.5302 - val_loss: 0.0573 - val_acc: 0.5511\n",
      "Epoch 48/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0579 - acc: 0.5327 - val_loss: 0.0571 - val_acc: 0.5536\n",
      "Epoch 49/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0578 - acc: 0.5380 - val_loss: 0.0571 - val_acc: 0.5528\n",
      "Epoch 50/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0578 - acc: 0.5457 - val_loss: 0.0573 - val_acc: 0.5520\n",
      "Epoch 51/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0577 - acc: 0.5463 - val_loss: 0.0570 - val_acc: 0.5503\n",
      "Epoch 52/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0577 - acc: 0.5485 - val_loss: 0.0573 - val_acc: 0.5520\n",
      "Epoch 53/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0574 - acc: 0.5479 - val_loss: 0.0569 - val_acc: 0.5528\n",
      "Epoch 54/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0575 - acc: 0.5482 - val_loss: 0.0573 - val_acc: 0.5503\n",
      "Epoch 55/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0576 - acc: 0.5488 - val_loss: 0.0570 - val_acc: 0.5495\n",
      "Epoch 56/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0577 - acc: 0.5474 - val_loss: 0.0574 - val_acc: 0.5536\n",
      "Epoch 57/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0572 - acc: 0.5510 - val_loss: 0.0568 - val_acc: 0.5503\n",
      "Epoch 58/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0574 - acc: 0.5507 - val_loss: 0.0570 - val_acc: 0.5528\n",
      "Epoch 59/500\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0573 - acc: 0.5466 - val_loss: 0.0574 - val_acc: 0.5495\n",
      "Epoch 60/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0572 - acc: 0.5452 - val_loss: 0.0568 - val_acc: 0.5553\n",
      "Epoch 61/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0572 - acc: 0.5510 - val_loss: 0.0579 - val_acc: 0.5411\n",
      "Epoch 62/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0575 - acc: 0.5479 - val_loss: 0.0568 - val_acc: 0.5561\n",
      "Epoch 63/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0572 - acc: 0.5527 - val_loss: 0.0567 - val_acc: 0.5520\n",
      "Epoch 64/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0572 - acc: 0.5504 - val_loss: 0.0566 - val_acc: 0.5578\n",
      "Epoch 65/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0570 - acc: 0.5552 - val_loss: 0.0569 - val_acc: 0.5611\n",
      "Epoch 66/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0568 - acc: 0.5635 - val_loss: 0.0568 - val_acc: 0.5619\n",
      "Epoch 67/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0568 - acc: 0.5651 - val_loss: 0.0568 - val_acc: 0.5644\n",
      "Epoch 68/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0570 - acc: 0.5574 - val_loss: 0.0567 - val_acc: 0.5586\n",
      "Epoch 69/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0569 - acc: 0.5654 - val_loss: 0.0568 - val_acc: 0.5644\n",
      "Epoch 70/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0568 - acc: 0.5607 - val_loss: 0.0570 - val_acc: 0.5619\n",
      "Epoch 71/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0568 - acc: 0.5665 - val_loss: 0.0568 - val_acc: 0.5644\n",
      "Epoch 72/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0568 - acc: 0.5696 - val_loss: 0.0568 - val_acc: 0.5669\n",
      "Epoch 73/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0566 - acc: 0.5687 - val_loss: 0.0619 - val_acc: 0.4988\n",
      "Epoch 74/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0568 - acc: 0.5635 - val_loss: 0.0574 - val_acc: 0.5619\n",
      "Epoch 75/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0565 - acc: 0.5698 - val_loss: 0.0574 - val_acc: 0.5594\n",
      "Epoch 76/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0567 - acc: 0.5649 - val_loss: 0.0572 - val_acc: 0.5677\n",
      "Epoch 77/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0567 - acc: 0.5701 - val_loss: 0.0569 - val_acc: 0.5653\n",
      "Epoch 78/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0566 - acc: 0.5687 - val_loss: 0.0568 - val_acc: 0.5619\n",
      "Epoch 79/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0565 - acc: 0.5674 - val_loss: 0.0570 - val_acc: 0.5636\n",
      "Epoch 80/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0565 - acc: 0.5660 - val_loss: 0.0579 - val_acc: 0.5603\n",
      "Epoch 81/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0565 - acc: 0.5660 - val_loss: 0.0568 - val_acc: 0.5636\n",
      "Epoch 82/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0563 - acc: 0.5682 - val_loss: 0.0568 - val_acc: 0.5702\n",
      "Epoch 83/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0566 - acc: 0.5676 - val_loss: 0.0566 - val_acc: 0.5677\n",
      "Epoch 84/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0564 - acc: 0.5723 - val_loss: 0.0565 - val_acc: 0.5711\n",
      "Epoch 85/500\n",
      "3608/3608 [==============================] - 1s 324us/step - loss: 0.0563 - acc: 0.5743 - val_loss: 0.0572 - val_acc: 0.5619\n",
      "Epoch 86/500\n",
      "3608/3608 [==============================] - 1s 352us/step - loss: 0.0563 - acc: 0.5754 - val_loss: 0.0589 - val_acc: 0.5520\n",
      "Epoch 87/500\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0563 - acc: 0.5732 - val_loss: 0.0568 - val_acc: 0.5619\n",
      "Epoch 88/500\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0564 - acc: 0.5712 - val_loss: 0.0568 - val_acc: 0.5661\n",
      "Epoch 89/500\n",
      "3608/3608 [==============================] - 1s 322us/step - loss: 0.0564 - acc: 0.5748 - val_loss: 0.0567 - val_acc: 0.5636\n",
      "Epoch 90/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0562 - acc: 0.5710 - val_loss: 0.0566 - val_acc: 0.5661\n",
      "Epoch 91/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0561 - acc: 0.5718 - val_loss: 0.0567 - val_acc: 0.5669\n",
      "Epoch 92/500\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0564 - acc: 0.5682 - val_loss: 0.0568 - val_acc: 0.5669\n",
      "Epoch 93/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0563 - acc: 0.5690 - val_loss: 0.0567 - val_acc: 0.5644\n",
      "Epoch 94/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0561 - acc: 0.5729 - val_loss: 0.0576 - val_acc: 0.5636\n",
      "Epoch 95/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0564 - acc: 0.5718 - val_loss: 0.0566 - val_acc: 0.5603\n",
      "Epoch 96/500\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0560 - acc: 0.5759 - val_loss: 0.0569 - val_acc: 0.5619\n",
      "Epoch 97/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0560 - acc: 0.5776 - val_loss: 0.0578 - val_acc: 0.5594\n",
      "Epoch 98/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0560 - acc: 0.5737 - val_loss: 0.0587 - val_acc: 0.5461\n",
      "Epoch 99/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0562 - acc: 0.5729 - val_loss: 0.0570 - val_acc: 0.5644\n",
      "Epoch 100/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0559 - acc: 0.5754 - val_loss: 0.0568 - val_acc: 0.5578\n",
      "Epoch 101/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0560 - acc: 0.5737 - val_loss: 0.0563 - val_acc: 0.5677\n",
      "Epoch 102/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0558 - acc: 0.5759 - val_loss: 0.0564 - val_acc: 0.5628\n",
      "Epoch 103/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0558 - acc: 0.5771 - val_loss: 0.0565 - val_acc: 0.5669\n",
      "Epoch 104/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0557 - acc: 0.5782 - val_loss: 0.0565 - val_acc: 0.5636\n",
      "Epoch 105/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0559 - acc: 0.5779 - val_loss: 0.0564 - val_acc: 0.5719\n",
      "Epoch 106/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0557 - acc: 0.5782 - val_loss: 0.0576 - val_acc: 0.5611\n",
      "Epoch 107/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0559 - acc: 0.5790 - val_loss: 0.0569 - val_acc: 0.5619\n",
      "Epoch 108/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0560 - acc: 0.5759 - val_loss: 0.0562 - val_acc: 0.5694\n",
      "Epoch 109/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0557 - acc: 0.5784 - val_loss: 0.0564 - val_acc: 0.5686\n",
      "Epoch 110/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0555 - acc: 0.5759 - val_loss: 0.0570 - val_acc: 0.5603\n",
      "Epoch 111/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0556 - acc: 0.5765 - val_loss: 0.0569 - val_acc: 0.5628\n",
      "Epoch 112/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0557 - acc: 0.5771 - val_loss: 0.0562 - val_acc: 0.5661\n",
      "Epoch 113/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0557 - acc: 0.5795 - val_loss: 0.0564 - val_acc: 0.5677\n",
      "Epoch 114/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0558 - acc: 0.5771 - val_loss: 0.0563 - val_acc: 0.5661\n",
      "Epoch 115/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0554 - acc: 0.5809 - val_loss: 0.0563 - val_acc: 0.5653\n",
      "Epoch 116/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0556 - acc: 0.5834 - val_loss: 0.0562 - val_acc: 0.5719\n",
      "Epoch 117/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0554 - acc: 0.5815 - val_loss: 0.0564 - val_acc: 0.5686\n",
      "Epoch 118/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0554 - acc: 0.5868 - val_loss: 0.0567 - val_acc: 0.5677\n",
      "Epoch 119/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0553 - acc: 0.5823 - val_loss: 0.0575 - val_acc: 0.5653\n",
      "Epoch 120/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0554 - acc: 0.5834 - val_loss: 0.0568 - val_acc: 0.5694\n",
      "Epoch 121/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0554 - acc: 0.5826 - val_loss: 0.0572 - val_acc: 0.5661\n",
      "Epoch 122/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0555 - acc: 0.5776 - val_loss: 0.0564 - val_acc: 0.5686\n",
      "Epoch 123/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0555 - acc: 0.5831 - val_loss: 0.0567 - val_acc: 0.5636\n",
      "Epoch 124/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0554 - acc: 0.5773 - val_loss: 0.0566 - val_acc: 0.5661\n",
      "Epoch 125/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0553 - acc: 0.5840 - val_loss: 0.0563 - val_acc: 0.5694\n",
      "Epoch 126/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0553 - acc: 0.5831 - val_loss: 0.0566 - val_acc: 0.5669\n",
      "Epoch 127/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0554 - acc: 0.5812 - val_loss: 0.0563 - val_acc: 0.5636\n",
      "Epoch 128/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0552 - acc: 0.5823 - val_loss: 0.0565 - val_acc: 0.5677\n",
      "Epoch 129/500\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0550 - acc: 0.5851 - val_loss: 0.0569 - val_acc: 0.5677\n",
      "Epoch 130/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0555 - acc: 0.5790 - val_loss: 0.0563 - val_acc: 0.5686\n",
      "Epoch 131/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0551 - acc: 0.5868 - val_loss: 0.0568 - val_acc: 0.5686\n",
      "Epoch 132/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0552 - acc: 0.5843 - val_loss: 0.0583 - val_acc: 0.5520\n",
      "Epoch 133/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0555 - acc: 0.5818 - val_loss: 0.0565 - val_acc: 0.5644\n",
      "Epoch 134/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0551 - acc: 0.5865 - val_loss: 0.0586 - val_acc: 0.5436\n",
      "Epoch 135/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0554 - acc: 0.5831 - val_loss: 0.0565 - val_acc: 0.5644\n",
      "Epoch 136/500\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0556 - acc: 0.5782 - val_loss: 0.0596 - val_acc: 0.5303\n",
      "Epoch 137/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0554 - acc: 0.5793 - val_loss: 0.0566 - val_acc: 0.5677\n",
      "Epoch 138/500\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0552 - acc: 0.5831 - val_loss: 0.0562 - val_acc: 0.5628\n",
      "Epoch 139/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0550 - acc: 0.5876 - val_loss: 0.0564 - val_acc: 0.5644\n",
      "Epoch 140/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0549 - acc: 0.5862 - val_loss: 0.0562 - val_acc: 0.5694\n",
      "Epoch 141/500\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0549 - acc: 0.5868 - val_loss: 0.0565 - val_acc: 0.5653\n",
      "Epoch 142/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0550 - acc: 0.5876 - val_loss: 0.0569 - val_acc: 0.5603\n",
      "Epoch 143/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0548 - acc: 0.5895 - val_loss: 0.0563 - val_acc: 0.5653\n",
      "Epoch 144/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0550 - acc: 0.5845 - val_loss: 0.0568 - val_acc: 0.5661\n",
      "Epoch 145/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0547 - acc: 0.5887 - val_loss: 0.0571 - val_acc: 0.5661\n",
      "Epoch 146/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0550 - acc: 0.5848 - val_loss: 0.0576 - val_acc: 0.5569\n",
      "Epoch 147/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0548 - acc: 0.5879 - val_loss: 0.0566 - val_acc: 0.5644\n",
      "Epoch 148/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0548 - acc: 0.5865 - val_loss: 0.0562 - val_acc: 0.5644\n",
      "Epoch 149/500\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0547 - acc: 0.5887 - val_loss: 0.0567 - val_acc: 0.5653\n",
      "Epoch 150/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0550 - acc: 0.5865 - val_loss: 0.0596 - val_acc: 0.5345\n",
      "Epoch 151/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0550 - acc: 0.5848 - val_loss: 0.0566 - val_acc: 0.5611\n",
      "Epoch 152/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0549 - acc: 0.5856 - val_loss: 0.0565 - val_acc: 0.5661\n",
      "Epoch 153/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0546 - acc: 0.5862 - val_loss: 0.0565 - val_acc: 0.5653\n",
      "Epoch 154/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0547 - acc: 0.5848 - val_loss: 0.0565 - val_acc: 0.5603\n",
      "Epoch 155/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0551 - acc: 0.5826 - val_loss: 0.0584 - val_acc: 0.5578\n",
      "Epoch 156/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0548 - acc: 0.5890 - val_loss: 0.0564 - val_acc: 0.5636\n",
      "Epoch 157/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0553 - acc: 0.5840 - val_loss: 0.0568 - val_acc: 0.5619\n",
      "Epoch 158/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0548 - acc: 0.5856 - val_loss: 0.0567 - val_acc: 0.5628\n",
      "Epoch 159/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0550 - acc: 0.5887 - val_loss: 0.0569 - val_acc: 0.5603\n",
      "Epoch 160/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0548 - acc: 0.5851 - val_loss: 0.0571 - val_acc: 0.5636\n",
      "Epoch 161/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0548 - acc: 0.5876 - val_loss: 0.0563 - val_acc: 0.5619\n",
      "Epoch 162/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0548 - acc: 0.5884 - val_loss: 0.0568 - val_acc: 0.5569\n",
      "Epoch 163/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0545 - acc: 0.5879 - val_loss: 0.0576 - val_acc: 0.5611\n",
      "Epoch 164/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0547 - acc: 0.5851 - val_loss: 0.0567 - val_acc: 0.5611\n",
      "Epoch 165/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0545 - acc: 0.5892 - val_loss: 0.0567 - val_acc: 0.5611\n",
      "Epoch 166/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0546 - acc: 0.5854 - val_loss: 0.0564 - val_acc: 0.5661\n",
      "Epoch 167/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0546 - acc: 0.5884 - val_loss: 0.0564 - val_acc: 0.5603\n",
      "Epoch 168/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0543 - acc: 0.5898 - val_loss: 0.0566 - val_acc: 0.5653\n",
      "Epoch 169/500\n",
      "3608/3608 [==============================] - 1s 303us/step - loss: 0.0548 - acc: 0.5876 - val_loss: 0.0564 - val_acc: 0.5644\n",
      "Epoch 170/500\n",
      "3608/3608 [==============================] - 1s 348us/step - loss: 0.0545 - acc: 0.5906 - val_loss: 0.0572 - val_acc: 0.5653\n",
      "Epoch 171/500\n",
      "3608/3608 [==============================] - 1s 351us/step - loss: 0.0546 - acc: 0.5868 - val_loss: 0.0578 - val_acc: 0.5603\n",
      "Epoch 172/500\n",
      "3608/3608 [==============================] - 1s 350us/step - loss: 0.0545 - acc: 0.5856 - val_loss: 0.0573 - val_acc: 0.5553\n",
      "Epoch 173/500\n",
      "3608/3608 [==============================] - 1s 350us/step - loss: 0.0543 - acc: 0.5901 - val_loss: 0.0568 - val_acc: 0.5611\n",
      "Epoch 174/500\n",
      "3608/3608 [==============================] - 1s 346us/step - loss: 0.0543 - acc: 0.5890 - val_loss: 0.0567 - val_acc: 0.5636\n",
      "Epoch 175/500\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0543 - acc: 0.5901 - val_loss: 0.0564 - val_acc: 0.5677\n",
      "Epoch 176/500\n",
      "3608/3608 [==============================] - 1s 350us/step - loss: 0.0549 - acc: 0.5843 - val_loss: 0.0566 - val_acc: 0.5677\n",
      "Epoch 177/500\n",
      "3608/3608 [==============================] - 1s 336us/step - loss: 0.0543 - acc: 0.5920 - val_loss: 0.0563 - val_acc: 0.5628\n",
      "Epoch 178/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0543 - acc: 0.5898 - val_loss: 0.0566 - val_acc: 0.5628\n",
      "Epoch 179/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0543 - acc: 0.5898 - val_loss: 0.0564 - val_acc: 0.5628\n",
      "Epoch 180/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0543 - acc: 0.5873 - val_loss: 0.0570 - val_acc: 0.5661\n",
      "Epoch 181/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0545 - acc: 0.5901 - val_loss: 0.0564 - val_acc: 0.5686\n",
      "Epoch 182/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0543 - acc: 0.5898 - val_loss: 0.0567 - val_acc: 0.5694\n",
      "Epoch 183/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0544 - acc: 0.5917 - val_loss: 0.0566 - val_acc: 0.5628\n",
      "Epoch 184/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0544 - acc: 0.5931 - val_loss: 0.0565 - val_acc: 0.5686\n",
      "Epoch 185/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0542 - acc: 0.5920 - val_loss: 0.0566 - val_acc: 0.5644\n",
      "Epoch 186/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0543 - acc: 0.5940 - val_loss: 0.0566 - val_acc: 0.5661\n",
      "Epoch 187/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0542 - acc: 0.5912 - val_loss: 0.0572 - val_acc: 0.5603\n",
      "Epoch 188/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0540 - acc: 0.5912 - val_loss: 0.0571 - val_acc: 0.5611\n",
      "Epoch 189/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0541 - acc: 0.5923 - val_loss: 0.0565 - val_acc: 0.5653\n",
      "Epoch 190/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0541 - acc: 0.5928 - val_loss: 0.0570 - val_acc: 0.5619\n",
      "Epoch 191/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0542 - acc: 0.5920 - val_loss: 0.0567 - val_acc: 0.5636\n",
      "Epoch 192/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0541 - acc: 0.5926 - val_loss: 0.0575 - val_acc: 0.5594\n",
      "Epoch 193/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0539 - acc: 0.5937 - val_loss: 0.0639 - val_acc: 0.4688\n",
      "Epoch 194/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0545 - acc: 0.5892 - val_loss: 0.0565 - val_acc: 0.5677\n",
      "Epoch 195/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0540 - acc: 0.5940 - val_loss: 0.0571 - val_acc: 0.5603\n",
      "Epoch 196/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0544 - acc: 0.5904 - val_loss: 0.0565 - val_acc: 0.5628\n",
      "Epoch 197/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0541 - acc: 0.5942 - val_loss: 0.0565 - val_acc: 0.5661\n",
      "Epoch 198/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0543 - acc: 0.5892 - val_loss: 0.0572 - val_acc: 0.5636\n",
      "Epoch 199/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0541 - acc: 0.5917 - val_loss: 0.0565 - val_acc: 0.5686\n",
      "Epoch 200/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0542 - acc: 0.5909 - val_loss: 0.0574 - val_acc: 0.5594\n",
      "Epoch 201/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0540 - acc: 0.5926 - val_loss: 0.0574 - val_acc: 0.5644\n",
      "Epoch 202/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0543 - acc: 0.5917 - val_loss: 0.0565 - val_acc: 0.5686\n",
      "Epoch 203/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0541 - acc: 0.5873 - val_loss: 0.0571 - val_acc: 0.5653\n",
      "Epoch 204/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0539 - acc: 0.5948 - val_loss: 0.0566 - val_acc: 0.5653\n",
      "Epoch 205/500\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0540 - acc: 0.5873 - val_loss: 0.0569 - val_acc: 0.5561\n",
      "Epoch 206/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0541 - acc: 0.5942 - val_loss: 0.0583 - val_acc: 0.5511\n",
      "Epoch 207/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0538 - acc: 0.5970 - val_loss: 0.0565 - val_acc: 0.5669\n",
      "Epoch 208/500\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0540 - acc: 0.5906 - val_loss: 0.0574 - val_acc: 0.5628\n",
      "Epoch 209/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0539 - acc: 0.5917 - val_loss: 0.0572 - val_acc: 0.5594\n",
      "Epoch 210/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0541 - acc: 0.5931 - val_loss: 0.0582 - val_acc: 0.5520\n",
      "Epoch 211/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0541 - acc: 0.5890 - val_loss: 0.0576 - val_acc: 0.5619\n",
      "Epoch 212/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0540 - acc: 0.5931 - val_loss: 0.0569 - val_acc: 0.5603\n",
      "Epoch 213/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0539 - acc: 0.5931 - val_loss: 0.0571 - val_acc: 0.5569\n",
      "Epoch 214/500\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0540 - acc: 0.5978 - val_loss: 0.0573 - val_acc: 0.5586\n",
      "Epoch 215/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0539 - acc: 0.5934 - val_loss: 0.0569 - val_acc: 0.5653\n",
      "Epoch 216/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0539 - acc: 0.5948 - val_loss: 0.0569 - val_acc: 0.5628\n",
      "Epoch 217/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0536 - acc: 0.5959 - val_loss: 0.0568 - val_acc: 0.5611\n",
      "Epoch 218/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0537 - acc: 0.5981 - val_loss: 0.0570 - val_acc: 0.5653\n",
      "Epoch 219/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0537 - acc: 0.5970 - val_loss: 0.0576 - val_acc: 0.5561\n",
      "Epoch 220/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0540 - acc: 0.5920 - val_loss: 0.0573 - val_acc: 0.5628\n",
      "Epoch 221/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0538 - acc: 0.5937 - val_loss: 0.0569 - val_acc: 0.5611\n",
      "Epoch 222/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0541 - acc: 0.5915 - val_loss: 0.0565 - val_acc: 0.5677\n",
      "Epoch 223/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0538 - acc: 0.5923 - val_loss: 0.0566 - val_acc: 0.5636\n",
      "Epoch 224/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0536 - acc: 0.5976 - val_loss: 0.0564 - val_acc: 0.5702\n",
      "Epoch 225/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0540 - acc: 0.5953 - val_loss: 0.0566 - val_acc: 0.5661\n",
      "Epoch 226/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0538 - acc: 0.5937 - val_loss: 0.0571 - val_acc: 0.5611\n",
      "Epoch 227/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0537 - acc: 0.5948 - val_loss: 0.0565 - val_acc: 0.5661\n",
      "Epoch 228/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0537 - acc: 0.5970 - val_loss: 0.0570 - val_acc: 0.5594\n",
      "Epoch 229/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0536 - acc: 0.5953 - val_loss: 0.0568 - val_acc: 0.5586\n",
      "Epoch 230/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0536 - acc: 0.5909 - val_loss: 0.0567 - val_acc: 0.5644\n",
      "Epoch 231/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0539 - acc: 0.5987 - val_loss: 0.0570 - val_acc: 0.5578\n",
      "Epoch 232/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0537 - acc: 0.5945 - val_loss: 0.0570 - val_acc: 0.5586\n",
      "Epoch 233/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0536 - acc: 0.5965 - val_loss: 0.0568 - val_acc: 0.5586\n",
      "Epoch 234/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0540 - acc: 0.5909 - val_loss: 0.0568 - val_acc: 0.5653\n",
      "Epoch 235/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0538 - acc: 0.5987 - val_loss: 0.0567 - val_acc: 0.5686\n",
      "Epoch 236/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0538 - acc: 0.5945 - val_loss: 0.0568 - val_acc: 0.5661\n",
      "Epoch 237/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0536 - acc: 0.5959 - val_loss: 0.0569 - val_acc: 0.5636\n",
      "Epoch 238/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0534 - acc: 0.5992 - val_loss: 0.0572 - val_acc: 0.5644\n",
      "Epoch 239/500\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0536 - acc: 0.5945 - val_loss: 0.0566 - val_acc: 0.5694\n",
      "Epoch 240/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0537 - acc: 0.5945 - val_loss: 0.0575 - val_acc: 0.5619\n",
      "Epoch 241/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0534 - acc: 0.5984 - val_loss: 0.0576 - val_acc: 0.5628\n",
      "Epoch 242/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0536 - acc: 0.5945 - val_loss: 0.0567 - val_acc: 0.5694\n",
      "Epoch 243/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0537 - acc: 0.5984 - val_loss: 0.0566 - val_acc: 0.5702\n",
      "Epoch 244/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0536 - acc: 0.5934 - val_loss: 0.0574 - val_acc: 0.5594\n",
      "Epoch 245/500\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0534 - acc: 0.6012 - val_loss: 0.0572 - val_acc: 0.5611\n",
      "Epoch 246/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0535 - acc: 0.5984 - val_loss: 0.0570 - val_acc: 0.5594\n",
      "Epoch 247/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0533 - acc: 0.5992 - val_loss: 0.0572 - val_acc: 0.5661\n",
      "Epoch 248/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0535 - acc: 0.5965 - val_loss: 0.0571 - val_acc: 0.5611\n",
      "Epoch 249/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0535 - acc: 0.5951 - val_loss: 0.0570 - val_acc: 0.5586\n",
      "Epoch 250/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0535 - acc: 0.6014 - val_loss: 0.0576 - val_acc: 0.5611\n",
      "Epoch 251/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0535 - acc: 0.5962 - val_loss: 0.0572 - val_acc: 0.5636\n",
      "Epoch 252/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0533 - acc: 0.5987 - val_loss: 0.0580 - val_acc: 0.5544\n",
      "Epoch 253/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0534 - acc: 0.5995 - val_loss: 0.0567 - val_acc: 0.5719\n",
      "Epoch 254/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0535 - acc: 0.5956 - val_loss: 0.0571 - val_acc: 0.5628\n",
      "Epoch 255/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0534 - acc: 0.5965 - val_loss: 0.0575 - val_acc: 0.5569\n",
      "Epoch 256/500\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0532 - acc: 0.5987 - val_loss: 0.0564 - val_acc: 0.5644\n",
      "Epoch 257/500\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0535 - acc: 0.5948 - val_loss: 0.0569 - val_acc: 0.5636\n",
      "Epoch 258/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0536 - acc: 0.5948 - val_loss: 0.0573 - val_acc: 0.5686\n",
      "Epoch 259/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0533 - acc: 0.5928 - val_loss: 0.0570 - val_acc: 0.5661\n",
      "Epoch 260/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0532 - acc: 0.5995 - val_loss: 0.0569 - val_acc: 0.5636\n",
      "Epoch 261/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0533 - acc: 0.5989 - val_loss: 0.0576 - val_acc: 0.5594\n",
      "Epoch 262/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0532 - acc: 0.5989 - val_loss: 0.0576 - val_acc: 0.5553\n",
      "Epoch 263/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0534 - acc: 0.5976 - val_loss: 0.0577 - val_acc: 0.5561\n",
      "Epoch 264/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0534 - acc: 0.5970 - val_loss: 0.0582 - val_acc: 0.5503\n",
      "Epoch 265/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0532 - acc: 0.5959 - val_loss: 0.0594 - val_acc: 0.5320\n",
      "Epoch 266/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0535 - acc: 0.5948 - val_loss: 0.0572 - val_acc: 0.5628\n",
      "Epoch 267/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0533 - acc: 0.5951 - val_loss: 0.0577 - val_acc: 0.5578\n",
      "Epoch 268/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0533 - acc: 0.5995 - val_loss: 0.0571 - val_acc: 0.5669\n",
      "Epoch 269/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0533 - acc: 0.6025 - val_loss: 0.0567 - val_acc: 0.5686\n",
      "Epoch 270/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0530 - acc: 0.6037 - val_loss: 0.0574 - val_acc: 0.5611\n",
      "Epoch 271/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0533 - acc: 0.5981 - val_loss: 0.0578 - val_acc: 0.5603\n",
      "Epoch 272/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0531 - acc: 0.6020 - val_loss: 0.0577 - val_acc: 0.5603\n",
      "Epoch 273/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0532 - acc: 0.6003 - val_loss: 0.0572 - val_acc: 0.5644\n",
      "Epoch 274/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0530 - acc: 0.6031 - val_loss: 0.0572 - val_acc: 0.5569\n",
      "Epoch 275/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0531 - acc: 0.6034 - val_loss: 0.0577 - val_acc: 0.5653\n",
      "Epoch 276/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0535 - acc: 0.5965 - val_loss: 0.0571 - val_acc: 0.5653\n",
      "Epoch 277/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0532 - acc: 0.5962 - val_loss: 0.0579 - val_acc: 0.5544\n",
      "Epoch 278/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0531 - acc: 0.6014 - val_loss: 0.0582 - val_acc: 0.5511\n",
      "Epoch 279/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0530 - acc: 0.6031 - val_loss: 0.0568 - val_acc: 0.5628\n",
      "Epoch 280/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0530 - acc: 0.6042 - val_loss: 0.0572 - val_acc: 0.5644\n",
      "Epoch 281/500\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0529 - acc: 0.6001 - val_loss: 0.0573 - val_acc: 0.5636\n",
      "Epoch 282/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0530 - acc: 0.6014 - val_loss: 0.0582 - val_acc: 0.5594\n",
      "Epoch 283/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0530 - acc: 0.5998 - val_loss: 0.0571 - val_acc: 0.5603\n",
      "Epoch 284/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0530 - acc: 0.5970 - val_loss: 0.0570 - val_acc: 0.5603\n",
      "Epoch 285/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0534 - acc: 0.5978 - val_loss: 0.0579 - val_acc: 0.5603\n",
      "Epoch 286/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0532 - acc: 0.5992 - val_loss: 0.0576 - val_acc: 0.5619\n",
      "Epoch 287/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0532 - acc: 0.6001 - val_loss: 0.0572 - val_acc: 0.5628\n",
      "Epoch 288/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0531 - acc: 0.6001 - val_loss: 0.0585 - val_acc: 0.5495\n",
      "Epoch 289/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0531 - acc: 0.6048 - val_loss: 0.0579 - val_acc: 0.5603\n",
      "Epoch 290/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0529 - acc: 0.6034 - val_loss: 0.0570 - val_acc: 0.5603\n",
      "Epoch 291/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0528 - acc: 0.6034 - val_loss: 0.0574 - val_acc: 0.5677\n",
      "Epoch 292/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0529 - acc: 0.6031 - val_loss: 0.0575 - val_acc: 0.5569\n",
      "Epoch 293/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0531 - acc: 0.6006 - val_loss: 0.0576 - val_acc: 0.5603\n",
      "Epoch 294/500\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0530 - acc: 0.6078 - val_loss: 0.0573 - val_acc: 0.5594\n",
      "Epoch 295/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0531 - acc: 0.5978 - val_loss: 0.0573 - val_acc: 0.5569\n",
      "Epoch 296/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0530 - acc: 0.6020 - val_loss: 0.0582 - val_acc: 0.5569\n",
      "Epoch 297/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0528 - acc: 0.6023 - val_loss: 0.0574 - val_acc: 0.5644\n",
      "Epoch 298/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0528 - acc: 0.5998 - val_loss: 0.0580 - val_acc: 0.5569\n",
      "Epoch 299/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0531 - acc: 0.6056 - val_loss: 0.0573 - val_acc: 0.5603\n",
      "Epoch 300/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0530 - acc: 0.6017 - val_loss: 0.0595 - val_acc: 0.5453\n",
      "Epoch 301/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0530 - acc: 0.5995 - val_loss: 0.0575 - val_acc: 0.5603\n",
      "Epoch 302/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0530 - acc: 0.6012 - val_loss: 0.0576 - val_acc: 0.5594\n",
      "Epoch 303/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0530 - acc: 0.6003 - val_loss: 0.0581 - val_acc: 0.5536\n",
      "Epoch 304/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0527 - acc: 0.6039 - val_loss: 0.0577 - val_acc: 0.5569\n",
      "Epoch 305/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0530 - acc: 0.6009 - val_loss: 0.0577 - val_acc: 0.5561\n",
      "Epoch 306/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0526 - acc: 0.6028 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 307/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0528 - acc: 0.6039 - val_loss: 0.0573 - val_acc: 0.5611\n",
      "Epoch 308/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0528 - acc: 0.6001 - val_loss: 0.0573 - val_acc: 0.5653\n",
      "Epoch 309/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0528 - acc: 0.6025 - val_loss: 0.0574 - val_acc: 0.5636\n",
      "Epoch 310/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0527 - acc: 0.5978 - val_loss: 0.0578 - val_acc: 0.5586\n",
      "Epoch 311/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0527 - acc: 0.6025 - val_loss: 0.0574 - val_acc: 0.5636\n",
      "Epoch 312/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0528 - acc: 0.6001 - val_loss: 0.0572 - val_acc: 0.5578\n",
      "Epoch 313/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0528 - acc: 0.6048 - val_loss: 0.0579 - val_acc: 0.5569\n",
      "Epoch 314/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0528 - acc: 0.6034 - val_loss: 0.0572 - val_acc: 0.5661\n",
      "Epoch 315/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0525 - acc: 0.6062 - val_loss: 0.0576 - val_acc: 0.5628\n",
      "Epoch 316/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0532 - acc: 0.5970 - val_loss: 0.0576 - val_acc: 0.5553\n",
      "Epoch 317/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0532 - acc: 0.5981 - val_loss: 0.0572 - val_acc: 0.5644\n",
      "Epoch 318/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0526 - acc: 0.6031 - val_loss: 0.0574 - val_acc: 0.5544\n",
      "Epoch 319/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0527 - acc: 0.6053 - val_loss: 0.0590 - val_acc: 0.5478\n",
      "Epoch 320/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0528 - acc: 0.6003 - val_loss: 0.0575 - val_acc: 0.5603\n",
      "Epoch 321/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0527 - acc: 0.6050 - val_loss: 0.0576 - val_acc: 0.5586\n",
      "Epoch 322/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0526 - acc: 0.6037 - val_loss: 0.0583 - val_acc: 0.5661\n",
      "Epoch 323/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0526 - acc: 0.6012 - val_loss: 0.0576 - val_acc: 0.5603\n",
      "Epoch 324/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0528 - acc: 0.5992 - val_loss: 0.0579 - val_acc: 0.5578\n",
      "Epoch 325/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0528 - acc: 0.6042 - val_loss: 0.0575 - val_acc: 0.5644\n",
      "Epoch 326/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0527 - acc: 0.6039 - val_loss: 0.0574 - val_acc: 0.5636\n",
      "Epoch 327/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0525 - acc: 0.6067 - val_loss: 0.0575 - val_acc: 0.5603\n",
      "Epoch 328/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0526 - acc: 0.6003 - val_loss: 0.0588 - val_acc: 0.5544\n",
      "Epoch 329/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0526 - acc: 0.6056 - val_loss: 0.0581 - val_acc: 0.5586\n",
      "Epoch 330/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0525 - acc: 0.6059 - val_loss: 0.0574 - val_acc: 0.5619\n",
      "Epoch 331/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0526 - acc: 0.6045 - val_loss: 0.0589 - val_acc: 0.5520\n",
      "Epoch 332/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0528 - acc: 0.6064 - val_loss: 0.0574 - val_acc: 0.5653\n",
      "Epoch 333/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0526 - acc: 0.6025 - val_loss: 0.0575 - val_acc: 0.5611\n",
      "Epoch 334/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0525 - acc: 0.6067 - val_loss: 0.0574 - val_acc: 0.5661\n",
      "Epoch 335/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0524 - acc: 0.6078 - val_loss: 0.0576 - val_acc: 0.5611\n",
      "Epoch 336/500\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0527 - acc: 0.5987 - val_loss: 0.0587 - val_acc: 0.5503\n",
      "Epoch 337/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0524 - acc: 0.6073 - val_loss: 0.0581 - val_acc: 0.5586\n",
      "Epoch 338/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0525 - acc: 0.6086 - val_loss: 0.0576 - val_acc: 0.5569\n",
      "Epoch 339/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0528 - acc: 0.6062 - val_loss: 0.0574 - val_acc: 0.5644\n",
      "Epoch 340/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0528 - acc: 0.6020 - val_loss: 0.0582 - val_acc: 0.5569\n",
      "Epoch 341/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0523 - acc: 0.6070 - val_loss: 0.0582 - val_acc: 0.5619\n",
      "Epoch 342/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0523 - acc: 0.6067 - val_loss: 0.0578 - val_acc: 0.5619\n",
      "Epoch 343/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0526 - acc: 0.5987 - val_loss: 0.0577 - val_acc: 0.5653\n",
      "Epoch 344/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0525 - acc: 0.6045 - val_loss: 0.0583 - val_acc: 0.5578\n",
      "Epoch 345/500\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0523 - acc: 0.6070 - val_loss: 0.0585 - val_acc: 0.5561\n",
      "Epoch 346/500\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0525 - acc: 0.6095 - val_loss: 0.0579 - val_acc: 0.5619\n",
      "Epoch 347/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0525 - acc: 0.6037 - val_loss: 0.0580 - val_acc: 0.5594\n",
      "Epoch 348/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0525 - acc: 0.6039 - val_loss: 0.0581 - val_acc: 0.5644\n",
      "Epoch 349/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0523 - acc: 0.6103 - val_loss: 0.0582 - val_acc: 0.5586\n",
      "Epoch 350/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0525 - acc: 0.6023 - val_loss: 0.0575 - val_acc: 0.5603\n",
      "Epoch 351/500\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0525 - acc: 0.6067 - val_loss: 0.0577 - val_acc: 0.5561\n",
      "Epoch 352/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0526 - acc: 0.6075 - val_loss: 0.0580 - val_acc: 0.5653\n",
      "Epoch 353/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0523 - acc: 0.6086 - val_loss: 0.0578 - val_acc: 0.5611\n",
      "Epoch 354/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0529 - acc: 0.6050 - val_loss: 0.0578 - val_acc: 0.5669\n",
      "Epoch 355/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0523 - acc: 0.6081 - val_loss: 0.0577 - val_acc: 0.5644\n",
      "Epoch 356/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0526 - acc: 0.6039 - val_loss: 0.0577 - val_acc: 0.5636\n",
      "Epoch 357/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0523 - acc: 0.6056 - val_loss: 0.0581 - val_acc: 0.5611\n",
      "Epoch 358/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0525 - acc: 0.6059 - val_loss: 0.0586 - val_acc: 0.5594\n",
      "Epoch 359/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0523 - acc: 0.6086 - val_loss: 0.0588 - val_acc: 0.5536\n",
      "Epoch 360/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0523 - acc: 0.6092 - val_loss: 0.0581 - val_acc: 0.5653\n",
      "Epoch 361/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0524 - acc: 0.6062 - val_loss: 0.0585 - val_acc: 0.5611\n",
      "Epoch 362/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0521 - acc: 0.6056 - val_loss: 0.0577 - val_acc: 0.5644\n",
      "Epoch 363/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0525 - acc: 0.6028 - val_loss: 0.0590 - val_acc: 0.5453\n",
      "Epoch 364/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0520 - acc: 0.6114 - val_loss: 0.0579 - val_acc: 0.5677\n",
      "Epoch 365/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0526 - acc: 0.6031 - val_loss: 0.0578 - val_acc: 0.5653\n",
      "Epoch 366/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0522 - acc: 0.6084 - val_loss: 0.0580 - val_acc: 0.5603\n",
      "Epoch 367/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0522 - acc: 0.6048 - val_loss: 0.0580 - val_acc: 0.5677\n",
      "Epoch 368/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0523 - acc: 0.6009 - val_loss: 0.0576 - val_acc: 0.5644\n",
      "Epoch 369/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0522 - acc: 0.6092 - val_loss: 0.0576 - val_acc: 0.5619\n",
      "Epoch 370/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0521 - acc: 0.6114 - val_loss: 0.0582 - val_acc: 0.5603\n",
      "Epoch 371/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0522 - acc: 0.6075 - val_loss: 0.0577 - val_acc: 0.5561\n",
      "Epoch 372/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0521 - acc: 0.6128 - val_loss: 0.0600 - val_acc: 0.5436\n",
      "Epoch 373/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0522 - acc: 0.6111 - val_loss: 0.0583 - val_acc: 0.5569\n",
      "Epoch 374/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0523 - acc: 0.6050 - val_loss: 0.0578 - val_acc: 0.5578\n",
      "Epoch 375/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0521 - acc: 0.6117 - val_loss: 0.0579 - val_acc: 0.5653\n",
      "Epoch 376/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0521 - acc: 0.6100 - val_loss: 0.0583 - val_acc: 0.5561\n",
      "Epoch 377/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0520 - acc: 0.6153 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 378/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0521 - acc: 0.6073 - val_loss: 0.0578 - val_acc: 0.5578\n",
      "Epoch 379/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0523 - acc: 0.6073 - val_loss: 0.0585 - val_acc: 0.5569\n",
      "Epoch 380/500\n",
      "3608/3608 [==============================] - 1s 327us/step - loss: 0.0523 - acc: 0.6081 - val_loss: 0.0582 - val_acc: 0.5619\n",
      "Epoch 381/500\n",
      "3608/3608 [==============================] - 1s 355us/step - loss: 0.0522 - acc: 0.6075 - val_loss: 0.0580 - val_acc: 0.5561\n",
      "Epoch 382/500\n",
      "3608/3608 [==============================] - 1s 348us/step - loss: 0.0520 - acc: 0.6103 - val_loss: 0.0576 - val_acc: 0.5653\n",
      "Epoch 383/500\n",
      "3608/3608 [==============================] - 1s 350us/step - loss: 0.0522 - acc: 0.6084 - val_loss: 0.0579 - val_acc: 0.5636\n",
      "Epoch 384/500\n",
      "3608/3608 [==============================] - 1s 320us/step - loss: 0.0524 - acc: 0.6067 - val_loss: 0.0582 - val_acc: 0.5561\n",
      "Epoch 385/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0521 - acc: 0.6098 - val_loss: 0.0582 - val_acc: 0.5636\n",
      "Epoch 386/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0522 - acc: 0.6098 - val_loss: 0.0578 - val_acc: 0.5611\n",
      "Epoch 387/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0520 - acc: 0.6114 - val_loss: 0.0600 - val_acc: 0.5403\n",
      "Epoch 388/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0521 - acc: 0.6123 - val_loss: 0.0581 - val_acc: 0.5544\n",
      "Epoch 389/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0521 - acc: 0.6103 - val_loss: 0.0578 - val_acc: 0.5653\n",
      "Epoch 390/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0519 - acc: 0.6150 - val_loss: 0.0580 - val_acc: 0.5628\n",
      "Epoch 391/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0520 - acc: 0.6100 - val_loss: 0.0594 - val_acc: 0.5503\n",
      "Epoch 392/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0521 - acc: 0.6125 - val_loss: 0.0586 - val_acc: 0.5495\n",
      "Epoch 393/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0519 - acc: 0.6120 - val_loss: 0.0589 - val_acc: 0.5569\n",
      "Epoch 394/500\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0520 - acc: 0.6123 - val_loss: 0.0578 - val_acc: 0.5644\n",
      "Epoch 395/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0518 - acc: 0.6131 - val_loss: 0.0576 - val_acc: 0.5594\n",
      "Epoch 396/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0518 - acc: 0.6100 - val_loss: 0.0585 - val_acc: 0.5611\n",
      "Epoch 397/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0521 - acc: 0.6081 - val_loss: 0.0590 - val_acc: 0.5528\n",
      "Epoch 398/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0520 - acc: 0.6075 - val_loss: 0.0585 - val_acc: 0.5594\n",
      "Epoch 399/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0521 - acc: 0.6120 - val_loss: 0.0589 - val_acc: 0.5553\n",
      "Epoch 400/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0522 - acc: 0.6086 - val_loss: 0.0580 - val_acc: 0.5677\n",
      "Epoch 401/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0517 - acc: 0.6092 - val_loss: 0.0587 - val_acc: 0.5586\n",
      "Epoch 402/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0520 - acc: 0.6084 - val_loss: 0.0583 - val_acc: 0.5636\n",
      "Epoch 403/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0520 - acc: 0.6084 - val_loss: 0.0617 - val_acc: 0.5137\n",
      "Epoch 404/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0521 - acc: 0.6125 - val_loss: 0.0594 - val_acc: 0.5528\n",
      "Epoch 405/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0521 - acc: 0.6086 - val_loss: 0.0580 - val_acc: 0.5611\n",
      "Epoch 406/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0517 - acc: 0.6095 - val_loss: 0.0586 - val_acc: 0.5628\n",
      "Epoch 407/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0519 - acc: 0.6095 - val_loss: 0.0582 - val_acc: 0.5644\n",
      "Epoch 408/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0521 - acc: 0.6106 - val_loss: 0.0584 - val_acc: 0.5636\n",
      "Epoch 409/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0522 - acc: 0.6056 - val_loss: 0.0597 - val_acc: 0.5470\n",
      "Epoch 410/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0518 - acc: 0.6120 - val_loss: 0.0586 - val_acc: 0.5644\n",
      "Epoch 411/500\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0517 - acc: 0.6150 - val_loss: 0.0584 - val_acc: 0.5636\n",
      "Epoch 412/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0519 - acc: 0.6117 - val_loss: 0.0586 - val_acc: 0.5619\n",
      "Epoch 413/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0519 - acc: 0.6100 - val_loss: 0.0586 - val_acc: 0.5569\n",
      "Epoch 414/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0518 - acc: 0.6156 - val_loss: 0.0586 - val_acc: 0.5569\n",
      "Epoch 415/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0518 - acc: 0.6170 - val_loss: 0.0579 - val_acc: 0.5644\n",
      "Epoch 416/500\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0518 - acc: 0.6147 - val_loss: 0.0585 - val_acc: 0.5628\n",
      "Epoch 417/500\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0520 - acc: 0.6153 - val_loss: 0.0585 - val_acc: 0.5628\n",
      "Epoch 418/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0520 - acc: 0.6123 - val_loss: 0.0581 - val_acc: 0.5644\n",
      "Epoch 419/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0519 - acc: 0.6131 - val_loss: 0.0587 - val_acc: 0.5594\n",
      "Epoch 420/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0520 - acc: 0.6109 - val_loss: 0.0581 - val_acc: 0.5653\n",
      "Epoch 421/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0520 - acc: 0.6098 - val_loss: 0.0612 - val_acc: 0.5270\n",
      "Epoch 422/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0522 - acc: 0.6084 - val_loss: 0.0583 - val_acc: 0.5511\n",
      "Epoch 423/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0522 - acc: 0.6095 - val_loss: 0.0580 - val_acc: 0.5686\n",
      "Epoch 424/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0518 - acc: 0.6136 - val_loss: 0.0581 - val_acc: 0.5661\n",
      "Epoch 425/500\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0516 - acc: 0.6189 - val_loss: 0.0584 - val_acc: 0.5611\n",
      "Epoch 426/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0517 - acc: 0.6125 - val_loss: 0.0585 - val_acc: 0.5561\n",
      "Epoch 427/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0515 - acc: 0.6161 - val_loss: 0.0582 - val_acc: 0.5561\n",
      "Epoch 428/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0517 - acc: 0.6145 - val_loss: 0.0596 - val_acc: 0.5511\n",
      "Epoch 429/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0515 - acc: 0.6100 - val_loss: 0.0581 - val_acc: 0.5611\n",
      "Epoch 430/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0520 - acc: 0.6125 - val_loss: 0.0587 - val_acc: 0.5528\n",
      "Epoch 431/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0517 - acc: 0.6159 - val_loss: 0.0585 - val_acc: 0.5619\n",
      "Epoch 432/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0516 - acc: 0.6183 - val_loss: 0.0586 - val_acc: 0.5653\n",
      "Epoch 433/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0519 - acc: 0.6131 - val_loss: 0.0587 - val_acc: 0.5569\n",
      "Epoch 434/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0517 - acc: 0.6117 - val_loss: 0.0583 - val_acc: 0.5677\n",
      "Epoch 435/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0516 - acc: 0.6200 - val_loss: 0.0580 - val_acc: 0.5586\n",
      "Epoch 436/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0516 - acc: 0.6117 - val_loss: 0.0588 - val_acc: 0.5578\n",
      "Epoch 437/500\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0516 - acc: 0.6181 - val_loss: 0.0589 - val_acc: 0.5561\n",
      "Epoch 438/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0519 - acc: 0.6106 - val_loss: 0.0585 - val_acc: 0.5661\n",
      "Epoch 439/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0517 - acc: 0.6095 - val_loss: 0.0585 - val_acc: 0.5544\n",
      "Epoch 440/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0519 - acc: 0.6131 - val_loss: 0.0587 - val_acc: 0.5636\n",
      "Epoch 441/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0516 - acc: 0.6206 - val_loss: 0.0582 - val_acc: 0.5603\n",
      "Epoch 442/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0515 - acc: 0.6150 - val_loss: 0.0586 - val_acc: 0.5528\n",
      "Epoch 443/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0517 - acc: 0.6142 - val_loss: 0.0591 - val_acc: 0.5553\n",
      "Epoch 444/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0516 - acc: 0.6117 - val_loss: 0.0592 - val_acc: 0.5594\n",
      "Epoch 445/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0514 - acc: 0.6153 - val_loss: 0.0589 - val_acc: 0.5594\n",
      "Epoch 446/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0515 - acc: 0.6178 - val_loss: 0.0582 - val_acc: 0.5694\n",
      "Epoch 447/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0518 - acc: 0.6120 - val_loss: 0.0590 - val_acc: 0.5528\n",
      "Epoch 448/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0516 - acc: 0.6161 - val_loss: 0.0590 - val_acc: 0.5586\n",
      "Epoch 449/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0516 - acc: 0.6159 - val_loss: 0.0594 - val_acc: 0.5486\n",
      "Epoch 450/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0514 - acc: 0.6117 - val_loss: 0.0585 - val_acc: 0.5569\n",
      "Epoch 451/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0520 - acc: 0.6123 - val_loss: 0.0600 - val_acc: 0.5436\n",
      "Epoch 452/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0516 - acc: 0.6106 - val_loss: 0.0583 - val_acc: 0.5594\n",
      "Epoch 453/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0516 - acc: 0.6147 - val_loss: 0.0593 - val_acc: 0.5594\n",
      "Epoch 454/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0517 - acc: 0.6147 - val_loss: 0.0589 - val_acc: 0.5586\n",
      "Epoch 455/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0516 - acc: 0.6136 - val_loss: 0.0584 - val_acc: 0.5636\n",
      "Epoch 456/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0518 - acc: 0.6109 - val_loss: 0.0582 - val_acc: 0.5603\n",
      "Epoch 457/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0513 - acc: 0.6170 - val_loss: 0.0600 - val_acc: 0.5436\n",
      "Epoch 458/500\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0517 - acc: 0.6159 - val_loss: 0.0589 - val_acc: 0.5578\n",
      "Epoch 459/500\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0515 - acc: 0.6156 - val_loss: 0.0592 - val_acc: 0.5528\n",
      "Epoch 460/500\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0513 - acc: 0.6139 - val_loss: 0.0583 - val_acc: 0.5636\n",
      "Epoch 461/500\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0515 - acc: 0.6136 - val_loss: 0.0588 - val_acc: 0.5544\n",
      "Epoch 462/500\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0514 - acc: 0.6206 - val_loss: 0.0586 - val_acc: 0.5653\n",
      "Epoch 463/500\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0513 - acc: 0.6214 - val_loss: 0.0602 - val_acc: 0.5453\n",
      "Epoch 464/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0514 - acc: 0.6197 - val_loss: 0.0582 - val_acc: 0.5594\n",
      "Epoch 465/500\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0515 - acc: 0.6183 - val_loss: 0.0583 - val_acc: 0.5544\n",
      "Epoch 466/500\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0514 - acc: 0.6175 - val_loss: 0.0587 - val_acc: 0.5578\n",
      "Epoch 467/500\n",
      "3608/3608 [==============================] - 1s 339us/step - loss: 0.0515 - acc: 0.6239 - val_loss: 0.0590 - val_acc: 0.5511\n",
      "Epoch 468/500\n",
      "3608/3608 [==============================] - 1s 346us/step - loss: 0.0513 - acc: 0.6211 - val_loss: 0.0596 - val_acc: 0.5536\n",
      "Epoch 469/500\n",
      "3608/3608 [==============================] - 1s 345us/step - loss: 0.0516 - acc: 0.6183 - val_loss: 0.0591 - val_acc: 0.5611\n",
      "Epoch 470/500\n",
      "3608/3608 [==============================] - 1s 349us/step - loss: 0.0512 - acc: 0.6159 - val_loss: 0.0586 - val_acc: 0.5628\n",
      "Epoch 471/500\n",
      "3608/3608 [==============================] - 1s 346us/step - loss: 0.0512 - acc: 0.6206 - val_loss: 0.0591 - val_acc: 0.5611\n",
      "Epoch 472/500\n",
      "3608/3608 [==============================] - 1s 346us/step - loss: 0.0512 - acc: 0.6159 - val_loss: 0.0590 - val_acc: 0.5544\n",
      "Epoch 473/500\n",
      "3608/3608 [==============================] - 1s 347us/step - loss: 0.0515 - acc: 0.6175 - val_loss: 0.0589 - val_acc: 0.5544\n",
      "Epoch 474/500\n",
      "3608/3608 [==============================] - 1s 349us/step - loss: 0.0517 - acc: 0.6156 - val_loss: 0.0591 - val_acc: 0.5520\n",
      "Epoch 475/500\n",
      "3608/3608 [==============================] - 1s 312us/step - loss: 0.0516 - acc: 0.6164 - val_loss: 0.0602 - val_acc: 0.5461\n",
      "Epoch 476/500\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0513 - acc: 0.6208 - val_loss: 0.0592 - val_acc: 0.5486\n",
      "Epoch 477/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0514 - acc: 0.6186 - val_loss: 0.0591 - val_acc: 0.5544\n",
      "Epoch 478/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0513 - acc: 0.6195 - val_loss: 0.0590 - val_acc: 0.5661\n",
      "Epoch 479/500\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0515 - acc: 0.6128 - val_loss: 0.0584 - val_acc: 0.5611\n",
      "Epoch 480/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0513 - acc: 0.6175 - val_loss: 0.0597 - val_acc: 0.5470\n",
      "Epoch 481/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0516 - acc: 0.6123 - val_loss: 0.0587 - val_acc: 0.5636\n",
      "Epoch 482/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0512 - acc: 0.6192 - val_loss: 0.0595 - val_acc: 0.5553\n",
      "Epoch 483/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0513 - acc: 0.6139 - val_loss: 0.0583 - val_acc: 0.5611\n",
      "Epoch 484/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0513 - acc: 0.6164 - val_loss: 0.0592 - val_acc: 0.5578\n",
      "Epoch 485/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0512 - acc: 0.6170 - val_loss: 0.0591 - val_acc: 0.5603\n",
      "Epoch 486/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0512 - acc: 0.6145 - val_loss: 0.0595 - val_acc: 0.5511\n",
      "Epoch 487/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0511 - acc: 0.6175 - val_loss: 0.0595 - val_acc: 0.5586\n",
      "Epoch 488/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0514 - acc: 0.6189 - val_loss: 0.0587 - val_acc: 0.5553\n",
      "Epoch 489/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0512 - acc: 0.6175 - val_loss: 0.0589 - val_acc: 0.5578\n",
      "Epoch 490/500\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0512 - acc: 0.6172 - val_loss: 0.0602 - val_acc: 0.5495\n",
      "Epoch 491/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0511 - acc: 0.6222 - val_loss: 0.0586 - val_acc: 0.5686\n",
      "Epoch 492/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0517 - acc: 0.6153 - val_loss: 0.0596 - val_acc: 0.5486\n",
      "Epoch 493/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0513 - acc: 0.6236 - val_loss: 0.0589 - val_acc: 0.5578\n",
      "Epoch 494/500\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0510 - acc: 0.6183 - val_loss: 0.0584 - val_acc: 0.5586\n",
      "Epoch 495/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0512 - acc: 0.6203 - val_loss: 0.0588 - val_acc: 0.5544\n",
      "Epoch 496/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0513 - acc: 0.6197 - val_loss: 0.0594 - val_acc: 0.5611\n",
      "Epoch 497/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0513 - acc: 0.6195 - val_loss: 0.0588 - val_acc: 0.5553\n",
      "Epoch 498/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0514 - acc: 0.6142 - val_loss: 0.0588 - val_acc: 0.5578\n",
      "Epoch 499/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0513 - acc: 0.6142 - val_loss: 0.0598 - val_acc: 0.5486\n",
      "Epoch 500/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0511 - acc: 0.6220 - val_loss: 0.0603 - val_acc: 0.5403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7810100d68>"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.summary()\n",
    "nn_model.add(Dense(units=32, activation='relu'))\n",
    "nn_model.add(Dense(units=16, activation='relu'))\n",
    "nn_model.add(Dense(units=8, activation='relu'))\n",
    "nn_model.add(Dense(units=4, activation='relu'))\n",
    "nn_model.add(Dense(units=2, activation='relu'))\n",
    "nn_model.add(Dense(units=4, activation='relu'))\n",
    "nn_model.add(Dense(units=8, activation='relu'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "nn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "nn_model.fit(x=train_x, y=train_y, epochs=500, validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zjfCta-Ky1R"
   },
   "source": [
    "Different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17391
    },
    "colab_type": "code",
    "id": "zyxXaJmK7ryI",
    "outputId": "5f5f8afb-74e5-4429-b711-9d59d96619b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 1,754\n",
      "Trainable params: 1,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3608 samples, validate on 1203 samples\n",
      "Epoch 1/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0696 - acc: 0.4329 - val_loss: 0.0655 - val_acc: 0.4705\n",
      "Epoch 2/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0660 - acc: 0.4471 - val_loss: 0.0636 - val_acc: 0.4730\n",
      "Epoch 3/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0646 - acc: 0.4512 - val_loss: 0.0635 - val_acc: 0.5129\n",
      "Epoch 4/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0637 - acc: 0.4615 - val_loss: 0.0617 - val_acc: 0.4796\n",
      "Epoch 5/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0619 - acc: 0.4853 - val_loss: 0.0613 - val_acc: 0.5187\n",
      "Epoch 6/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0608 - acc: 0.5105 - val_loss: 0.0596 - val_acc: 0.5245\n",
      "Epoch 7/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0598 - acc: 0.5188 - val_loss: 0.0590 - val_acc: 0.5503\n",
      "Epoch 8/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0596 - acc: 0.5313 - val_loss: 0.0611 - val_acc: 0.5195\n",
      "Epoch 9/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0591 - acc: 0.5388 - val_loss: 0.0575 - val_acc: 0.5544\n",
      "Epoch 10/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0581 - acc: 0.5518 - val_loss: 0.0566 - val_acc: 0.5594\n",
      "Epoch 11/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0577 - acc: 0.5576 - val_loss: 0.0588 - val_acc: 0.5428\n",
      "Epoch 12/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0571 - acc: 0.5668 - val_loss: 0.0566 - val_acc: 0.5752\n",
      "Epoch 13/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0573 - acc: 0.5565 - val_loss: 0.0570 - val_acc: 0.5711\n",
      "Epoch 14/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0572 - acc: 0.5610 - val_loss: 0.0577 - val_acc: 0.5619\n",
      "Epoch 15/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0573 - acc: 0.5604 - val_loss: 0.0570 - val_acc: 0.5661\n",
      "Epoch 16/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0568 - acc: 0.5646 - val_loss: 0.0569 - val_acc: 0.5628\n",
      "Epoch 17/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0569 - acc: 0.5660 - val_loss: 0.0575 - val_acc: 0.5544\n",
      "Epoch 18/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0569 - acc: 0.5698 - val_loss: 0.0570 - val_acc: 0.5702\n",
      "Epoch 19/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0565 - acc: 0.5732 - val_loss: 0.0575 - val_acc: 0.5711\n",
      "Epoch 20/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0567 - acc: 0.5701 - val_loss: 0.0573 - val_acc: 0.5594\n",
      "Epoch 21/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0564 - acc: 0.5737 - val_loss: 0.0563 - val_acc: 0.5702\n",
      "Epoch 22/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0566 - acc: 0.5715 - val_loss: 0.0569 - val_acc: 0.5694\n",
      "Epoch 23/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0563 - acc: 0.5762 - val_loss: 0.0560 - val_acc: 0.5744\n",
      "Epoch 24/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0566 - acc: 0.5665 - val_loss: 0.0572 - val_acc: 0.5761\n",
      "Epoch 25/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0565 - acc: 0.5687 - val_loss: 0.0566 - val_acc: 0.5727\n",
      "Epoch 26/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0561 - acc: 0.5818 - val_loss: 0.0585 - val_acc: 0.5328\n",
      "Epoch 27/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0565 - acc: 0.5712 - val_loss: 0.0572 - val_acc: 0.5586\n",
      "Epoch 28/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0564 - acc: 0.5723 - val_loss: 0.0561 - val_acc: 0.5852\n",
      "Epoch 29/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0565 - acc: 0.5643 - val_loss: 0.0575 - val_acc: 0.5694\n",
      "Epoch 30/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0558 - acc: 0.5746 - val_loss: 0.0574 - val_acc: 0.5719\n",
      "Epoch 31/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0563 - acc: 0.5701 - val_loss: 0.0563 - val_acc: 0.5736\n",
      "Epoch 32/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0561 - acc: 0.5746 - val_loss: 0.0565 - val_acc: 0.5894\n",
      "Epoch 33/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0557 - acc: 0.5793 - val_loss: 0.0567 - val_acc: 0.5677\n",
      "Epoch 34/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0556 - acc: 0.5823 - val_loss: 0.0563 - val_acc: 0.5752\n",
      "Epoch 35/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0557 - acc: 0.5782 - val_loss: 0.0577 - val_acc: 0.5702\n",
      "Epoch 36/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0556 - acc: 0.5812 - val_loss: 0.0567 - val_acc: 0.5744\n",
      "Epoch 37/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0555 - acc: 0.5773 - val_loss: 0.0589 - val_acc: 0.5511\n",
      "Epoch 38/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0557 - acc: 0.5812 - val_loss: 0.0562 - val_acc: 0.5761\n",
      "Epoch 39/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0553 - acc: 0.5840 - val_loss: 0.0603 - val_acc: 0.5254\n",
      "Epoch 40/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0556 - acc: 0.5884 - val_loss: 0.0562 - val_acc: 0.5819\n",
      "Epoch 41/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0556 - acc: 0.5837 - val_loss: 0.0562 - val_acc: 0.5677\n",
      "Epoch 42/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0557 - acc: 0.5801 - val_loss: 0.0567 - val_acc: 0.5636\n",
      "Epoch 43/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0555 - acc: 0.5820 - val_loss: 0.0564 - val_acc: 0.5786\n",
      "Epoch 44/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0557 - acc: 0.5798 - val_loss: 0.0576 - val_acc: 0.5569\n",
      "Epoch 45/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0550 - acc: 0.5862 - val_loss: 0.0564 - val_acc: 0.5711\n",
      "Epoch 46/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0551 - acc: 0.5804 - val_loss: 0.0565 - val_acc: 0.5736\n",
      "Epoch 47/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0557 - acc: 0.5818 - val_loss: 0.0568 - val_acc: 0.5744\n",
      "Epoch 48/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0551 - acc: 0.5845 - val_loss: 0.0570 - val_acc: 0.5752\n",
      "Epoch 49/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0551 - acc: 0.5854 - val_loss: 0.0575 - val_acc: 0.5603\n",
      "Epoch 50/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0551 - acc: 0.5854 - val_loss: 0.0566 - val_acc: 0.5777\n",
      "Epoch 51/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0556 - acc: 0.5737 - val_loss: 0.0563 - val_acc: 0.5752\n",
      "Epoch 52/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0554 - acc: 0.5795 - val_loss: 0.0565 - val_acc: 0.5719\n",
      "Epoch 53/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0548 - acc: 0.5868 - val_loss: 0.0565 - val_acc: 0.5761\n",
      "Epoch 54/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0552 - acc: 0.5829 - val_loss: 0.0573 - val_acc: 0.5694\n",
      "Epoch 55/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0555 - acc: 0.5845 - val_loss: 0.0560 - val_acc: 0.5810\n",
      "Epoch 56/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0549 - acc: 0.5904 - val_loss: 0.0570 - val_acc: 0.5661\n",
      "Epoch 57/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0551 - acc: 0.5868 - val_loss: 0.0565 - val_acc: 0.5786\n",
      "Epoch 58/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0548 - acc: 0.5843 - val_loss: 0.0583 - val_acc: 0.5511\n",
      "Epoch 59/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0554 - acc: 0.5779 - val_loss: 0.0591 - val_acc: 0.5470\n",
      "Epoch 60/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0550 - acc: 0.5890 - val_loss: 0.0577 - val_acc: 0.5653\n",
      "Epoch 61/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0546 - acc: 0.5940 - val_loss: 0.0570 - val_acc: 0.5736\n",
      "Epoch 62/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0548 - acc: 0.5868 - val_loss: 0.0580 - val_acc: 0.5711\n",
      "Epoch 63/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0547 - acc: 0.5934 - val_loss: 0.0576 - val_acc: 0.5686\n",
      "Epoch 64/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0546 - acc: 0.5873 - val_loss: 0.0576 - val_acc: 0.5653\n",
      "Epoch 65/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0551 - acc: 0.5887 - val_loss: 0.0561 - val_acc: 0.5819\n",
      "Epoch 66/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0545 - acc: 0.5884 - val_loss: 0.0564 - val_acc: 0.5819\n",
      "Epoch 67/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0546 - acc: 0.5859 - val_loss: 0.0577 - val_acc: 0.5528\n",
      "Epoch 68/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0547 - acc: 0.5862 - val_loss: 0.0566 - val_acc: 0.5661\n",
      "Epoch 69/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0546 - acc: 0.5859 - val_loss: 0.0566 - val_acc: 0.5719\n",
      "Epoch 70/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0547 - acc: 0.5831 - val_loss: 0.0571 - val_acc: 0.5702\n",
      "Epoch 71/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0547 - acc: 0.5887 - val_loss: 0.0561 - val_acc: 0.5777\n",
      "Epoch 72/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0549 - acc: 0.5848 - val_loss: 0.0570 - val_acc: 0.5794\n",
      "Epoch 73/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0548 - acc: 0.5865 - val_loss: 0.0565 - val_acc: 0.5711\n",
      "Epoch 74/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0544 - acc: 0.5937 - val_loss: 0.0582 - val_acc: 0.5503\n",
      "Epoch 75/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0539 - acc: 0.5989 - val_loss: 0.0583 - val_acc: 0.5436\n",
      "Epoch 76/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0548 - acc: 0.5887 - val_loss: 0.0568 - val_acc: 0.5744\n",
      "Epoch 77/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0542 - acc: 0.5945 - val_loss: 0.0564 - val_acc: 0.5802\n",
      "Epoch 78/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0546 - acc: 0.5879 - val_loss: 0.0567 - val_acc: 0.5694\n",
      "Epoch 79/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0542 - acc: 0.5953 - val_loss: 0.0569 - val_acc: 0.5702\n",
      "Epoch 80/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0543 - acc: 0.5915 - val_loss: 0.0570 - val_acc: 0.5786\n",
      "Epoch 81/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0547 - acc: 0.5887 - val_loss: 0.0567 - val_acc: 0.5802\n",
      "Epoch 82/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0540 - acc: 0.5970 - val_loss: 0.0569 - val_acc: 0.5727\n",
      "Epoch 83/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0542 - acc: 0.5940 - val_loss: 0.0578 - val_acc: 0.5603\n",
      "Epoch 84/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0541 - acc: 0.5962 - val_loss: 0.0577 - val_acc: 0.5702\n",
      "Epoch 85/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0543 - acc: 0.5942 - val_loss: 0.0567 - val_acc: 0.5727\n",
      "Epoch 86/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0546 - acc: 0.5862 - val_loss: 0.0572 - val_acc: 0.5686\n",
      "Epoch 87/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0541 - acc: 0.5981 - val_loss: 0.0567 - val_acc: 0.5661\n",
      "Epoch 88/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0542 - acc: 0.5945 - val_loss: 0.0569 - val_acc: 0.5786\n",
      "Epoch 89/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0541 - acc: 0.5934 - val_loss: 0.0579 - val_acc: 0.5736\n",
      "Epoch 90/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0543 - acc: 0.5926 - val_loss: 0.0564 - val_acc: 0.5761\n",
      "Epoch 91/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0542 - acc: 0.5945 - val_loss: 0.0575 - val_acc: 0.5769\n",
      "Epoch 92/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0547 - acc: 0.5909 - val_loss: 0.0571 - val_acc: 0.5744\n",
      "Epoch 93/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0539 - acc: 0.5906 - val_loss: 0.0571 - val_acc: 0.5677\n",
      "Epoch 94/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0544 - acc: 0.5895 - val_loss: 0.0566 - val_acc: 0.5694\n",
      "Epoch 95/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0539 - acc: 0.5945 - val_loss: 0.0574 - val_acc: 0.5411\n",
      "Epoch 96/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0539 - acc: 0.5981 - val_loss: 0.0574 - val_acc: 0.5603\n",
      "Epoch 97/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0541 - acc: 0.5934 - val_loss: 0.0561 - val_acc: 0.5777\n",
      "Epoch 98/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0539 - acc: 0.5948 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 99/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0545 - acc: 0.5909 - val_loss: 0.0573 - val_acc: 0.5719\n",
      "Epoch 100/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0543 - acc: 0.5904 - val_loss: 0.0564 - val_acc: 0.5727\n",
      "Epoch 101/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0540 - acc: 0.5926 - val_loss: 0.0566 - val_acc: 0.5702\n",
      "Epoch 102/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0542 - acc: 0.5962 - val_loss: 0.0567 - val_acc: 0.5736\n",
      "Epoch 103/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0538 - acc: 0.6014 - val_loss: 0.0605 - val_acc: 0.5303\n",
      "Epoch 104/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0536 - acc: 0.5989 - val_loss: 0.0568 - val_acc: 0.5686\n",
      "Epoch 105/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0537 - acc: 0.5967 - val_loss: 0.0567 - val_acc: 0.5752\n",
      "Epoch 106/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0537 - acc: 0.5981 - val_loss: 0.0588 - val_acc: 0.5478\n",
      "Epoch 107/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0544 - acc: 0.5926 - val_loss: 0.0579 - val_acc: 0.5528\n",
      "Epoch 108/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0537 - acc: 0.5995 - val_loss: 0.0560 - val_acc: 0.5736\n",
      "Epoch 109/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0537 - acc: 0.5953 - val_loss: 0.0578 - val_acc: 0.5528\n",
      "Epoch 110/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0535 - acc: 0.6031 - val_loss: 0.0573 - val_acc: 0.5702\n",
      "Epoch 111/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0539 - acc: 0.5948 - val_loss: 0.0576 - val_acc: 0.5752\n",
      "Epoch 112/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0542 - acc: 0.5917 - val_loss: 0.0566 - val_acc: 0.5752\n",
      "Epoch 113/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0536 - acc: 0.6001 - val_loss: 0.0590 - val_acc: 0.5511\n",
      "Epoch 114/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0539 - acc: 0.5965 - val_loss: 0.0568 - val_acc: 0.5744\n",
      "Epoch 115/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0535 - acc: 0.5992 - val_loss: 0.0566 - val_acc: 0.5744\n",
      "Epoch 116/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0539 - acc: 0.5970 - val_loss: 0.0577 - val_acc: 0.5669\n",
      "Epoch 117/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0535 - acc: 0.5992 - val_loss: 0.0567 - val_acc: 0.5777\n",
      "Epoch 118/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0534 - acc: 0.5965 - val_loss: 0.0564 - val_acc: 0.5711\n",
      "Epoch 119/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0537 - acc: 0.5973 - val_loss: 0.0577 - val_acc: 0.5561\n",
      "Epoch 120/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0536 - acc: 0.6012 - val_loss: 0.0577 - val_acc: 0.5544\n",
      "Epoch 121/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0536 - acc: 0.6001 - val_loss: 0.0567 - val_acc: 0.5794\n",
      "Epoch 122/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0534 - acc: 0.6020 - val_loss: 0.0569 - val_acc: 0.5694\n",
      "Epoch 123/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0534 - acc: 0.5987 - val_loss: 0.0568 - val_acc: 0.5711\n",
      "Epoch 124/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0540 - acc: 0.5962 - val_loss: 0.0575 - val_acc: 0.5702\n",
      "Epoch 125/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0541 - acc: 0.5856 - val_loss: 0.0578 - val_acc: 0.5677\n",
      "Epoch 126/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0543 - acc: 0.5865 - val_loss: 0.0576 - val_acc: 0.5628\n",
      "Epoch 127/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0542 - acc: 0.5923 - val_loss: 0.0574 - val_acc: 0.5653\n",
      "Epoch 128/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0532 - acc: 0.6039 - val_loss: 0.0572 - val_acc: 0.5686\n",
      "Epoch 129/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0533 - acc: 0.6042 - val_loss: 0.0564 - val_acc: 0.5769\n",
      "Epoch 130/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0532 - acc: 0.6020 - val_loss: 0.0571 - val_acc: 0.5677\n",
      "Epoch 131/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0537 - acc: 0.5942 - val_loss: 0.0592 - val_acc: 0.5544\n",
      "Epoch 132/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0541 - acc: 0.5940 - val_loss: 0.0576 - val_acc: 0.5677\n",
      "Epoch 133/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0534 - acc: 0.6017 - val_loss: 0.0571 - val_acc: 0.5736\n",
      "Epoch 134/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0531 - acc: 0.6020 - val_loss: 0.0570 - val_acc: 0.5744\n",
      "Epoch 135/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0536 - acc: 0.5998 - val_loss: 0.0601 - val_acc: 0.5486\n",
      "Epoch 136/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0537 - acc: 0.5973 - val_loss: 0.0586 - val_acc: 0.5694\n",
      "Epoch 137/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0533 - acc: 0.6014 - val_loss: 0.0587 - val_acc: 0.5511\n",
      "Epoch 138/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0532 - acc: 0.6003 - val_loss: 0.0570 - val_acc: 0.5810\n",
      "Epoch 139/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0534 - acc: 0.5970 - val_loss: 0.0573 - val_acc: 0.5819\n",
      "Epoch 140/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0531 - acc: 0.6009 - val_loss: 0.0588 - val_acc: 0.5669\n",
      "Epoch 141/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0540 - acc: 0.5948 - val_loss: 0.0571 - val_acc: 0.5752\n",
      "Epoch 142/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0531 - acc: 0.6025 - val_loss: 0.0566 - val_acc: 0.5810\n",
      "Epoch 143/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0532 - acc: 0.5965 - val_loss: 0.0574 - val_acc: 0.5669\n",
      "Epoch 144/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0535 - acc: 0.5995 - val_loss: 0.0578 - val_acc: 0.5653\n",
      "Epoch 145/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0535 - acc: 0.6014 - val_loss: 0.0568 - val_acc: 0.5744\n",
      "Epoch 146/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0538 - acc: 0.5942 - val_loss: 0.0571 - val_acc: 0.5777\n",
      "Epoch 147/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0530 - acc: 0.5995 - val_loss: 0.0574 - val_acc: 0.5727\n",
      "Epoch 148/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0532 - acc: 0.6025 - val_loss: 0.0569 - val_acc: 0.5694\n",
      "Epoch 149/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0533 - acc: 0.5978 - val_loss: 0.0578 - val_acc: 0.5677\n",
      "Epoch 150/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0529 - acc: 0.6006 - val_loss: 0.0576 - val_acc: 0.5511\n",
      "Epoch 151/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0530 - acc: 0.6037 - val_loss: 0.0578 - val_acc: 0.5661\n",
      "Epoch 152/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0534 - acc: 0.5940 - val_loss: 0.0570 - val_acc: 0.5719\n",
      "Epoch 153/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0529 - acc: 0.6037 - val_loss: 0.0574 - val_acc: 0.5686\n",
      "Epoch 154/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0532 - acc: 0.5981 - val_loss: 0.0573 - val_acc: 0.5669\n",
      "Epoch 155/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0531 - acc: 0.6056 - val_loss: 0.0572 - val_acc: 0.5752\n",
      "Epoch 156/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0533 - acc: 0.5981 - val_loss: 0.0571 - val_acc: 0.5694\n",
      "Epoch 157/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0528 - acc: 0.6050 - val_loss: 0.0576 - val_acc: 0.5802\n",
      "Epoch 158/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0527 - acc: 0.6098 - val_loss: 0.0577 - val_acc: 0.5653\n",
      "Epoch 159/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0532 - acc: 0.5976 - val_loss: 0.0574 - val_acc: 0.5686\n",
      "Epoch 160/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0528 - acc: 0.6045 - val_loss: 0.0574 - val_acc: 0.5719\n",
      "Epoch 161/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0529 - acc: 0.6053 - val_loss: 0.0572 - val_acc: 0.5677\n",
      "Epoch 162/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0533 - acc: 0.5931 - val_loss: 0.0587 - val_acc: 0.5569\n",
      "Epoch 163/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0532 - acc: 0.6025 - val_loss: 0.0585 - val_acc: 0.5619\n",
      "Epoch 164/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0529 - acc: 0.6012 - val_loss: 0.0573 - val_acc: 0.5727\n",
      "Epoch 165/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0529 - acc: 0.6020 - val_loss: 0.0586 - val_acc: 0.5619\n",
      "Epoch 166/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0533 - acc: 0.6017 - val_loss: 0.0574 - val_acc: 0.5702\n",
      "Epoch 167/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0529 - acc: 0.5967 - val_loss: 0.0575 - val_acc: 0.5702\n",
      "Epoch 168/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0529 - acc: 0.6025 - val_loss: 0.0575 - val_acc: 0.5719\n",
      "Epoch 169/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0532 - acc: 0.6025 - val_loss: 0.0577 - val_acc: 0.5727\n",
      "Epoch 170/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0530 - acc: 0.6039 - val_loss: 0.0582 - val_acc: 0.5636\n",
      "Epoch 171/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0535 - acc: 0.5976 - val_loss: 0.0575 - val_acc: 0.5761\n",
      "Epoch 172/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0528 - acc: 0.6081 - val_loss: 0.0577 - val_acc: 0.5702\n",
      "Epoch 173/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0528 - acc: 0.6017 - val_loss: 0.0587 - val_acc: 0.5561\n",
      "Epoch 174/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0530 - acc: 0.6014 - val_loss: 0.0577 - val_acc: 0.5702\n",
      "Epoch 175/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0528 - acc: 0.6106 - val_loss: 0.0579 - val_acc: 0.5686\n",
      "Epoch 176/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0528 - acc: 0.6070 - val_loss: 0.0579 - val_acc: 0.5594\n",
      "Epoch 177/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0529 - acc: 0.6034 - val_loss: 0.0588 - val_acc: 0.5594\n",
      "Epoch 178/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0535 - acc: 0.6001 - val_loss: 0.0595 - val_acc: 0.5578\n",
      "Epoch 179/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0534 - acc: 0.6042 - val_loss: 0.0592 - val_acc: 0.5569\n",
      "Epoch 180/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0532 - acc: 0.6006 - val_loss: 0.0583 - val_acc: 0.5661\n",
      "Epoch 181/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0532 - acc: 0.6059 - val_loss: 0.0579 - val_acc: 0.5644\n",
      "Epoch 182/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0528 - acc: 0.6059 - val_loss: 0.0582 - val_acc: 0.5694\n",
      "Epoch 183/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0528 - acc: 0.6050 - val_loss: 0.0583 - val_acc: 0.5661\n",
      "Epoch 184/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0530 - acc: 0.6017 - val_loss: 0.0580 - val_acc: 0.5736\n",
      "Epoch 185/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0527 - acc: 0.6092 - val_loss: 0.0588 - val_acc: 0.5686\n",
      "Epoch 186/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0529 - acc: 0.6039 - val_loss: 0.0594 - val_acc: 0.5453\n",
      "Epoch 187/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0531 - acc: 0.6031 - val_loss: 0.0576 - val_acc: 0.5694\n",
      "Epoch 188/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0528 - acc: 0.6075 - val_loss: 0.0581 - val_acc: 0.5636\n",
      "Epoch 189/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0533 - acc: 0.6006 - val_loss: 0.0593 - val_acc: 0.5594\n",
      "Epoch 190/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0526 - acc: 0.6078 - val_loss: 0.0575 - val_acc: 0.5711\n",
      "Epoch 191/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0529 - acc: 0.6050 - val_loss: 0.0577 - val_acc: 0.5653\n",
      "Epoch 192/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0525 - acc: 0.6075 - val_loss: 0.0595 - val_acc: 0.5561\n",
      "Epoch 193/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0523 - acc: 0.6139 - val_loss: 0.0577 - val_acc: 0.5611\n",
      "Epoch 194/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0531 - acc: 0.6014 - val_loss: 0.0574 - val_acc: 0.5677\n",
      "Epoch 195/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0535 - acc: 0.5989 - val_loss: 0.0590 - val_acc: 0.5569\n",
      "Epoch 196/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0527 - acc: 0.6073 - val_loss: 0.0582 - val_acc: 0.5761\n",
      "Epoch 197/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0526 - acc: 0.6081 - val_loss: 0.0582 - val_acc: 0.5727\n",
      "Epoch 198/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0525 - acc: 0.6084 - val_loss: 0.0586 - val_acc: 0.5644\n",
      "Epoch 199/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0527 - acc: 0.6078 - val_loss: 0.0589 - val_acc: 0.5677\n",
      "Epoch 200/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0531 - acc: 0.6020 - val_loss: 0.0604 - val_acc: 0.5378\n",
      "Epoch 201/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0527 - acc: 0.6070 - val_loss: 0.0579 - val_acc: 0.5669\n",
      "Epoch 202/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0526 - acc: 0.6020 - val_loss: 0.0580 - val_acc: 0.5636\n",
      "Epoch 203/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0528 - acc: 0.6045 - val_loss: 0.0573 - val_acc: 0.5661\n",
      "Epoch 204/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0525 - acc: 0.6089 - val_loss: 0.0581 - val_acc: 0.5653\n",
      "Epoch 205/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0526 - acc: 0.6075 - val_loss: 0.0577 - val_acc: 0.5686\n",
      "Epoch 206/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0530 - acc: 0.5989 - val_loss: 0.0577 - val_acc: 0.5611\n",
      "Epoch 207/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0528 - acc: 0.6064 - val_loss: 0.0577 - val_acc: 0.5702\n",
      "Epoch 208/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0529 - acc: 0.6034 - val_loss: 0.0589 - val_acc: 0.5636\n",
      "Epoch 209/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0526 - acc: 0.6050 - val_loss: 0.0582 - val_acc: 0.5694\n",
      "Epoch 210/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0528 - acc: 0.6056 - val_loss: 0.0582 - val_acc: 0.5744\n",
      "Epoch 211/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0525 - acc: 0.6064 - val_loss: 0.0589 - val_acc: 0.5777\n",
      "Epoch 212/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0527 - acc: 0.5987 - val_loss: 0.0576 - val_acc: 0.5719\n",
      "Epoch 213/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0529 - acc: 0.6009 - val_loss: 0.0605 - val_acc: 0.5594\n",
      "Epoch 214/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0527 - acc: 0.6042 - val_loss: 0.0586 - val_acc: 0.5669\n",
      "Epoch 215/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0528 - acc: 0.6039 - val_loss: 0.0577 - val_acc: 0.5669\n",
      "Epoch 216/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0529 - acc: 0.5981 - val_loss: 0.0577 - val_acc: 0.5744\n",
      "Epoch 217/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0527 - acc: 0.6064 - val_loss: 0.0578 - val_acc: 0.5636\n",
      "Epoch 218/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0526 - acc: 0.6084 - val_loss: 0.0590 - val_acc: 0.5578\n",
      "Epoch 219/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0525 - acc: 0.6059 - val_loss: 0.0581 - val_acc: 0.5661\n",
      "Epoch 220/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0524 - acc: 0.6017 - val_loss: 0.0578 - val_acc: 0.5769\n",
      "Epoch 221/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0527 - acc: 0.6067 - val_loss: 0.0581 - val_acc: 0.5677\n",
      "Epoch 222/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0525 - acc: 0.6064 - val_loss: 0.0578 - val_acc: 0.5694\n",
      "Epoch 223/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0526 - acc: 0.6070 - val_loss: 0.0581 - val_acc: 0.5661\n",
      "Epoch 224/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0529 - acc: 0.6053 - val_loss: 0.0585 - val_acc: 0.5653\n",
      "Epoch 225/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0523 - acc: 0.6120 - val_loss: 0.0581 - val_acc: 0.5777\n",
      "Epoch 226/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0522 - acc: 0.6059 - val_loss: 0.0578 - val_acc: 0.5686\n",
      "Epoch 227/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0525 - acc: 0.6092 - val_loss: 0.0619 - val_acc: 0.5262\n",
      "Epoch 228/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0526 - acc: 0.5998 - val_loss: 0.0592 - val_acc: 0.5578\n",
      "Epoch 229/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0523 - acc: 0.6120 - val_loss: 0.0592 - val_acc: 0.5644\n",
      "Epoch 230/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0523 - acc: 0.6100 - val_loss: 0.0582 - val_acc: 0.5653\n",
      "Epoch 231/500\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0521 - acc: 0.6092 - val_loss: 0.0601 - val_acc: 0.5420\n",
      "Epoch 232/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0525 - acc: 0.6073 - val_loss: 0.0583 - val_acc: 0.5711\n",
      "Epoch 233/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0525 - acc: 0.6092 - val_loss: 0.0592 - val_acc: 0.5619\n",
      "Epoch 234/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0526 - acc: 0.6073 - val_loss: 0.0575 - val_acc: 0.5727\n",
      "Epoch 235/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0527 - acc: 0.5998 - val_loss: 0.0581 - val_acc: 0.5761\n",
      "Epoch 236/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0523 - acc: 0.6078 - val_loss: 0.0590 - val_acc: 0.5586\n",
      "Epoch 237/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0517 - acc: 0.6150 - val_loss: 0.0595 - val_acc: 0.5536\n",
      "Epoch 238/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0522 - acc: 0.6092 - val_loss: 0.0593 - val_acc: 0.5561\n",
      "Epoch 239/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0521 - acc: 0.6145 - val_loss: 0.0586 - val_acc: 0.5653\n",
      "Epoch 240/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0527 - acc: 0.6075 - val_loss: 0.0582 - val_acc: 0.5661\n",
      "Epoch 241/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0521 - acc: 0.6117 - val_loss: 0.0588 - val_acc: 0.5553\n",
      "Epoch 242/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0522 - acc: 0.6131 - val_loss: 0.0595 - val_acc: 0.5619\n",
      "Epoch 243/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0528 - acc: 0.6064 - val_loss: 0.0587 - val_acc: 0.5578\n",
      "Epoch 244/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0523 - acc: 0.6056 - val_loss: 0.0584 - val_acc: 0.5702\n",
      "Epoch 245/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0524 - acc: 0.6081 - val_loss: 0.0585 - val_acc: 0.5644\n",
      "Epoch 246/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0522 - acc: 0.6106 - val_loss: 0.0592 - val_acc: 0.5628\n",
      "Epoch 247/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0529 - acc: 0.6073 - val_loss: 0.0578 - val_acc: 0.5719\n",
      "Epoch 248/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0521 - acc: 0.6098 - val_loss: 0.0580 - val_acc: 0.5669\n",
      "Epoch 249/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0526 - acc: 0.6059 - val_loss: 0.0580 - val_acc: 0.5719\n",
      "Epoch 250/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0520 - acc: 0.6131 - val_loss: 0.0589 - val_acc: 0.5561\n",
      "Epoch 251/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0520 - acc: 0.6081 - val_loss: 0.0586 - val_acc: 0.5694\n",
      "Epoch 252/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0521 - acc: 0.6025 - val_loss: 0.0587 - val_acc: 0.5619\n",
      "Epoch 253/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0520 - acc: 0.6150 - val_loss: 0.0585 - val_acc: 0.5619\n",
      "Epoch 254/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0524 - acc: 0.6064 - val_loss: 0.0594 - val_acc: 0.5644\n",
      "Epoch 255/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0521 - acc: 0.6100 - val_loss: 0.0599 - val_acc: 0.5561\n",
      "Epoch 256/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0527 - acc: 0.6073 - val_loss: 0.0590 - val_acc: 0.5669\n",
      "Epoch 257/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0523 - acc: 0.6034 - val_loss: 0.0586 - val_acc: 0.5727\n",
      "Epoch 258/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0524 - acc: 0.6078 - val_loss: 0.0579 - val_acc: 0.5686\n",
      "Epoch 259/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0524 - acc: 0.6114 - val_loss: 0.0593 - val_acc: 0.5628\n",
      "Epoch 260/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0520 - acc: 0.6084 - val_loss: 0.0593 - val_acc: 0.5628\n",
      "Epoch 261/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0521 - acc: 0.6064 - val_loss: 0.0590 - val_acc: 0.5520\n",
      "Epoch 262/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0522 - acc: 0.6161 - val_loss: 0.0591 - val_acc: 0.5653\n",
      "Epoch 263/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0520 - acc: 0.6131 - val_loss: 0.0590 - val_acc: 0.5628\n",
      "Epoch 264/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0519 - acc: 0.6120 - val_loss: 0.0591 - val_acc: 0.5694\n",
      "Epoch 265/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0524 - acc: 0.6062 - val_loss: 0.0579 - val_acc: 0.5761\n",
      "Epoch 266/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0524 - acc: 0.6042 - val_loss: 0.0590 - val_acc: 0.5603\n",
      "Epoch 267/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0525 - acc: 0.6100 - val_loss: 0.0598 - val_acc: 0.5594\n",
      "Epoch 268/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0518 - acc: 0.6114 - val_loss: 0.0595 - val_acc: 0.5686\n",
      "Epoch 269/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0519 - acc: 0.6161 - val_loss: 0.0593 - val_acc: 0.5669\n",
      "Epoch 270/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0523 - acc: 0.6031 - val_loss: 0.0592 - val_acc: 0.5586\n",
      "Epoch 271/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0520 - acc: 0.6134 - val_loss: 0.0595 - val_acc: 0.5661\n",
      "Epoch 272/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0522 - acc: 0.6136 - val_loss: 0.0585 - val_acc: 0.5636\n",
      "Epoch 273/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0524 - acc: 0.6106 - val_loss: 0.0587 - val_acc: 0.5611\n",
      "Epoch 274/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0517 - acc: 0.6134 - val_loss: 0.0601 - val_acc: 0.5553\n",
      "Epoch 275/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0525 - acc: 0.6067 - val_loss: 0.0594 - val_acc: 0.5569\n",
      "Epoch 276/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0519 - acc: 0.6131 - val_loss: 0.0589 - val_acc: 0.5644\n",
      "Epoch 277/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0523 - acc: 0.6086 - val_loss: 0.0634 - val_acc: 0.5170\n",
      "Epoch 278/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0520 - acc: 0.6081 - val_loss: 0.0584 - val_acc: 0.5694\n",
      "Epoch 279/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0520 - acc: 0.6131 - val_loss: 0.0594 - val_acc: 0.5611\n",
      "Epoch 280/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0521 - acc: 0.6095 - val_loss: 0.0590 - val_acc: 0.5594\n",
      "Epoch 281/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0519 - acc: 0.6120 - val_loss: 0.0605 - val_acc: 0.5453\n",
      "Epoch 282/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0525 - acc: 0.6062 - val_loss: 0.0615 - val_acc: 0.5461\n",
      "Epoch 283/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0529 - acc: 0.6064 - val_loss: 0.0593 - val_acc: 0.5619\n",
      "Epoch 284/500\n",
      "3608/3608 [==============================] - 1s 199us/step - loss: 0.0524 - acc: 0.6081 - val_loss: 0.0582 - val_acc: 0.5694\n",
      "Epoch 285/500\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0526 - acc: 0.6034 - val_loss: 0.0597 - val_acc: 0.5586\n",
      "Epoch 286/500\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0519 - acc: 0.6150 - val_loss: 0.0591 - val_acc: 0.5636\n",
      "Epoch 287/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0522 - acc: 0.6081 - val_loss: 0.0581 - val_acc: 0.5661\n",
      "Epoch 288/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0520 - acc: 0.6111 - val_loss: 0.0585 - val_acc: 0.5636\n",
      "Epoch 289/500\n",
      "3608/3608 [==============================] - 1s 205us/step - loss: 0.0517 - acc: 0.6159 - val_loss: 0.0593 - val_acc: 0.5694\n",
      "Epoch 290/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0518 - acc: 0.6134 - val_loss: 0.0587 - val_acc: 0.5653\n",
      "Epoch 291/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0519 - acc: 0.6098 - val_loss: 0.0587 - val_acc: 0.5661\n",
      "Epoch 292/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0522 - acc: 0.6067 - val_loss: 0.0597 - val_acc: 0.5694\n",
      "Epoch 293/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0519 - acc: 0.6089 - val_loss: 0.0590 - val_acc: 0.5628\n",
      "Epoch 294/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0520 - acc: 0.6109 - val_loss: 0.0588 - val_acc: 0.5644\n",
      "Epoch 295/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0518 - acc: 0.6120 - val_loss: 0.0601 - val_acc: 0.5495\n",
      "Epoch 296/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0517 - acc: 0.6100 - val_loss: 0.0588 - val_acc: 0.5636\n",
      "Epoch 297/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0521 - acc: 0.6111 - val_loss: 0.0583 - val_acc: 0.5719\n",
      "Epoch 298/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0519 - acc: 0.6123 - val_loss: 0.0594 - val_acc: 0.5636\n",
      "Epoch 299/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0519 - acc: 0.6100 - val_loss: 0.0601 - val_acc: 0.5628\n",
      "Epoch 300/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0519 - acc: 0.6123 - val_loss: 0.0585 - val_acc: 0.5786\n",
      "Epoch 301/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0520 - acc: 0.6084 - val_loss: 0.0594 - val_acc: 0.5553\n",
      "Epoch 302/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0524 - acc: 0.6117 - val_loss: 0.0584 - val_acc: 0.5611\n",
      "Epoch 303/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0521 - acc: 0.6073 - val_loss: 0.0580 - val_acc: 0.5711\n",
      "Epoch 304/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0518 - acc: 0.6100 - val_loss: 0.0583 - val_acc: 0.5727\n",
      "Epoch 305/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0515 - acc: 0.6153 - val_loss: 0.0589 - val_acc: 0.5669\n",
      "Epoch 306/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0517 - acc: 0.6078 - val_loss: 0.0597 - val_acc: 0.5578\n",
      "Epoch 307/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0516 - acc: 0.6142 - val_loss: 0.0618 - val_acc: 0.5403\n",
      "Epoch 308/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0524 - acc: 0.6120 - val_loss: 0.0594 - val_acc: 0.5669\n",
      "Epoch 309/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0520 - acc: 0.6120 - val_loss: 0.0586 - val_acc: 0.5694\n",
      "Epoch 310/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0518 - acc: 0.6117 - val_loss: 0.0595 - val_acc: 0.5628\n",
      "Epoch 311/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0517 - acc: 0.6145 - val_loss: 0.0603 - val_acc: 0.5553\n",
      "Epoch 312/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0516 - acc: 0.6123 - val_loss: 0.0599 - val_acc: 0.5603\n",
      "Epoch 313/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0517 - acc: 0.6147 - val_loss: 0.0596 - val_acc: 0.5594\n",
      "Epoch 314/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0521 - acc: 0.6103 - val_loss: 0.0585 - val_acc: 0.5711\n",
      "Epoch 315/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0518 - acc: 0.6106 - val_loss: 0.0590 - val_acc: 0.5686\n",
      "Epoch 316/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0522 - acc: 0.6120 - val_loss: 0.0607 - val_acc: 0.5544\n",
      "Epoch 317/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0518 - acc: 0.6098 - val_loss: 0.0591 - val_acc: 0.5653\n",
      "Epoch 318/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0516 - acc: 0.6145 - val_loss: 0.0596 - val_acc: 0.5611\n",
      "Epoch 319/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0519 - acc: 0.6128 - val_loss: 0.0586 - val_acc: 0.5653\n",
      "Epoch 320/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0517 - acc: 0.6123 - val_loss: 0.0589 - val_acc: 0.5636\n",
      "Epoch 321/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0519 - acc: 0.6159 - val_loss: 0.0585 - val_acc: 0.5752\n",
      "Epoch 322/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0519 - acc: 0.6109 - val_loss: 0.0587 - val_acc: 0.5677\n",
      "Epoch 323/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0519 - acc: 0.6086 - val_loss: 0.0585 - val_acc: 0.5719\n",
      "Epoch 324/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0512 - acc: 0.6192 - val_loss: 0.0592 - val_acc: 0.5636\n",
      "Epoch 325/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0515 - acc: 0.6181 - val_loss: 0.0597 - val_acc: 0.5544\n",
      "Epoch 326/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0515 - acc: 0.6128 - val_loss: 0.0591 - val_acc: 0.5594\n",
      "Epoch 327/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0519 - acc: 0.6123 - val_loss: 0.0588 - val_acc: 0.5611\n",
      "Epoch 328/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0516 - acc: 0.6147 - val_loss: 0.0594 - val_acc: 0.5628\n",
      "Epoch 329/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0513 - acc: 0.6159 - val_loss: 0.0583 - val_acc: 0.5736\n",
      "Epoch 330/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0513 - acc: 0.6153 - val_loss: 0.0587 - val_acc: 0.5653\n",
      "Epoch 331/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0524 - acc: 0.6078 - val_loss: 0.0613 - val_acc: 0.5353\n",
      "Epoch 332/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0516 - acc: 0.6128 - val_loss: 0.0597 - val_acc: 0.5486\n",
      "Epoch 333/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0518 - acc: 0.6142 - val_loss: 0.0591 - val_acc: 0.5694\n",
      "Epoch 334/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0513 - acc: 0.6172 - val_loss: 0.0591 - val_acc: 0.5636\n",
      "Epoch 335/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0522 - acc: 0.6123 - val_loss: 0.0598 - val_acc: 0.5611\n",
      "Epoch 336/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0519 - acc: 0.6109 - val_loss: 0.0592 - val_acc: 0.5611\n",
      "Epoch 337/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0515 - acc: 0.6136 - val_loss: 0.0593 - val_acc: 0.5661\n",
      "Epoch 338/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0517 - acc: 0.6150 - val_loss: 0.0592 - val_acc: 0.5686\n",
      "Epoch 339/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0517 - acc: 0.6073 - val_loss: 0.0587 - val_acc: 0.5661\n",
      "Epoch 340/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0513 - acc: 0.6114 - val_loss: 0.0588 - val_acc: 0.5661\n",
      "Epoch 341/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0519 - acc: 0.6139 - val_loss: 0.0589 - val_acc: 0.5603\n",
      "Epoch 342/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0517 - acc: 0.6109 - val_loss: 0.0605 - val_acc: 0.5553\n",
      "Epoch 343/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0514 - acc: 0.6147 - val_loss: 0.0592 - val_acc: 0.5636\n",
      "Epoch 344/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0516 - acc: 0.6145 - val_loss: 0.0586 - val_acc: 0.5669\n",
      "Epoch 345/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0515 - acc: 0.6164 - val_loss: 0.0589 - val_acc: 0.5578\n",
      "Epoch 346/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0515 - acc: 0.6189 - val_loss: 0.0593 - val_acc: 0.5578\n",
      "Epoch 347/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0521 - acc: 0.6114 - val_loss: 0.0591 - val_acc: 0.5619\n",
      "Epoch 348/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0518 - acc: 0.6103 - val_loss: 0.0595 - val_acc: 0.5520\n",
      "Epoch 349/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0518 - acc: 0.6161 - val_loss: 0.0587 - val_acc: 0.5644\n",
      "Epoch 350/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0515 - acc: 0.6114 - val_loss: 0.0594 - val_acc: 0.5694\n",
      "Epoch 351/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0515 - acc: 0.6164 - val_loss: 0.0591 - val_acc: 0.5661\n",
      "Epoch 352/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0521 - acc: 0.6084 - val_loss: 0.0602 - val_acc: 0.5528\n",
      "Epoch 353/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0519 - acc: 0.6086 - val_loss: 0.0598 - val_acc: 0.5536\n",
      "Epoch 354/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0516 - acc: 0.6189 - val_loss: 0.0597 - val_acc: 0.5628\n",
      "Epoch 355/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0514 - acc: 0.6136 - val_loss: 0.0596 - val_acc: 0.5644\n",
      "Epoch 356/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0518 - acc: 0.6100 - val_loss: 0.0601 - val_acc: 0.5536\n",
      "Epoch 357/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0512 - acc: 0.6175 - val_loss: 0.0597 - val_acc: 0.5661\n",
      "Epoch 358/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0513 - acc: 0.6197 - val_loss: 0.0594 - val_acc: 0.5628\n",
      "Epoch 359/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0517 - acc: 0.6120 - val_loss: 0.0592 - val_acc: 0.5661\n",
      "Epoch 360/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0512 - acc: 0.6183 - val_loss: 0.0591 - val_acc: 0.5719\n",
      "Epoch 361/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0514 - acc: 0.6195 - val_loss: 0.0596 - val_acc: 0.5628\n",
      "Epoch 362/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0511 - acc: 0.6172 - val_loss: 0.0597 - val_acc: 0.5653\n",
      "Epoch 363/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0517 - acc: 0.6106 - val_loss: 0.0599 - val_acc: 0.5636\n",
      "Epoch 364/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0516 - acc: 0.6147 - val_loss: 0.0592 - val_acc: 0.5569\n",
      "Epoch 365/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0515 - acc: 0.6136 - val_loss: 0.0598 - val_acc: 0.5677\n",
      "Epoch 366/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0519 - acc: 0.6136 - val_loss: 0.0605 - val_acc: 0.5594\n",
      "Epoch 367/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0517 - acc: 0.6161 - val_loss: 0.0597 - val_acc: 0.5586\n",
      "Epoch 368/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0514 - acc: 0.6139 - val_loss: 0.0593 - val_acc: 0.5603\n",
      "Epoch 369/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0515 - acc: 0.6131 - val_loss: 0.0596 - val_acc: 0.5636\n",
      "Epoch 370/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0512 - acc: 0.6161 - val_loss: 0.0600 - val_acc: 0.5619\n",
      "Epoch 371/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0512 - acc: 0.6189 - val_loss: 0.0596 - val_acc: 0.5628\n",
      "Epoch 372/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0510 - acc: 0.6220 - val_loss: 0.0592 - val_acc: 0.5603\n",
      "Epoch 373/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0509 - acc: 0.6264 - val_loss: 0.0595 - val_acc: 0.5653\n",
      "Epoch 374/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0511 - acc: 0.6217 - val_loss: 0.0601 - val_acc: 0.5461\n",
      "Epoch 375/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0514 - acc: 0.6150 - val_loss: 0.0595 - val_acc: 0.5686\n",
      "Epoch 376/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0516 - acc: 0.6142 - val_loss: 0.0604 - val_acc: 0.5544\n",
      "Epoch 377/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0513 - acc: 0.6167 - val_loss: 0.0602 - val_acc: 0.5561\n",
      "Epoch 378/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0509 - acc: 0.6217 - val_loss: 0.0596 - val_acc: 0.5636\n",
      "Epoch 379/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0513 - acc: 0.6200 - val_loss: 0.0597 - val_acc: 0.5686\n",
      "Epoch 380/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0518 - acc: 0.6167 - val_loss: 0.0593 - val_acc: 0.5611\n",
      "Epoch 381/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0512 - acc: 0.6175 - val_loss: 0.0592 - val_acc: 0.5619\n",
      "Epoch 382/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0511 - acc: 0.6183 - val_loss: 0.0588 - val_acc: 0.5686\n",
      "Epoch 383/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0508 - acc: 0.6206 - val_loss: 0.0592 - val_acc: 0.5702\n",
      "Epoch 384/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0512 - acc: 0.6228 - val_loss: 0.0597 - val_acc: 0.5669\n",
      "Epoch 385/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0510 - acc: 0.6208 - val_loss: 0.0614 - val_acc: 0.5594\n",
      "Epoch 386/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0513 - acc: 0.6175 - val_loss: 0.0593 - val_acc: 0.5711\n",
      "Epoch 387/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0521 - acc: 0.6089 - val_loss: 0.0598 - val_acc: 0.5653\n",
      "Epoch 388/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0511 - acc: 0.6195 - val_loss: 0.0593 - val_acc: 0.5644\n",
      "Epoch 389/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0513 - acc: 0.6206 - val_loss: 0.0595 - val_acc: 0.5669\n",
      "Epoch 390/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0515 - acc: 0.6178 - val_loss: 0.0588 - val_acc: 0.5653\n",
      "Epoch 391/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0513 - acc: 0.6172 - val_loss: 0.0602 - val_acc: 0.5536\n",
      "Epoch 392/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0515 - acc: 0.6161 - val_loss: 0.0595 - val_acc: 0.5561\n",
      "Epoch 393/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0510 - acc: 0.6197 - val_loss: 0.0596 - val_acc: 0.5586\n",
      "Epoch 394/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0512 - acc: 0.6206 - val_loss: 0.0596 - val_acc: 0.5569\n",
      "Epoch 395/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0510 - acc: 0.6214 - val_loss: 0.0605 - val_acc: 0.5561\n",
      "Epoch 396/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0513 - acc: 0.6170 - val_loss: 0.0599 - val_acc: 0.5669\n",
      "Epoch 397/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0511 - acc: 0.6195 - val_loss: 0.0607 - val_acc: 0.5544\n",
      "Epoch 398/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0507 - acc: 0.6214 - val_loss: 0.0603 - val_acc: 0.5611\n",
      "Epoch 399/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0514 - acc: 0.6150 - val_loss: 0.0598 - val_acc: 0.5677\n",
      "Epoch 400/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0514 - acc: 0.6175 - val_loss: 0.0605 - val_acc: 0.5553\n",
      "Epoch 401/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0513 - acc: 0.6211 - val_loss: 0.0598 - val_acc: 0.5553\n",
      "Epoch 402/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0511 - acc: 0.6225 - val_loss: 0.0595 - val_acc: 0.5578\n",
      "Epoch 403/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0511 - acc: 0.6206 - val_loss: 0.0610 - val_acc: 0.5544\n",
      "Epoch 404/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0513 - acc: 0.6192 - val_loss: 0.0594 - val_acc: 0.5644\n",
      "Epoch 405/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0509 - acc: 0.6267 - val_loss: 0.0605 - val_acc: 0.5511\n",
      "Epoch 406/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0510 - acc: 0.6203 - val_loss: 0.0598 - val_acc: 0.5544\n",
      "Epoch 407/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0513 - acc: 0.6197 - val_loss: 0.0597 - val_acc: 0.5611\n",
      "Epoch 408/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0508 - acc: 0.6186 - val_loss: 0.0595 - val_acc: 0.5677\n",
      "Epoch 409/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0512 - acc: 0.6167 - val_loss: 0.0599 - val_acc: 0.5611\n",
      "Epoch 410/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0513 - acc: 0.6208 - val_loss: 0.0616 - val_acc: 0.5553\n",
      "Epoch 411/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0514 - acc: 0.6211 - val_loss: 0.0613 - val_acc: 0.5403\n",
      "Epoch 412/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0511 - acc: 0.6239 - val_loss: 0.0613 - val_acc: 0.5520\n",
      "Epoch 413/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0510 - acc: 0.6189 - val_loss: 0.0592 - val_acc: 0.5686\n",
      "Epoch 414/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0516 - acc: 0.6145 - val_loss: 0.0604 - val_acc: 0.5586\n",
      "Epoch 415/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0511 - acc: 0.6236 - val_loss: 0.0604 - val_acc: 0.5586\n",
      "Epoch 416/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0511 - acc: 0.6236 - val_loss: 0.0600 - val_acc: 0.5536\n",
      "Epoch 417/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0512 - acc: 0.6195 - val_loss: 0.0599 - val_acc: 0.5603\n",
      "Epoch 418/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0512 - acc: 0.6217 - val_loss: 0.0597 - val_acc: 0.5561\n",
      "Epoch 419/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0514 - acc: 0.6211 - val_loss: 0.0594 - val_acc: 0.5653\n",
      "Epoch 420/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0510 - acc: 0.6170 - val_loss: 0.0595 - val_acc: 0.5586\n",
      "Epoch 421/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0507 - acc: 0.6269 - val_loss: 0.0603 - val_acc: 0.5553\n",
      "Epoch 422/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0517 - acc: 0.6134 - val_loss: 0.0597 - val_acc: 0.5578\n",
      "Epoch 423/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0510 - acc: 0.6211 - val_loss: 0.0597 - val_acc: 0.5644\n",
      "Epoch 424/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0514 - acc: 0.6200 - val_loss: 0.0590 - val_acc: 0.5711\n",
      "Epoch 425/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0512 - acc: 0.6178 - val_loss: 0.0605 - val_acc: 0.5561\n",
      "Epoch 426/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0514 - acc: 0.6172 - val_loss: 0.0601 - val_acc: 0.5553\n",
      "Epoch 427/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0508 - acc: 0.6214 - val_loss: 0.0601 - val_acc: 0.5644\n",
      "Epoch 428/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0510 - acc: 0.6192 - val_loss: 0.0604 - val_acc: 0.5578\n",
      "Epoch 429/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0508 - acc: 0.6242 - val_loss: 0.0599 - val_acc: 0.5661\n",
      "Epoch 430/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0509 - acc: 0.6183 - val_loss: 0.0604 - val_acc: 0.5553\n",
      "Epoch 431/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0510 - acc: 0.6220 - val_loss: 0.0594 - val_acc: 0.5628\n",
      "Epoch 432/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0508 - acc: 0.6244 - val_loss: 0.0607 - val_acc: 0.5528\n",
      "Epoch 433/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0507 - acc: 0.6264 - val_loss: 0.0604 - val_acc: 0.5644\n",
      "Epoch 434/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0505 - acc: 0.6256 - val_loss: 0.0599 - val_acc: 0.5561\n",
      "Epoch 435/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0506 - acc: 0.6242 - val_loss: 0.0609 - val_acc: 0.5428\n",
      "Epoch 436/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0512 - acc: 0.6214 - val_loss: 0.0604 - val_acc: 0.5686\n",
      "Epoch 437/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0511 - acc: 0.6156 - val_loss: 0.0600 - val_acc: 0.5661\n",
      "Epoch 438/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0509 - acc: 0.6231 - val_loss: 0.0598 - val_acc: 0.5611\n",
      "Epoch 439/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0513 - acc: 0.6220 - val_loss: 0.0598 - val_acc: 0.5694\n",
      "Epoch 440/500\n",
      "3608/3608 [==============================] - 1s 205us/step - loss: 0.0523 - acc: 0.6125 - val_loss: 0.0610 - val_acc: 0.5461\n",
      "Epoch 441/500\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0516 - acc: 0.6175 - val_loss: 0.0601 - val_acc: 0.5628\n",
      "Epoch 442/500\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0517 - acc: 0.6192 - val_loss: 0.0601 - val_acc: 0.5694\n",
      "Epoch 443/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0508 - acc: 0.6208 - val_loss: 0.0619 - val_acc: 0.5528\n",
      "Epoch 444/500\n",
      "3608/3608 [==============================] - 1s 208us/step - loss: 0.0508 - acc: 0.6236 - val_loss: 0.0599 - val_acc: 0.5644\n",
      "Epoch 445/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0503 - acc: 0.6300 - val_loss: 0.0605 - val_acc: 0.5636\n",
      "Epoch 446/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0507 - acc: 0.6206 - val_loss: 0.0617 - val_acc: 0.5420\n",
      "Epoch 447/500\n",
      "3608/3608 [==============================] - 1s 208us/step - loss: 0.0506 - acc: 0.6267 - val_loss: 0.0612 - val_acc: 0.5594\n",
      "Epoch 448/500\n",
      "3608/3608 [==============================] - 1s 207us/step - loss: 0.0512 - acc: 0.6233 - val_loss: 0.0603 - val_acc: 0.5603\n",
      "Epoch 449/500\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0510 - acc: 0.6225 - val_loss: 0.0605 - val_acc: 0.5553\n",
      "Epoch 450/500\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0505 - acc: 0.6267 - val_loss: 0.0611 - val_acc: 0.5536\n",
      "Epoch 451/500\n",
      "3608/3608 [==============================] - 1s 205us/step - loss: 0.0507 - acc: 0.6192 - val_loss: 0.0611 - val_acc: 0.5594\n",
      "Epoch 452/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0513 - acc: 0.6220 - val_loss: 0.0621 - val_acc: 0.5553\n",
      "Epoch 453/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0509 - acc: 0.6192 - val_loss: 0.0623 - val_acc: 0.5411\n",
      "Epoch 454/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0508 - acc: 0.6222 - val_loss: 0.0610 - val_acc: 0.5536\n",
      "Epoch 455/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0506 - acc: 0.6261 - val_loss: 0.0603 - val_acc: 0.5586\n",
      "Epoch 456/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0516 - acc: 0.6178 - val_loss: 0.0598 - val_acc: 0.5619\n",
      "Epoch 457/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0505 - acc: 0.6231 - val_loss: 0.0597 - val_acc: 0.5669\n",
      "Epoch 458/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0509 - acc: 0.6172 - val_loss: 0.0608 - val_acc: 0.5578\n",
      "Epoch 459/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0509 - acc: 0.6197 - val_loss: 0.0612 - val_acc: 0.5461\n",
      "Epoch 460/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0506 - acc: 0.6261 - val_loss: 0.0612 - val_acc: 0.5603\n",
      "Epoch 461/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0505 - acc: 0.6239 - val_loss: 0.0604 - val_acc: 0.5644\n",
      "Epoch 462/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0509 - acc: 0.6197 - val_loss: 0.0603 - val_acc: 0.5694\n",
      "Epoch 463/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0509 - acc: 0.6200 - val_loss: 0.0602 - val_acc: 0.5619\n",
      "Epoch 464/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0509 - acc: 0.6269 - val_loss: 0.0600 - val_acc: 0.5586\n",
      "Epoch 465/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0507 - acc: 0.6239 - val_loss: 0.0605 - val_acc: 0.5594\n",
      "Epoch 466/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0502 - acc: 0.6283 - val_loss: 0.0617 - val_acc: 0.5486\n",
      "Epoch 467/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0512 - acc: 0.6189 - val_loss: 0.0601 - val_acc: 0.5669\n",
      "Epoch 468/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0509 - acc: 0.6183 - val_loss: 0.0603 - val_acc: 0.5644\n",
      "Epoch 469/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0512 - acc: 0.6189 - val_loss: 0.0608 - val_acc: 0.5569\n",
      "Epoch 470/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0513 - acc: 0.6178 - val_loss: 0.0606 - val_acc: 0.5611\n",
      "Epoch 471/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0507 - acc: 0.6236 - val_loss: 0.0606 - val_acc: 0.5653\n",
      "Epoch 472/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0511 - acc: 0.6192 - val_loss: 0.0618 - val_acc: 0.5644\n",
      "Epoch 473/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0513 - acc: 0.6181 - val_loss: 0.0603 - val_acc: 0.5594\n",
      "Epoch 474/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0507 - acc: 0.6203 - val_loss: 0.0624 - val_acc: 0.5486\n",
      "Epoch 475/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0509 - acc: 0.6228 - val_loss: 0.0602 - val_acc: 0.5586\n",
      "Epoch 476/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0509 - acc: 0.6203 - val_loss: 0.0609 - val_acc: 0.5520\n",
      "Epoch 477/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0513 - acc: 0.6231 - val_loss: 0.0605 - val_acc: 0.5569\n",
      "Epoch 478/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0510 - acc: 0.6247 - val_loss: 0.0605 - val_acc: 0.5553\n",
      "Epoch 479/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0506 - acc: 0.6175 - val_loss: 0.0621 - val_acc: 0.5453\n",
      "Epoch 480/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0505 - acc: 0.6272 - val_loss: 0.0601 - val_acc: 0.5553\n",
      "Epoch 481/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0512 - acc: 0.6159 - val_loss: 0.0610 - val_acc: 0.5544\n",
      "Epoch 482/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0510 - acc: 0.6200 - val_loss: 0.0603 - val_acc: 0.5653\n",
      "Epoch 483/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0505 - acc: 0.6239 - val_loss: 0.0600 - val_acc: 0.5677\n",
      "Epoch 484/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0512 - acc: 0.6217 - val_loss: 0.0606 - val_acc: 0.5619\n",
      "Epoch 485/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0508 - acc: 0.6231 - val_loss: 0.0600 - val_acc: 0.5586\n",
      "Epoch 486/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0511 - acc: 0.6222 - val_loss: 0.0606 - val_acc: 0.5511\n",
      "Epoch 487/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0507 - acc: 0.6208 - val_loss: 0.0610 - val_acc: 0.5503\n",
      "Epoch 488/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0506 - acc: 0.6239 - val_loss: 0.0616 - val_acc: 0.5553\n",
      "Epoch 489/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0509 - acc: 0.6244 - val_loss: 0.0609 - val_acc: 0.5503\n",
      "Epoch 490/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0507 - acc: 0.6233 - val_loss: 0.0599 - val_acc: 0.5653\n",
      "Epoch 491/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0507 - acc: 0.6244 - val_loss: 0.0603 - val_acc: 0.5603\n",
      "Epoch 492/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0505 - acc: 0.6264 - val_loss: 0.0603 - val_acc: 0.5653\n",
      "Epoch 493/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0508 - acc: 0.6228 - val_loss: 0.0602 - val_acc: 0.5594\n",
      "Epoch 494/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0506 - acc: 0.6195 - val_loss: 0.0600 - val_acc: 0.5536\n",
      "Epoch 495/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0506 - acc: 0.6233 - val_loss: 0.0606 - val_acc: 0.5544\n",
      "Epoch 496/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0506 - acc: 0.6233 - val_loss: 0.0616 - val_acc: 0.5453\n",
      "Epoch 497/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0508 - acc: 0.6178 - val_loss: 0.0609 - val_acc: 0.5586\n",
      "Epoch 498/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0508 - acc: 0.6167 - val_loss: 0.0617 - val_acc: 0.5470\n",
      "Epoch 499/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0505 - acc: 0.6195 - val_loss: 0.0604 - val_acc: 0.5636\n",
      "Epoch 500/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0509 - acc: 0.6225 - val_loss: 0.0617 - val_acc: 0.5495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77ffa6f320>"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.summary()\n",
    "nn_model.add(Dense(units=32, activation='relu'))\n",
    "nn_model.add(Dense(units=16, activation='relu'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "nn_model.fit(x=train_x, y=train_y, epochs=500, validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17391
    },
    "colab_type": "code",
    "id": "h26cvMfc-4HC",
    "outputId": "749c597c-cdc2-4b6e-c674-d8bab5bf2314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 1,754\n",
      "Trainable params: 1,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3608 samples, validate on 1203 samples\n",
      "Epoch 1/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0688 - acc: 0.4335 - val_loss: 0.0656 - val_acc: 0.4372\n",
      "Epoch 2/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0645 - acc: 0.4584 - val_loss: 0.0621 - val_acc: 0.4713\n",
      "Epoch 3/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0631 - acc: 0.4673 - val_loss: 0.0618 - val_acc: 0.5021\n",
      "Epoch 4/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0620 - acc: 0.4997 - val_loss: 0.0658 - val_acc: 0.4422\n",
      "Epoch 5/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0606 - acc: 0.5086 - val_loss: 0.0585 - val_acc: 0.5220\n",
      "Epoch 6/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0599 - acc: 0.5244 - val_loss: 0.0582 - val_acc: 0.5370\n",
      "Epoch 7/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0608 - acc: 0.5150 - val_loss: 0.0599 - val_acc: 0.5220\n",
      "Epoch 8/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0590 - acc: 0.5316 - val_loss: 0.0564 - val_acc: 0.5520\n",
      "Epoch 9/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0593 - acc: 0.5280 - val_loss: 0.0570 - val_acc: 0.5561\n",
      "Epoch 10/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0589 - acc: 0.5413 - val_loss: 0.0582 - val_acc: 0.5428\n",
      "Epoch 11/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0589 - acc: 0.5366 - val_loss: 0.0612 - val_acc: 0.5370\n",
      "Epoch 12/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0587 - acc: 0.5432 - val_loss: 0.0565 - val_acc: 0.5461\n",
      "Epoch 13/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0578 - acc: 0.5502 - val_loss: 0.0560 - val_acc: 0.5736\n",
      "Epoch 14/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0581 - acc: 0.5488 - val_loss: 0.0556 - val_acc: 0.5786\n",
      "Epoch 15/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0579 - acc: 0.5513 - val_loss: 0.0573 - val_acc: 0.5478\n",
      "Epoch 16/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0571 - acc: 0.5618 - val_loss: 0.0584 - val_acc: 0.5553\n",
      "Epoch 17/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0571 - acc: 0.5574 - val_loss: 0.0590 - val_acc: 0.5528\n",
      "Epoch 18/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0580 - acc: 0.5452 - val_loss: 0.0555 - val_acc: 0.5744\n",
      "Epoch 19/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0574 - acc: 0.5571 - val_loss: 0.0571 - val_acc: 0.5719\n",
      "Epoch 20/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0579 - acc: 0.5496 - val_loss: 0.0572 - val_acc: 0.5611\n",
      "Epoch 21/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0570 - acc: 0.5618 - val_loss: 0.0581 - val_acc: 0.5403\n",
      "Epoch 22/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0568 - acc: 0.5649 - val_loss: 0.0557 - val_acc: 0.5752\n",
      "Epoch 23/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0565 - acc: 0.5696 - val_loss: 0.0553 - val_acc: 0.5769\n",
      "Epoch 24/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0566 - acc: 0.5740 - val_loss: 0.0569 - val_acc: 0.5420\n",
      "Epoch 25/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0566 - acc: 0.5671 - val_loss: 0.0568 - val_acc: 0.5661\n",
      "Epoch 26/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0572 - acc: 0.5557 - val_loss: 0.0565 - val_acc: 0.5694\n",
      "Epoch 27/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0570 - acc: 0.5610 - val_loss: 0.0568 - val_acc: 0.5686\n",
      "Epoch 28/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0568 - acc: 0.5649 - val_loss: 0.0561 - val_acc: 0.5727\n",
      "Epoch 29/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0568 - acc: 0.5621 - val_loss: 0.0592 - val_acc: 0.5478\n",
      "Epoch 30/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0571 - acc: 0.5571 - val_loss: 0.0558 - val_acc: 0.5686\n",
      "Epoch 31/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0572 - acc: 0.5585 - val_loss: 0.0561 - val_acc: 0.5669\n",
      "Epoch 32/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0566 - acc: 0.5693 - val_loss: 0.0561 - val_acc: 0.5644\n",
      "Epoch 33/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0565 - acc: 0.5660 - val_loss: 0.0553 - val_acc: 0.5694\n",
      "Epoch 34/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0565 - acc: 0.5671 - val_loss: 0.0553 - val_acc: 0.5802\n",
      "Epoch 35/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0566 - acc: 0.5629 - val_loss: 0.0569 - val_acc: 0.5644\n",
      "Epoch 36/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0580 - acc: 0.5554 - val_loss: 0.0570 - val_acc: 0.5520\n",
      "Epoch 37/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0563 - acc: 0.5693 - val_loss: 0.0570 - val_acc: 0.5611\n",
      "Epoch 38/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0562 - acc: 0.5718 - val_loss: 0.0565 - val_acc: 0.5636\n",
      "Epoch 39/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0562 - acc: 0.5710 - val_loss: 0.0564 - val_acc: 0.5661\n",
      "Epoch 40/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0581 - acc: 0.5516 - val_loss: 0.0573 - val_acc: 0.5353\n",
      "Epoch 41/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0562 - acc: 0.5674 - val_loss: 0.0566 - val_acc: 0.5644\n",
      "Epoch 42/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0563 - acc: 0.5715 - val_loss: 0.0589 - val_acc: 0.5553\n",
      "Epoch 43/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0568 - acc: 0.5607 - val_loss: 0.0584 - val_acc: 0.5486\n",
      "Epoch 44/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0571 - acc: 0.5599 - val_loss: 0.0580 - val_acc: 0.5594\n",
      "Epoch 45/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0562 - acc: 0.5746 - val_loss: 0.0558 - val_acc: 0.5719\n",
      "Epoch 46/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0562 - acc: 0.5693 - val_loss: 0.0558 - val_acc: 0.5769\n",
      "Epoch 47/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0560 - acc: 0.5768 - val_loss: 0.0558 - val_acc: 0.5761\n",
      "Epoch 48/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0559 - acc: 0.5726 - val_loss: 0.0566 - val_acc: 0.5686\n",
      "Epoch 49/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0569 - acc: 0.5621 - val_loss: 0.0568 - val_acc: 0.5719\n",
      "Epoch 50/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0558 - acc: 0.5712 - val_loss: 0.0578 - val_acc: 0.5544\n",
      "Epoch 51/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0566 - acc: 0.5654 - val_loss: 0.0572 - val_acc: 0.5736\n",
      "Epoch 52/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0558 - acc: 0.5732 - val_loss: 0.0580 - val_acc: 0.5420\n",
      "Epoch 53/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0557 - acc: 0.5718 - val_loss: 0.0568 - val_acc: 0.5669\n",
      "Epoch 54/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0556 - acc: 0.5748 - val_loss: 0.0582 - val_acc: 0.5619\n",
      "Epoch 55/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0566 - acc: 0.5629 - val_loss: 0.0561 - val_acc: 0.5761\n",
      "Epoch 56/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0565 - acc: 0.5746 - val_loss: 0.0571 - val_acc: 0.5619\n",
      "Epoch 57/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0557 - acc: 0.5715 - val_loss: 0.0559 - val_acc: 0.5711\n",
      "Epoch 58/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0560 - acc: 0.5754 - val_loss: 0.0578 - val_acc: 0.5611\n",
      "Epoch 59/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0567 - acc: 0.5668 - val_loss: 0.0570 - val_acc: 0.5677\n",
      "Epoch 60/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0554 - acc: 0.5768 - val_loss: 0.0563 - val_acc: 0.5661\n",
      "Epoch 61/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0562 - acc: 0.5635 - val_loss: 0.0557 - val_acc: 0.5727\n",
      "Epoch 62/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0559 - acc: 0.5704 - val_loss: 0.0570 - val_acc: 0.5686\n",
      "Epoch 63/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0557 - acc: 0.5729 - val_loss: 0.0589 - val_acc: 0.5420\n",
      "Epoch 64/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0554 - acc: 0.5773 - val_loss: 0.0566 - val_acc: 0.5702\n",
      "Epoch 65/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0561 - acc: 0.5707 - val_loss: 0.0573 - val_acc: 0.5727\n",
      "Epoch 66/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0553 - acc: 0.5793 - val_loss: 0.0579 - val_acc: 0.5611\n",
      "Epoch 67/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0552 - acc: 0.5804 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 68/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0570 - acc: 0.5668 - val_loss: 0.0578 - val_acc: 0.5428\n",
      "Epoch 69/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0552 - acc: 0.5854 - val_loss: 0.0566 - val_acc: 0.5702\n",
      "Epoch 70/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0552 - acc: 0.5807 - val_loss: 0.0579 - val_acc: 0.5644\n",
      "Epoch 71/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0554 - acc: 0.5804 - val_loss: 0.0575 - val_acc: 0.5661\n",
      "Epoch 72/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0554 - acc: 0.5795 - val_loss: 0.0574 - val_acc: 0.5727\n",
      "Epoch 73/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0555 - acc: 0.5768 - val_loss: 0.0567 - val_acc: 0.5636\n",
      "Epoch 74/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0548 - acc: 0.5818 - val_loss: 0.0567 - val_acc: 0.5744\n",
      "Epoch 75/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0552 - acc: 0.5782 - val_loss: 0.0575 - val_acc: 0.5594\n",
      "Epoch 76/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0552 - acc: 0.5743 - val_loss: 0.0567 - val_acc: 0.5653\n",
      "Epoch 77/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0558 - acc: 0.5826 - val_loss: 0.0565 - val_acc: 0.5619\n",
      "Epoch 78/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0557 - acc: 0.5748 - val_loss: 0.0584 - val_acc: 0.5561\n",
      "Epoch 79/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0556 - acc: 0.5757 - val_loss: 0.0565 - val_acc: 0.5628\n",
      "Epoch 80/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0558 - acc: 0.5676 - val_loss: 0.0579 - val_acc: 0.5603\n",
      "Epoch 81/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0557 - acc: 0.5698 - val_loss: 0.0573 - val_acc: 0.5611\n",
      "Epoch 82/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0554 - acc: 0.5726 - val_loss: 0.0572 - val_acc: 0.5669\n",
      "Epoch 83/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0551 - acc: 0.5793 - val_loss: 0.0566 - val_acc: 0.5653\n",
      "Epoch 84/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0554 - acc: 0.5784 - val_loss: 0.0563 - val_acc: 0.5719\n",
      "Epoch 85/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0558 - acc: 0.5765 - val_loss: 0.0593 - val_acc: 0.5428\n",
      "Epoch 86/500\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0554 - acc: 0.5818 - val_loss: 0.0569 - val_acc: 0.5653\n",
      "Epoch 87/500\n",
      "3608/3608 [==============================] - 1s 204us/step - loss: 0.0550 - acc: 0.5834 - val_loss: 0.0578 - val_acc: 0.5553\n",
      "Epoch 88/500\n",
      "3608/3608 [==============================] - 1s 185us/step - loss: 0.0556 - acc: 0.5793 - val_loss: 0.0588 - val_acc: 0.5320\n",
      "Epoch 89/500\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0557 - acc: 0.5712 - val_loss: 0.0580 - val_acc: 0.5653\n",
      "Epoch 90/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0553 - acc: 0.5826 - val_loss: 0.0574 - val_acc: 0.5653\n",
      "Epoch 91/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0554 - acc: 0.5782 - val_loss: 0.0563 - val_acc: 0.5761\n",
      "Epoch 92/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0549 - acc: 0.5848 - val_loss: 0.0575 - val_acc: 0.5578\n",
      "Epoch 93/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0552 - acc: 0.5784 - val_loss: 0.0565 - val_acc: 0.5702\n",
      "Epoch 94/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0553 - acc: 0.5776 - val_loss: 0.0566 - val_acc: 0.5686\n",
      "Epoch 95/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0552 - acc: 0.5823 - val_loss: 0.0573 - val_acc: 0.5661\n",
      "Epoch 96/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0557 - acc: 0.5765 - val_loss: 0.0570 - val_acc: 0.5711\n",
      "Epoch 97/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0550 - acc: 0.5804 - val_loss: 0.0574 - val_acc: 0.5677\n",
      "Epoch 98/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0550 - acc: 0.5801 - val_loss: 0.0583 - val_acc: 0.5603\n",
      "Epoch 99/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0557 - acc: 0.5723 - val_loss: 0.0574 - val_acc: 0.5561\n",
      "Epoch 100/500\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0552 - acc: 0.5787 - val_loss: 0.0566 - val_acc: 0.5719\n",
      "Epoch 101/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0549 - acc: 0.5812 - val_loss: 0.0571 - val_acc: 0.5719\n",
      "Epoch 102/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0550 - acc: 0.5818 - val_loss: 0.0577 - val_acc: 0.5611\n",
      "Epoch 103/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0552 - acc: 0.5784 - val_loss: 0.0565 - val_acc: 0.5711\n",
      "Epoch 104/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0554 - acc: 0.5812 - val_loss: 0.0575 - val_acc: 0.5611\n",
      "Epoch 105/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0555 - acc: 0.5718 - val_loss: 0.0578 - val_acc: 0.5362\n",
      "Epoch 106/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0551 - acc: 0.5826 - val_loss: 0.0575 - val_acc: 0.5653\n",
      "Epoch 107/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0553 - acc: 0.5798 - val_loss: 0.0589 - val_acc: 0.5503\n",
      "Epoch 108/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0550 - acc: 0.5831 - val_loss: 0.0568 - val_acc: 0.5644\n",
      "Epoch 109/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0553 - acc: 0.5771 - val_loss: 0.0575 - val_acc: 0.5619\n",
      "Epoch 110/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0557 - acc: 0.5746 - val_loss: 0.0567 - val_acc: 0.5677\n",
      "Epoch 111/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0554 - acc: 0.5721 - val_loss: 0.0563 - val_acc: 0.5677\n",
      "Epoch 112/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0554 - acc: 0.5804 - val_loss: 0.0564 - val_acc: 0.5761\n",
      "Epoch 113/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0549 - acc: 0.5809 - val_loss: 0.0584 - val_acc: 0.5578\n",
      "Epoch 114/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0554 - acc: 0.5815 - val_loss: 0.0570 - val_acc: 0.5677\n",
      "Epoch 115/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0551 - acc: 0.5771 - val_loss: 0.0585 - val_acc: 0.5669\n",
      "Epoch 116/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0552 - acc: 0.5818 - val_loss: 0.0590 - val_acc: 0.5661\n",
      "Epoch 117/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0553 - acc: 0.5793 - val_loss: 0.0602 - val_acc: 0.5353\n",
      "Epoch 118/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0560 - acc: 0.5704 - val_loss: 0.0584 - val_acc: 0.5528\n",
      "Epoch 119/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0552 - acc: 0.5776 - val_loss: 0.0572 - val_acc: 0.5744\n",
      "Epoch 120/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0549 - acc: 0.5895 - val_loss: 0.0566 - val_acc: 0.5727\n",
      "Epoch 121/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0550 - acc: 0.5815 - val_loss: 0.0577 - val_acc: 0.5611\n",
      "Epoch 122/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0549 - acc: 0.5881 - val_loss: 0.0561 - val_acc: 0.5744\n",
      "Epoch 123/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0550 - acc: 0.5826 - val_loss: 0.0564 - val_acc: 0.5736\n",
      "Epoch 124/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0550 - acc: 0.5826 - val_loss: 0.0573 - val_acc: 0.5653\n",
      "Epoch 125/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0548 - acc: 0.5829 - val_loss: 0.0571 - val_acc: 0.5744\n",
      "Epoch 126/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0550 - acc: 0.5837 - val_loss: 0.0573 - val_acc: 0.5727\n",
      "Epoch 127/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0551 - acc: 0.5815 - val_loss: 0.0574 - val_acc: 0.5702\n",
      "Epoch 128/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0553 - acc: 0.5787 - val_loss: 0.0568 - val_acc: 0.5694\n",
      "Epoch 129/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0552 - acc: 0.5801 - val_loss: 0.0588 - val_acc: 0.5603\n",
      "Epoch 130/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0558 - acc: 0.5804 - val_loss: 0.0577 - val_acc: 0.5686\n",
      "Epoch 131/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0558 - acc: 0.5754 - val_loss: 0.0574 - val_acc: 0.5653\n",
      "Epoch 132/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0548 - acc: 0.5845 - val_loss: 0.0570 - val_acc: 0.5736\n",
      "Epoch 133/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0551 - acc: 0.5840 - val_loss: 0.0580 - val_acc: 0.5603\n",
      "Epoch 134/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0545 - acc: 0.5856 - val_loss: 0.0567 - val_acc: 0.5769\n",
      "Epoch 135/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0548 - acc: 0.5779 - val_loss: 0.0564 - val_acc: 0.5819\n",
      "Epoch 136/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0548 - acc: 0.5848 - val_loss: 0.0576 - val_acc: 0.5677\n",
      "Epoch 137/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0551 - acc: 0.5776 - val_loss: 0.0565 - val_acc: 0.5727\n",
      "Epoch 138/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0553 - acc: 0.5787 - val_loss: 0.0567 - val_acc: 0.5711\n",
      "Epoch 139/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0560 - acc: 0.5779 - val_loss: 0.0590 - val_acc: 0.5278\n",
      "Epoch 140/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0550 - acc: 0.5812 - val_loss: 0.0592 - val_acc: 0.5586\n",
      "Epoch 141/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0547 - acc: 0.5862 - val_loss: 0.0569 - val_acc: 0.5727\n",
      "Epoch 142/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0549 - acc: 0.5818 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 143/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0550 - acc: 0.5793 - val_loss: 0.0570 - val_acc: 0.5769\n",
      "Epoch 144/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0548 - acc: 0.5837 - val_loss: 0.0575 - val_acc: 0.5653\n",
      "Epoch 145/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0554 - acc: 0.5751 - val_loss: 0.0564 - val_acc: 0.5719\n",
      "Epoch 146/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0553 - acc: 0.5740 - val_loss: 0.0568 - val_acc: 0.5827\n",
      "Epoch 147/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0548 - acc: 0.5862 - val_loss: 0.0586 - val_acc: 0.5661\n",
      "Epoch 148/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0546 - acc: 0.5848 - val_loss: 0.0571 - val_acc: 0.5744\n",
      "Epoch 149/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0547 - acc: 0.5848 - val_loss: 0.0574 - val_acc: 0.5611\n",
      "Epoch 150/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0548 - acc: 0.5854 - val_loss: 0.0575 - val_acc: 0.5686\n",
      "Epoch 151/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0548 - acc: 0.5779 - val_loss: 0.0564 - val_acc: 0.5711\n",
      "Epoch 152/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0548 - acc: 0.5795 - val_loss: 0.0585 - val_acc: 0.5395\n",
      "Epoch 153/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0547 - acc: 0.5845 - val_loss: 0.0570 - val_acc: 0.5677\n",
      "Epoch 154/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0548 - acc: 0.5823 - val_loss: 0.0598 - val_acc: 0.5528\n",
      "Epoch 155/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0549 - acc: 0.5793 - val_loss: 0.0566 - val_acc: 0.5669\n",
      "Epoch 156/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0546 - acc: 0.5826 - val_loss: 0.0576 - val_acc: 0.5644\n",
      "Epoch 157/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0545 - acc: 0.5837 - val_loss: 0.0583 - val_acc: 0.5561\n",
      "Epoch 158/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0549 - acc: 0.5776 - val_loss: 0.0579 - val_acc: 0.5586\n",
      "Epoch 159/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0553 - acc: 0.5782 - val_loss: 0.0597 - val_acc: 0.5436\n",
      "Epoch 160/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0548 - acc: 0.5818 - val_loss: 0.0577 - val_acc: 0.5644\n",
      "Epoch 161/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0548 - acc: 0.5795 - val_loss: 0.0563 - val_acc: 0.5736\n",
      "Epoch 162/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0545 - acc: 0.5909 - val_loss: 0.0571 - val_acc: 0.5769\n",
      "Epoch 163/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0545 - acc: 0.5862 - val_loss: 0.0575 - val_acc: 0.5761\n",
      "Epoch 164/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0546 - acc: 0.5862 - val_loss: 0.0569 - val_acc: 0.5686\n",
      "Epoch 165/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0552 - acc: 0.5809 - val_loss: 0.0568 - val_acc: 0.5744\n",
      "Epoch 166/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0549 - acc: 0.5801 - val_loss: 0.0573 - val_acc: 0.5711\n",
      "Epoch 167/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0545 - acc: 0.5854 - val_loss: 0.0565 - val_acc: 0.5736\n",
      "Epoch 168/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0559 - acc: 0.5751 - val_loss: 0.0569 - val_acc: 0.5628\n",
      "Epoch 169/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0560 - acc: 0.5732 - val_loss: 0.0560 - val_acc: 0.5736\n",
      "Epoch 170/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0545 - acc: 0.5876 - val_loss: 0.0570 - val_acc: 0.5702\n",
      "Epoch 171/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0547 - acc: 0.5782 - val_loss: 0.0558 - val_acc: 0.5794\n",
      "Epoch 172/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0548 - acc: 0.5768 - val_loss: 0.0562 - val_acc: 0.5819\n",
      "Epoch 173/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0545 - acc: 0.5837 - val_loss: 0.0570 - val_acc: 0.5711\n",
      "Epoch 174/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0551 - acc: 0.5773 - val_loss: 0.0577 - val_acc: 0.5677\n",
      "Epoch 175/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0546 - acc: 0.5818 - val_loss: 0.0562 - val_acc: 0.5677\n",
      "Epoch 176/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0547 - acc: 0.5798 - val_loss: 0.0568 - val_acc: 0.5628\n",
      "Epoch 177/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0544 - acc: 0.5904 - val_loss: 0.0578 - val_acc: 0.5644\n",
      "Epoch 178/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0546 - acc: 0.5884 - val_loss: 0.0577 - val_acc: 0.5719\n",
      "Epoch 179/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0552 - acc: 0.5843 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 180/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0545 - acc: 0.5848 - val_loss: 0.0583 - val_acc: 0.5478\n",
      "Epoch 181/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0549 - acc: 0.5809 - val_loss: 0.0628 - val_acc: 0.5187\n",
      "Epoch 182/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0548 - acc: 0.5807 - val_loss: 0.0568 - val_acc: 0.5694\n",
      "Epoch 183/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0544 - acc: 0.5787 - val_loss: 0.0568 - val_acc: 0.5744\n",
      "Epoch 184/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0543 - acc: 0.5826 - val_loss: 0.0578 - val_acc: 0.5603\n",
      "Epoch 185/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0545 - acc: 0.5804 - val_loss: 0.0589 - val_acc: 0.5520\n",
      "Epoch 186/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0552 - acc: 0.5798 - val_loss: 0.0572 - val_acc: 0.5677\n",
      "Epoch 187/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0545 - acc: 0.5848 - val_loss: 0.0578 - val_acc: 0.5561\n",
      "Epoch 188/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0546 - acc: 0.5868 - val_loss: 0.0571 - val_acc: 0.5702\n",
      "Epoch 189/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0550 - acc: 0.5801 - val_loss: 0.0582 - val_acc: 0.5702\n",
      "Epoch 190/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0547 - acc: 0.5895 - val_loss: 0.0587 - val_acc: 0.5686\n",
      "Epoch 191/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0551 - acc: 0.5795 - val_loss: 0.0568 - val_acc: 0.5777\n",
      "Epoch 192/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0548 - acc: 0.5798 - val_loss: 0.0588 - val_acc: 0.5578\n",
      "Epoch 193/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0544 - acc: 0.5820 - val_loss: 0.0589 - val_acc: 0.5461\n",
      "Epoch 194/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0551 - acc: 0.5721 - val_loss: 0.0567 - val_acc: 0.5669\n",
      "Epoch 195/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0549 - acc: 0.5776 - val_loss: 0.0576 - val_acc: 0.5702\n",
      "Epoch 196/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0542 - acc: 0.5909 - val_loss: 0.0569 - val_acc: 0.5744\n",
      "Epoch 197/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0545 - acc: 0.5845 - val_loss: 0.0575 - val_acc: 0.5677\n",
      "Epoch 198/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0544 - acc: 0.5848 - val_loss: 0.0586 - val_acc: 0.5569\n",
      "Epoch 199/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0547 - acc: 0.5831 - val_loss: 0.0566 - val_acc: 0.5769\n",
      "Epoch 200/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0546 - acc: 0.5751 - val_loss: 0.0577 - val_acc: 0.5628\n",
      "Epoch 201/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0545 - acc: 0.5787 - val_loss: 0.0575 - val_acc: 0.5719\n",
      "Epoch 202/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0547 - acc: 0.5820 - val_loss: 0.0576 - val_acc: 0.5653\n",
      "Epoch 203/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0541 - acc: 0.5901 - val_loss: 0.0568 - val_acc: 0.5769\n",
      "Epoch 204/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0543 - acc: 0.5831 - val_loss: 0.0586 - val_acc: 0.5653\n",
      "Epoch 205/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0540 - acc: 0.5820 - val_loss: 0.0576 - val_acc: 0.5619\n",
      "Epoch 206/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0547 - acc: 0.5804 - val_loss: 0.0590 - val_acc: 0.5553\n",
      "Epoch 207/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0545 - acc: 0.5831 - val_loss: 0.0581 - val_acc: 0.5711\n",
      "Epoch 208/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0543 - acc: 0.5856 - val_loss: 0.0585 - val_acc: 0.5511\n",
      "Epoch 209/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0542 - acc: 0.5912 - val_loss: 0.0573 - val_acc: 0.5594\n",
      "Epoch 210/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0542 - acc: 0.5881 - val_loss: 0.0569 - val_acc: 0.5727\n",
      "Epoch 211/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0542 - acc: 0.5876 - val_loss: 0.0575 - val_acc: 0.5761\n",
      "Epoch 212/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0549 - acc: 0.5851 - val_loss: 0.0581 - val_acc: 0.5569\n",
      "Epoch 213/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0548 - acc: 0.5798 - val_loss: 0.0579 - val_acc: 0.5636\n",
      "Epoch 214/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0541 - acc: 0.5809 - val_loss: 0.0570 - val_acc: 0.5653\n",
      "Epoch 215/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0542 - acc: 0.5917 - val_loss: 0.0587 - val_acc: 0.5528\n",
      "Epoch 216/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0544 - acc: 0.5868 - val_loss: 0.0588 - val_acc: 0.5594\n",
      "Epoch 217/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0545 - acc: 0.5782 - val_loss: 0.0575 - val_acc: 0.5669\n",
      "Epoch 218/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0545 - acc: 0.5757 - val_loss: 0.0599 - val_acc: 0.5270\n",
      "Epoch 219/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0542 - acc: 0.5909 - val_loss: 0.0586 - val_acc: 0.5536\n",
      "Epoch 220/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0547 - acc: 0.5865 - val_loss: 0.0584 - val_acc: 0.5511\n",
      "Epoch 221/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0540 - acc: 0.5862 - val_loss: 0.0568 - val_acc: 0.5802\n",
      "Epoch 222/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0544 - acc: 0.5804 - val_loss: 0.0581 - val_acc: 0.5578\n",
      "Epoch 223/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0544 - acc: 0.5818 - val_loss: 0.0568 - val_acc: 0.5794\n",
      "Epoch 224/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0538 - acc: 0.5962 - val_loss: 0.0587 - val_acc: 0.5528\n",
      "Epoch 225/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0545 - acc: 0.5837 - val_loss: 0.0581 - val_acc: 0.5669\n",
      "Epoch 226/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0546 - acc: 0.5873 - val_loss: 0.0588 - val_acc: 0.5603\n",
      "Epoch 227/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0542 - acc: 0.5820 - val_loss: 0.0588 - val_acc: 0.5636\n",
      "Epoch 228/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0541 - acc: 0.5881 - val_loss: 0.0581 - val_acc: 0.5628\n",
      "Epoch 229/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0538 - acc: 0.5862 - val_loss: 0.0572 - val_acc: 0.5686\n",
      "Epoch 230/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0537 - acc: 0.5912 - val_loss: 0.0573 - val_acc: 0.5686\n",
      "Epoch 231/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0537 - acc: 0.5904 - val_loss: 0.0582 - val_acc: 0.5594\n",
      "Epoch 232/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0540 - acc: 0.5948 - val_loss: 0.0573 - val_acc: 0.5594\n",
      "Epoch 233/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0538 - acc: 0.5884 - val_loss: 0.0593 - val_acc: 0.5486\n",
      "Epoch 234/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0543 - acc: 0.5856 - val_loss: 0.0568 - val_acc: 0.5736\n",
      "Epoch 235/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0542 - acc: 0.5904 - val_loss: 0.0589 - val_acc: 0.5628\n",
      "Epoch 236/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0546 - acc: 0.5837 - val_loss: 0.0576 - val_acc: 0.5586\n",
      "Epoch 237/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0546 - acc: 0.5848 - val_loss: 0.0574 - val_acc: 0.5686\n",
      "Epoch 238/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0540 - acc: 0.5840 - val_loss: 0.0570 - val_acc: 0.5694\n",
      "Epoch 239/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0540 - acc: 0.5934 - val_loss: 0.0617 - val_acc: 0.5129\n",
      "Epoch 240/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0544 - acc: 0.5831 - val_loss: 0.0592 - val_acc: 0.5594\n",
      "Epoch 241/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0540 - acc: 0.5865 - val_loss: 0.0565 - val_acc: 0.5686\n",
      "Epoch 242/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0540 - acc: 0.5887 - val_loss: 0.0568 - val_acc: 0.5802\n",
      "Epoch 243/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0544 - acc: 0.5920 - val_loss: 0.0582 - val_acc: 0.5586\n",
      "Epoch 244/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0539 - acc: 0.5928 - val_loss: 0.0568 - val_acc: 0.5719\n",
      "Epoch 245/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0538 - acc: 0.5901 - val_loss: 0.0600 - val_acc: 0.5436\n",
      "Epoch 246/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0545 - acc: 0.5845 - val_loss: 0.0582 - val_acc: 0.5653\n",
      "Epoch 247/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0540 - acc: 0.5876 - val_loss: 0.0584 - val_acc: 0.5511\n",
      "Epoch 248/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0541 - acc: 0.5892 - val_loss: 0.0579 - val_acc: 0.5694\n",
      "Epoch 249/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0538 - acc: 0.5909 - val_loss: 0.0573 - val_acc: 0.5686\n",
      "Epoch 250/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0538 - acc: 0.5959 - val_loss: 0.0597 - val_acc: 0.5653\n",
      "Epoch 251/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0552 - acc: 0.5798 - val_loss: 0.0608 - val_acc: 0.5436\n",
      "Epoch 252/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0555 - acc: 0.5812 - val_loss: 0.0574 - val_acc: 0.5752\n",
      "Epoch 253/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0542 - acc: 0.5870 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 254/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0546 - acc: 0.5837 - val_loss: 0.0573 - val_acc: 0.5769\n",
      "Epoch 255/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0542 - acc: 0.5901 - val_loss: 0.0582 - val_acc: 0.5636\n",
      "Epoch 256/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0543 - acc: 0.5868 - val_loss: 0.0571 - val_acc: 0.5752\n",
      "Epoch 257/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0536 - acc: 0.5904 - val_loss: 0.0579 - val_acc: 0.5536\n",
      "Epoch 258/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0542 - acc: 0.5923 - val_loss: 0.0594 - val_acc: 0.5378\n",
      "Epoch 259/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0537 - acc: 0.5901 - val_loss: 0.0581 - val_acc: 0.5611\n",
      "Epoch 260/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0542 - acc: 0.5848 - val_loss: 0.0570 - val_acc: 0.5694\n",
      "Epoch 261/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0540 - acc: 0.5876 - val_loss: 0.0583 - val_acc: 0.5653\n",
      "Epoch 262/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0542 - acc: 0.5831 - val_loss: 0.0588 - val_acc: 0.5486\n",
      "Epoch 263/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0540 - acc: 0.5887 - val_loss: 0.0571 - val_acc: 0.5727\n",
      "Epoch 264/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0545 - acc: 0.5804 - val_loss: 0.0576 - val_acc: 0.5727\n",
      "Epoch 265/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0538 - acc: 0.5856 - val_loss: 0.0602 - val_acc: 0.5486\n",
      "Epoch 266/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0544 - acc: 0.5804 - val_loss: 0.0574 - val_acc: 0.5744\n",
      "Epoch 267/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0551 - acc: 0.5790 - val_loss: 0.0585 - val_acc: 0.5511\n",
      "Epoch 268/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0542 - acc: 0.5923 - val_loss: 0.0589 - val_acc: 0.5686\n",
      "Epoch 269/500\n",
      "3608/3608 [==============================] - 1s 184us/step - loss: 0.0539 - acc: 0.5887 - val_loss: 0.0575 - val_acc: 0.5628\n",
      "Epoch 270/500\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0541 - acc: 0.5837 - val_loss: 0.0576 - val_acc: 0.5603\n",
      "Epoch 271/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0538 - acc: 0.5873 - val_loss: 0.0568 - val_acc: 0.5877\n",
      "Epoch 272/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0542 - acc: 0.5876 - val_loss: 0.0580 - val_acc: 0.5553\n",
      "Epoch 273/500\n",
      "3608/3608 [==============================] - 1s 207us/step - loss: 0.0541 - acc: 0.5845 - val_loss: 0.0577 - val_acc: 0.5669\n",
      "Epoch 274/500\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0543 - acc: 0.5801 - val_loss: 0.0578 - val_acc: 0.5628\n",
      "Epoch 275/500\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0540 - acc: 0.5856 - val_loss: 0.0573 - val_acc: 0.5702\n",
      "Epoch 276/500\n",
      "3608/3608 [==============================] - 1s 192us/step - loss: 0.0544 - acc: 0.5837 - val_loss: 0.0605 - val_acc: 0.5345\n",
      "Epoch 277/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0540 - acc: 0.5890 - val_loss: 0.0580 - val_acc: 0.5644\n",
      "Epoch 278/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0539 - acc: 0.5895 - val_loss: 0.0587 - val_acc: 0.5561\n",
      "Epoch 279/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0544 - acc: 0.5823 - val_loss: 0.0587 - val_acc: 0.5495\n",
      "Epoch 280/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0540 - acc: 0.5854 - val_loss: 0.0575 - val_acc: 0.5644\n",
      "Epoch 281/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0538 - acc: 0.5901 - val_loss: 0.0581 - val_acc: 0.5744\n",
      "Epoch 282/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0544 - acc: 0.5862 - val_loss: 0.0572 - val_acc: 0.5711\n",
      "Epoch 283/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0543 - acc: 0.5876 - val_loss: 0.0569 - val_acc: 0.5694\n",
      "Epoch 284/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0541 - acc: 0.5892 - val_loss: 0.0592 - val_acc: 0.5528\n",
      "Epoch 285/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0536 - acc: 0.5928 - val_loss: 0.0579 - val_acc: 0.5569\n",
      "Epoch 286/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0544 - acc: 0.5854 - val_loss: 0.0581 - val_acc: 0.5628\n",
      "Epoch 287/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0544 - acc: 0.5826 - val_loss: 0.0581 - val_acc: 0.5644\n",
      "Epoch 288/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0551 - acc: 0.5809 - val_loss: 0.0579 - val_acc: 0.5677\n",
      "Epoch 289/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0545 - acc: 0.5904 - val_loss: 0.0581 - val_acc: 0.5752\n",
      "Epoch 290/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0537 - acc: 0.5917 - val_loss: 0.0581 - val_acc: 0.5478\n",
      "Epoch 291/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0543 - acc: 0.5843 - val_loss: 0.0571 - val_acc: 0.5727\n",
      "Epoch 292/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0538 - acc: 0.5876 - val_loss: 0.0580 - val_acc: 0.5653\n",
      "Epoch 293/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0540 - acc: 0.5906 - val_loss: 0.0581 - val_acc: 0.5594\n",
      "Epoch 294/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0547 - acc: 0.5809 - val_loss: 0.0578 - val_acc: 0.5694\n",
      "Epoch 295/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0543 - acc: 0.5895 - val_loss: 0.0588 - val_acc: 0.5378\n",
      "Epoch 296/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0537 - acc: 0.5873 - val_loss: 0.0577 - val_acc: 0.5653\n",
      "Epoch 297/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0536 - acc: 0.5879 - val_loss: 0.0575 - val_acc: 0.5694\n",
      "Epoch 298/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0534 - acc: 0.5917 - val_loss: 0.0578 - val_acc: 0.5611\n",
      "Epoch 299/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0535 - acc: 0.5881 - val_loss: 0.0577 - val_acc: 0.5719\n",
      "Epoch 300/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0540 - acc: 0.5868 - val_loss: 0.0591 - val_acc: 0.5353\n",
      "Epoch 301/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0544 - acc: 0.5837 - val_loss: 0.0577 - val_acc: 0.5669\n",
      "Epoch 302/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0538 - acc: 0.5890 - val_loss: 0.0589 - val_acc: 0.5677\n",
      "Epoch 303/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0536 - acc: 0.5906 - val_loss: 0.0584 - val_acc: 0.5594\n",
      "Epoch 304/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0538 - acc: 0.5892 - val_loss: 0.0601 - val_acc: 0.5337\n",
      "Epoch 305/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0534 - acc: 0.5934 - val_loss: 0.0583 - val_acc: 0.5578\n",
      "Epoch 306/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0540 - acc: 0.5856 - val_loss: 0.0603 - val_acc: 0.5420\n",
      "Epoch 307/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0542 - acc: 0.5831 - val_loss: 0.0578 - val_acc: 0.5619\n",
      "Epoch 308/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0540 - acc: 0.5887 - val_loss: 0.0570 - val_acc: 0.5702\n",
      "Epoch 309/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0540 - acc: 0.5859 - val_loss: 0.0597 - val_acc: 0.5586\n",
      "Epoch 310/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0536 - acc: 0.5909 - val_loss: 0.0579 - val_acc: 0.5636\n",
      "Epoch 311/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0536 - acc: 0.5843 - val_loss: 0.0574 - val_acc: 0.5594\n",
      "Epoch 312/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0539 - acc: 0.5859 - val_loss: 0.0572 - val_acc: 0.5677\n",
      "Epoch 313/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0539 - acc: 0.5862 - val_loss: 0.0587 - val_acc: 0.5653\n",
      "Epoch 314/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0539 - acc: 0.5868 - val_loss: 0.0580 - val_acc: 0.5644\n",
      "Epoch 315/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0546 - acc: 0.5856 - val_loss: 0.0579 - val_acc: 0.5553\n",
      "Epoch 316/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0538 - acc: 0.5904 - val_loss: 0.0583 - val_acc: 0.5619\n",
      "Epoch 317/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0539 - acc: 0.5865 - val_loss: 0.0576 - val_acc: 0.5628\n",
      "Epoch 318/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0536 - acc: 0.5884 - val_loss: 0.0583 - val_acc: 0.5619\n",
      "Epoch 319/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0543 - acc: 0.5865 - val_loss: 0.0583 - val_acc: 0.5536\n",
      "Epoch 320/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0538 - acc: 0.5912 - val_loss: 0.0576 - val_acc: 0.5686\n",
      "Epoch 321/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0553 - acc: 0.5826 - val_loss: 0.0585 - val_acc: 0.5520\n",
      "Epoch 322/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0544 - acc: 0.5859 - val_loss: 0.0572 - val_acc: 0.5719\n",
      "Epoch 323/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0534 - acc: 0.5906 - val_loss: 0.0574 - val_acc: 0.5644\n",
      "Epoch 324/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0537 - acc: 0.5906 - val_loss: 0.0573 - val_acc: 0.5769\n",
      "Epoch 325/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0545 - acc: 0.5862 - val_loss: 0.0568 - val_acc: 0.5769\n",
      "Epoch 326/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0548 - acc: 0.5776 - val_loss: 0.0589 - val_acc: 0.5520\n",
      "Epoch 327/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0537 - acc: 0.5906 - val_loss: 0.0579 - val_acc: 0.5702\n",
      "Epoch 328/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0538 - acc: 0.5881 - val_loss: 0.0572 - val_acc: 0.5636\n",
      "Epoch 329/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0533 - acc: 0.5923 - val_loss: 0.0575 - val_acc: 0.5644\n",
      "Epoch 330/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0539 - acc: 0.5912 - val_loss: 0.0581 - val_acc: 0.5636\n",
      "Epoch 331/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0542 - acc: 0.5859 - val_loss: 0.0589 - val_acc: 0.5420\n",
      "Epoch 332/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0537 - acc: 0.5906 - val_loss: 0.0573 - val_acc: 0.5819\n",
      "Epoch 333/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0538 - acc: 0.5879 - val_loss: 0.0574 - val_acc: 0.5694\n",
      "Epoch 334/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0539 - acc: 0.5870 - val_loss: 0.0578 - val_acc: 0.5702\n",
      "Epoch 335/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0539 - acc: 0.5887 - val_loss: 0.0584 - val_acc: 0.5628\n",
      "Epoch 336/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0540 - acc: 0.5909 - val_loss: 0.0581 - val_acc: 0.5636\n",
      "Epoch 337/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0542 - acc: 0.5862 - val_loss: 0.0579 - val_acc: 0.5619\n",
      "Epoch 338/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0539 - acc: 0.5915 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 339/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0539 - acc: 0.5848 - val_loss: 0.0589 - val_acc: 0.5569\n",
      "Epoch 340/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0539 - acc: 0.5926 - val_loss: 0.0591 - val_acc: 0.5561\n",
      "Epoch 341/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0545 - acc: 0.5851 - val_loss: 0.0570 - val_acc: 0.5669\n",
      "Epoch 342/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0543 - acc: 0.5845 - val_loss: 0.0580 - val_acc: 0.5719\n",
      "Epoch 343/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0532 - acc: 0.5923 - val_loss: 0.0578 - val_acc: 0.5677\n",
      "Epoch 344/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0539 - acc: 0.5862 - val_loss: 0.0576 - val_acc: 0.5653\n",
      "Epoch 345/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0539 - acc: 0.5859 - val_loss: 0.0577 - val_acc: 0.5644\n",
      "Epoch 346/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0541 - acc: 0.5862 - val_loss: 0.0577 - val_acc: 0.5644\n",
      "Epoch 347/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0534 - acc: 0.5942 - val_loss: 0.0586 - val_acc: 0.5694\n",
      "Epoch 348/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0537 - acc: 0.5879 - val_loss: 0.0581 - val_acc: 0.5619\n",
      "Epoch 349/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0535 - acc: 0.5862 - val_loss: 0.0578 - val_acc: 0.5761\n",
      "Epoch 350/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0533 - acc: 0.6001 - val_loss: 0.0577 - val_acc: 0.5686\n",
      "Epoch 351/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0536 - acc: 0.5915 - val_loss: 0.0569 - val_acc: 0.5769\n",
      "Epoch 352/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0536 - acc: 0.5917 - val_loss: 0.0577 - val_acc: 0.5619\n",
      "Epoch 353/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0538 - acc: 0.5848 - val_loss: 0.0574 - val_acc: 0.5702\n",
      "Epoch 354/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0535 - acc: 0.5973 - val_loss: 0.0586 - val_acc: 0.5486\n",
      "Epoch 355/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0536 - acc: 0.5912 - val_loss: 0.0575 - val_acc: 0.5553\n",
      "Epoch 356/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0533 - acc: 0.5942 - val_loss: 0.0576 - val_acc: 0.5761\n",
      "Epoch 357/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0538 - acc: 0.5926 - val_loss: 0.0593 - val_acc: 0.5495\n",
      "Epoch 358/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0535 - acc: 0.5962 - val_loss: 0.0583 - val_acc: 0.5628\n",
      "Epoch 359/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0535 - acc: 0.5881 - val_loss: 0.0584 - val_acc: 0.5520\n",
      "Epoch 360/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0534 - acc: 0.5862 - val_loss: 0.0587 - val_acc: 0.5553\n",
      "Epoch 361/500\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0534 - acc: 0.5912 - val_loss: 0.0577 - val_acc: 0.5727\n",
      "Epoch 362/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0538 - acc: 0.5898 - val_loss: 0.0583 - val_acc: 0.5636\n",
      "Epoch 363/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0539 - acc: 0.5917 - val_loss: 0.0582 - val_acc: 0.5711\n",
      "Epoch 364/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0541 - acc: 0.5901 - val_loss: 0.0577 - val_acc: 0.5686\n",
      "Epoch 365/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0535 - acc: 0.5904 - val_loss: 0.0599 - val_acc: 0.5470\n",
      "Epoch 366/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0536 - acc: 0.5917 - val_loss: 0.0574 - val_acc: 0.5694\n",
      "Epoch 367/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0536 - acc: 0.5970 - val_loss: 0.0577 - val_acc: 0.5736\n",
      "Epoch 368/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0552 - acc: 0.5743 - val_loss: 0.0573 - val_acc: 0.5653\n",
      "Epoch 369/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0541 - acc: 0.5848 - val_loss: 0.0577 - val_acc: 0.5694\n",
      "Epoch 370/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0538 - acc: 0.5876 - val_loss: 0.0574 - val_acc: 0.5669\n",
      "Epoch 371/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0534 - acc: 0.5934 - val_loss: 0.0590 - val_acc: 0.5495\n",
      "Epoch 372/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0535 - acc: 0.5948 - val_loss: 0.0580 - val_acc: 0.5669\n",
      "Epoch 373/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0535 - acc: 0.5912 - val_loss: 0.0568 - val_acc: 0.5677\n",
      "Epoch 374/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0537 - acc: 0.5887 - val_loss: 0.0575 - val_acc: 0.5761\n",
      "Epoch 375/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0537 - acc: 0.5920 - val_loss: 0.0572 - val_acc: 0.5677\n",
      "Epoch 376/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0541 - acc: 0.5887 - val_loss: 0.0589 - val_acc: 0.5445\n",
      "Epoch 377/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0537 - acc: 0.5928 - val_loss: 0.0573 - val_acc: 0.5719\n",
      "Epoch 378/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0535 - acc: 0.5862 - val_loss: 0.0576 - val_acc: 0.5694\n",
      "Epoch 379/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0538 - acc: 0.5884 - val_loss: 0.0574 - val_acc: 0.5702\n",
      "Epoch 380/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0542 - acc: 0.5848 - val_loss: 0.0574 - val_acc: 0.5669\n",
      "Epoch 381/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0537 - acc: 0.5862 - val_loss: 0.0579 - val_acc: 0.5761\n",
      "Epoch 382/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0541 - acc: 0.5820 - val_loss: 0.0575 - val_acc: 0.5769\n",
      "Epoch 383/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0540 - acc: 0.5887 - val_loss: 0.0577 - val_acc: 0.5628\n",
      "Epoch 384/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0535 - acc: 0.5915 - val_loss: 0.0592 - val_acc: 0.5503\n",
      "Epoch 385/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0535 - acc: 0.5917 - val_loss: 0.0590 - val_acc: 0.5619\n",
      "Epoch 386/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0538 - acc: 0.5970 - val_loss: 0.0605 - val_acc: 0.5503\n",
      "Epoch 387/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0538 - acc: 0.5904 - val_loss: 0.0575 - val_acc: 0.5736\n",
      "Epoch 388/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0539 - acc: 0.5926 - val_loss: 0.0596 - val_acc: 0.5520\n",
      "Epoch 389/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0541 - acc: 0.5937 - val_loss: 0.0575 - val_acc: 0.5653\n",
      "Epoch 390/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0535 - acc: 0.5901 - val_loss: 0.0585 - val_acc: 0.5520\n",
      "Epoch 391/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0535 - acc: 0.5976 - val_loss: 0.0576 - val_acc: 0.5702\n",
      "Epoch 392/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0533 - acc: 0.5965 - val_loss: 0.0595 - val_acc: 0.5445\n",
      "Epoch 393/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0534 - acc: 0.5926 - val_loss: 0.0568 - val_acc: 0.5711\n",
      "Epoch 394/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0531 - acc: 0.5948 - val_loss: 0.0581 - val_acc: 0.5711\n",
      "Epoch 395/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0532 - acc: 0.5887 - val_loss: 0.0590 - val_acc: 0.5686\n",
      "Epoch 396/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0542 - acc: 0.5868 - val_loss: 0.0574 - val_acc: 0.5686\n",
      "Epoch 397/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0531 - acc: 0.6031 - val_loss: 0.0578 - val_acc: 0.5677\n",
      "Epoch 398/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0531 - acc: 0.5934 - val_loss: 0.0582 - val_acc: 0.5594\n",
      "Epoch 399/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0531 - acc: 0.5940 - val_loss: 0.0583 - val_acc: 0.5611\n",
      "Epoch 400/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0541 - acc: 0.5920 - val_loss: 0.0582 - val_acc: 0.5636\n",
      "Epoch 401/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0535 - acc: 0.5942 - val_loss: 0.0586 - val_acc: 0.5644\n",
      "Epoch 402/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0537 - acc: 0.5928 - val_loss: 0.0581 - val_acc: 0.5644\n",
      "Epoch 403/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0533 - acc: 0.5931 - val_loss: 0.0589 - val_acc: 0.5511\n",
      "Epoch 404/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0536 - acc: 0.5901 - val_loss: 0.0577 - val_acc: 0.5586\n",
      "Epoch 405/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0540 - acc: 0.5884 - val_loss: 0.0578 - val_acc: 0.5719\n",
      "Epoch 406/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0539 - acc: 0.5879 - val_loss: 0.0588 - val_acc: 0.5553\n",
      "Epoch 407/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0537 - acc: 0.5901 - val_loss: 0.0577 - val_acc: 0.5603\n",
      "Epoch 408/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0540 - acc: 0.5884 - val_loss: 0.0583 - val_acc: 0.5644\n",
      "Epoch 409/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0540 - acc: 0.5854 - val_loss: 0.0582 - val_acc: 0.5611\n",
      "Epoch 410/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0538 - acc: 0.5923 - val_loss: 0.0596 - val_acc: 0.5561\n",
      "Epoch 411/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0532 - acc: 0.5926 - val_loss: 0.0580 - val_acc: 0.5794\n",
      "Epoch 412/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0539 - acc: 0.5928 - val_loss: 0.0596 - val_acc: 0.5428\n",
      "Epoch 413/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0536 - acc: 0.5937 - val_loss: 0.0569 - val_acc: 0.5736\n",
      "Epoch 414/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0537 - acc: 0.5909 - val_loss: 0.0574 - val_acc: 0.5669\n",
      "Epoch 415/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0534 - acc: 0.5934 - val_loss: 0.0571 - val_acc: 0.5752\n",
      "Epoch 416/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0537 - acc: 0.5895 - val_loss: 0.0574 - val_acc: 0.5719\n",
      "Epoch 417/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0537 - acc: 0.5909 - val_loss: 0.0571 - val_acc: 0.5702\n",
      "Epoch 418/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0534 - acc: 0.5904 - val_loss: 0.0578 - val_acc: 0.5727\n",
      "Epoch 419/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0537 - acc: 0.5962 - val_loss: 0.0587 - val_acc: 0.5528\n",
      "Epoch 420/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0535 - acc: 0.5926 - val_loss: 0.0593 - val_acc: 0.5578\n",
      "Epoch 421/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0545 - acc: 0.5912 - val_loss: 0.0596 - val_acc: 0.5420\n",
      "Epoch 422/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0542 - acc: 0.5870 - val_loss: 0.0597 - val_acc: 0.5495\n",
      "Epoch 423/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0540 - acc: 0.5865 - val_loss: 0.0580 - val_acc: 0.5628\n",
      "Epoch 424/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0544 - acc: 0.5790 - val_loss: 0.0580 - val_acc: 0.5603\n",
      "Epoch 425/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0535 - acc: 0.5881 - val_loss: 0.0576 - val_acc: 0.5711\n",
      "Epoch 426/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0533 - acc: 0.5843 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 427/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0531 - acc: 0.5940 - val_loss: 0.0595 - val_acc: 0.5569\n",
      "Epoch 428/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0539 - acc: 0.5892 - val_loss: 0.0594 - val_acc: 0.5520\n",
      "Epoch 429/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0539 - acc: 0.5831 - val_loss: 0.0588 - val_acc: 0.5520\n",
      "Epoch 430/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0538 - acc: 0.5895 - val_loss: 0.0583 - val_acc: 0.5653\n",
      "Epoch 431/500\n",
      "3608/3608 [==============================] - 1s 204us/step - loss: 0.0536 - acc: 0.5881 - val_loss: 0.0606 - val_acc: 0.5270\n",
      "Epoch 432/500\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0541 - acc: 0.5876 - val_loss: 0.0594 - val_acc: 0.5569\n",
      "Epoch 433/500\n",
      "3608/3608 [==============================] - 1s 206us/step - loss: 0.0536 - acc: 0.5856 - val_loss: 0.0581 - val_acc: 0.5619\n",
      "Epoch 434/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0542 - acc: 0.5890 - val_loss: 0.0605 - val_acc: 0.5503\n",
      "Epoch 435/500\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0534 - acc: 0.5931 - val_loss: 0.0576 - val_acc: 0.5719\n",
      "Epoch 436/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0535 - acc: 0.5940 - val_loss: 0.0576 - val_acc: 0.5669\n",
      "Epoch 437/500\n",
      "3608/3608 [==============================] - 1s 206us/step - loss: 0.0537 - acc: 0.5951 - val_loss: 0.0574 - val_acc: 0.5686\n",
      "Epoch 438/500\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0540 - acc: 0.5876 - val_loss: 0.0568 - val_acc: 0.5744\n",
      "Epoch 439/500\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0535 - acc: 0.5917 - val_loss: 0.0581 - val_acc: 0.5520\n",
      "Epoch 440/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0538 - acc: 0.5862 - val_loss: 0.0578 - val_acc: 0.5661\n",
      "Epoch 441/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0533 - acc: 0.5976 - val_loss: 0.0579 - val_acc: 0.5702\n",
      "Epoch 442/500\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0540 - acc: 0.5868 - val_loss: 0.0588 - val_acc: 0.5744\n",
      "Epoch 443/500\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0539 - acc: 0.5848 - val_loss: 0.0579 - val_acc: 0.5769\n",
      "Epoch 444/500\n",
      "3608/3608 [==============================] - 1s 191us/step - loss: 0.0535 - acc: 0.5881 - val_loss: 0.0582 - val_acc: 0.5644\n",
      "Epoch 445/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0535 - acc: 0.5928 - val_loss: 0.0569 - val_acc: 0.5727\n",
      "Epoch 446/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0539 - acc: 0.5868 - val_loss: 0.0584 - val_acc: 0.5495\n",
      "Epoch 447/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0535 - acc: 0.5887 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 448/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0536 - acc: 0.5945 - val_loss: 0.0589 - val_acc: 0.5511\n",
      "Epoch 449/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0543 - acc: 0.5834 - val_loss: 0.0588 - val_acc: 0.5520\n",
      "Epoch 450/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0537 - acc: 0.5928 - val_loss: 0.0591 - val_acc: 0.5653\n",
      "Epoch 451/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0537 - acc: 0.5887 - val_loss: 0.0583 - val_acc: 0.5661\n",
      "Epoch 452/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0533 - acc: 0.5934 - val_loss: 0.0573 - val_acc: 0.5761\n",
      "Epoch 453/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0535 - acc: 0.5945 - val_loss: 0.0579 - val_acc: 0.5686\n",
      "Epoch 454/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0533 - acc: 0.5945 - val_loss: 0.0584 - val_acc: 0.5702\n",
      "Epoch 455/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0533 - acc: 0.5956 - val_loss: 0.0571 - val_acc: 0.5736\n",
      "Epoch 456/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0535 - acc: 0.5909 - val_loss: 0.0598 - val_acc: 0.5461\n",
      "Epoch 457/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0541 - acc: 0.5884 - val_loss: 0.0589 - val_acc: 0.5578\n",
      "Epoch 458/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0535 - acc: 0.5953 - val_loss: 0.0572 - val_acc: 0.5736\n",
      "Epoch 459/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0536 - acc: 0.5901 - val_loss: 0.0585 - val_acc: 0.5578\n",
      "Epoch 460/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0532 - acc: 0.5942 - val_loss: 0.0598 - val_acc: 0.5495\n",
      "Epoch 461/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0535 - acc: 0.5912 - val_loss: 0.0580 - val_acc: 0.5644\n",
      "Epoch 462/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0529 - acc: 0.5992 - val_loss: 0.0580 - val_acc: 0.5702\n",
      "Epoch 463/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0534 - acc: 0.5904 - val_loss: 0.0587 - val_acc: 0.5553\n",
      "Epoch 464/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0535 - acc: 0.5940 - val_loss: 0.0579 - val_acc: 0.5644\n",
      "Epoch 465/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0536 - acc: 0.5931 - val_loss: 0.0573 - val_acc: 0.5669\n",
      "Epoch 466/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0535 - acc: 0.5951 - val_loss: 0.0585 - val_acc: 0.5669\n",
      "Epoch 467/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0536 - acc: 0.5928 - val_loss: 0.0574 - val_acc: 0.5711\n",
      "Epoch 468/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0542 - acc: 0.5851 - val_loss: 0.0581 - val_acc: 0.5586\n",
      "Epoch 469/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0534 - acc: 0.5917 - val_loss: 0.0567 - val_acc: 0.5744\n",
      "Epoch 470/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0532 - acc: 0.5937 - val_loss: 0.0586 - val_acc: 0.5619\n",
      "Epoch 471/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0534 - acc: 0.5892 - val_loss: 0.0565 - val_acc: 0.5885\n",
      "Epoch 472/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0536 - acc: 0.5892 - val_loss: 0.0603 - val_acc: 0.5395\n",
      "Epoch 473/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0539 - acc: 0.5909 - val_loss: 0.0569 - val_acc: 0.5802\n",
      "Epoch 474/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0543 - acc: 0.5890 - val_loss: 0.0583 - val_acc: 0.5702\n",
      "Epoch 475/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0538 - acc: 0.5890 - val_loss: 0.0581 - val_acc: 0.5686\n",
      "Epoch 476/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0530 - acc: 0.5928 - val_loss: 0.0581 - val_acc: 0.5711\n",
      "Epoch 477/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0535 - acc: 0.5937 - val_loss: 0.0575 - val_acc: 0.5686\n",
      "Epoch 478/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0537 - acc: 0.5931 - val_loss: 0.0576 - val_acc: 0.5702\n",
      "Epoch 479/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0535 - acc: 0.5945 - val_loss: 0.0571 - val_acc: 0.5810\n",
      "Epoch 480/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0532 - acc: 0.5995 - val_loss: 0.0588 - val_acc: 0.5578\n",
      "Epoch 481/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0538 - acc: 0.5865 - val_loss: 0.0584 - val_acc: 0.5636\n",
      "Epoch 482/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0536 - acc: 0.5948 - val_loss: 0.0593 - val_acc: 0.5520\n",
      "Epoch 483/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0540 - acc: 0.5884 - val_loss: 0.0588 - val_acc: 0.5594\n",
      "Epoch 484/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0538 - acc: 0.5895 - val_loss: 0.0578 - val_acc: 0.5786\n",
      "Epoch 485/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0536 - acc: 0.5892 - val_loss: 0.0573 - val_acc: 0.5702\n",
      "Epoch 486/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0539 - acc: 0.5884 - val_loss: 0.0579 - val_acc: 0.5628\n",
      "Epoch 487/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0538 - acc: 0.5892 - val_loss: 0.0583 - val_acc: 0.5611\n",
      "Epoch 488/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0535 - acc: 0.5940 - val_loss: 0.0578 - val_acc: 0.5686\n",
      "Epoch 489/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0536 - acc: 0.5956 - val_loss: 0.0571 - val_acc: 0.5619\n",
      "Epoch 490/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0537 - acc: 0.5898 - val_loss: 0.0594 - val_acc: 0.5578\n",
      "Epoch 491/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0532 - acc: 0.5976 - val_loss: 0.0572 - val_acc: 0.5628\n",
      "Epoch 492/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0532 - acc: 0.5909 - val_loss: 0.0570 - val_acc: 0.5719\n",
      "Epoch 493/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0537 - acc: 0.5931 - val_loss: 0.0569 - val_acc: 0.5694\n",
      "Epoch 494/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0528 - acc: 0.6012 - val_loss: 0.0575 - val_acc: 0.5769\n",
      "Epoch 495/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0534 - acc: 0.5995 - val_loss: 0.0585 - val_acc: 0.5677\n",
      "Epoch 496/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0538 - acc: 0.5845 - val_loss: 0.0576 - val_acc: 0.5594\n",
      "Epoch 497/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0535 - acc: 0.5906 - val_loss: 0.0581 - val_acc: 0.5702\n",
      "Epoch 498/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0535 - acc: 0.5940 - val_loss: 0.0574 - val_acc: 0.5653\n",
      "Epoch 499/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0534 - acc: 0.5917 - val_loss: 0.0575 - val_acc: 0.5677\n",
      "Epoch 500/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0540 - acc: 0.5848 - val_loss: 0.0584 - val_acc: 0.5569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77ff6faef0>"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.summary()\n",
    "nn_model.add(Dense(units=32, activation='relu'))\n",
    "nn_model.add(Dense(units=16, activation='relu'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.02)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "nn_model.fit(x=train_x, y=train_y, epochs=500, validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17272
    },
    "colab_type": "code",
    "id": "ll4e5pCb_ffy",
    "outputId": "ad4d399c-aaa4-4bff-e923-83e07896f898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 1,754\n",
      "Trainable params: 1,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3608 samples, validate on 1203 samples\n",
      "Epoch 1/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0682 - acc: 0.4290 - val_loss: 0.0661 - val_acc: 0.4489\n",
      "Epoch 2/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0654 - acc: 0.4462 - val_loss: 0.0625 - val_acc: 0.5037\n",
      "Epoch 3/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0648 - acc: 0.4601 - val_loss: 0.0611 - val_acc: 0.4680\n",
      "Epoch 4/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0627 - acc: 0.4789 - val_loss: 0.0589 - val_acc: 0.5229\n",
      "Epoch 5/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0630 - acc: 0.4884 - val_loss: 0.0599 - val_acc: 0.5353\n",
      "Epoch 6/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0626 - acc: 0.4903 - val_loss: 0.0634 - val_acc: 0.5012\n",
      "Epoch 7/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0623 - acc: 0.5019 - val_loss: 0.0579 - val_acc: 0.5528\n",
      "Epoch 8/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0621 - acc: 0.5125 - val_loss: 0.0586 - val_acc: 0.5428\n",
      "Epoch 9/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0599 - acc: 0.5316 - val_loss: 0.0576 - val_acc: 0.5478\n",
      "Epoch 10/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0612 - acc: 0.5042 - val_loss: 0.0606 - val_acc: 0.5004\n",
      "Epoch 11/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0603 - acc: 0.5227 - val_loss: 0.0674 - val_acc: 0.3973\n",
      "Epoch 12/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0602 - acc: 0.5369 - val_loss: 0.0572 - val_acc: 0.5553\n",
      "Epoch 13/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0601 - acc: 0.5255 - val_loss: 0.0577 - val_acc: 0.5495\n",
      "Epoch 14/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0604 - acc: 0.5252 - val_loss: 0.0570 - val_acc: 0.5536\n",
      "Epoch 15/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0612 - acc: 0.5122 - val_loss: 0.0578 - val_acc: 0.5445\n",
      "Epoch 16/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0600 - acc: 0.5305 - val_loss: 0.0582 - val_acc: 0.5528\n",
      "Epoch 17/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0592 - acc: 0.5416 - val_loss: 0.0573 - val_acc: 0.5553\n",
      "Epoch 18/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0601 - acc: 0.5313 - val_loss: 0.0571 - val_acc: 0.5694\n",
      "Epoch 19/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0593 - acc: 0.5349 - val_loss: 0.0605 - val_acc: 0.5337\n",
      "Epoch 20/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0598 - acc: 0.5310 - val_loss: 0.0589 - val_acc: 0.5478\n",
      "Epoch 21/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0591 - acc: 0.5399 - val_loss: 0.0572 - val_acc: 0.5611\n",
      "Epoch 22/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0592 - acc: 0.5344 - val_loss: 0.0591 - val_acc: 0.5320\n",
      "Epoch 23/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0595 - acc: 0.5313 - val_loss: 0.0572 - val_acc: 0.5661\n",
      "Epoch 24/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0594 - acc: 0.5413 - val_loss: 0.0590 - val_acc: 0.5037\n",
      "Epoch 25/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0588 - acc: 0.5410 - val_loss: 0.0564 - val_acc: 0.5727\n",
      "Epoch 26/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0589 - acc: 0.5377 - val_loss: 0.0559 - val_acc: 0.5727\n",
      "Epoch 27/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0593 - acc: 0.5330 - val_loss: 0.0583 - val_acc: 0.5619\n",
      "Epoch 28/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0594 - acc: 0.5419 - val_loss: 0.0569 - val_acc: 0.5702\n",
      "Epoch 29/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0604 - acc: 0.5261 - val_loss: 0.0579 - val_acc: 0.5328\n",
      "Epoch 30/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0591 - acc: 0.5385 - val_loss: 0.0569 - val_acc: 0.5711\n",
      "Epoch 31/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0588 - acc: 0.5421 - val_loss: 0.0567 - val_acc: 0.5644\n",
      "Epoch 32/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0586 - acc: 0.5438 - val_loss: 0.0568 - val_acc: 0.5752\n",
      "Epoch 33/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0599 - acc: 0.5324 - val_loss: 0.0576 - val_acc: 0.5561\n",
      "Epoch 34/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0597 - acc: 0.5410 - val_loss: 0.0600 - val_acc: 0.5520\n",
      "Epoch 35/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0599 - acc: 0.5319 - val_loss: 0.0563 - val_acc: 0.5644\n",
      "Epoch 36/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0586 - acc: 0.5430 - val_loss: 0.0563 - val_acc: 0.5752\n",
      "Epoch 37/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0589 - acc: 0.5427 - val_loss: 0.0652 - val_acc: 0.4480\n",
      "Epoch 38/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0594 - acc: 0.5374 - val_loss: 0.0583 - val_acc: 0.5578\n",
      "Epoch 39/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0591 - acc: 0.5363 - val_loss: 0.0572 - val_acc: 0.5603\n",
      "Epoch 40/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0594 - acc: 0.5405 - val_loss: 0.0567 - val_acc: 0.5711\n",
      "Epoch 41/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0590 - acc: 0.5419 - val_loss: 0.0570 - val_acc: 0.5744\n",
      "Epoch 42/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0593 - acc: 0.5382 - val_loss: 0.0567 - val_acc: 0.5669\n",
      "Epoch 43/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0587 - acc: 0.5513 - val_loss: 0.0575 - val_acc: 0.5544\n",
      "Epoch 44/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0589 - acc: 0.5443 - val_loss: 0.0577 - val_acc: 0.5553\n",
      "Epoch 45/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0597 - acc: 0.5352 - val_loss: 0.0582 - val_acc: 0.5653\n",
      "Epoch 46/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0611 - acc: 0.5291 - val_loss: 0.0616 - val_acc: 0.5270\n",
      "Epoch 47/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0594 - acc: 0.5330 - val_loss: 0.0577 - val_acc: 0.5569\n",
      "Epoch 48/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0593 - acc: 0.5396 - val_loss: 0.0581 - val_acc: 0.5628\n",
      "Epoch 49/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0595 - acc: 0.5330 - val_loss: 0.0583 - val_acc: 0.5320\n",
      "Epoch 50/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0587 - acc: 0.5385 - val_loss: 0.0584 - val_acc: 0.5312\n",
      "Epoch 51/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0595 - acc: 0.5349 - val_loss: 0.0574 - val_acc: 0.5702\n",
      "Epoch 52/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0590 - acc: 0.5396 - val_loss: 0.0584 - val_acc: 0.5586\n",
      "Epoch 53/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0604 - acc: 0.5244 - val_loss: 0.0581 - val_acc: 0.5478\n",
      "Epoch 54/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0586 - acc: 0.5485 - val_loss: 0.0584 - val_acc: 0.5636\n",
      "Epoch 55/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0589 - acc: 0.5493 - val_loss: 0.0587 - val_acc: 0.5395\n",
      "Epoch 56/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0589 - acc: 0.5499 - val_loss: 0.0572 - val_acc: 0.5628\n",
      "Epoch 57/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0590 - acc: 0.5485 - val_loss: 0.0571 - val_acc: 0.5686\n",
      "Epoch 58/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0585 - acc: 0.5504 - val_loss: 0.0571 - val_acc: 0.5686\n",
      "Epoch 59/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0586 - acc: 0.5438 - val_loss: 0.0591 - val_acc: 0.5262\n",
      "Epoch 60/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0606 - acc: 0.5255 - val_loss: 0.0579 - val_acc: 0.5661\n",
      "Epoch 61/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0583 - acc: 0.5502 - val_loss: 0.0572 - val_acc: 0.5603\n",
      "Epoch 62/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0583 - acc: 0.5396 - val_loss: 0.0561 - val_acc: 0.5719\n",
      "Epoch 63/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0585 - acc: 0.5485 - val_loss: 0.0595 - val_acc: 0.5262\n",
      "Epoch 64/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0589 - acc: 0.5416 - val_loss: 0.0577 - val_acc: 0.5536\n",
      "Epoch 65/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0604 - acc: 0.5266 - val_loss: 0.0563 - val_acc: 0.5744\n",
      "Epoch 66/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0593 - acc: 0.5313 - val_loss: 0.0581 - val_acc: 0.5520\n",
      "Epoch 67/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0588 - acc: 0.5438 - val_loss: 0.0566 - val_acc: 0.5827\n",
      "Epoch 68/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0584 - acc: 0.5477 - val_loss: 0.0621 - val_acc: 0.5046\n",
      "Epoch 69/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0586 - acc: 0.5532 - val_loss: 0.0563 - val_acc: 0.5719\n",
      "Epoch 70/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0584 - acc: 0.5482 - val_loss: 0.0573 - val_acc: 0.5644\n",
      "Epoch 71/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0585 - acc: 0.5507 - val_loss: 0.0574 - val_acc: 0.5470\n",
      "Epoch 72/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0588 - acc: 0.5407 - val_loss: 0.0564 - val_acc: 0.5661\n",
      "Epoch 73/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0584 - acc: 0.5496 - val_loss: 0.0571 - val_acc: 0.5603\n",
      "Epoch 74/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0593 - acc: 0.5430 - val_loss: 0.0577 - val_acc: 0.5669\n",
      "Epoch 75/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0586 - acc: 0.5460 - val_loss: 0.0591 - val_acc: 0.5245\n",
      "Epoch 76/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0584 - acc: 0.5479 - val_loss: 0.0577 - val_acc: 0.5702\n",
      "Epoch 77/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0584 - acc: 0.5538 - val_loss: 0.0587 - val_acc: 0.5495\n",
      "Epoch 78/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0588 - acc: 0.5441 - val_loss: 0.0570 - val_acc: 0.5686\n",
      "Epoch 79/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0583 - acc: 0.5499 - val_loss: 0.0579 - val_acc: 0.5619\n",
      "Epoch 80/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0589 - acc: 0.5449 - val_loss: 0.0578 - val_acc: 0.5544\n",
      "Epoch 81/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0582 - acc: 0.5549 - val_loss: 0.0601 - val_acc: 0.5436\n",
      "Epoch 82/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0587 - acc: 0.5382 - val_loss: 0.0565 - val_acc: 0.5553\n",
      "Epoch 83/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0581 - acc: 0.5513 - val_loss: 0.0582 - val_acc: 0.5553\n",
      "Epoch 84/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0592 - acc: 0.5410 - val_loss: 0.0573 - val_acc: 0.5603\n",
      "Epoch 85/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0583 - acc: 0.5507 - val_loss: 0.0565 - val_acc: 0.5686\n",
      "Epoch 86/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0586 - acc: 0.5524 - val_loss: 0.0562 - val_acc: 0.5686\n",
      "Epoch 87/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0587 - acc: 0.5394 - val_loss: 0.0564 - val_acc: 0.5702\n",
      "Epoch 88/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0580 - acc: 0.5463 - val_loss: 0.0579 - val_acc: 0.5320\n",
      "Epoch 89/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0585 - acc: 0.5518 - val_loss: 0.0571 - val_acc: 0.5636\n",
      "Epoch 90/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0584 - acc: 0.5504 - val_loss: 0.0583 - val_acc: 0.5578\n",
      "Epoch 91/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0584 - acc: 0.5496 - val_loss: 0.0570 - val_acc: 0.5694\n",
      "Epoch 92/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0593 - acc: 0.5457 - val_loss: 0.0571 - val_acc: 0.5578\n",
      "Epoch 93/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0593 - acc: 0.5402 - val_loss: 0.0571 - val_acc: 0.5619\n",
      "Epoch 94/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0594 - acc: 0.5327 - val_loss: 0.0614 - val_acc: 0.4996\n",
      "Epoch 95/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0587 - acc: 0.5460 - val_loss: 0.0572 - val_acc: 0.5586\n",
      "Epoch 96/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0595 - acc: 0.5399 - val_loss: 0.0618 - val_acc: 0.5121\n",
      "Epoch 97/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0591 - acc: 0.5366 - val_loss: 0.0565 - val_acc: 0.5677\n",
      "Epoch 98/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0583 - acc: 0.5479 - val_loss: 0.0568 - val_acc: 0.5628\n",
      "Epoch 99/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0581 - acc: 0.5513 - val_loss: 0.0572 - val_acc: 0.5677\n",
      "Epoch 100/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0587 - acc: 0.5474 - val_loss: 0.0574 - val_acc: 0.5661\n",
      "Epoch 101/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0586 - acc: 0.5468 - val_loss: 0.0589 - val_acc: 0.5495\n",
      "Epoch 102/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0584 - acc: 0.5521 - val_loss: 0.0586 - val_acc: 0.5603\n",
      "Epoch 103/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0591 - acc: 0.5427 - val_loss: 0.0571 - val_acc: 0.5744\n",
      "Epoch 104/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0589 - acc: 0.5424 - val_loss: 0.0656 - val_acc: 0.4181\n",
      "Epoch 105/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0591 - acc: 0.5435 - val_loss: 0.0580 - val_acc: 0.5553\n",
      "Epoch 106/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0588 - acc: 0.5513 - val_loss: 0.0568 - val_acc: 0.5569\n",
      "Epoch 107/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0586 - acc: 0.5510 - val_loss: 0.0568 - val_acc: 0.5653\n",
      "Epoch 108/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0591 - acc: 0.5446 - val_loss: 0.0581 - val_acc: 0.5628\n",
      "Epoch 109/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0587 - acc: 0.5532 - val_loss: 0.0584 - val_acc: 0.5644\n",
      "Epoch 110/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0586 - acc: 0.5499 - val_loss: 0.0585 - val_acc: 0.5561\n",
      "Epoch 111/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0588 - acc: 0.5427 - val_loss: 0.0571 - val_acc: 0.5677\n",
      "Epoch 112/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0586 - acc: 0.5485 - val_loss: 0.0577 - val_acc: 0.5561\n",
      "Epoch 113/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0584 - acc: 0.5446 - val_loss: 0.0577 - val_acc: 0.5528\n",
      "Epoch 114/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0598 - acc: 0.5391 - val_loss: 0.0578 - val_acc: 0.5561\n",
      "Epoch 115/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0603 - acc: 0.5371 - val_loss: 0.0579 - val_acc: 0.5686\n",
      "Epoch 116/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0602 - acc: 0.5310 - val_loss: 0.0579 - val_acc: 0.5486\n",
      "Epoch 117/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0597 - acc: 0.5322 - val_loss: 0.0579 - val_acc: 0.5636\n",
      "Epoch 118/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0595 - acc: 0.5338 - val_loss: 0.0565 - val_acc: 0.5736\n",
      "Epoch 119/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0596 - acc: 0.5391 - val_loss: 0.0572 - val_acc: 0.5536\n",
      "Epoch 120/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0596 - acc: 0.5338 - val_loss: 0.0581 - val_acc: 0.5520\n",
      "Epoch 121/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0587 - acc: 0.5460 - val_loss: 0.0571 - val_acc: 0.5802\n",
      "Epoch 122/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0589 - acc: 0.5380 - val_loss: 0.0570 - val_acc: 0.5578\n",
      "Epoch 123/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0584 - acc: 0.5416 - val_loss: 0.0599 - val_acc: 0.5528\n",
      "Epoch 124/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0584 - acc: 0.5457 - val_loss: 0.0576 - val_acc: 0.5544\n",
      "Epoch 125/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0591 - acc: 0.5416 - val_loss: 0.0573 - val_acc: 0.5786\n",
      "Epoch 126/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0588 - acc: 0.5557 - val_loss: 0.0574 - val_acc: 0.5661\n",
      "Epoch 127/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0585 - acc: 0.5535 - val_loss: 0.0570 - val_acc: 0.5644\n",
      "Epoch 128/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0587 - acc: 0.5460 - val_loss: 0.0566 - val_acc: 0.5694\n",
      "Epoch 129/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0583 - acc: 0.5491 - val_loss: 0.0579 - val_acc: 0.5661\n",
      "Epoch 130/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0593 - acc: 0.5396 - val_loss: 0.0581 - val_acc: 0.5553\n",
      "Epoch 131/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0585 - acc: 0.5527 - val_loss: 0.0572 - val_acc: 0.5769\n",
      "Epoch 132/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0583 - acc: 0.5452 - val_loss: 0.0567 - val_acc: 0.5711\n",
      "Epoch 133/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0591 - acc: 0.5394 - val_loss: 0.0581 - val_acc: 0.5694\n",
      "Epoch 134/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0588 - acc: 0.5427 - val_loss: 0.0569 - val_acc: 0.5603\n",
      "Epoch 135/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0587 - acc: 0.5499 - val_loss: 0.0569 - val_acc: 0.5744\n",
      "Epoch 136/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0586 - acc: 0.5468 - val_loss: 0.0567 - val_acc: 0.5669\n",
      "Epoch 137/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0588 - acc: 0.5441 - val_loss: 0.0574 - val_acc: 0.5702\n",
      "Epoch 138/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0586 - acc: 0.5502 - val_loss: 0.0592 - val_acc: 0.5511\n",
      "Epoch 139/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0592 - acc: 0.5416 - val_loss: 0.0575 - val_acc: 0.5744\n",
      "Epoch 140/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0589 - acc: 0.5424 - val_loss: 0.0566 - val_acc: 0.5653\n",
      "Epoch 141/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0584 - acc: 0.5518 - val_loss: 0.0567 - val_acc: 0.5694\n",
      "Epoch 142/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0589 - acc: 0.5410 - val_loss: 0.0570 - val_acc: 0.5736\n",
      "Epoch 143/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0585 - acc: 0.5471 - val_loss: 0.0570 - val_acc: 0.5553\n",
      "Epoch 144/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0583 - acc: 0.5474 - val_loss: 0.0565 - val_acc: 0.5702\n",
      "Epoch 145/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0593 - acc: 0.5457 - val_loss: 0.0566 - val_acc: 0.5661\n",
      "Epoch 146/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0586 - acc: 0.5443 - val_loss: 0.0567 - val_acc: 0.5935\n",
      "Epoch 147/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0586 - acc: 0.5435 - val_loss: 0.0569 - val_acc: 0.5686\n",
      "Epoch 148/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0585 - acc: 0.5416 - val_loss: 0.0566 - val_acc: 0.5694\n",
      "Epoch 149/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0581 - acc: 0.5491 - val_loss: 0.0571 - val_acc: 0.5536\n",
      "Epoch 150/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0589 - acc: 0.5435 - val_loss: 0.0596 - val_acc: 0.5486\n",
      "Epoch 151/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0585 - acc: 0.5471 - val_loss: 0.0577 - val_acc: 0.5669\n",
      "Epoch 152/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0588 - acc: 0.5479 - val_loss: 0.0570 - val_acc: 0.5711\n",
      "Epoch 153/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0585 - acc: 0.5430 - val_loss: 0.0569 - val_acc: 0.5628\n",
      "Epoch 154/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0591 - acc: 0.5421 - val_loss: 0.0584 - val_acc: 0.5644\n",
      "Epoch 155/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0590 - acc: 0.5496 - val_loss: 0.0563 - val_acc: 0.5644\n",
      "Epoch 156/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0583 - acc: 0.5535 - val_loss: 0.0572 - val_acc: 0.5686\n",
      "Epoch 157/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0582 - acc: 0.5471 - val_loss: 0.0570 - val_acc: 0.5794\n",
      "Epoch 158/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0583 - acc: 0.5479 - val_loss: 0.0570 - val_acc: 0.5586\n",
      "Epoch 159/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0584 - acc: 0.5479 - val_loss: 0.0569 - val_acc: 0.5619\n",
      "Epoch 160/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0583 - acc: 0.5516 - val_loss: 0.0566 - val_acc: 0.5661\n",
      "Epoch 161/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0582 - acc: 0.5474 - val_loss: 0.0565 - val_acc: 0.5677\n",
      "Epoch 162/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0583 - acc: 0.5441 - val_loss: 0.0567 - val_acc: 0.5636\n",
      "Epoch 163/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0590 - acc: 0.5360 - val_loss: 0.0591 - val_acc: 0.5370\n",
      "Epoch 164/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0583 - acc: 0.5413 - val_loss: 0.0567 - val_acc: 0.5636\n",
      "Epoch 165/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0592 - acc: 0.5419 - val_loss: 0.0570 - val_acc: 0.5644\n",
      "Epoch 166/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0586 - acc: 0.5449 - val_loss: 0.0569 - val_acc: 0.5569\n",
      "Epoch 167/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0585 - acc: 0.5460 - val_loss: 0.0567 - val_acc: 0.5603\n",
      "Epoch 168/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0584 - acc: 0.5435 - val_loss: 0.0594 - val_acc: 0.5411\n",
      "Epoch 169/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0594 - acc: 0.5449 - val_loss: 0.0571 - val_acc: 0.5686\n",
      "Epoch 170/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0585 - acc: 0.5477 - val_loss: 0.0566 - val_acc: 0.5744\n",
      "Epoch 171/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0586 - acc: 0.5432 - val_loss: 0.0562 - val_acc: 0.5769\n",
      "Epoch 172/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0586 - acc: 0.5424 - val_loss: 0.0565 - val_acc: 0.5736\n",
      "Epoch 173/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0588 - acc: 0.5441 - val_loss: 0.0594 - val_acc: 0.5403\n",
      "Epoch 174/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0601 - acc: 0.5261 - val_loss: 0.0580 - val_acc: 0.5561\n",
      "Epoch 175/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0594 - acc: 0.5421 - val_loss: 0.0585 - val_acc: 0.5428\n",
      "Epoch 176/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0591 - acc: 0.5377 - val_loss: 0.0572 - val_acc: 0.5511\n",
      "Epoch 177/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0582 - acc: 0.5488 - val_loss: 0.0575 - val_acc: 0.5586\n",
      "Epoch 178/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0584 - acc: 0.5396 - val_loss: 0.0572 - val_acc: 0.5694\n",
      "Epoch 179/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0583 - acc: 0.5527 - val_loss: 0.0576 - val_acc: 0.5420\n",
      "Epoch 180/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0592 - acc: 0.5358 - val_loss: 0.0578 - val_acc: 0.5528\n",
      "Epoch 181/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0586 - acc: 0.5482 - val_loss: 0.0563 - val_acc: 0.5769\n",
      "Epoch 182/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0584 - acc: 0.5516 - val_loss: 0.0586 - val_acc: 0.5569\n",
      "Epoch 183/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0591 - acc: 0.5377 - val_loss: 0.0578 - val_acc: 0.5495\n",
      "Epoch 184/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0583 - acc: 0.5491 - val_loss: 0.0569 - val_acc: 0.5603\n",
      "Epoch 185/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0587 - acc: 0.5443 - val_loss: 0.0574 - val_acc: 0.5786\n",
      "Epoch 186/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0585 - acc: 0.5463 - val_loss: 0.0571 - val_acc: 0.5669\n",
      "Epoch 187/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0581 - acc: 0.5513 - val_loss: 0.0572 - val_acc: 0.5586\n",
      "Epoch 188/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0587 - acc: 0.5358 - val_loss: 0.0568 - val_acc: 0.5536\n",
      "Epoch 189/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0586 - acc: 0.5394 - val_loss: 0.0572 - val_acc: 0.5702\n",
      "Epoch 190/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0599 - acc: 0.5261 - val_loss: 0.0573 - val_acc: 0.5611\n",
      "Epoch 191/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0583 - acc: 0.5474 - val_loss: 0.0595 - val_acc: 0.5503\n",
      "Epoch 192/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0591 - acc: 0.5366 - val_loss: 0.0582 - val_acc: 0.5677\n",
      "Epoch 193/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0588 - acc: 0.5360 - val_loss: 0.0589 - val_acc: 0.5619\n",
      "Epoch 194/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0591 - acc: 0.5399 - val_loss: 0.0586 - val_acc: 0.5520\n",
      "Epoch 195/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0583 - acc: 0.5430 - val_loss: 0.0569 - val_acc: 0.5603\n",
      "Epoch 196/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0581 - acc: 0.5507 - val_loss: 0.0575 - val_acc: 0.5470\n",
      "Epoch 197/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0584 - acc: 0.5424 - val_loss: 0.0579 - val_acc: 0.5486\n",
      "Epoch 198/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0585 - acc: 0.5471 - val_loss: 0.0581 - val_acc: 0.5495\n",
      "Epoch 199/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0588 - acc: 0.5441 - val_loss: 0.0577 - val_acc: 0.5619\n",
      "Epoch 200/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0591 - acc: 0.5407 - val_loss: 0.0574 - val_acc: 0.5694\n",
      "Epoch 201/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0579 - acc: 0.5571 - val_loss: 0.0566 - val_acc: 0.5711\n",
      "Epoch 202/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0586 - acc: 0.5441 - val_loss: 0.0565 - val_acc: 0.5495\n",
      "Epoch 203/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0588 - acc: 0.5396 - val_loss: 0.0577 - val_acc: 0.5586\n",
      "Epoch 204/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0584 - acc: 0.5491 - val_loss: 0.0576 - val_acc: 0.5727\n",
      "Epoch 205/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0593 - acc: 0.5355 - val_loss: 0.0571 - val_acc: 0.5536\n",
      "Epoch 206/500\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0584 - acc: 0.5479 - val_loss: 0.0562 - val_acc: 0.5628\n",
      "Epoch 207/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0584 - acc: 0.5471 - val_loss: 0.0566 - val_acc: 0.5603\n",
      "Epoch 208/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0583 - acc: 0.5493 - val_loss: 0.0577 - val_acc: 0.5752\n",
      "Epoch 209/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0581 - acc: 0.5527 - val_loss: 0.0578 - val_acc: 0.5578\n",
      "Epoch 210/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0581 - acc: 0.5474 - val_loss: 0.0571 - val_acc: 0.5869\n",
      "Epoch 211/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0590 - acc: 0.5502 - val_loss: 0.0569 - val_acc: 0.5752\n",
      "Epoch 212/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0583 - acc: 0.5510 - val_loss: 0.0566 - val_acc: 0.5653\n",
      "Epoch 213/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0586 - acc: 0.5443 - val_loss: 0.0577 - val_acc: 0.5711\n",
      "Epoch 214/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0588 - acc: 0.5452 - val_loss: 0.0604 - val_acc: 0.5254\n",
      "Epoch 215/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0590 - acc: 0.5396 - val_loss: 0.0577 - val_acc: 0.5603\n",
      "Epoch 216/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0589 - acc: 0.5435 - val_loss: 0.0576 - val_acc: 0.5719\n",
      "Epoch 217/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0591 - acc: 0.5388 - val_loss: 0.0580 - val_acc: 0.5428\n",
      "Epoch 218/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0592 - acc: 0.5327 - val_loss: 0.0591 - val_acc: 0.5478\n",
      "Epoch 219/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0597 - acc: 0.5352 - val_loss: 0.0583 - val_acc: 0.5278\n",
      "Epoch 220/500\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0597 - acc: 0.5310 - val_loss: 0.0569 - val_acc: 0.5761\n",
      "Epoch 221/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0596 - acc: 0.5366 - val_loss: 0.0572 - val_acc: 0.5777\n",
      "Epoch 222/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0595 - acc: 0.5333 - val_loss: 0.0600 - val_acc: 0.5445\n",
      "Epoch 223/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0600 - acc: 0.5299 - val_loss: 0.0575 - val_acc: 0.5603\n",
      "Epoch 224/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0596 - acc: 0.5338 - val_loss: 0.0570 - val_acc: 0.5711\n",
      "Epoch 225/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0598 - acc: 0.5327 - val_loss: 0.0576 - val_acc: 0.5586\n",
      "Epoch 226/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0595 - acc: 0.5366 - val_loss: 0.0574 - val_acc: 0.5736\n",
      "Epoch 227/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0594 - acc: 0.5360 - val_loss: 0.0582 - val_acc: 0.5486\n",
      "Epoch 228/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0605 - acc: 0.5299 - val_loss: 0.0586 - val_acc: 0.5387\n",
      "Epoch 229/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0600 - acc: 0.5346 - val_loss: 0.0573 - val_acc: 0.5569\n",
      "Epoch 230/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0593 - acc: 0.5316 - val_loss: 0.0578 - val_acc: 0.5478\n",
      "Epoch 231/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0600 - acc: 0.5327 - val_loss: 0.0571 - val_acc: 0.5777\n",
      "Epoch 232/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0595 - acc: 0.5291 - val_loss: 0.0583 - val_acc: 0.5436\n",
      "Epoch 233/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0597 - acc: 0.5324 - val_loss: 0.0576 - val_acc: 0.5686\n",
      "Epoch 234/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0599 - acc: 0.5280 - val_loss: 0.0578 - val_acc: 0.5486\n",
      "Epoch 235/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0591 - acc: 0.5394 - val_loss: 0.0582 - val_acc: 0.5486\n",
      "Epoch 236/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0598 - acc: 0.5327 - val_loss: 0.0584 - val_acc: 0.5420\n",
      "Epoch 237/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0596 - acc: 0.5322 - val_loss: 0.0589 - val_acc: 0.5544\n",
      "Epoch 238/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0597 - acc: 0.5341 - val_loss: 0.0573 - val_acc: 0.5644\n",
      "Epoch 239/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0597 - acc: 0.5358 - val_loss: 0.0578 - val_acc: 0.5553\n",
      "Epoch 240/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0600 - acc: 0.5280 - val_loss: 0.0578 - val_acc: 0.5520\n",
      "Epoch 241/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0593 - acc: 0.5360 - val_loss: 0.0579 - val_acc: 0.5553\n",
      "Epoch 242/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0597 - acc: 0.5355 - val_loss: 0.0592 - val_acc: 0.5362\n",
      "Epoch 243/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0598 - acc: 0.5277 - val_loss: 0.0592 - val_acc: 0.5303\n",
      "Epoch 244/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0598 - acc: 0.5355 - val_loss: 0.0586 - val_acc: 0.5628\n",
      "Epoch 245/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0601 - acc: 0.5294 - val_loss: 0.0578 - val_acc: 0.5719\n",
      "Epoch 246/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0599 - acc: 0.5355 - val_loss: 0.0572 - val_acc: 0.5719\n",
      "Epoch 247/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0594 - acc: 0.5335 - val_loss: 0.0585 - val_acc: 0.5628\n",
      "Epoch 248/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0592 - acc: 0.5344 - val_loss: 0.0574 - val_acc: 0.5528\n",
      "Epoch 249/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0590 - acc: 0.5405 - val_loss: 0.0585 - val_acc: 0.5486\n",
      "Epoch 250/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0596 - acc: 0.5308 - val_loss: 0.0573 - val_acc: 0.5553\n",
      "Epoch 251/500\n",
      "3608/3608 [==============================] - 1s 159us/step - loss: 0.0593 - acc: 0.5399 - val_loss: 0.0601 - val_acc: 0.5420\n",
      "Epoch 252/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0593 - acc: 0.5341 - val_loss: 0.0582 - val_acc: 0.5461\n",
      "Epoch 253/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0594 - acc: 0.5316 - val_loss: 0.0571 - val_acc: 0.5736\n",
      "Epoch 254/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0594 - acc: 0.5371 - val_loss: 0.0573 - val_acc: 0.5544\n",
      "Epoch 255/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0592 - acc: 0.5322 - val_loss: 0.0578 - val_acc: 0.5453\n",
      "Epoch 256/500\n",
      "3608/3608 [==============================] - 1s 192us/step - loss: 0.0596 - acc: 0.5338 - val_loss: 0.0578 - val_acc: 0.5511\n",
      "Epoch 257/500\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0596 - acc: 0.5308 - val_loss: 0.0594 - val_acc: 0.5520\n",
      "Epoch 258/500\n",
      "3608/3608 [==============================] - 1s 218us/step - loss: 0.0595 - acc: 0.5346 - val_loss: 0.0597 - val_acc: 0.5278\n",
      "Epoch 259/500\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0591 - acc: 0.5352 - val_loss: 0.0580 - val_acc: 0.5470\n",
      "Epoch 260/500\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0591 - acc: 0.5366 - val_loss: 0.0571 - val_acc: 0.5694\n",
      "Epoch 261/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0593 - acc: 0.5394 - val_loss: 0.0576 - val_acc: 0.5553\n",
      "Epoch 262/500\n",
      "3608/3608 [==============================] - 1s 218us/step - loss: 0.0599 - acc: 0.5358 - val_loss: 0.0575 - val_acc: 0.5470\n",
      "Epoch 263/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0593 - acc: 0.5394 - val_loss: 0.0576 - val_acc: 0.5528\n",
      "Epoch 264/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0591 - acc: 0.5344 - val_loss: 0.0570 - val_acc: 0.5736\n",
      "Epoch 265/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0591 - acc: 0.5382 - val_loss: 0.0574 - val_acc: 0.5544\n",
      "Epoch 266/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0590 - acc: 0.5360 - val_loss: 0.0574 - val_acc: 0.5644\n",
      "Epoch 267/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0596 - acc: 0.5322 - val_loss: 0.0571 - val_acc: 0.5594\n",
      "Epoch 268/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0602 - acc: 0.5200 - val_loss: 0.0640 - val_acc: 0.4913\n",
      "Epoch 269/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0597 - acc: 0.5258 - val_loss: 0.0584 - val_acc: 0.5578\n",
      "Epoch 270/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0597 - acc: 0.5358 - val_loss: 0.0576 - val_acc: 0.5561\n",
      "Epoch 271/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0591 - acc: 0.5355 - val_loss: 0.0580 - val_acc: 0.5486\n",
      "Epoch 272/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0595 - acc: 0.5366 - val_loss: 0.0574 - val_acc: 0.5536\n",
      "Epoch 273/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0592 - acc: 0.5380 - val_loss: 0.0573 - val_acc: 0.5711\n",
      "Epoch 274/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0590 - acc: 0.5382 - val_loss: 0.0573 - val_acc: 0.5686\n",
      "Epoch 275/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0591 - acc: 0.5369 - val_loss: 0.0581 - val_acc: 0.5702\n",
      "Epoch 276/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0596 - acc: 0.5308 - val_loss: 0.0584 - val_acc: 0.5436\n",
      "Epoch 277/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0592 - acc: 0.5355 - val_loss: 0.0591 - val_acc: 0.5544\n",
      "Epoch 278/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0594 - acc: 0.5305 - val_loss: 0.0580 - val_acc: 0.5520\n",
      "Epoch 279/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0594 - acc: 0.5299 - val_loss: 0.0583 - val_acc: 0.5628\n",
      "Epoch 280/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0594 - acc: 0.5322 - val_loss: 0.0580 - val_acc: 0.5345\n",
      "Epoch 281/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0602 - acc: 0.5216 - val_loss: 0.0583 - val_acc: 0.5486\n",
      "Epoch 282/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0595 - acc: 0.5371 - val_loss: 0.0591 - val_acc: 0.5544\n",
      "Epoch 283/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0595 - acc: 0.5299 - val_loss: 0.0580 - val_acc: 0.5436\n",
      "Epoch 284/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0593 - acc: 0.5333 - val_loss: 0.0579 - val_acc: 0.5428\n",
      "Epoch 285/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0592 - acc: 0.5335 - val_loss: 0.0576 - val_acc: 0.5536\n",
      "Epoch 286/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0595 - acc: 0.5308 - val_loss: 0.0573 - val_acc: 0.5561\n",
      "Epoch 287/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0592 - acc: 0.5374 - val_loss: 0.0574 - val_acc: 0.5536\n",
      "Epoch 288/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0594 - acc: 0.5369 - val_loss: 0.0601 - val_acc: 0.5220\n",
      "Epoch 289/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0594 - acc: 0.5294 - val_loss: 0.0577 - val_acc: 0.5661\n",
      "Epoch 290/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0594 - acc: 0.5313 - val_loss: 0.0577 - val_acc: 0.5544\n",
      "Epoch 291/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0593 - acc: 0.5313 - val_loss: 0.0601 - val_acc: 0.5245\n",
      "Epoch 292/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0591 - acc: 0.5302 - val_loss: 0.0590 - val_acc: 0.5461\n",
      "Epoch 293/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0595 - acc: 0.5283 - val_loss: 0.0580 - val_acc: 0.5503\n",
      "Epoch 294/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0591 - acc: 0.5391 - val_loss: 0.0592 - val_acc: 0.5603\n",
      "Epoch 295/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0592 - acc: 0.5410 - val_loss: 0.0579 - val_acc: 0.5495\n",
      "Epoch 296/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0594 - acc: 0.5324 - val_loss: 0.0591 - val_acc: 0.5378\n",
      "Epoch 297/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0590 - acc: 0.5319 - val_loss: 0.0580 - val_acc: 0.5702\n",
      "Epoch 298/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0590 - acc: 0.5327 - val_loss: 0.0577 - val_acc: 0.5636\n",
      "Epoch 299/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0595 - acc: 0.5341 - val_loss: 0.0579 - val_acc: 0.5636\n",
      "Epoch 300/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0597 - acc: 0.5283 - val_loss: 0.0574 - val_acc: 0.5677\n",
      "Epoch 301/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0596 - acc: 0.5305 - val_loss: 0.0577 - val_acc: 0.5511\n",
      "Epoch 302/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0591 - acc: 0.5346 - val_loss: 0.0592 - val_acc: 0.5619\n",
      "Epoch 303/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0593 - acc: 0.5358 - val_loss: 0.0576 - val_acc: 0.5669\n",
      "Epoch 304/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0593 - acc: 0.5308 - val_loss: 0.0585 - val_acc: 0.5503\n",
      "Epoch 305/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0591 - acc: 0.5360 - val_loss: 0.0574 - val_acc: 0.5669\n",
      "Epoch 306/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0592 - acc: 0.5341 - val_loss: 0.0574 - val_acc: 0.5511\n",
      "Epoch 307/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0596 - acc: 0.5299 - val_loss: 0.0588 - val_acc: 0.5636\n",
      "Epoch 308/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0589 - acc: 0.5374 - val_loss: 0.0584 - val_acc: 0.5553\n",
      "Epoch 309/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0592 - acc: 0.5385 - val_loss: 0.0589 - val_acc: 0.5362\n",
      "Epoch 310/500\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0591 - acc: 0.5360 - val_loss: 0.0570 - val_acc: 0.5578\n",
      "Epoch 311/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0592 - acc: 0.5280 - val_loss: 0.0571 - val_acc: 0.5744\n",
      "Epoch 312/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0596 - acc: 0.5261 - val_loss: 0.0624 - val_acc: 0.5096\n",
      "Epoch 313/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0593 - acc: 0.5291 - val_loss: 0.0581 - val_acc: 0.5520\n",
      "Epoch 314/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0593 - acc: 0.5330 - val_loss: 0.0574 - val_acc: 0.5520\n",
      "Epoch 315/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0591 - acc: 0.5280 - val_loss: 0.0580 - val_acc: 0.5503\n",
      "Epoch 316/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0588 - acc: 0.5346 - val_loss: 0.0575 - val_acc: 0.5528\n",
      "Epoch 317/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0591 - acc: 0.5269 - val_loss: 0.0573 - val_acc: 0.5669\n",
      "Epoch 318/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0592 - acc: 0.5399 - val_loss: 0.0581 - val_acc: 0.5561\n",
      "Epoch 319/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0592 - acc: 0.5308 - val_loss: 0.0585 - val_acc: 0.5204\n",
      "Epoch 320/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0589 - acc: 0.5341 - val_loss: 0.0591 - val_acc: 0.5245\n",
      "Epoch 321/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0590 - acc: 0.5305 - val_loss: 0.0581 - val_acc: 0.5653\n",
      "Epoch 322/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0607 - acc: 0.5188 - val_loss: 0.0584 - val_acc: 0.5536\n",
      "Epoch 323/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0598 - acc: 0.5261 - val_loss: 0.0577 - val_acc: 0.5544\n",
      "Epoch 324/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0595 - acc: 0.5346 - val_loss: 0.0586 - val_acc: 0.5569\n",
      "Epoch 325/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0594 - acc: 0.5266 - val_loss: 0.0581 - val_acc: 0.5486\n",
      "Epoch 326/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0592 - acc: 0.5294 - val_loss: 0.0571 - val_acc: 0.5586\n",
      "Epoch 327/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0595 - acc: 0.5310 - val_loss: 0.0592 - val_acc: 0.5287\n",
      "Epoch 328/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0595 - acc: 0.5302 - val_loss: 0.0597 - val_acc: 0.5337\n",
      "Epoch 329/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0596 - acc: 0.5297 - val_loss: 0.0585 - val_acc: 0.5453\n",
      "Epoch 330/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0597 - acc: 0.5330 - val_loss: 0.0581 - val_acc: 0.5503\n",
      "Epoch 331/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0594 - acc: 0.5310 - val_loss: 0.0580 - val_acc: 0.5603\n",
      "Epoch 332/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0593 - acc: 0.5330 - val_loss: 0.0586 - val_acc: 0.5578\n",
      "Epoch 333/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0596 - acc: 0.5266 - val_loss: 0.0601 - val_acc: 0.5445\n",
      "Epoch 334/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0594 - acc: 0.5294 - val_loss: 0.0573 - val_acc: 0.5677\n",
      "Epoch 335/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0595 - acc: 0.5272 - val_loss: 0.0581 - val_acc: 0.5495\n",
      "Epoch 336/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0595 - acc: 0.5277 - val_loss: 0.0582 - val_acc: 0.5403\n",
      "Epoch 337/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0598 - acc: 0.5269 - val_loss: 0.0589 - val_acc: 0.5353\n",
      "Epoch 338/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0602 - acc: 0.5274 - val_loss: 0.0582 - val_acc: 0.5520\n",
      "Epoch 339/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0594 - acc: 0.5285 - val_loss: 0.0583 - val_acc: 0.5495\n",
      "Epoch 340/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0599 - acc: 0.5299 - val_loss: 0.0582 - val_acc: 0.5611\n",
      "Epoch 341/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0597 - acc: 0.5297 - val_loss: 0.0591 - val_acc: 0.5503\n",
      "Epoch 342/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0601 - acc: 0.5263 - val_loss: 0.0603 - val_acc: 0.5195\n",
      "Epoch 343/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0598 - acc: 0.5280 - val_loss: 0.0573 - val_acc: 0.5702\n",
      "Epoch 344/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0596 - acc: 0.5335 - val_loss: 0.0606 - val_acc: 0.5278\n",
      "Epoch 345/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0596 - acc: 0.5244 - val_loss: 0.0574 - val_acc: 0.5536\n",
      "Epoch 346/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0594 - acc: 0.5299 - val_loss: 0.0577 - val_acc: 0.5561\n",
      "Epoch 347/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0591 - acc: 0.5346 - val_loss: 0.0577 - val_acc: 0.5578\n",
      "Epoch 348/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0594 - acc: 0.5294 - val_loss: 0.0573 - val_acc: 0.5569\n",
      "Epoch 349/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0592 - acc: 0.5308 - val_loss: 0.0574 - val_acc: 0.5694\n",
      "Epoch 350/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0593 - acc: 0.5305 - val_loss: 0.0574 - val_acc: 0.5528\n",
      "Epoch 351/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0601 - acc: 0.5238 - val_loss: 0.0574 - val_acc: 0.5669\n",
      "Epoch 352/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0596 - acc: 0.5277 - val_loss: 0.0574 - val_acc: 0.5711\n",
      "Epoch 353/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0594 - acc: 0.5349 - val_loss: 0.0578 - val_acc: 0.5478\n",
      "Epoch 354/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0597 - acc: 0.5302 - val_loss: 0.0576 - val_acc: 0.5553\n",
      "Epoch 355/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0593 - acc: 0.5349 - val_loss: 0.0594 - val_acc: 0.5137\n",
      "Epoch 356/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0598 - acc: 0.5255 - val_loss: 0.0577 - val_acc: 0.5461\n",
      "Epoch 357/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0595 - acc: 0.5358 - val_loss: 0.0627 - val_acc: 0.5004\n",
      "Epoch 358/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0596 - acc: 0.5297 - val_loss: 0.0601 - val_acc: 0.5195\n",
      "Epoch 359/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0607 - acc: 0.5122 - val_loss: 0.0577 - val_acc: 0.5520\n",
      "Epoch 360/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0604 - acc: 0.5244 - val_loss: 0.0592 - val_acc: 0.5511\n",
      "Epoch 361/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0594 - acc: 0.5335 - val_loss: 0.0575 - val_acc: 0.5428\n",
      "Epoch 362/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0595 - acc: 0.5341 - val_loss: 0.0605 - val_acc: 0.5062\n",
      "Epoch 363/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0599 - acc: 0.5263 - val_loss: 0.0577 - val_acc: 0.5586\n",
      "Epoch 364/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0601 - acc: 0.5222 - val_loss: 0.0581 - val_acc: 0.5644\n",
      "Epoch 365/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0598 - acc: 0.5266 - val_loss: 0.0573 - val_acc: 0.5603\n",
      "Epoch 366/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0593 - acc: 0.5322 - val_loss: 0.0580 - val_acc: 0.5553\n",
      "Epoch 367/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0594 - acc: 0.5322 - val_loss: 0.0574 - val_acc: 0.5503\n",
      "Epoch 368/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0599 - acc: 0.5233 - val_loss: 0.0582 - val_acc: 0.5337\n",
      "Epoch 369/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0596 - acc: 0.5330 - val_loss: 0.0573 - val_acc: 0.5702\n",
      "Epoch 370/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0596 - acc: 0.5305 - val_loss: 0.0575 - val_acc: 0.5736\n",
      "Epoch 371/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0596 - acc: 0.5280 - val_loss: 0.0572 - val_acc: 0.5736\n",
      "Epoch 372/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0593 - acc: 0.5338 - val_loss: 0.0576 - val_acc: 0.5603\n",
      "Epoch 373/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0593 - acc: 0.5335 - val_loss: 0.0574 - val_acc: 0.5561\n",
      "Epoch 374/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0594 - acc: 0.5330 - val_loss: 0.0573 - val_acc: 0.5461\n",
      "Epoch 375/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0593 - acc: 0.5280 - val_loss: 0.0599 - val_acc: 0.5046\n",
      "Epoch 376/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0596 - acc: 0.5310 - val_loss: 0.0576 - val_acc: 0.5586\n",
      "Epoch 377/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0594 - acc: 0.5294 - val_loss: 0.0576 - val_acc: 0.5694\n",
      "Epoch 378/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0594 - acc: 0.5327 - val_loss: 0.0578 - val_acc: 0.5544\n",
      "Epoch 379/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0593 - acc: 0.5283 - val_loss: 0.0590 - val_acc: 0.5362\n",
      "Epoch 380/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0597 - acc: 0.5236 - val_loss: 0.0578 - val_acc: 0.5495\n",
      "Epoch 381/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0594 - acc: 0.5291 - val_loss: 0.0588 - val_acc: 0.5411\n",
      "Epoch 382/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0597 - acc: 0.5263 - val_loss: 0.0577 - val_acc: 0.5644\n",
      "Epoch 383/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0594 - acc: 0.5291 - val_loss: 0.0573 - val_acc: 0.5677\n",
      "Epoch 384/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0595 - acc: 0.5327 - val_loss: 0.0584 - val_acc: 0.5636\n",
      "Epoch 385/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0595 - acc: 0.5305 - val_loss: 0.0573 - val_acc: 0.5636\n",
      "Epoch 386/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0593 - acc: 0.5346 - val_loss: 0.0580 - val_acc: 0.5320\n",
      "Epoch 387/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0595 - acc: 0.5333 - val_loss: 0.0573 - val_acc: 0.5603\n",
      "Epoch 388/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0596 - acc: 0.5255 - val_loss: 0.0606 - val_acc: 0.5162\n",
      "Epoch 389/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0596 - acc: 0.5236 - val_loss: 0.0572 - val_acc: 0.5553\n",
      "Epoch 390/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0592 - acc: 0.5330 - val_loss: 0.0574 - val_acc: 0.5603\n",
      "Epoch 391/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0596 - acc: 0.5272 - val_loss: 0.0572 - val_acc: 0.5495\n",
      "Epoch 392/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0594 - acc: 0.5349 - val_loss: 0.0601 - val_acc: 0.5287\n",
      "Epoch 393/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0597 - acc: 0.5269 - val_loss: 0.0573 - val_acc: 0.5719\n",
      "Epoch 394/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0595 - acc: 0.5338 - val_loss: 0.0573 - val_acc: 0.5453\n",
      "Epoch 395/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0592 - acc: 0.5349 - val_loss: 0.0578 - val_acc: 0.5628\n",
      "Epoch 396/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0596 - acc: 0.5297 - val_loss: 0.0578 - val_acc: 0.5445\n",
      "Epoch 397/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0596 - acc: 0.5305 - val_loss: 0.0570 - val_acc: 0.5603\n",
      "Epoch 398/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0594 - acc: 0.5327 - val_loss: 0.0576 - val_acc: 0.5520\n",
      "Epoch 399/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0590 - acc: 0.5335 - val_loss: 0.0572 - val_acc: 0.5553\n",
      "Epoch 400/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0593 - acc: 0.5258 - val_loss: 0.0574 - val_acc: 0.5586\n",
      "Epoch 401/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0593 - acc: 0.5274 - val_loss: 0.0583 - val_acc: 0.5653\n",
      "Epoch 402/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0592 - acc: 0.5360 - val_loss: 0.0610 - val_acc: 0.5370\n",
      "Epoch 403/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0597 - acc: 0.5344 - val_loss: 0.0573 - val_acc: 0.5470\n",
      "Epoch 404/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0596 - acc: 0.5302 - val_loss: 0.0623 - val_acc: 0.4938\n",
      "Epoch 405/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0594 - acc: 0.5310 - val_loss: 0.0581 - val_acc: 0.5536\n",
      "Epoch 406/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0592 - acc: 0.5283 - val_loss: 0.0574 - val_acc: 0.5528\n",
      "Epoch 407/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0592 - acc: 0.5335 - val_loss: 0.0577 - val_acc: 0.5578\n",
      "Epoch 408/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0594 - acc: 0.5324 - val_loss: 0.0584 - val_acc: 0.5661\n",
      "Epoch 409/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0594 - acc: 0.5316 - val_loss: 0.0583 - val_acc: 0.5486\n",
      "Epoch 410/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0597 - acc: 0.5291 - val_loss: 0.0583 - val_acc: 0.5544\n",
      "Epoch 411/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0596 - acc: 0.5305 - val_loss: 0.0593 - val_acc: 0.5461\n",
      "Epoch 412/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0595 - acc: 0.5294 - val_loss: 0.0584 - val_acc: 0.5495\n",
      "Epoch 413/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0594 - acc: 0.5313 - val_loss: 0.0578 - val_acc: 0.5461\n",
      "Epoch 414/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0599 - acc: 0.5225 - val_loss: 0.0586 - val_acc: 0.5403\n",
      "Epoch 415/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0600 - acc: 0.5277 - val_loss: 0.0581 - val_acc: 0.5561\n",
      "Epoch 416/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0594 - acc: 0.5299 - val_loss: 0.0580 - val_acc: 0.5470\n",
      "Epoch 417/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0602 - acc: 0.5211 - val_loss: 0.0574 - val_acc: 0.5520\n",
      "Epoch 418/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0596 - acc: 0.5274 - val_loss: 0.0575 - val_acc: 0.5661\n",
      "Epoch 419/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0592 - acc: 0.5355 - val_loss: 0.0570 - val_acc: 0.5511\n",
      "Epoch 420/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0594 - acc: 0.5333 - val_loss: 0.0583 - val_acc: 0.5578\n",
      "Epoch 421/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0595 - acc: 0.5261 - val_loss: 0.0578 - val_acc: 0.5686\n",
      "Epoch 422/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0594 - acc: 0.5316 - val_loss: 0.0577 - val_acc: 0.5603\n",
      "Epoch 423/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0594 - acc: 0.5333 - val_loss: 0.0575 - val_acc: 0.5603\n",
      "Epoch 424/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0591 - acc: 0.5366 - val_loss: 0.0575 - val_acc: 0.5569\n",
      "Epoch 425/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0599 - acc: 0.5258 - val_loss: 0.0585 - val_acc: 0.5520\n",
      "Epoch 426/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0592 - acc: 0.5349 - val_loss: 0.0576 - val_acc: 0.5628\n",
      "Epoch 427/500\n",
      "3608/3608 [==============================] - 1s 194us/step - loss: 0.0593 - acc: 0.5360 - val_loss: 0.0576 - val_acc: 0.5561\n",
      "Epoch 428/500\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0592 - acc: 0.5335 - val_loss: 0.0621 - val_acc: 0.5112\n",
      "Epoch 429/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0595 - acc: 0.5335 - val_loss: 0.0571 - val_acc: 0.5677\n",
      "Epoch 430/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0592 - acc: 0.5352 - val_loss: 0.0573 - val_acc: 0.5586\n",
      "Epoch 431/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0597 - acc: 0.5285 - val_loss: 0.0573 - val_acc: 0.5694\n",
      "Epoch 432/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0594 - acc: 0.5272 - val_loss: 0.0574 - val_acc: 0.5553\n",
      "Epoch 433/500\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0597 - acc: 0.5288 - val_loss: 0.0582 - val_acc: 0.5702\n",
      "Epoch 434/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0592 - acc: 0.5338 - val_loss: 0.0575 - val_acc: 0.5702\n",
      "Epoch 435/500\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0595 - acc: 0.5297 - val_loss: 0.0574 - val_acc: 0.5644\n",
      "Epoch 436/500\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0599 - acc: 0.5238 - val_loss: 0.0576 - val_acc: 0.5594\n",
      "Epoch 437/500\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0596 - acc: 0.5305 - val_loss: 0.0572 - val_acc: 0.5644\n",
      "Epoch 438/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0592 - acc: 0.5299 - val_loss: 0.0583 - val_acc: 0.5445\n",
      "Epoch 439/500\n",
      "3608/3608 [==============================] - 1s 207us/step - loss: 0.0590 - acc: 0.5335 - val_loss: 0.0581 - val_acc: 0.5520\n",
      "Epoch 440/500\n",
      "3608/3608 [==============================] - 1s 200us/step - loss: 0.0598 - acc: 0.5205 - val_loss: 0.0594 - val_acc: 0.5345\n",
      "Epoch 441/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0593 - acc: 0.5344 - val_loss: 0.0592 - val_acc: 0.5553\n",
      "Epoch 442/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0592 - acc: 0.5346 - val_loss: 0.0580 - val_acc: 0.5420\n",
      "Epoch 443/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0592 - acc: 0.5308 - val_loss: 0.0578 - val_acc: 0.5528\n",
      "Epoch 444/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0591 - acc: 0.5302 - val_loss: 0.0581 - val_acc: 0.5528\n",
      "Epoch 445/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0592 - acc: 0.5338 - val_loss: 0.0579 - val_acc: 0.5594\n",
      "Epoch 446/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0593 - acc: 0.5330 - val_loss: 0.0582 - val_acc: 0.5436\n",
      "Epoch 447/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0591 - acc: 0.5297 - val_loss: 0.0579 - val_acc: 0.5586\n",
      "Epoch 448/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0593 - acc: 0.5330 - val_loss: 0.0574 - val_acc: 0.5719\n",
      "Epoch 449/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0597 - acc: 0.5258 - val_loss: 0.0601 - val_acc: 0.5212\n",
      "Epoch 450/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0594 - acc: 0.5344 - val_loss: 0.0584 - val_acc: 0.5511\n",
      "Epoch 451/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0592 - acc: 0.5352 - val_loss: 0.0588 - val_acc: 0.5611\n",
      "Epoch 452/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0595 - acc: 0.5355 - val_loss: 0.0572 - val_acc: 0.5578\n",
      "Epoch 453/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0594 - acc: 0.5299 - val_loss: 0.0575 - val_acc: 0.5628\n",
      "Epoch 454/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0593 - acc: 0.5341 - val_loss: 0.0579 - val_acc: 0.5486\n",
      "Epoch 455/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0592 - acc: 0.5358 - val_loss: 0.0578 - val_acc: 0.5727\n",
      "Epoch 456/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0591 - acc: 0.5344 - val_loss: 0.0576 - val_acc: 0.5561\n",
      "Epoch 457/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0597 - acc: 0.5310 - val_loss: 0.0576 - val_acc: 0.5569\n",
      "Epoch 458/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0598 - acc: 0.5291 - val_loss: 0.0571 - val_acc: 0.5578\n",
      "Epoch 459/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0595 - acc: 0.5310 - val_loss: 0.0582 - val_acc: 0.5486\n",
      "Epoch 460/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0594 - acc: 0.5346 - val_loss: 0.0575 - val_acc: 0.5578\n",
      "Epoch 461/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0591 - acc: 0.5308 - val_loss: 0.0580 - val_acc: 0.5669\n",
      "Epoch 462/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0591 - acc: 0.5380 - val_loss: 0.0574 - val_acc: 0.5503\n",
      "Epoch 463/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0594 - acc: 0.5261 - val_loss: 0.0577 - val_acc: 0.5636\n",
      "Epoch 464/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0595 - acc: 0.5310 - val_loss: 0.0576 - val_acc: 0.5436\n",
      "Epoch 465/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0597 - acc: 0.5338 - val_loss: 0.0584 - val_acc: 0.5677\n",
      "Epoch 466/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0591 - acc: 0.5308 - val_loss: 0.0576 - val_acc: 0.5578\n",
      "Epoch 467/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0597 - acc: 0.5272 - val_loss: 0.0576 - val_acc: 0.5544\n",
      "Epoch 468/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0597 - acc: 0.5285 - val_loss: 0.0576 - val_acc: 0.5711\n",
      "Epoch 469/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0600 - acc: 0.5188 - val_loss: 0.0573 - val_acc: 0.5586\n",
      "Epoch 470/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0592 - acc: 0.5308 - val_loss: 0.0575 - val_acc: 0.5586\n",
      "Epoch 471/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0594 - acc: 0.5294 - val_loss: 0.0571 - val_acc: 0.5702\n",
      "Epoch 472/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0594 - acc: 0.5341 - val_loss: 0.0580 - val_acc: 0.5528\n",
      "Epoch 473/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0592 - acc: 0.5349 - val_loss: 0.0599 - val_acc: 0.5303\n",
      "Epoch 474/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0596 - acc: 0.5291 - val_loss: 0.0580 - val_acc: 0.5495\n",
      "Epoch 475/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0595 - acc: 0.5335 - val_loss: 0.0580 - val_acc: 0.5536\n",
      "Epoch 476/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0596 - acc: 0.5297 - val_loss: 0.0584 - val_acc: 0.5445\n",
      "Epoch 477/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0590 - acc: 0.5310 - val_loss: 0.0575 - val_acc: 0.5536\n",
      "Epoch 478/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0590 - acc: 0.5322 - val_loss: 0.0578 - val_acc: 0.5353\n",
      "Epoch 479/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0592 - acc: 0.5313 - val_loss: 0.0591 - val_acc: 0.5495\n",
      "Epoch 480/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0596 - acc: 0.5322 - val_loss: 0.0598 - val_acc: 0.5187\n",
      "Epoch 481/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0598 - acc: 0.5252 - val_loss: 0.0625 - val_acc: 0.4988\n",
      "Epoch 482/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0595 - acc: 0.5316 - val_loss: 0.0573 - val_acc: 0.5544\n",
      "Epoch 483/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0594 - acc: 0.5349 - val_loss: 0.0573 - val_acc: 0.5395\n",
      "Epoch 484/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0598 - acc: 0.5261 - val_loss: 0.0569 - val_acc: 0.5661\n",
      "Epoch 485/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0592 - acc: 0.5352 - val_loss: 0.0580 - val_acc: 0.5644\n",
      "Epoch 486/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0601 - acc: 0.5169 - val_loss: 0.0573 - val_acc: 0.5636\n",
      "Epoch 487/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0596 - acc: 0.5299 - val_loss: 0.0592 - val_acc: 0.5328\n",
      "Epoch 488/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0595 - acc: 0.5291 - val_loss: 0.0600 - val_acc: 0.5561\n",
      "Epoch 489/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0597 - acc: 0.5277 - val_loss: 0.0576 - val_acc: 0.5561\n",
      "Epoch 490/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0594 - acc: 0.5299 - val_loss: 0.0591 - val_acc: 0.5603\n",
      "Epoch 491/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0595 - acc: 0.5280 - val_loss: 0.0622 - val_acc: 0.4963\n",
      "Epoch 492/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0592 - acc: 0.5316 - val_loss: 0.0584 - val_acc: 0.5636\n",
      "Epoch 493/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0598 - acc: 0.5283 - val_loss: 0.0585 - val_acc: 0.5553\n",
      "Epoch 494/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0596 - acc: 0.5252 - val_loss: 0.0583 - val_acc: 0.5536\n",
      "Epoch 495/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0591 - acc: 0.5316 - val_loss: 0.0579 - val_acc: 0.5536\n",
      "Epoch 496/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0593 - acc: 0.5327 - val_loss: 0.0571 - val_acc: 0.5495\n",
      "Epoch 497/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0590 - acc: 0.5341 - val_loss: 0.0572 - val_acc: 0.5495\n",
      "Epoch 498/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0589 - acc: 0.5324 - val_loss: 0.0606 - val_acc: 0.5387\n",
      "Epoch 499/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0591 - acc: 0.5333 - val_loss: 0.0572 - val_acc: 0.5619\n",
      "Epoch 500/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0593 - acc: 0.5344 - val_loss: 0.0587 - val_acc: 0.5453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77ff30e208>"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='relu'))\n",
    "nn_model.add(Dense(units=16, activation='relu'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.05)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "nn_model.fit(x=train_x, y=train_y, epochs=500, validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2DaXZOLP3BD"
   },
   "source": [
    "Validation result seems quite same. I consider 0.1 is the best with the highest training accuracy.\n",
    "Now I changed activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17272
    },
    "colab_type": "code",
    "id": "uWmNRnWJA7h7",
    "outputId": "5fe24db7-ef35-4fde-fe90-a4b1ab0b1e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 1,754\n",
      "Trainable params: 1,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3608 samples, validate on 1203 samples\n",
      "Epoch 1/500\n",
      "3608/3608 [==============================] - 1s 258us/step - loss: 0.0724 - acc: 0.4271 - val_loss: 0.0658 - val_acc: 0.4622\n",
      "Epoch 2/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0674 - acc: 0.4446 - val_loss: 0.0653 - val_acc: 0.4697\n",
      "Epoch 3/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0660 - acc: 0.4529 - val_loss: 0.0639 - val_acc: 0.4763\n",
      "Epoch 4/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0647 - acc: 0.4615 - val_loss: 0.0639 - val_acc: 0.4688\n",
      "Epoch 5/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0636 - acc: 0.4709 - val_loss: 0.0620 - val_acc: 0.4913\n",
      "Epoch 6/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0626 - acc: 0.4812 - val_loss: 0.0601 - val_acc: 0.4963\n",
      "Epoch 7/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0616 - acc: 0.4975 - val_loss: 0.0596 - val_acc: 0.5362\n",
      "Epoch 8/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0606 - acc: 0.5075 - val_loss: 0.0588 - val_acc: 0.5461\n",
      "Epoch 9/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0602 - acc: 0.5277 - val_loss: 0.0581 - val_acc: 0.5536\n",
      "Epoch 10/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0600 - acc: 0.5277 - val_loss: 0.0579 - val_acc: 0.5303\n",
      "Epoch 11/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0594 - acc: 0.5382 - val_loss: 0.0588 - val_acc: 0.5254\n",
      "Epoch 12/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0588 - acc: 0.5477 - val_loss: 0.0581 - val_acc: 0.5420\n",
      "Epoch 13/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0586 - acc: 0.5540 - val_loss: 0.0575 - val_acc: 0.5694\n",
      "Epoch 14/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0585 - acc: 0.5529 - val_loss: 0.0566 - val_acc: 0.5769\n",
      "Epoch 15/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0584 - acc: 0.5576 - val_loss: 0.0566 - val_acc: 0.5677\n",
      "Epoch 16/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0581 - acc: 0.5546 - val_loss: 0.0562 - val_acc: 0.5819\n",
      "Epoch 17/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0581 - acc: 0.5524 - val_loss: 0.0579 - val_acc: 0.5378\n",
      "Epoch 18/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0583 - acc: 0.5552 - val_loss: 0.0563 - val_acc: 0.5761\n",
      "Epoch 19/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0577 - acc: 0.5629 - val_loss: 0.0564 - val_acc: 0.5786\n",
      "Epoch 20/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0577 - acc: 0.5629 - val_loss: 0.0569 - val_acc: 0.5653\n",
      "Epoch 21/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0576 - acc: 0.5632 - val_loss: 0.0567 - val_acc: 0.5686\n",
      "Epoch 22/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0573 - acc: 0.5646 - val_loss: 0.0556 - val_acc: 0.5869\n",
      "Epoch 23/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0574 - acc: 0.5665 - val_loss: 0.0571 - val_acc: 0.5653\n",
      "Epoch 24/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0574 - acc: 0.5662 - val_loss: 0.0566 - val_acc: 0.5711\n",
      "Epoch 25/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0574 - acc: 0.5637 - val_loss: 0.0560 - val_acc: 0.5794\n",
      "Epoch 26/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0572 - acc: 0.5629 - val_loss: 0.0564 - val_acc: 0.5636\n",
      "Epoch 27/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0570 - acc: 0.5710 - val_loss: 0.0561 - val_acc: 0.5802\n",
      "Epoch 28/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0569 - acc: 0.5734 - val_loss: 0.0559 - val_acc: 0.5786\n",
      "Epoch 29/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0571 - acc: 0.5693 - val_loss: 0.0556 - val_acc: 0.5794\n",
      "Epoch 30/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0565 - acc: 0.5732 - val_loss: 0.0555 - val_acc: 0.5835\n",
      "Epoch 31/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0567 - acc: 0.5757 - val_loss: 0.0562 - val_acc: 0.5786\n",
      "Epoch 32/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0569 - acc: 0.5674 - val_loss: 0.0567 - val_acc: 0.5752\n",
      "Epoch 33/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0569 - acc: 0.5693 - val_loss: 0.0582 - val_acc: 0.5453\n",
      "Epoch 34/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0566 - acc: 0.5773 - val_loss: 0.0555 - val_acc: 0.5786\n",
      "Epoch 35/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0564 - acc: 0.5734 - val_loss: 0.0554 - val_acc: 0.5835\n",
      "Epoch 36/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0564 - acc: 0.5704 - val_loss: 0.0557 - val_acc: 0.5802\n",
      "Epoch 37/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0562 - acc: 0.5771 - val_loss: 0.0554 - val_acc: 0.5877\n",
      "Epoch 38/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0563 - acc: 0.5771 - val_loss: 0.0567 - val_acc: 0.5611\n",
      "Epoch 39/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0562 - acc: 0.5718 - val_loss: 0.0553 - val_acc: 0.5827\n",
      "Epoch 40/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0564 - acc: 0.5751 - val_loss: 0.0554 - val_acc: 0.5860\n",
      "Epoch 41/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0561 - acc: 0.5765 - val_loss: 0.0554 - val_acc: 0.5794\n",
      "Epoch 42/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0560 - acc: 0.5776 - val_loss: 0.0554 - val_acc: 0.5844\n",
      "Epoch 43/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0561 - acc: 0.5721 - val_loss: 0.0559 - val_acc: 0.5819\n",
      "Epoch 44/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0559 - acc: 0.5790 - val_loss: 0.0555 - val_acc: 0.5794\n",
      "Epoch 45/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0558 - acc: 0.5779 - val_loss: 0.0559 - val_acc: 0.5802\n",
      "Epoch 46/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0558 - acc: 0.5757 - val_loss: 0.0556 - val_acc: 0.5769\n",
      "Epoch 47/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0557 - acc: 0.5793 - val_loss: 0.0563 - val_acc: 0.5761\n",
      "Epoch 48/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0559 - acc: 0.5754 - val_loss: 0.0557 - val_acc: 0.5786\n",
      "Epoch 49/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0561 - acc: 0.5784 - val_loss: 0.0552 - val_acc: 0.5835\n",
      "Epoch 50/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0557 - acc: 0.5759 - val_loss: 0.0560 - val_acc: 0.5744\n",
      "Epoch 51/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0554 - acc: 0.5862 - val_loss: 0.0568 - val_acc: 0.5744\n",
      "Epoch 52/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0557 - acc: 0.5793 - val_loss: 0.0560 - val_acc: 0.5794\n",
      "Epoch 53/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0560 - acc: 0.5795 - val_loss: 0.0572 - val_acc: 0.5736\n",
      "Epoch 54/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0556 - acc: 0.5798 - val_loss: 0.0555 - val_acc: 0.5786\n",
      "Epoch 55/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0555 - acc: 0.5818 - val_loss: 0.0554 - val_acc: 0.5810\n",
      "Epoch 56/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0554 - acc: 0.5826 - val_loss: 0.0560 - val_acc: 0.5802\n",
      "Epoch 57/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0555 - acc: 0.5818 - val_loss: 0.0558 - val_acc: 0.5819\n",
      "Epoch 58/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0551 - acc: 0.5876 - val_loss: 0.0563 - val_acc: 0.5719\n",
      "Epoch 59/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0554 - acc: 0.5826 - val_loss: 0.0577 - val_acc: 0.5694\n",
      "Epoch 60/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0553 - acc: 0.5845 - val_loss: 0.0562 - val_acc: 0.5761\n",
      "Epoch 61/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0555 - acc: 0.5843 - val_loss: 0.0564 - val_acc: 0.5653\n",
      "Epoch 62/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0552 - acc: 0.5856 - val_loss: 0.0560 - val_acc: 0.5761\n",
      "Epoch 63/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0550 - acc: 0.5854 - val_loss: 0.0556 - val_acc: 0.5786\n",
      "Epoch 64/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0550 - acc: 0.5856 - val_loss: 0.0557 - val_acc: 0.5736\n",
      "Epoch 65/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0552 - acc: 0.5843 - val_loss: 0.0562 - val_acc: 0.5744\n",
      "Epoch 66/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0551 - acc: 0.5812 - val_loss: 0.0557 - val_acc: 0.5736\n",
      "Epoch 67/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0549 - acc: 0.5859 - val_loss: 0.0565 - val_acc: 0.5677\n",
      "Epoch 68/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0548 - acc: 0.5870 - val_loss: 0.0560 - val_acc: 0.5761\n",
      "Epoch 69/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0548 - acc: 0.5851 - val_loss: 0.0571 - val_acc: 0.5736\n",
      "Epoch 70/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0550 - acc: 0.5890 - val_loss: 0.0570 - val_acc: 0.5694\n",
      "Epoch 71/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0549 - acc: 0.5923 - val_loss: 0.0555 - val_acc: 0.5769\n",
      "Epoch 72/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0547 - acc: 0.5904 - val_loss: 0.0563 - val_acc: 0.5619\n",
      "Epoch 73/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0547 - acc: 0.5887 - val_loss: 0.0563 - val_acc: 0.5702\n",
      "Epoch 74/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0547 - acc: 0.5879 - val_loss: 0.0565 - val_acc: 0.5694\n",
      "Epoch 75/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0547 - acc: 0.5906 - val_loss: 0.0569 - val_acc: 0.5769\n",
      "Epoch 76/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0545 - acc: 0.5904 - val_loss: 0.0558 - val_acc: 0.5769\n",
      "Epoch 77/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0544 - acc: 0.5962 - val_loss: 0.0561 - val_acc: 0.5777\n",
      "Epoch 78/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0544 - acc: 0.5884 - val_loss: 0.0558 - val_acc: 0.5744\n",
      "Epoch 79/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0543 - acc: 0.5895 - val_loss: 0.0569 - val_acc: 0.5727\n",
      "Epoch 80/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0546 - acc: 0.5934 - val_loss: 0.0567 - val_acc: 0.5677\n",
      "Epoch 81/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0541 - acc: 0.5940 - val_loss: 0.0561 - val_acc: 0.5711\n",
      "Epoch 82/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0545 - acc: 0.5904 - val_loss: 0.0562 - val_acc: 0.5786\n",
      "Epoch 83/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0546 - acc: 0.5879 - val_loss: 0.0562 - val_acc: 0.5669\n",
      "Epoch 84/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0544 - acc: 0.5901 - val_loss: 0.0573 - val_acc: 0.5636\n",
      "Epoch 85/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0543 - acc: 0.5928 - val_loss: 0.0568 - val_acc: 0.5669\n",
      "Epoch 86/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0541 - acc: 0.5940 - val_loss: 0.0562 - val_acc: 0.5744\n",
      "Epoch 87/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0542 - acc: 0.5962 - val_loss: 0.0557 - val_acc: 0.5736\n",
      "Epoch 88/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0539 - acc: 0.5959 - val_loss: 0.0571 - val_acc: 0.5661\n",
      "Epoch 89/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0541 - acc: 0.5981 - val_loss: 0.0568 - val_acc: 0.5669\n",
      "Epoch 90/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0538 - acc: 0.5945 - val_loss: 0.0570 - val_acc: 0.5653\n",
      "Epoch 91/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0541 - acc: 0.5967 - val_loss: 0.0563 - val_acc: 0.5769\n",
      "Epoch 92/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0540 - acc: 0.6003 - val_loss: 0.0561 - val_acc: 0.5686\n",
      "Epoch 93/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0539 - acc: 0.5970 - val_loss: 0.0570 - val_acc: 0.5736\n",
      "Epoch 94/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0539 - acc: 0.6020 - val_loss: 0.0569 - val_acc: 0.5677\n",
      "Epoch 95/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0535 - acc: 0.5978 - val_loss: 0.0570 - val_acc: 0.5694\n",
      "Epoch 96/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0535 - acc: 0.6006 - val_loss: 0.0564 - val_acc: 0.5761\n",
      "Epoch 97/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0536 - acc: 0.5965 - val_loss: 0.0570 - val_acc: 0.5686\n",
      "Epoch 98/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0537 - acc: 0.5987 - val_loss: 0.0567 - val_acc: 0.5736\n",
      "Epoch 99/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0537 - acc: 0.6001 - val_loss: 0.0566 - val_acc: 0.5702\n",
      "Epoch 100/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0536 - acc: 0.5976 - val_loss: 0.0590 - val_acc: 0.5561\n",
      "Epoch 101/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0535 - acc: 0.5942 - val_loss: 0.0561 - val_acc: 0.5786\n",
      "Epoch 102/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0533 - acc: 0.5984 - val_loss: 0.0576 - val_acc: 0.5569\n",
      "Epoch 103/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0534 - acc: 0.5976 - val_loss: 0.0566 - val_acc: 0.5694\n",
      "Epoch 104/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0532 - acc: 0.6006 - val_loss: 0.0570 - val_acc: 0.5744\n",
      "Epoch 105/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0534 - acc: 0.5995 - val_loss: 0.0565 - val_acc: 0.5711\n",
      "Epoch 106/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0532 - acc: 0.6039 - val_loss: 0.0569 - val_acc: 0.5619\n",
      "Epoch 107/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0537 - acc: 0.5956 - val_loss: 0.0564 - val_acc: 0.5736\n",
      "Epoch 108/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0533 - acc: 0.5989 - val_loss: 0.0568 - val_acc: 0.5786\n",
      "Epoch 109/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0533 - acc: 0.6037 - val_loss: 0.0574 - val_acc: 0.5628\n",
      "Epoch 110/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0538 - acc: 0.5940 - val_loss: 0.0573 - val_acc: 0.5661\n",
      "Epoch 111/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0529 - acc: 0.6064 - val_loss: 0.0578 - val_acc: 0.5669\n",
      "Epoch 112/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0531 - acc: 0.6045 - val_loss: 0.0567 - val_acc: 0.5761\n",
      "Epoch 113/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0528 - acc: 0.6028 - val_loss: 0.0570 - val_acc: 0.5677\n",
      "Epoch 114/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0528 - acc: 0.6039 - val_loss: 0.0564 - val_acc: 0.5752\n",
      "Epoch 115/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0528 - acc: 0.6070 - val_loss: 0.0570 - val_acc: 0.5727\n",
      "Epoch 116/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0531 - acc: 0.6050 - val_loss: 0.0571 - val_acc: 0.5653\n",
      "Epoch 117/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0528 - acc: 0.6064 - val_loss: 0.0565 - val_acc: 0.5794\n",
      "Epoch 118/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0527 - acc: 0.6084 - val_loss: 0.0570 - val_acc: 0.5603\n",
      "Epoch 119/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0528 - acc: 0.6025 - val_loss: 0.0568 - val_acc: 0.5711\n",
      "Epoch 120/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0526 - acc: 0.6070 - val_loss: 0.0585 - val_acc: 0.5594\n",
      "Epoch 121/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0527 - acc: 0.6067 - val_loss: 0.0592 - val_acc: 0.5619\n",
      "Epoch 122/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0526 - acc: 0.6086 - val_loss: 0.0566 - val_acc: 0.5752\n",
      "Epoch 123/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0524 - acc: 0.6134 - val_loss: 0.0569 - val_acc: 0.5677\n",
      "Epoch 124/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0525 - acc: 0.6100 - val_loss: 0.0573 - val_acc: 0.5727\n",
      "Epoch 125/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0523 - acc: 0.6109 - val_loss: 0.0568 - val_acc: 0.5711\n",
      "Epoch 126/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0524 - acc: 0.6098 - val_loss: 0.0577 - val_acc: 0.5702\n",
      "Epoch 127/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0527 - acc: 0.6103 - val_loss: 0.0582 - val_acc: 0.5586\n",
      "Epoch 128/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0522 - acc: 0.6159 - val_loss: 0.0571 - val_acc: 0.5727\n",
      "Epoch 129/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0523 - acc: 0.6081 - val_loss: 0.0577 - val_acc: 0.5603\n",
      "Epoch 130/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0524 - acc: 0.6123 - val_loss: 0.0570 - val_acc: 0.5769\n",
      "Epoch 131/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0523 - acc: 0.6089 - val_loss: 0.0569 - val_acc: 0.5702\n",
      "Epoch 132/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0524 - acc: 0.6098 - val_loss: 0.0574 - val_acc: 0.5694\n",
      "Epoch 133/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0519 - acc: 0.6142 - val_loss: 0.0576 - val_acc: 0.5677\n",
      "Epoch 134/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0522 - acc: 0.6139 - val_loss: 0.0573 - val_acc: 0.5694\n",
      "Epoch 135/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0521 - acc: 0.6142 - val_loss: 0.0571 - val_acc: 0.5719\n",
      "Epoch 136/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0521 - acc: 0.6134 - val_loss: 0.0573 - val_acc: 0.5744\n",
      "Epoch 137/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0520 - acc: 0.6128 - val_loss: 0.0578 - val_acc: 0.5677\n",
      "Epoch 138/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0519 - acc: 0.6131 - val_loss: 0.0578 - val_acc: 0.5619\n",
      "Epoch 139/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0519 - acc: 0.6150 - val_loss: 0.0572 - val_acc: 0.5702\n",
      "Epoch 140/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0518 - acc: 0.6153 - val_loss: 0.0570 - val_acc: 0.5786\n",
      "Epoch 141/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0519 - acc: 0.6145 - val_loss: 0.0576 - val_acc: 0.5686\n",
      "Epoch 142/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0518 - acc: 0.6145 - val_loss: 0.0573 - val_acc: 0.5702\n",
      "Epoch 143/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0515 - acc: 0.6111 - val_loss: 0.0573 - val_acc: 0.5744\n",
      "Epoch 144/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0519 - acc: 0.6189 - val_loss: 0.0585 - val_acc: 0.5586\n",
      "Epoch 145/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0516 - acc: 0.6186 - val_loss: 0.0575 - val_acc: 0.5677\n",
      "Epoch 146/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0519 - acc: 0.6172 - val_loss: 0.0575 - val_acc: 0.5686\n",
      "Epoch 147/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0515 - acc: 0.6178 - val_loss: 0.0577 - val_acc: 0.5810\n",
      "Epoch 148/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0516 - acc: 0.6175 - val_loss: 0.0578 - val_acc: 0.5636\n",
      "Epoch 149/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0518 - acc: 0.6200 - val_loss: 0.0586 - val_acc: 0.5478\n",
      "Epoch 150/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0515 - acc: 0.6156 - val_loss: 0.0576 - val_acc: 0.5653\n",
      "Epoch 151/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0515 - acc: 0.6195 - val_loss: 0.0575 - val_acc: 0.5711\n",
      "Epoch 152/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0515 - acc: 0.6186 - val_loss: 0.0586 - val_acc: 0.5561\n",
      "Epoch 153/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0514 - acc: 0.6206 - val_loss: 0.0577 - val_acc: 0.5686\n",
      "Epoch 154/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0514 - acc: 0.6192 - val_loss: 0.0576 - val_acc: 0.5644\n",
      "Epoch 155/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0513 - acc: 0.6250 - val_loss: 0.0573 - val_acc: 0.5727\n",
      "Epoch 156/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0512 - acc: 0.6181 - val_loss: 0.0573 - val_acc: 0.5677\n",
      "Epoch 157/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0511 - acc: 0.6217 - val_loss: 0.0576 - val_acc: 0.5769\n",
      "Epoch 158/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0511 - acc: 0.6206 - val_loss: 0.0583 - val_acc: 0.5669\n",
      "Epoch 159/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0512 - acc: 0.6200 - val_loss: 0.0575 - val_acc: 0.5702\n",
      "Epoch 160/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0511 - acc: 0.6239 - val_loss: 0.0582 - val_acc: 0.5719\n",
      "Epoch 161/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0510 - acc: 0.6283 - val_loss: 0.0587 - val_acc: 0.5619\n",
      "Epoch 162/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0511 - acc: 0.6228 - val_loss: 0.0579 - val_acc: 0.5677\n",
      "Epoch 163/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0511 - acc: 0.6186 - val_loss: 0.0590 - val_acc: 0.5686\n",
      "Epoch 164/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0511 - acc: 0.6211 - val_loss: 0.0580 - val_acc: 0.5653\n",
      "Epoch 165/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0512 - acc: 0.6197 - val_loss: 0.0579 - val_acc: 0.5761\n",
      "Epoch 166/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0513 - acc: 0.6178 - val_loss: 0.0584 - val_acc: 0.5603\n",
      "Epoch 167/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0511 - acc: 0.6186 - val_loss: 0.0581 - val_acc: 0.5727\n",
      "Epoch 168/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0508 - acc: 0.6236 - val_loss: 0.0584 - val_acc: 0.5661\n",
      "Epoch 169/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0507 - acc: 0.6236 - val_loss: 0.0579 - val_acc: 0.5677\n",
      "Epoch 170/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0511 - acc: 0.6211 - val_loss: 0.0578 - val_acc: 0.5702\n",
      "Epoch 171/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0511 - acc: 0.6195 - val_loss: 0.0583 - val_acc: 0.5727\n",
      "Epoch 172/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0507 - acc: 0.6228 - val_loss: 0.0581 - val_acc: 0.5769\n",
      "Epoch 173/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0507 - acc: 0.6261 - val_loss: 0.0585 - val_acc: 0.5761\n",
      "Epoch 174/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0505 - acc: 0.6303 - val_loss: 0.0582 - val_acc: 0.5702\n",
      "Epoch 175/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0507 - acc: 0.6247 - val_loss: 0.0584 - val_acc: 0.5669\n",
      "Epoch 176/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0507 - acc: 0.6253 - val_loss: 0.0589 - val_acc: 0.5669\n",
      "Epoch 177/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0506 - acc: 0.6222 - val_loss: 0.0584 - val_acc: 0.5702\n",
      "Epoch 178/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0504 - acc: 0.6314 - val_loss: 0.0585 - val_acc: 0.5578\n",
      "Epoch 179/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0509 - acc: 0.6228 - val_loss: 0.0590 - val_acc: 0.5653\n",
      "Epoch 180/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0503 - acc: 0.6253 - val_loss: 0.0593 - val_acc: 0.5528\n",
      "Epoch 181/500\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0509 - acc: 0.6294 - val_loss: 0.0583 - val_acc: 0.5636\n",
      "Epoch 182/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0507 - acc: 0.6256 - val_loss: 0.0596 - val_acc: 0.5528\n",
      "Epoch 183/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0503 - acc: 0.6269 - val_loss: 0.0584 - val_acc: 0.5694\n",
      "Epoch 184/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0505 - acc: 0.6244 - val_loss: 0.0581 - val_acc: 0.5736\n",
      "Epoch 185/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0504 - acc: 0.6294 - val_loss: 0.0590 - val_acc: 0.5702\n",
      "Epoch 186/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0503 - acc: 0.6275 - val_loss: 0.0595 - val_acc: 0.5619\n",
      "Epoch 187/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0503 - acc: 0.6330 - val_loss: 0.0600 - val_acc: 0.5653\n",
      "Epoch 188/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0504 - acc: 0.6258 - val_loss: 0.0587 - val_acc: 0.5719\n",
      "Epoch 189/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0502 - acc: 0.6300 - val_loss: 0.0586 - val_acc: 0.5686\n",
      "Epoch 190/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0507 - acc: 0.6189 - val_loss: 0.0591 - val_acc: 0.5686\n",
      "Epoch 191/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0503 - acc: 0.6280 - val_loss: 0.0585 - val_acc: 0.5569\n",
      "Epoch 192/500\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0501 - acc: 0.6300 - val_loss: 0.0588 - val_acc: 0.5744\n",
      "Epoch 193/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0502 - acc: 0.6308 - val_loss: 0.0594 - val_acc: 0.5669\n",
      "Epoch 194/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0500 - acc: 0.6239 - val_loss: 0.0607 - val_acc: 0.5511\n",
      "Epoch 195/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0504 - acc: 0.6247 - val_loss: 0.0583 - val_acc: 0.5702\n",
      "Epoch 196/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0501 - acc: 0.6294 - val_loss: 0.0587 - val_acc: 0.5761\n",
      "Epoch 197/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0500 - acc: 0.6325 - val_loss: 0.0596 - val_acc: 0.5636\n",
      "Epoch 198/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0500 - acc: 0.6294 - val_loss: 0.0592 - val_acc: 0.5644\n",
      "Epoch 199/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0502 - acc: 0.6278 - val_loss: 0.0589 - val_acc: 0.5752\n",
      "Epoch 200/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0499 - acc: 0.6325 - val_loss: 0.0593 - val_acc: 0.5619\n",
      "Epoch 201/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0500 - acc: 0.6358 - val_loss: 0.0589 - val_acc: 0.5686\n",
      "Epoch 202/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0501 - acc: 0.6325 - val_loss: 0.0589 - val_acc: 0.5644\n",
      "Epoch 203/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0501 - acc: 0.6286 - val_loss: 0.0592 - val_acc: 0.5727\n",
      "Epoch 204/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0498 - acc: 0.6311 - val_loss: 0.0603 - val_acc: 0.5653\n",
      "Epoch 205/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0498 - acc: 0.6322 - val_loss: 0.0596 - val_acc: 0.5628\n",
      "Epoch 206/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0502 - acc: 0.6292 - val_loss: 0.0586 - val_acc: 0.5653\n",
      "Epoch 207/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0495 - acc: 0.6333 - val_loss: 0.0587 - val_acc: 0.5653\n",
      "Epoch 208/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0496 - acc: 0.6283 - val_loss: 0.0595 - val_acc: 0.5694\n",
      "Epoch 209/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0498 - acc: 0.6278 - val_loss: 0.0587 - val_acc: 0.5711\n",
      "Epoch 210/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0496 - acc: 0.6300 - val_loss: 0.0592 - val_acc: 0.5719\n",
      "Epoch 211/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0493 - acc: 0.6389 - val_loss: 0.0610 - val_acc: 0.5445\n",
      "Epoch 212/500\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0497 - acc: 0.6319 - val_loss: 0.0588 - val_acc: 0.5719\n",
      "Epoch 213/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0496 - acc: 0.6308 - val_loss: 0.0594 - val_acc: 0.5644\n",
      "Epoch 214/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0495 - acc: 0.6372 - val_loss: 0.0597 - val_acc: 0.5661\n",
      "Epoch 215/500\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0494 - acc: 0.6339 - val_loss: 0.0598 - val_acc: 0.5603\n",
      "Epoch 216/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0495 - acc: 0.6347 - val_loss: 0.0591 - val_acc: 0.5736\n",
      "Epoch 217/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0496 - acc: 0.6322 - val_loss: 0.0590 - val_acc: 0.5736\n",
      "Epoch 218/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0494 - acc: 0.6350 - val_loss: 0.0589 - val_acc: 0.5677\n",
      "Epoch 219/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0495 - acc: 0.6300 - val_loss: 0.0598 - val_acc: 0.5628\n",
      "Epoch 220/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0493 - acc: 0.6350 - val_loss: 0.0604 - val_acc: 0.5603\n",
      "Epoch 221/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0495 - acc: 0.6361 - val_loss: 0.0600 - val_acc: 0.5611\n",
      "Epoch 222/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0495 - acc: 0.6333 - val_loss: 0.0593 - val_acc: 0.5702\n",
      "Epoch 223/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0493 - acc: 0.6336 - val_loss: 0.0604 - val_acc: 0.5636\n",
      "Epoch 224/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0494 - acc: 0.6336 - val_loss: 0.0599 - val_acc: 0.5636\n",
      "Epoch 225/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0493 - acc: 0.6366 - val_loss: 0.0603 - val_acc: 0.5702\n",
      "Epoch 226/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0494 - acc: 0.6386 - val_loss: 0.0595 - val_acc: 0.5611\n",
      "Epoch 227/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0492 - acc: 0.6414 - val_loss: 0.0600 - val_acc: 0.5619\n",
      "Epoch 228/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0494 - acc: 0.6355 - val_loss: 0.0593 - val_acc: 0.5653\n",
      "Epoch 229/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0491 - acc: 0.6389 - val_loss: 0.0593 - val_acc: 0.5677\n",
      "Epoch 230/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0491 - acc: 0.6366 - val_loss: 0.0598 - val_acc: 0.5628\n",
      "Epoch 231/500\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0493 - acc: 0.6366 - val_loss: 0.0599 - val_acc: 0.5661\n",
      "Epoch 232/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0490 - acc: 0.6391 - val_loss: 0.0604 - val_acc: 0.5586\n",
      "Epoch 233/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0491 - acc: 0.6328 - val_loss: 0.0600 - val_acc: 0.5636\n",
      "Epoch 234/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0491 - acc: 0.6397 - val_loss: 0.0608 - val_acc: 0.5603\n",
      "Epoch 235/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0491 - acc: 0.6369 - val_loss: 0.0605 - val_acc: 0.5586\n",
      "Epoch 236/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0489 - acc: 0.6408 - val_loss: 0.0616 - val_acc: 0.5320\n",
      "Epoch 237/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0489 - acc: 0.6380 - val_loss: 0.0602 - val_acc: 0.5619\n",
      "Epoch 238/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0488 - acc: 0.6433 - val_loss: 0.0604 - val_acc: 0.5561\n",
      "Epoch 239/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0490 - acc: 0.6408 - val_loss: 0.0606 - val_acc: 0.5586\n",
      "Epoch 240/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0491 - acc: 0.6377 - val_loss: 0.0611 - val_acc: 0.5528\n",
      "Epoch 241/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0486 - acc: 0.6430 - val_loss: 0.0597 - val_acc: 0.5611\n",
      "Epoch 242/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0491 - acc: 0.6355 - val_loss: 0.0602 - val_acc: 0.5594\n",
      "Epoch 243/500\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0489 - acc: 0.6416 - val_loss: 0.0598 - val_acc: 0.5669\n",
      "Epoch 244/500\n",
      "3608/3608 [==============================] - 1s 220us/step - loss: 0.0488 - acc: 0.6441 - val_loss: 0.0605 - val_acc: 0.5669\n",
      "Epoch 245/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0488 - acc: 0.6419 - val_loss: 0.0602 - val_acc: 0.5686\n",
      "Epoch 246/500\n",
      "3608/3608 [==============================] - 1s 218us/step - loss: 0.0489 - acc: 0.6347 - val_loss: 0.0607 - val_acc: 0.5569\n",
      "Epoch 247/500\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0488 - acc: 0.6416 - val_loss: 0.0595 - val_acc: 0.5628\n",
      "Epoch 248/500\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0488 - acc: 0.6411 - val_loss: 0.0609 - val_acc: 0.5578\n",
      "Epoch 249/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0486 - acc: 0.6441 - val_loss: 0.0607 - val_acc: 0.5603\n",
      "Epoch 250/500\n",
      "3608/3608 [==============================] - 1s 197us/step - loss: 0.0487 - acc: 0.6452 - val_loss: 0.0606 - val_acc: 0.5586\n",
      "Epoch 251/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0489 - acc: 0.6402 - val_loss: 0.0602 - val_acc: 0.5586\n",
      "Epoch 252/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0487 - acc: 0.6422 - val_loss: 0.0602 - val_acc: 0.5586\n",
      "Epoch 253/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0486 - acc: 0.6441 - val_loss: 0.0603 - val_acc: 0.5686\n",
      "Epoch 254/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0484 - acc: 0.6455 - val_loss: 0.0603 - val_acc: 0.5611\n",
      "Epoch 255/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0487 - acc: 0.6391 - val_loss: 0.0625 - val_acc: 0.5544\n",
      "Epoch 256/500\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0486 - acc: 0.6386 - val_loss: 0.0602 - val_acc: 0.5636\n",
      "Epoch 257/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0483 - acc: 0.6480 - val_loss: 0.0608 - val_acc: 0.5544\n",
      "Epoch 258/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0485 - acc: 0.6430 - val_loss: 0.0613 - val_acc: 0.5619\n",
      "Epoch 259/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0484 - acc: 0.6447 - val_loss: 0.0607 - val_acc: 0.5619\n",
      "Epoch 260/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0486 - acc: 0.6397 - val_loss: 0.0607 - val_acc: 0.5702\n",
      "Epoch 261/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0485 - acc: 0.6416 - val_loss: 0.0607 - val_acc: 0.5636\n",
      "Epoch 262/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0484 - acc: 0.6422 - val_loss: 0.0612 - val_acc: 0.5561\n",
      "Epoch 263/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0481 - acc: 0.6475 - val_loss: 0.0608 - val_acc: 0.5619\n",
      "Epoch 264/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0484 - acc: 0.6463 - val_loss: 0.0606 - val_acc: 0.5436\n",
      "Epoch 265/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0484 - acc: 0.6430 - val_loss: 0.0614 - val_acc: 0.5578\n",
      "Epoch 266/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0484 - acc: 0.6447 - val_loss: 0.0611 - val_acc: 0.5594\n",
      "Epoch 267/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0486 - acc: 0.6425 - val_loss: 0.0607 - val_acc: 0.5636\n",
      "Epoch 268/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0481 - acc: 0.6433 - val_loss: 0.0603 - val_acc: 0.5603\n",
      "Epoch 269/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0481 - acc: 0.6488 - val_loss: 0.0608 - val_acc: 0.5603\n",
      "Epoch 270/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0483 - acc: 0.6499 - val_loss: 0.0615 - val_acc: 0.5569\n",
      "Epoch 271/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0482 - acc: 0.6483 - val_loss: 0.0615 - val_acc: 0.5561\n",
      "Epoch 272/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0485 - acc: 0.6455 - val_loss: 0.0613 - val_acc: 0.5619\n",
      "Epoch 273/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0478 - acc: 0.6508 - val_loss: 0.0606 - val_acc: 0.5586\n",
      "Epoch 274/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0483 - acc: 0.6469 - val_loss: 0.0609 - val_acc: 0.5611\n",
      "Epoch 275/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0482 - acc: 0.6463 - val_loss: 0.0610 - val_acc: 0.5653\n",
      "Epoch 276/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0482 - acc: 0.6511 - val_loss: 0.0606 - val_acc: 0.5611\n",
      "Epoch 277/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0479 - acc: 0.6472 - val_loss: 0.0603 - val_acc: 0.5628\n",
      "Epoch 278/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0483 - acc: 0.6427 - val_loss: 0.0618 - val_acc: 0.5511\n",
      "Epoch 279/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0477 - acc: 0.6508 - val_loss: 0.0608 - val_acc: 0.5594\n",
      "Epoch 280/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0481 - acc: 0.6427 - val_loss: 0.0627 - val_acc: 0.5387\n",
      "Epoch 281/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0480 - acc: 0.6502 - val_loss: 0.0604 - val_acc: 0.5578\n",
      "Epoch 282/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0482 - acc: 0.6402 - val_loss: 0.0619 - val_acc: 0.5503\n",
      "Epoch 283/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0480 - acc: 0.6502 - val_loss: 0.0619 - val_acc: 0.5536\n",
      "Epoch 284/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0479 - acc: 0.6469 - val_loss: 0.0618 - val_acc: 0.5569\n",
      "Epoch 285/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0481 - acc: 0.6488 - val_loss: 0.0611 - val_acc: 0.5603\n",
      "Epoch 286/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6458 - val_loss: 0.0612 - val_acc: 0.5561\n",
      "Epoch 287/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0482 - acc: 0.6455 - val_loss: 0.0610 - val_acc: 0.5619\n",
      "Epoch 288/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0479 - acc: 0.6438 - val_loss: 0.0618 - val_acc: 0.5553\n",
      "Epoch 289/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0478 - acc: 0.6472 - val_loss: 0.0625 - val_acc: 0.5511\n",
      "Epoch 290/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0480 - acc: 0.6447 - val_loss: 0.0612 - val_acc: 0.5619\n",
      "Epoch 291/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0477 - acc: 0.6544 - val_loss: 0.0612 - val_acc: 0.5619\n",
      "Epoch 292/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6511 - val_loss: 0.0619 - val_acc: 0.5644\n",
      "Epoch 293/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0477 - acc: 0.6541 - val_loss: 0.0618 - val_acc: 0.5644\n",
      "Epoch 294/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0478 - acc: 0.6483 - val_loss: 0.0617 - val_acc: 0.5578\n",
      "Epoch 295/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0476 - acc: 0.6466 - val_loss: 0.0623 - val_acc: 0.5594\n",
      "Epoch 296/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0475 - acc: 0.6516 - val_loss: 0.0617 - val_acc: 0.5586\n",
      "Epoch 297/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0479 - acc: 0.6488 - val_loss: 0.0608 - val_acc: 0.5594\n",
      "Epoch 298/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0477 - acc: 0.6499 - val_loss: 0.0619 - val_acc: 0.5536\n",
      "Epoch 299/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0475 - acc: 0.6549 - val_loss: 0.0630 - val_acc: 0.5461\n",
      "Epoch 300/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0476 - acc: 0.6502 - val_loss: 0.0617 - val_acc: 0.5578\n",
      "Epoch 301/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0476 - acc: 0.6486 - val_loss: 0.0625 - val_acc: 0.5520\n",
      "Epoch 302/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0478 - acc: 0.6483 - val_loss: 0.0617 - val_acc: 0.5603\n",
      "Epoch 303/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0477 - acc: 0.6508 - val_loss: 0.0614 - val_acc: 0.5694\n",
      "Epoch 304/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0477 - acc: 0.6499 - val_loss: 0.0619 - val_acc: 0.5594\n",
      "Epoch 305/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0475 - acc: 0.6524 - val_loss: 0.0630 - val_acc: 0.5486\n",
      "Epoch 306/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0476 - acc: 0.6511 - val_loss: 0.0610 - val_acc: 0.5611\n",
      "Epoch 307/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6530 - val_loss: 0.0622 - val_acc: 0.5478\n",
      "Epoch 308/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0475 - acc: 0.6497 - val_loss: 0.0609 - val_acc: 0.5594\n",
      "Epoch 309/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0475 - acc: 0.6508 - val_loss: 0.0613 - val_acc: 0.5520\n",
      "Epoch 310/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0474 - acc: 0.6594 - val_loss: 0.0626 - val_acc: 0.5445\n",
      "Epoch 311/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0473 - acc: 0.6519 - val_loss: 0.0626 - val_acc: 0.5503\n",
      "Epoch 312/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6569 - val_loss: 0.0627 - val_acc: 0.5578\n",
      "Epoch 313/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0473 - acc: 0.6527 - val_loss: 0.0622 - val_acc: 0.5536\n",
      "Epoch 314/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0472 - acc: 0.6499 - val_loss: 0.0621 - val_acc: 0.5611\n",
      "Epoch 315/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0471 - acc: 0.6522 - val_loss: 0.0624 - val_acc: 0.5495\n",
      "Epoch 316/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0471 - acc: 0.6555 - val_loss: 0.0624 - val_acc: 0.5594\n",
      "Epoch 317/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0471 - acc: 0.6497 - val_loss: 0.0633 - val_acc: 0.5445\n",
      "Epoch 318/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6555 - val_loss: 0.0626 - val_acc: 0.5511\n",
      "Epoch 319/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0471 - acc: 0.6535 - val_loss: 0.0626 - val_acc: 0.5520\n",
      "Epoch 320/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6502 - val_loss: 0.0618 - val_acc: 0.5628\n",
      "Epoch 321/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6555 - val_loss: 0.0627 - val_acc: 0.5528\n",
      "Epoch 322/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0474 - acc: 0.6547 - val_loss: 0.0619 - val_acc: 0.5628\n",
      "Epoch 323/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0473 - acc: 0.6524 - val_loss: 0.0629 - val_acc: 0.5528\n",
      "Epoch 324/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0471 - acc: 0.6516 - val_loss: 0.0620 - val_acc: 0.5603\n",
      "Epoch 325/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6541 - val_loss: 0.0624 - val_acc: 0.5520\n",
      "Epoch 326/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0473 - acc: 0.6549 - val_loss: 0.0618 - val_acc: 0.5653\n",
      "Epoch 327/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0471 - acc: 0.6619 - val_loss: 0.0618 - val_acc: 0.5544\n",
      "Epoch 328/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0471 - acc: 0.6544 - val_loss: 0.0620 - val_acc: 0.5503\n",
      "Epoch 329/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0469 - acc: 0.6558 - val_loss: 0.0620 - val_acc: 0.5578\n",
      "Epoch 330/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0471 - acc: 0.6502 - val_loss: 0.0628 - val_acc: 0.5520\n",
      "Epoch 331/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0471 - acc: 0.6547 - val_loss: 0.0636 - val_acc: 0.5387\n",
      "Epoch 332/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0466 - acc: 0.6585 - val_loss: 0.0631 - val_acc: 0.5486\n",
      "Epoch 333/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0472 - acc: 0.6555 - val_loss: 0.0619 - val_acc: 0.5611\n",
      "Epoch 334/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0469 - acc: 0.6513 - val_loss: 0.0618 - val_acc: 0.5569\n",
      "Epoch 335/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0469 - acc: 0.6616 - val_loss: 0.0632 - val_acc: 0.5561\n",
      "Epoch 336/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0473 - acc: 0.6533 - val_loss: 0.0627 - val_acc: 0.5495\n",
      "Epoch 337/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0468 - acc: 0.6552 - val_loss: 0.0617 - val_acc: 0.5553\n",
      "Epoch 338/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0471 - acc: 0.6599 - val_loss: 0.0619 - val_acc: 0.5628\n",
      "Epoch 339/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0468 - acc: 0.6569 - val_loss: 0.0628 - val_acc: 0.5520\n",
      "Epoch 340/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0469 - acc: 0.6585 - val_loss: 0.0631 - val_acc: 0.5486\n",
      "Epoch 341/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6572 - val_loss: 0.0629 - val_acc: 0.5453\n",
      "Epoch 342/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6621 - val_loss: 0.0620 - val_acc: 0.5586\n",
      "Epoch 343/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0469 - acc: 0.6558 - val_loss: 0.0619 - val_acc: 0.5520\n",
      "Epoch 344/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6594 - val_loss: 0.0621 - val_acc: 0.5503\n",
      "Epoch 345/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0468 - acc: 0.6544 - val_loss: 0.0625 - val_acc: 0.5611\n",
      "Epoch 346/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0470 - acc: 0.6563 - val_loss: 0.0626 - val_acc: 0.5578\n",
      "Epoch 347/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6585 - val_loss: 0.0638 - val_acc: 0.5436\n",
      "Epoch 348/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0466 - acc: 0.6588 - val_loss: 0.0628 - val_acc: 0.5486\n",
      "Epoch 349/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0468 - acc: 0.6547 - val_loss: 0.0637 - val_acc: 0.5445\n",
      "Epoch 350/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0469 - acc: 0.6594 - val_loss: 0.0627 - val_acc: 0.5528\n",
      "Epoch 351/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0468 - acc: 0.6594 - val_loss: 0.0636 - val_acc: 0.5470\n",
      "Epoch 352/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6566 - val_loss: 0.0628 - val_acc: 0.5586\n",
      "Epoch 353/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0467 - acc: 0.6594 - val_loss: 0.0631 - val_acc: 0.5503\n",
      "Epoch 354/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0465 - acc: 0.6596 - val_loss: 0.0621 - val_acc: 0.5603\n",
      "Epoch 355/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6552 - val_loss: 0.0629 - val_acc: 0.5503\n",
      "Epoch 356/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0466 - acc: 0.6566 - val_loss: 0.0631 - val_acc: 0.5470\n",
      "Epoch 357/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6574 - val_loss: 0.0622 - val_acc: 0.5569\n",
      "Epoch 358/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0469 - acc: 0.6574 - val_loss: 0.0629 - val_acc: 0.5569\n",
      "Epoch 359/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0463 - acc: 0.6655 - val_loss: 0.0627 - val_acc: 0.5586\n",
      "Epoch 360/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0465 - acc: 0.6563 - val_loss: 0.0637 - val_acc: 0.5503\n",
      "Epoch 361/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0464 - acc: 0.6594 - val_loss: 0.0629 - val_acc: 0.5553\n",
      "Epoch 362/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0466 - acc: 0.6547 - val_loss: 0.0629 - val_acc: 0.5478\n",
      "Epoch 363/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0463 - acc: 0.6608 - val_loss: 0.0624 - val_acc: 0.5661\n",
      "Epoch 364/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0466 - acc: 0.6605 - val_loss: 0.0629 - val_acc: 0.5528\n",
      "Epoch 365/500\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0462 - acc: 0.6624 - val_loss: 0.0638 - val_acc: 0.5428\n",
      "Epoch 366/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0465 - acc: 0.6577 - val_loss: 0.0625 - val_acc: 0.5653\n",
      "Epoch 367/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6588 - val_loss: 0.0629 - val_acc: 0.5578\n",
      "Epoch 368/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0464 - acc: 0.6608 - val_loss: 0.0630 - val_acc: 0.5511\n",
      "Epoch 369/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0462 - acc: 0.6596 - val_loss: 0.0630 - val_acc: 0.5578\n",
      "Epoch 370/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0468 - acc: 0.6516 - val_loss: 0.0623 - val_acc: 0.5561\n",
      "Epoch 371/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0463 - acc: 0.6594 - val_loss: 0.0625 - val_acc: 0.5569\n",
      "Epoch 372/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6635 - val_loss: 0.0625 - val_acc: 0.5561\n",
      "Epoch 373/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0464 - acc: 0.6585 - val_loss: 0.0619 - val_acc: 0.5544\n",
      "Epoch 374/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6591 - val_loss: 0.0634 - val_acc: 0.5561\n",
      "Epoch 375/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0466 - acc: 0.6624 - val_loss: 0.0659 - val_acc: 0.5129\n",
      "Epoch 376/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0464 - acc: 0.6574 - val_loss: 0.0615 - val_acc: 0.5628\n",
      "Epoch 377/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0462 - acc: 0.6605 - val_loss: 0.0653 - val_acc: 0.5170\n",
      "Epoch 378/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0465 - acc: 0.6552 - val_loss: 0.0631 - val_acc: 0.5495\n",
      "Epoch 379/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0466 - acc: 0.6596 - val_loss: 0.0623 - val_acc: 0.5578\n",
      "Epoch 380/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6585 - val_loss: 0.0632 - val_acc: 0.5594\n",
      "Epoch 381/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6605 - val_loss: 0.0624 - val_acc: 0.5619\n",
      "Epoch 382/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0460 - acc: 0.6588 - val_loss: 0.0638 - val_acc: 0.5436\n",
      "Epoch 383/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0465 - acc: 0.6558 - val_loss: 0.0626 - val_acc: 0.5528\n",
      "Epoch 384/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0461 - acc: 0.6638 - val_loss: 0.0628 - val_acc: 0.5495\n",
      "Epoch 385/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0464 - acc: 0.6594 - val_loss: 0.0628 - val_acc: 0.5470\n",
      "Epoch 386/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0462 - acc: 0.6652 - val_loss: 0.0641 - val_acc: 0.5345\n",
      "Epoch 387/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6574 - val_loss: 0.0628 - val_acc: 0.5561\n",
      "Epoch 388/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0460 - acc: 0.6657 - val_loss: 0.0630 - val_acc: 0.5478\n",
      "Epoch 389/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0461 - acc: 0.6613 - val_loss: 0.0641 - val_acc: 0.5445\n",
      "Epoch 390/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0463 - acc: 0.6608 - val_loss: 0.0624 - val_acc: 0.5561\n",
      "Epoch 391/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0461 - acc: 0.6610 - val_loss: 0.0629 - val_acc: 0.5503\n",
      "Epoch 392/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0460 - acc: 0.6646 - val_loss: 0.0633 - val_acc: 0.5420\n",
      "Epoch 393/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0463 - acc: 0.6627 - val_loss: 0.0636 - val_acc: 0.5387\n",
      "Epoch 394/500\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0460 - acc: 0.6638 - val_loss: 0.0627 - val_acc: 0.5520\n",
      "Epoch 395/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0459 - acc: 0.6566 - val_loss: 0.0626 - val_acc: 0.5461\n",
      "Epoch 396/500\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0459 - acc: 0.6649 - val_loss: 0.0640 - val_acc: 0.5362\n",
      "Epoch 397/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0461 - acc: 0.6577 - val_loss: 0.0629 - val_acc: 0.5520\n",
      "Epoch 398/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0462 - acc: 0.6596 - val_loss: 0.0640 - val_acc: 0.5278\n",
      "Epoch 399/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0462 - acc: 0.6635 - val_loss: 0.0622 - val_acc: 0.5511\n",
      "Epoch 400/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0462 - acc: 0.6669 - val_loss: 0.0633 - val_acc: 0.5478\n",
      "Epoch 401/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0459 - acc: 0.6644 - val_loss: 0.0622 - val_acc: 0.5511\n",
      "Epoch 402/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0458 - acc: 0.6646 - val_loss: 0.0631 - val_acc: 0.5536\n",
      "Epoch 403/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0458 - acc: 0.6632 - val_loss: 0.0633 - val_acc: 0.5478\n",
      "Epoch 404/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0461 - acc: 0.6630 - val_loss: 0.0644 - val_acc: 0.5411\n",
      "Epoch 405/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0460 - acc: 0.6641 - val_loss: 0.0641 - val_acc: 0.5561\n",
      "Epoch 406/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0458 - acc: 0.6624 - val_loss: 0.0627 - val_acc: 0.5553\n",
      "Epoch 407/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0457 - acc: 0.6632 - val_loss: 0.0630 - val_acc: 0.5511\n",
      "Epoch 408/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0461 - acc: 0.6602 - val_loss: 0.0628 - val_acc: 0.5536\n",
      "Epoch 409/500\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0459 - acc: 0.6621 - val_loss: 0.0626 - val_acc: 0.5544\n",
      "Epoch 410/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0458 - acc: 0.6638 - val_loss: 0.0627 - val_acc: 0.5478\n",
      "Epoch 411/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0457 - acc: 0.6655 - val_loss: 0.0625 - val_acc: 0.5536\n",
      "Epoch 412/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0459 - acc: 0.6594 - val_loss: 0.0629 - val_acc: 0.5594\n",
      "Epoch 413/500\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0459 - acc: 0.6569 - val_loss: 0.0636 - val_acc: 0.5503\n",
      "Epoch 414/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0455 - acc: 0.6663 - val_loss: 0.0628 - val_acc: 0.5478\n",
      "Epoch 415/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0457 - acc: 0.6616 - val_loss: 0.0639 - val_acc: 0.5411\n",
      "Epoch 416/500\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0459 - acc: 0.6605 - val_loss: 0.0630 - val_acc: 0.5428\n",
      "Epoch 417/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0456 - acc: 0.6635 - val_loss: 0.0641 - val_acc: 0.5395\n",
      "Epoch 418/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0459 - acc: 0.6580 - val_loss: 0.0631 - val_acc: 0.5503\n",
      "Epoch 419/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0458 - acc: 0.6638 - val_loss: 0.0635 - val_acc: 0.5520\n",
      "Epoch 420/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0455 - acc: 0.6666 - val_loss: 0.0640 - val_acc: 0.5470\n",
      "Epoch 421/500\n",
      "3608/3608 [==============================] - 1s 194us/step - loss: 0.0455 - acc: 0.6691 - val_loss: 0.0631 - val_acc: 0.5528\n",
      "Epoch 422/500\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0457 - acc: 0.6652 - val_loss: 0.0637 - val_acc: 0.5470\n",
      "Epoch 423/500\n",
      "3608/3608 [==============================] - 1s 208us/step - loss: 0.0458 - acc: 0.6655 - val_loss: 0.0648 - val_acc: 0.5312\n",
      "Epoch 424/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0456 - acc: 0.6641 - val_loss: 0.0626 - val_acc: 0.5478\n",
      "Epoch 425/500\n",
      "3608/3608 [==============================] - 1s 218us/step - loss: 0.0453 - acc: 0.6638 - val_loss: 0.0630 - val_acc: 0.5536\n",
      "Epoch 426/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0455 - acc: 0.6632 - val_loss: 0.0626 - val_acc: 0.5569\n",
      "Epoch 427/500\n",
      "3608/3608 [==============================] - 1s 208us/step - loss: 0.0458 - acc: 0.6585 - val_loss: 0.0633 - val_acc: 0.5553\n",
      "Epoch 428/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0454 - acc: 0.6649 - val_loss: 0.0637 - val_acc: 0.5395\n",
      "Epoch 429/500\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0455 - acc: 0.6635 - val_loss: 0.0636 - val_acc: 0.5503\n",
      "Epoch 430/500\n",
      "3608/3608 [==============================] - 1s 207us/step - loss: 0.0456 - acc: 0.6644 - val_loss: 0.0638 - val_acc: 0.5486\n",
      "Epoch 431/500\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0452 - acc: 0.6682 - val_loss: 0.0630 - val_acc: 0.5520\n",
      "Epoch 432/500\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0457 - acc: 0.6652 - val_loss: 0.0635 - val_acc: 0.5470\n",
      "Epoch 433/500\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0455 - acc: 0.6644 - val_loss: 0.0637 - val_acc: 0.5445\n",
      "Epoch 434/500\n",
      "3608/3608 [==============================] - 1s 198us/step - loss: 0.0453 - acc: 0.6657 - val_loss: 0.0637 - val_acc: 0.5461\n",
      "Epoch 435/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0454 - acc: 0.6669 - val_loss: 0.0646 - val_acc: 0.5395\n",
      "Epoch 436/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0453 - acc: 0.6691 - val_loss: 0.0649 - val_acc: 0.5320\n",
      "Epoch 437/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0458 - acc: 0.6616 - val_loss: 0.0638 - val_acc: 0.5495\n",
      "Epoch 438/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0454 - acc: 0.6660 - val_loss: 0.0644 - val_acc: 0.5395\n",
      "Epoch 439/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0456 - acc: 0.6594 - val_loss: 0.0636 - val_acc: 0.5511\n",
      "Epoch 440/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0454 - acc: 0.6677 - val_loss: 0.0632 - val_acc: 0.5461\n",
      "Epoch 441/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0457 - acc: 0.6610 - val_loss: 0.0642 - val_acc: 0.5353\n",
      "Epoch 442/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0452 - acc: 0.6657 - val_loss: 0.0642 - val_acc: 0.5461\n",
      "Epoch 443/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0452 - acc: 0.6666 - val_loss: 0.0640 - val_acc: 0.5461\n",
      "Epoch 444/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0458 - acc: 0.6644 - val_loss: 0.0632 - val_acc: 0.5594\n",
      "Epoch 445/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0453 - acc: 0.6691 - val_loss: 0.0630 - val_acc: 0.5594\n",
      "Epoch 446/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0454 - acc: 0.6666 - val_loss: 0.0630 - val_acc: 0.5511\n",
      "Epoch 447/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0454 - acc: 0.6663 - val_loss: 0.0642 - val_acc: 0.5387\n",
      "Epoch 448/500\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0452 - acc: 0.6688 - val_loss: 0.0629 - val_acc: 0.5586\n",
      "Epoch 449/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0455 - acc: 0.6646 - val_loss: 0.0638 - val_acc: 0.5436\n",
      "Epoch 450/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0452 - acc: 0.6682 - val_loss: 0.0631 - val_acc: 0.5520\n",
      "Epoch 451/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0449 - acc: 0.6735 - val_loss: 0.0658 - val_acc: 0.5312\n",
      "Epoch 452/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0452 - acc: 0.6693 - val_loss: 0.0641 - val_acc: 0.5486\n",
      "Epoch 453/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0456 - acc: 0.6613 - val_loss: 0.0631 - val_acc: 0.5544\n",
      "Epoch 454/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0454 - acc: 0.6674 - val_loss: 0.0629 - val_acc: 0.5528\n",
      "Epoch 455/500\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0452 - acc: 0.6721 - val_loss: 0.0638 - val_acc: 0.5495\n",
      "Epoch 456/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0455 - acc: 0.6699 - val_loss: 0.0634 - val_acc: 0.5528\n",
      "Epoch 457/500\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0452 - acc: 0.6754 - val_loss: 0.0640 - val_acc: 0.5428\n",
      "Epoch 458/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0452 - acc: 0.6671 - val_loss: 0.0639 - val_acc: 0.5478\n",
      "Epoch 459/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0455 - acc: 0.6630 - val_loss: 0.0635 - val_acc: 0.5411\n",
      "Epoch 460/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0450 - acc: 0.6705 - val_loss: 0.0637 - val_acc: 0.5495\n",
      "Epoch 461/500\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0449 - acc: 0.6724 - val_loss: 0.0644 - val_acc: 0.5411\n",
      "Epoch 462/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0452 - acc: 0.6696 - val_loss: 0.0635 - val_acc: 0.5478\n",
      "Epoch 463/500\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0456 - acc: 0.6608 - val_loss: 0.0642 - val_acc: 0.5461\n",
      "Epoch 464/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0451 - acc: 0.6671 - val_loss: 0.0642 - val_acc: 0.5478\n",
      "Epoch 465/500\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0450 - acc: 0.6660 - val_loss: 0.0650 - val_acc: 0.5403\n",
      "Epoch 466/500\n",
      "3608/3608 [==============================] - 1s 158us/step - loss: 0.0451 - acc: 0.6644 - val_loss: 0.0632 - val_acc: 0.5436\n",
      "Epoch 467/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0452 - acc: 0.6635 - val_loss: 0.0636 - val_acc: 0.5511\n",
      "Epoch 468/500\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0447 - acc: 0.6716 - val_loss: 0.0635 - val_acc: 0.5619\n",
      "Epoch 469/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0452 - acc: 0.6729 - val_loss: 0.0637 - val_acc: 0.5544\n",
      "Epoch 470/500\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0451 - acc: 0.6669 - val_loss: 0.0652 - val_acc: 0.5212\n",
      "Epoch 471/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0448 - acc: 0.6738 - val_loss: 0.0639 - val_acc: 0.5528\n",
      "Epoch 472/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0450 - acc: 0.6680 - val_loss: 0.0633 - val_acc: 0.5495\n",
      "Epoch 473/500\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0451 - acc: 0.6727 - val_loss: 0.0654 - val_acc: 0.5387\n",
      "Epoch 474/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0451 - acc: 0.6693 - val_loss: 0.0637 - val_acc: 0.5536\n",
      "Epoch 475/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0450 - acc: 0.6718 - val_loss: 0.0644 - val_acc: 0.5420\n",
      "Epoch 476/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0448 - acc: 0.6732 - val_loss: 0.0632 - val_acc: 0.5503\n",
      "Epoch 477/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0448 - acc: 0.6682 - val_loss: 0.0642 - val_acc: 0.5411\n",
      "Epoch 478/500\n",
      "3608/3608 [==============================] - 1s 157us/step - loss: 0.0448 - acc: 0.6710 - val_loss: 0.0639 - val_acc: 0.5528\n",
      "Epoch 479/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0450 - acc: 0.6738 - val_loss: 0.0633 - val_acc: 0.5503\n",
      "Epoch 480/500\n",
      "3608/3608 [==============================] - 1s 159us/step - loss: 0.0448 - acc: 0.6718 - val_loss: 0.0640 - val_acc: 0.5486\n",
      "Epoch 481/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0449 - acc: 0.6682 - val_loss: 0.0644 - val_acc: 0.5503\n",
      "Epoch 482/500\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0451 - acc: 0.6671 - val_loss: 0.0639 - val_acc: 0.5453\n",
      "Epoch 483/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0455 - acc: 0.6663 - val_loss: 0.0631 - val_acc: 0.5511\n",
      "Epoch 484/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0453 - acc: 0.6660 - val_loss: 0.0645 - val_acc: 0.5503\n",
      "Epoch 485/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0447 - acc: 0.6691 - val_loss: 0.0662 - val_acc: 0.5378\n",
      "Epoch 486/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0449 - acc: 0.6705 - val_loss: 0.0636 - val_acc: 0.5569\n",
      "Epoch 487/500\n",
      "3608/3608 [==============================] - 1s 158us/step - loss: 0.0449 - acc: 0.6671 - val_loss: 0.0639 - val_acc: 0.5420\n",
      "Epoch 488/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0450 - acc: 0.6685 - val_loss: 0.0645 - val_acc: 0.5428\n",
      "Epoch 489/500\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0448 - acc: 0.6721 - val_loss: 0.0642 - val_acc: 0.5320\n",
      "Epoch 490/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0449 - acc: 0.6680 - val_loss: 0.0652 - val_acc: 0.5295\n",
      "Epoch 491/500\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0452 - acc: 0.6660 - val_loss: 0.0653 - val_acc: 0.5387\n",
      "Epoch 492/500\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0449 - acc: 0.6718 - val_loss: 0.0641 - val_acc: 0.5486\n",
      "Epoch 493/500\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0446 - acc: 0.6732 - val_loss: 0.0644 - val_acc: 0.5528\n",
      "Epoch 494/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0449 - acc: 0.6727 - val_loss: 0.0637 - val_acc: 0.5503\n",
      "Epoch 495/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0447 - acc: 0.6713 - val_loss: 0.0635 - val_acc: 0.5553\n",
      "Epoch 496/500\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0450 - acc: 0.6602 - val_loss: 0.0646 - val_acc: 0.5445\n",
      "Epoch 497/500\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0447 - acc: 0.6702 - val_loss: 0.0643 - val_acc: 0.5461\n",
      "Epoch 498/500\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0451 - acc: 0.6729 - val_loss: 0.0649 - val_acc: 0.5436\n",
      "Epoch 499/500\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0449 - acc: 0.6655 - val_loss: 0.0636 - val_acc: 0.5536\n",
      "Epoch 500/500\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0448 - acc: 0.6685 - val_loss: 0.0646 - val_acc: 0.5337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77fef56588>"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=16, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "nn_model.fit(x=train_x, y=train_y, epochs=500, validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0jGJza2RZZ7"
   },
   "source": [
    "Now I changed epoches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102272
    },
    "colab_type": "code",
    "id": "GliNY2CWCaSr",
    "outputId": "19147672-f502-43f0-cb15-8ecaca2f18b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 1,754\n",
      "Trainable params: 1,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3608 samples, validate on 1203 samples\n",
      "Epoch 1/3000\n",
      "3608/3608 [==============================] - 1s 257us/step - loss: 0.0691 - acc: 0.4257 - val_loss: 0.0629 - val_acc: 0.4680\n",
      "Epoch 2/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0636 - acc: 0.4656 - val_loss: 0.0607 - val_acc: 0.4821\n",
      "Epoch 3/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0622 - acc: 0.4881 - val_loss: 0.0637 - val_acc: 0.5179\n",
      "Epoch 4/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0618 - acc: 0.5033 - val_loss: 0.0596 - val_acc: 0.5287\n",
      "Epoch 5/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0599 - acc: 0.5247 - val_loss: 0.0574 - val_acc: 0.5594\n",
      "Epoch 6/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0593 - acc: 0.5294 - val_loss: 0.0599 - val_acc: 0.5170\n",
      "Epoch 7/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0588 - acc: 0.5438 - val_loss: 0.0579 - val_acc: 0.5470\n",
      "Epoch 8/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0586 - acc: 0.5435 - val_loss: 0.0591 - val_acc: 0.5395\n",
      "Epoch 9/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0580 - acc: 0.5491 - val_loss: 0.0575 - val_acc: 0.5553\n",
      "Epoch 10/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0582 - acc: 0.5529 - val_loss: 0.0577 - val_acc: 0.5528\n",
      "Epoch 11/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0578 - acc: 0.5574 - val_loss: 0.0579 - val_acc: 0.5520\n",
      "Epoch 12/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0575 - acc: 0.5579 - val_loss: 0.0577 - val_acc: 0.5561\n",
      "Epoch 13/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0581 - acc: 0.5496 - val_loss: 0.0576 - val_acc: 0.5694\n",
      "Epoch 14/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0573 - acc: 0.5660 - val_loss: 0.0582 - val_acc: 0.5411\n",
      "Epoch 15/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0577 - acc: 0.5596 - val_loss: 0.0573 - val_acc: 0.5403\n",
      "Epoch 16/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0570 - acc: 0.5637 - val_loss: 0.0571 - val_acc: 0.5653\n",
      "Epoch 17/3000\n",
      "3608/3608 [==============================] - 1s 159us/step - loss: 0.0572 - acc: 0.5618 - val_loss: 0.0565 - val_acc: 0.5686\n",
      "Epoch 18/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0571 - acc: 0.5610 - val_loss: 0.0574 - val_acc: 0.5569\n",
      "Epoch 19/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0563 - acc: 0.5743 - val_loss: 0.0581 - val_acc: 0.5495\n",
      "Epoch 20/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0566 - acc: 0.5654 - val_loss: 0.0566 - val_acc: 0.5436\n",
      "Epoch 21/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0569 - acc: 0.5565 - val_loss: 0.0577 - val_acc: 0.5561\n",
      "Epoch 22/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0563 - acc: 0.5701 - val_loss: 0.0560 - val_acc: 0.5711\n",
      "Epoch 23/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0562 - acc: 0.5704 - val_loss: 0.0581 - val_acc: 0.5478\n",
      "Epoch 24/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0562 - acc: 0.5690 - val_loss: 0.0569 - val_acc: 0.5553\n",
      "Epoch 25/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0562 - acc: 0.5674 - val_loss: 0.0580 - val_acc: 0.5445\n",
      "Epoch 26/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0562 - acc: 0.5729 - val_loss: 0.0575 - val_acc: 0.5428\n",
      "Epoch 27/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0556 - acc: 0.5759 - val_loss: 0.0574 - val_acc: 0.5619\n",
      "Epoch 28/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0561 - acc: 0.5707 - val_loss: 0.0572 - val_acc: 0.5520\n",
      "Epoch 29/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0559 - acc: 0.5751 - val_loss: 0.0560 - val_acc: 0.5777\n",
      "Epoch 30/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0554 - acc: 0.5748 - val_loss: 0.0563 - val_acc: 0.5819\n",
      "Epoch 31/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0553 - acc: 0.5834 - val_loss: 0.0573 - val_acc: 0.5594\n",
      "Epoch 32/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0555 - acc: 0.5751 - val_loss: 0.0570 - val_acc: 0.5644\n",
      "Epoch 33/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0557 - acc: 0.5782 - val_loss: 0.0591 - val_acc: 0.5486\n",
      "Epoch 34/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0563 - acc: 0.5649 - val_loss: 0.0574 - val_acc: 0.5669\n",
      "Epoch 35/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0559 - acc: 0.5759 - val_loss: 0.0578 - val_acc: 0.5628\n",
      "Epoch 36/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0552 - acc: 0.5773 - val_loss: 0.0573 - val_acc: 0.5653\n",
      "Epoch 37/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0553 - acc: 0.5820 - val_loss: 0.0567 - val_acc: 0.5677\n",
      "Epoch 38/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0549 - acc: 0.5795 - val_loss: 0.0573 - val_acc: 0.5669\n",
      "Epoch 39/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0550 - acc: 0.5818 - val_loss: 0.0563 - val_acc: 0.5719\n",
      "Epoch 40/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0549 - acc: 0.5876 - val_loss: 0.0566 - val_acc: 0.5694\n",
      "Epoch 41/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0547 - acc: 0.5840 - val_loss: 0.0579 - val_acc: 0.5503\n",
      "Epoch 42/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0547 - acc: 0.5887 - val_loss: 0.0572 - val_acc: 0.5561\n",
      "Epoch 43/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0546 - acc: 0.5856 - val_loss: 0.0567 - val_acc: 0.5702\n",
      "Epoch 44/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0561 - acc: 0.5759 - val_loss: 0.0572 - val_acc: 0.5586\n",
      "Epoch 45/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0548 - acc: 0.5845 - val_loss: 0.0569 - val_acc: 0.5636\n",
      "Epoch 46/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0545 - acc: 0.5876 - val_loss: 0.0592 - val_acc: 0.5553\n",
      "Epoch 47/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0545 - acc: 0.5865 - val_loss: 0.0575 - val_acc: 0.5511\n",
      "Epoch 48/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0549 - acc: 0.5820 - val_loss: 0.0579 - val_acc: 0.5511\n",
      "Epoch 49/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0547 - acc: 0.5843 - val_loss: 0.0565 - val_acc: 0.5686\n",
      "Epoch 50/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0545 - acc: 0.5881 - val_loss: 0.0568 - val_acc: 0.5719\n",
      "Epoch 51/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0546 - acc: 0.5834 - val_loss: 0.0584 - val_acc: 0.5536\n",
      "Epoch 52/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0542 - acc: 0.5892 - val_loss: 0.0583 - val_acc: 0.5495\n",
      "Epoch 53/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0546 - acc: 0.5865 - val_loss: 0.0580 - val_acc: 0.5628\n",
      "Epoch 54/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0545 - acc: 0.5881 - val_loss: 0.0584 - val_acc: 0.5594\n",
      "Epoch 55/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0552 - acc: 0.5754 - val_loss: 0.0587 - val_acc: 0.5569\n",
      "Epoch 56/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0542 - acc: 0.5873 - val_loss: 0.0571 - val_acc: 0.5528\n",
      "Epoch 57/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0544 - acc: 0.5831 - val_loss: 0.0569 - val_acc: 0.5603\n",
      "Epoch 58/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0540 - acc: 0.5906 - val_loss: 0.0578 - val_acc: 0.5669\n",
      "Epoch 59/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0541 - acc: 0.5895 - val_loss: 0.0580 - val_acc: 0.5553\n",
      "Epoch 60/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0543 - acc: 0.5876 - val_loss: 0.0569 - val_acc: 0.5628\n",
      "Epoch 61/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0538 - acc: 0.5934 - val_loss: 0.0574 - val_acc: 0.5694\n",
      "Epoch 62/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0544 - acc: 0.5840 - val_loss: 0.0600 - val_acc: 0.5428\n",
      "Epoch 63/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0540 - acc: 0.5926 - val_loss: 0.0570 - val_acc: 0.5603\n",
      "Epoch 64/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0546 - acc: 0.5845 - val_loss: 0.0573 - val_acc: 0.5636\n",
      "Epoch 65/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0548 - acc: 0.5784 - val_loss: 0.0587 - val_acc: 0.5278\n",
      "Epoch 66/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0540 - acc: 0.5906 - val_loss: 0.0573 - val_acc: 0.5603\n",
      "Epoch 67/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0545 - acc: 0.5840 - val_loss: 0.0573 - val_acc: 0.5578\n",
      "Epoch 68/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0537 - acc: 0.5904 - val_loss: 0.0579 - val_acc: 0.5619\n",
      "Epoch 69/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0542 - acc: 0.5873 - val_loss: 0.0589 - val_acc: 0.5544\n",
      "Epoch 70/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0540 - acc: 0.5895 - val_loss: 0.0582 - val_acc: 0.5586\n",
      "Epoch 71/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0539 - acc: 0.5967 - val_loss: 0.0574 - val_acc: 0.5603\n",
      "Epoch 72/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0536 - acc: 0.5931 - val_loss: 0.0568 - val_acc: 0.5669\n",
      "Epoch 73/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0534 - acc: 0.5906 - val_loss: 0.0645 - val_acc: 0.4497\n",
      "Epoch 74/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0536 - acc: 0.6001 - val_loss: 0.0575 - val_acc: 0.5677\n",
      "Epoch 75/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0534 - acc: 0.5989 - val_loss: 0.0589 - val_acc: 0.5669\n",
      "Epoch 76/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0543 - acc: 0.5906 - val_loss: 0.0579 - val_acc: 0.5653\n",
      "Epoch 77/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0535 - acc: 0.5923 - val_loss: 0.0598 - val_acc: 0.5470\n",
      "Epoch 78/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0544 - acc: 0.5890 - val_loss: 0.0582 - val_acc: 0.5486\n",
      "Epoch 79/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0532 - acc: 0.6028 - val_loss: 0.0572 - val_acc: 0.5677\n",
      "Epoch 80/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0536 - acc: 0.5898 - val_loss: 0.0578 - val_acc: 0.5636\n",
      "Epoch 81/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0534 - acc: 0.5962 - val_loss: 0.0585 - val_acc: 0.5661\n",
      "Epoch 82/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0534 - acc: 0.6001 - val_loss: 0.0581 - val_acc: 0.5520\n",
      "Epoch 83/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0533 - acc: 0.5965 - val_loss: 0.0636 - val_acc: 0.4830\n",
      "Epoch 84/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0536 - acc: 0.5942 - val_loss: 0.0609 - val_acc: 0.5328\n",
      "Epoch 85/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0536 - acc: 0.5962 - val_loss: 0.0575 - val_acc: 0.5578\n",
      "Epoch 86/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0539 - acc: 0.5940 - val_loss: 0.0604 - val_acc: 0.5378\n",
      "Epoch 87/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0535 - acc: 0.5937 - val_loss: 0.0600 - val_acc: 0.5436\n",
      "Epoch 88/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0543 - acc: 0.5915 - val_loss: 0.0578 - val_acc: 0.5619\n",
      "Epoch 89/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0540 - acc: 0.5956 - val_loss: 0.0583 - val_acc: 0.5553\n",
      "Epoch 90/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0536 - acc: 0.5984 - val_loss: 0.0587 - val_acc: 0.5653\n",
      "Epoch 91/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0532 - acc: 0.5931 - val_loss: 0.0578 - val_acc: 0.5528\n",
      "Epoch 92/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0533 - acc: 0.6025 - val_loss: 0.0579 - val_acc: 0.5702\n",
      "Epoch 93/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0531 - acc: 0.5987 - val_loss: 0.0592 - val_acc: 0.5362\n",
      "Epoch 94/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0534 - acc: 0.5973 - val_loss: 0.0593 - val_acc: 0.5503\n",
      "Epoch 95/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0538 - acc: 0.5942 - val_loss: 0.0584 - val_acc: 0.5578\n",
      "Epoch 96/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0531 - acc: 0.5987 - val_loss: 0.0585 - val_acc: 0.5603\n",
      "Epoch 97/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0534 - acc: 0.6025 - val_loss: 0.0603 - val_acc: 0.5403\n",
      "Epoch 98/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0536 - acc: 0.5909 - val_loss: 0.0575 - val_acc: 0.5719\n",
      "Epoch 99/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0531 - acc: 0.5992 - val_loss: 0.0580 - val_acc: 0.5520\n",
      "Epoch 100/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0527 - acc: 0.6103 - val_loss: 0.0583 - val_acc: 0.5528\n",
      "Epoch 101/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0532 - acc: 0.5989 - val_loss: 0.0580 - val_acc: 0.5661\n",
      "Epoch 102/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0533 - acc: 0.5962 - val_loss: 0.0592 - val_acc: 0.5520\n",
      "Epoch 103/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0532 - acc: 0.5984 - val_loss: 0.0585 - val_acc: 0.5661\n",
      "Epoch 104/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0530 - acc: 0.5989 - val_loss: 0.0586 - val_acc: 0.5503\n",
      "Epoch 105/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0529 - acc: 0.6045 - val_loss: 0.0593 - val_acc: 0.5470\n",
      "Epoch 106/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0531 - acc: 0.5981 - val_loss: 0.0602 - val_acc: 0.5395\n",
      "Epoch 107/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0530 - acc: 0.6023 - val_loss: 0.0580 - val_acc: 0.5520\n",
      "Epoch 108/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0527 - acc: 0.5956 - val_loss: 0.0581 - val_acc: 0.5628\n",
      "Epoch 109/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0534 - acc: 0.6042 - val_loss: 0.0581 - val_acc: 0.5586\n",
      "Epoch 110/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0529 - acc: 0.6012 - val_loss: 0.0651 - val_acc: 0.4622\n",
      "Epoch 111/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0531 - acc: 0.6031 - val_loss: 0.0590 - val_acc: 0.5586\n",
      "Epoch 112/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0529 - acc: 0.5962 - val_loss: 0.0581 - val_acc: 0.5520\n",
      "Epoch 113/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0526 - acc: 0.5987 - val_loss: 0.0577 - val_acc: 0.5736\n",
      "Epoch 114/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0525 - acc: 0.6089 - val_loss: 0.0581 - val_acc: 0.5569\n",
      "Epoch 115/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0525 - acc: 0.6025 - val_loss: 0.0581 - val_acc: 0.5644\n",
      "Epoch 116/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0531 - acc: 0.6023 - val_loss: 0.0589 - val_acc: 0.5611\n",
      "Epoch 117/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0529 - acc: 0.5973 - val_loss: 0.0584 - val_acc: 0.5511\n",
      "Epoch 118/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0529 - acc: 0.5970 - val_loss: 0.0586 - val_acc: 0.5644\n",
      "Epoch 119/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0526 - acc: 0.6078 - val_loss: 0.0590 - val_acc: 0.5636\n",
      "Epoch 120/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0526 - acc: 0.6048 - val_loss: 0.0589 - val_acc: 0.5736\n",
      "Epoch 121/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0523 - acc: 0.6103 - val_loss: 0.0589 - val_acc: 0.5387\n",
      "Epoch 122/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0526 - acc: 0.6001 - val_loss: 0.0589 - val_acc: 0.5528\n",
      "Epoch 123/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0528 - acc: 0.6023 - val_loss: 0.0584 - val_acc: 0.5677\n",
      "Epoch 124/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0523 - acc: 0.6064 - val_loss: 0.0590 - val_acc: 0.5445\n",
      "Epoch 125/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0527 - acc: 0.6059 - val_loss: 0.0595 - val_acc: 0.5403\n",
      "Epoch 126/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0528 - acc: 0.6020 - val_loss: 0.0593 - val_acc: 0.5495\n",
      "Epoch 127/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0530 - acc: 0.6034 - val_loss: 0.0591 - val_acc: 0.5420\n",
      "Epoch 128/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0523 - acc: 0.6023 - val_loss: 0.0589 - val_acc: 0.5503\n",
      "Epoch 129/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0524 - acc: 0.6050 - val_loss: 0.0595 - val_acc: 0.5370\n",
      "Epoch 130/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0527 - acc: 0.6039 - val_loss: 0.0587 - val_acc: 0.5520\n",
      "Epoch 131/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0526 - acc: 0.6067 - val_loss: 0.0597 - val_acc: 0.5553\n",
      "Epoch 132/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0529 - acc: 0.6042 - val_loss: 0.0594 - val_acc: 0.5544\n",
      "Epoch 133/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0525 - acc: 0.6025 - val_loss: 0.0590 - val_acc: 0.5578\n",
      "Epoch 134/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0529 - acc: 0.6014 - val_loss: 0.0587 - val_acc: 0.5661\n",
      "Epoch 135/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0526 - acc: 0.6059 - val_loss: 0.0591 - val_acc: 0.5536\n",
      "Epoch 136/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0521 - acc: 0.6098 - val_loss: 0.0592 - val_acc: 0.5553\n",
      "Epoch 137/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0525 - acc: 0.6012 - val_loss: 0.0589 - val_acc: 0.5636\n",
      "Epoch 138/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0528 - acc: 0.6001 - val_loss: 0.0608 - val_acc: 0.5337\n",
      "Epoch 139/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0524 - acc: 0.6070 - val_loss: 0.0602 - val_acc: 0.5403\n",
      "Epoch 140/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0525 - acc: 0.6006 - val_loss: 0.0595 - val_acc: 0.5495\n",
      "Epoch 141/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0526 - acc: 0.6050 - val_loss: 0.0588 - val_acc: 0.5594\n",
      "Epoch 142/3000\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0525 - acc: 0.6001 - val_loss: 0.0590 - val_acc: 0.5603\n",
      "Epoch 143/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0528 - acc: 0.5984 - val_loss: 0.0590 - val_acc: 0.5503\n",
      "Epoch 144/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0523 - acc: 0.6056 - val_loss: 0.0586 - val_acc: 0.5561\n",
      "Epoch 145/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0523 - acc: 0.6075 - val_loss: 0.0590 - val_acc: 0.5536\n",
      "Epoch 146/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0522 - acc: 0.6075 - val_loss: 0.0598 - val_acc: 0.5470\n",
      "Epoch 147/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0523 - acc: 0.6048 - val_loss: 0.0589 - val_acc: 0.5511\n",
      "Epoch 148/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0523 - acc: 0.6084 - val_loss: 0.0592 - val_acc: 0.5536\n",
      "Epoch 149/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0521 - acc: 0.6081 - val_loss: 0.0593 - val_acc: 0.5553\n",
      "Epoch 150/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0524 - acc: 0.6023 - val_loss: 0.0587 - val_acc: 0.5653\n",
      "Epoch 151/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0520 - acc: 0.6078 - val_loss: 0.0595 - val_acc: 0.5520\n",
      "Epoch 152/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0524 - acc: 0.6064 - val_loss: 0.0594 - val_acc: 0.5603\n",
      "Epoch 153/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0525 - acc: 0.5984 - val_loss: 0.0596 - val_acc: 0.5578\n",
      "Epoch 154/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0523 - acc: 0.6031 - val_loss: 0.0592 - val_acc: 0.5520\n",
      "Epoch 155/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0521 - acc: 0.6073 - val_loss: 0.0594 - val_acc: 0.5503\n",
      "Epoch 156/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0521 - acc: 0.6100 - val_loss: 0.0596 - val_acc: 0.5403\n",
      "Epoch 157/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0523 - acc: 0.6037 - val_loss: 0.0593 - val_acc: 0.5461\n",
      "Epoch 158/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0520 - acc: 0.6064 - val_loss: 0.0594 - val_acc: 0.5569\n",
      "Epoch 159/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0517 - acc: 0.6098 - val_loss: 0.0597 - val_acc: 0.5495\n",
      "Epoch 160/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0521 - acc: 0.6075 - val_loss: 0.0598 - val_acc: 0.5453\n",
      "Epoch 161/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0522 - acc: 0.6062 - val_loss: 0.0588 - val_acc: 0.5578\n",
      "Epoch 162/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0520 - acc: 0.6081 - val_loss: 0.0590 - val_acc: 0.5569\n",
      "Epoch 163/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0523 - acc: 0.6123 - val_loss: 0.0591 - val_acc: 0.5528\n",
      "Epoch 164/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0523 - acc: 0.6086 - val_loss: 0.0599 - val_acc: 0.5470\n",
      "Epoch 165/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0522 - acc: 0.6081 - val_loss: 0.0593 - val_acc: 0.5486\n",
      "Epoch 166/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0522 - acc: 0.6100 - val_loss: 0.0591 - val_acc: 0.5644\n",
      "Epoch 167/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0521 - acc: 0.6098 - val_loss: 0.0586 - val_acc: 0.5669\n",
      "Epoch 168/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0518 - acc: 0.6056 - val_loss: 0.0603 - val_acc: 0.5387\n",
      "Epoch 169/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0520 - acc: 0.6075 - val_loss: 0.0596 - val_acc: 0.5561\n",
      "Epoch 170/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0520 - acc: 0.6098 - val_loss: 0.0601 - val_acc: 0.5594\n",
      "Epoch 171/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0519 - acc: 0.6106 - val_loss: 0.0600 - val_acc: 0.5603\n",
      "Epoch 172/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0520 - acc: 0.6095 - val_loss: 0.0595 - val_acc: 0.5536\n",
      "Epoch 173/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0518 - acc: 0.6125 - val_loss: 0.0600 - val_acc: 0.5594\n",
      "Epoch 174/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0525 - acc: 0.6031 - val_loss: 0.0605 - val_acc: 0.5303\n",
      "Epoch 175/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0517 - acc: 0.6159 - val_loss: 0.0606 - val_acc: 0.5328\n",
      "Epoch 176/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0519 - acc: 0.6156 - val_loss: 0.0602 - val_acc: 0.5395\n",
      "Epoch 177/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0519 - acc: 0.6084 - val_loss: 0.0593 - val_acc: 0.5578\n",
      "Epoch 178/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0521 - acc: 0.6070 - val_loss: 0.0599 - val_acc: 0.5395\n",
      "Epoch 179/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0519 - acc: 0.6134 - val_loss: 0.0595 - val_acc: 0.5503\n",
      "Epoch 180/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0520 - acc: 0.6020 - val_loss: 0.0614 - val_acc: 0.5453\n",
      "Epoch 181/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0517 - acc: 0.6123 - val_loss: 0.0608 - val_acc: 0.5420\n",
      "Epoch 182/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0518 - acc: 0.6111 - val_loss: 0.0612 - val_acc: 0.5387\n",
      "Epoch 183/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0515 - acc: 0.6142 - val_loss: 0.0602 - val_acc: 0.5461\n",
      "Epoch 184/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0516 - acc: 0.6106 - val_loss: 0.0618 - val_acc: 0.5220\n",
      "Epoch 185/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0521 - acc: 0.6062 - val_loss: 0.0596 - val_acc: 0.5536\n",
      "Epoch 186/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0516 - acc: 0.6147 - val_loss: 0.0606 - val_acc: 0.5461\n",
      "Epoch 187/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0518 - acc: 0.6100 - val_loss: 0.0592 - val_acc: 0.5569\n",
      "Epoch 188/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0519 - acc: 0.6086 - val_loss: 0.0617 - val_acc: 0.5362\n",
      "Epoch 189/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0516 - acc: 0.6156 - val_loss: 0.0603 - val_acc: 0.5503\n",
      "Epoch 190/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0519 - acc: 0.6067 - val_loss: 0.0597 - val_acc: 0.5511\n",
      "Epoch 191/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0518 - acc: 0.6092 - val_loss: 0.0599 - val_acc: 0.5603\n",
      "Epoch 192/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0527 - acc: 0.6014 - val_loss: 0.0600 - val_acc: 0.5486\n",
      "Epoch 193/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0514 - acc: 0.6075 - val_loss: 0.0601 - val_acc: 0.5611\n",
      "Epoch 194/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0516 - acc: 0.6120 - val_loss: 0.0597 - val_acc: 0.5470\n",
      "Epoch 195/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0512 - acc: 0.6117 - val_loss: 0.0608 - val_acc: 0.5503\n",
      "Epoch 196/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0515 - acc: 0.6142 - val_loss: 0.0601 - val_acc: 0.5561\n",
      "Epoch 197/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0517 - acc: 0.6106 - val_loss: 0.0598 - val_acc: 0.5569\n",
      "Epoch 198/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0515 - acc: 0.6153 - val_loss: 0.0615 - val_acc: 0.5295\n",
      "Epoch 199/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0517 - acc: 0.6109 - val_loss: 0.0603 - val_acc: 0.5553\n",
      "Epoch 200/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0512 - acc: 0.6139 - val_loss: 0.0607 - val_acc: 0.5470\n",
      "Epoch 201/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0519 - acc: 0.6086 - val_loss: 0.0608 - val_acc: 0.5420\n",
      "Epoch 202/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0513 - acc: 0.6142 - val_loss: 0.0607 - val_acc: 0.5511\n",
      "Epoch 203/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0514 - acc: 0.6111 - val_loss: 0.0599 - val_acc: 0.5461\n",
      "Epoch 204/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0512 - acc: 0.6189 - val_loss: 0.0609 - val_acc: 0.5395\n",
      "Epoch 205/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0518 - acc: 0.6095 - val_loss: 0.0600 - val_acc: 0.5511\n",
      "Epoch 206/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0516 - acc: 0.6145 - val_loss: 0.0601 - val_acc: 0.5453\n",
      "Epoch 207/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0521 - acc: 0.6078 - val_loss: 0.0610 - val_acc: 0.5495\n",
      "Epoch 208/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0516 - acc: 0.6120 - val_loss: 0.0605 - val_acc: 0.5544\n",
      "Epoch 209/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0517 - acc: 0.6081 - val_loss: 0.0611 - val_acc: 0.5453\n",
      "Epoch 210/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0515 - acc: 0.6136 - val_loss: 0.0610 - val_acc: 0.5278\n",
      "Epoch 211/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0512 - acc: 0.6106 - val_loss: 0.0597 - val_acc: 0.5544\n",
      "Epoch 212/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0519 - acc: 0.6134 - val_loss: 0.0599 - val_acc: 0.5403\n",
      "Epoch 213/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0512 - acc: 0.6136 - val_loss: 0.0599 - val_acc: 0.5561\n",
      "Epoch 214/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0513 - acc: 0.6095 - val_loss: 0.0610 - val_acc: 0.5461\n",
      "Epoch 215/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0512 - acc: 0.6195 - val_loss: 0.0607 - val_acc: 0.5420\n",
      "Epoch 216/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0512 - acc: 0.6181 - val_loss: 0.0605 - val_acc: 0.5436\n",
      "Epoch 217/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0511 - acc: 0.6161 - val_loss: 0.0608 - val_acc: 0.5453\n",
      "Epoch 218/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0514 - acc: 0.6123 - val_loss: 0.0599 - val_acc: 0.5553\n",
      "Epoch 219/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0515 - acc: 0.6128 - val_loss: 0.0616 - val_acc: 0.5370\n",
      "Epoch 220/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0521 - acc: 0.6142 - val_loss: 0.0598 - val_acc: 0.5569\n",
      "Epoch 221/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0518 - acc: 0.6125 - val_loss: 0.0619 - val_acc: 0.5420\n",
      "Epoch 222/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0518 - acc: 0.6203 - val_loss: 0.0602 - val_acc: 0.5520\n",
      "Epoch 223/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0512 - acc: 0.6142 - val_loss: 0.0608 - val_acc: 0.5411\n",
      "Epoch 224/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0517 - acc: 0.6070 - val_loss: 0.0600 - val_acc: 0.5436\n",
      "Epoch 225/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0511 - acc: 0.6197 - val_loss: 0.0612 - val_acc: 0.5445\n",
      "Epoch 226/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0516 - acc: 0.6142 - val_loss: 0.0602 - val_acc: 0.5578\n",
      "Epoch 227/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0511 - acc: 0.6139 - val_loss: 0.0603 - val_acc: 0.5511\n",
      "Epoch 228/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0512 - acc: 0.6172 - val_loss: 0.0611 - val_acc: 0.5470\n",
      "Epoch 229/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0511 - acc: 0.6206 - val_loss: 0.0596 - val_acc: 0.5603\n",
      "Epoch 230/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0511 - acc: 0.6123 - val_loss: 0.0603 - val_acc: 0.5478\n",
      "Epoch 231/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0510 - acc: 0.6225 - val_loss: 0.0601 - val_acc: 0.5594\n",
      "Epoch 232/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0512 - acc: 0.6142 - val_loss: 0.0609 - val_acc: 0.5362\n",
      "Epoch 233/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0509 - acc: 0.6206 - val_loss: 0.0601 - val_acc: 0.5553\n",
      "Epoch 234/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0511 - acc: 0.6192 - val_loss: 0.0601 - val_acc: 0.5569\n",
      "Epoch 235/3000\n",
      "3608/3608 [==============================] - 1s 194us/step - loss: 0.0512 - acc: 0.6153 - val_loss: 0.0600 - val_acc: 0.5553\n",
      "Epoch 236/3000\n",
      "3608/3608 [==============================] - 1s 218us/step - loss: 0.0510 - acc: 0.6161 - val_loss: 0.0610 - val_acc: 0.5395\n",
      "Epoch 237/3000\n",
      "3608/3608 [==============================] - 1s 218us/step - loss: 0.0509 - acc: 0.6225 - val_loss: 0.0607 - val_acc: 0.5594\n",
      "Epoch 238/3000\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0510 - acc: 0.6159 - val_loss: 0.0602 - val_acc: 0.5420\n",
      "Epoch 239/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0507 - acc: 0.6170 - val_loss: 0.0612 - val_acc: 0.5453\n",
      "Epoch 240/3000\n",
      "3608/3608 [==============================] - 1s 220us/step - loss: 0.0512 - acc: 0.6186 - val_loss: 0.0607 - val_acc: 0.5586\n",
      "Epoch 241/3000\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0510 - acc: 0.6225 - val_loss: 0.0611 - val_acc: 0.5536\n",
      "Epoch 242/3000\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0512 - acc: 0.6114 - val_loss: 0.0609 - val_acc: 0.5370\n",
      "Epoch 243/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0511 - acc: 0.6197 - val_loss: 0.0603 - val_acc: 0.5536\n",
      "Epoch 244/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0508 - acc: 0.6214 - val_loss: 0.0603 - val_acc: 0.5553\n",
      "Epoch 245/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0511 - acc: 0.6159 - val_loss: 0.0610 - val_acc: 0.5411\n",
      "Epoch 246/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0522 - acc: 0.6075 - val_loss: 0.0616 - val_acc: 0.5328\n",
      "Epoch 247/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0515 - acc: 0.6159 - val_loss: 0.0600 - val_acc: 0.5553\n",
      "Epoch 248/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0513 - acc: 0.6178 - val_loss: 0.0599 - val_acc: 0.5586\n",
      "Epoch 249/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0506 - acc: 0.6183 - val_loss: 0.0601 - val_acc: 0.5470\n",
      "Epoch 250/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0505 - acc: 0.6292 - val_loss: 0.0604 - val_acc: 0.5520\n",
      "Epoch 251/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0508 - acc: 0.6225 - val_loss: 0.0607 - val_acc: 0.5461\n",
      "Epoch 252/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0509 - acc: 0.6172 - val_loss: 0.0611 - val_acc: 0.5395\n",
      "Epoch 253/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0509 - acc: 0.6134 - val_loss: 0.0608 - val_acc: 0.5511\n",
      "Epoch 254/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0508 - acc: 0.6189 - val_loss: 0.0604 - val_acc: 0.5445\n",
      "Epoch 255/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0509 - acc: 0.6225 - val_loss: 0.0612 - val_acc: 0.5461\n",
      "Epoch 256/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0504 - acc: 0.6303 - val_loss: 0.0625 - val_acc: 0.5378\n",
      "Epoch 257/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0511 - acc: 0.6161 - val_loss: 0.0604 - val_acc: 0.5486\n",
      "Epoch 258/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0508 - acc: 0.6236 - val_loss: 0.0599 - val_acc: 0.5619\n",
      "Epoch 259/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0515 - acc: 0.6125 - val_loss: 0.0613 - val_acc: 0.5470\n",
      "Epoch 260/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0509 - acc: 0.6211 - val_loss: 0.0611 - val_acc: 0.5578\n",
      "Epoch 261/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0507 - acc: 0.6175 - val_loss: 0.0603 - val_acc: 0.5528\n",
      "Epoch 262/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0504 - acc: 0.6200 - val_loss: 0.0610 - val_acc: 0.5461\n",
      "Epoch 263/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0507 - acc: 0.6142 - val_loss: 0.0607 - val_acc: 0.5544\n",
      "Epoch 264/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0506 - acc: 0.6214 - val_loss: 0.0609 - val_acc: 0.5578\n",
      "Epoch 265/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0508 - acc: 0.6181 - val_loss: 0.0616 - val_acc: 0.5420\n",
      "Epoch 266/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0505 - acc: 0.6225 - val_loss: 0.0606 - val_acc: 0.5436\n",
      "Epoch 267/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0509 - acc: 0.6195 - val_loss: 0.0607 - val_acc: 0.5511\n",
      "Epoch 268/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0509 - acc: 0.6253 - val_loss: 0.0620 - val_acc: 0.5420\n",
      "Epoch 269/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0512 - acc: 0.6170 - val_loss: 0.0607 - val_acc: 0.5503\n",
      "Epoch 270/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0507 - acc: 0.6239 - val_loss: 0.0617 - val_acc: 0.5511\n",
      "Epoch 271/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0505 - acc: 0.6244 - val_loss: 0.0623 - val_acc: 0.5237\n",
      "Epoch 272/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0508 - acc: 0.6161 - val_loss: 0.0617 - val_acc: 0.5470\n",
      "Epoch 273/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0504 - acc: 0.6242 - val_loss: 0.0613 - val_acc: 0.5362\n",
      "Epoch 274/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0506 - acc: 0.6208 - val_loss: 0.0622 - val_acc: 0.5478\n",
      "Epoch 275/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0506 - acc: 0.6225 - val_loss: 0.0610 - val_acc: 0.5470\n",
      "Epoch 276/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0502 - acc: 0.6292 - val_loss: 0.0626 - val_acc: 0.5262\n",
      "Epoch 277/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0506 - acc: 0.6195 - val_loss: 0.0618 - val_acc: 0.5353\n",
      "Epoch 278/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0508 - acc: 0.6167 - val_loss: 0.0616 - val_acc: 0.5536\n",
      "Epoch 279/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0507 - acc: 0.6256 - val_loss: 0.0615 - val_acc: 0.5420\n",
      "Epoch 280/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0503 - acc: 0.6220 - val_loss: 0.0617 - val_acc: 0.5478\n",
      "Epoch 281/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0505 - acc: 0.6269 - val_loss: 0.0605 - val_acc: 0.5520\n",
      "Epoch 282/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0501 - acc: 0.6269 - val_loss: 0.0606 - val_acc: 0.5470\n",
      "Epoch 283/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0505 - acc: 0.6272 - val_loss: 0.0610 - val_acc: 0.5436\n",
      "Epoch 284/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0505 - acc: 0.6267 - val_loss: 0.0607 - val_acc: 0.5428\n",
      "Epoch 285/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0505 - acc: 0.6264 - val_loss: 0.0618 - val_acc: 0.5395\n",
      "Epoch 286/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0508 - acc: 0.6189 - val_loss: 0.0616 - val_acc: 0.5536\n",
      "Epoch 287/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0511 - acc: 0.6225 - val_loss: 0.0608 - val_acc: 0.5520\n",
      "Epoch 288/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0510 - acc: 0.6208 - val_loss: 0.0611 - val_acc: 0.5478\n",
      "Epoch 289/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0504 - acc: 0.6228 - val_loss: 0.0610 - val_acc: 0.5511\n",
      "Epoch 290/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0507 - acc: 0.6214 - val_loss: 0.0611 - val_acc: 0.5403\n",
      "Epoch 291/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0503 - acc: 0.6242 - val_loss: 0.0604 - val_acc: 0.5520\n",
      "Epoch 292/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0502 - acc: 0.6242 - val_loss: 0.0618 - val_acc: 0.5436\n",
      "Epoch 293/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0507 - acc: 0.6181 - val_loss: 0.0611 - val_acc: 0.5636\n",
      "Epoch 294/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0510 - acc: 0.6161 - val_loss: 0.0615 - val_acc: 0.5461\n",
      "Epoch 295/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0510 - acc: 0.6208 - val_loss: 0.0613 - val_acc: 0.5411\n",
      "Epoch 296/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0507 - acc: 0.6195 - val_loss: 0.0619 - val_acc: 0.5503\n",
      "Epoch 297/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0510 - acc: 0.6211 - val_loss: 0.0613 - val_acc: 0.5387\n",
      "Epoch 298/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0505 - acc: 0.6225 - val_loss: 0.0614 - val_acc: 0.5486\n",
      "Epoch 299/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0503 - acc: 0.6256 - val_loss: 0.0613 - val_acc: 0.5453\n",
      "Epoch 300/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0500 - acc: 0.6311 - val_loss: 0.0607 - val_acc: 0.5461\n",
      "Epoch 301/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0501 - acc: 0.6250 - val_loss: 0.0620 - val_acc: 0.5387\n",
      "Epoch 302/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0505 - acc: 0.6222 - val_loss: 0.0637 - val_acc: 0.5229\n",
      "Epoch 303/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0501 - acc: 0.6261 - val_loss: 0.0612 - val_acc: 0.5503\n",
      "Epoch 304/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0501 - acc: 0.6278 - val_loss: 0.0612 - val_acc: 0.5470\n",
      "Epoch 305/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0502 - acc: 0.6272 - val_loss: 0.0618 - val_acc: 0.5461\n",
      "Epoch 306/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0504 - acc: 0.6308 - val_loss: 0.0616 - val_acc: 0.5503\n",
      "Epoch 307/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0502 - acc: 0.6275 - val_loss: 0.0617 - val_acc: 0.5528\n",
      "Epoch 308/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0501 - acc: 0.6247 - val_loss: 0.0615 - val_acc: 0.5495\n",
      "Epoch 309/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0504 - acc: 0.6231 - val_loss: 0.0613 - val_acc: 0.5478\n",
      "Epoch 310/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0503 - acc: 0.6217 - val_loss: 0.0613 - val_acc: 0.5478\n",
      "Epoch 311/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0505 - acc: 0.6222 - val_loss: 0.0625 - val_acc: 0.5378\n",
      "Epoch 312/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0504 - acc: 0.6206 - val_loss: 0.0628 - val_acc: 0.5478\n",
      "Epoch 313/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0507 - acc: 0.6231 - val_loss: 0.0621 - val_acc: 0.5362\n",
      "Epoch 314/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0512 - acc: 0.6161 - val_loss: 0.0614 - val_acc: 0.5461\n",
      "Epoch 315/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0500 - acc: 0.6289 - val_loss: 0.0613 - val_acc: 0.5520\n",
      "Epoch 316/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0502 - acc: 0.6225 - val_loss: 0.0613 - val_acc: 0.5503\n",
      "Epoch 317/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0503 - acc: 0.6233 - val_loss: 0.0613 - val_acc: 0.5478\n",
      "Epoch 318/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0502 - acc: 0.6211 - val_loss: 0.0621 - val_acc: 0.5436\n",
      "Epoch 319/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0513 - acc: 0.6189 - val_loss: 0.0628 - val_acc: 0.5378\n",
      "Epoch 320/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0503 - acc: 0.6264 - val_loss: 0.0610 - val_acc: 0.5503\n",
      "Epoch 321/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0505 - acc: 0.6206 - val_loss: 0.0622 - val_acc: 0.5337\n",
      "Epoch 322/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0505 - acc: 0.6220 - val_loss: 0.0612 - val_acc: 0.5569\n",
      "Epoch 323/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0498 - acc: 0.6256 - val_loss: 0.0631 - val_acc: 0.5436\n",
      "Epoch 324/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0505 - acc: 0.6233 - val_loss: 0.0615 - val_acc: 0.5486\n",
      "Epoch 325/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0501 - acc: 0.6250 - val_loss: 0.0630 - val_acc: 0.5312\n",
      "Epoch 326/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0504 - acc: 0.6278 - val_loss: 0.0612 - val_acc: 0.5520\n",
      "Epoch 327/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0501 - acc: 0.6314 - val_loss: 0.0616 - val_acc: 0.5503\n",
      "Epoch 328/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0501 - acc: 0.6308 - val_loss: 0.0617 - val_acc: 0.5594\n",
      "Epoch 329/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0501 - acc: 0.6319 - val_loss: 0.0616 - val_acc: 0.5511\n",
      "Epoch 330/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0503 - acc: 0.6275 - val_loss: 0.0614 - val_acc: 0.5569\n",
      "Epoch 331/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0500 - acc: 0.6253 - val_loss: 0.0609 - val_acc: 0.5586\n",
      "Epoch 332/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0511 - acc: 0.6244 - val_loss: 0.0613 - val_acc: 0.5453\n",
      "Epoch 333/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0498 - acc: 0.6314 - val_loss: 0.0619 - val_acc: 0.5586\n",
      "Epoch 334/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0504 - acc: 0.6247 - val_loss: 0.0615 - val_acc: 0.5503\n",
      "Epoch 335/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0502 - acc: 0.6325 - val_loss: 0.0611 - val_acc: 0.5553\n",
      "Epoch 336/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0503 - acc: 0.6292 - val_loss: 0.0611 - val_acc: 0.5445\n",
      "Epoch 337/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0501 - acc: 0.6336 - val_loss: 0.0614 - val_acc: 0.5387\n",
      "Epoch 338/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0506 - acc: 0.6292 - val_loss: 0.0620 - val_acc: 0.5453\n",
      "Epoch 339/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0504 - acc: 0.6261 - val_loss: 0.0642 - val_acc: 0.5270\n",
      "Epoch 340/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0502 - acc: 0.6275 - val_loss: 0.0624 - val_acc: 0.5544\n",
      "Epoch 341/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0500 - acc: 0.6344 - val_loss: 0.0613 - val_acc: 0.5553\n",
      "Epoch 342/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0500 - acc: 0.6377 - val_loss: 0.0610 - val_acc: 0.5569\n",
      "Epoch 343/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0503 - acc: 0.6322 - val_loss: 0.0635 - val_acc: 0.5312\n",
      "Epoch 344/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0503 - acc: 0.6325 - val_loss: 0.0612 - val_acc: 0.5578\n",
      "Epoch 345/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0499 - acc: 0.6289 - val_loss: 0.0617 - val_acc: 0.5544\n",
      "Epoch 346/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0500 - acc: 0.6311 - val_loss: 0.0628 - val_acc: 0.5461\n",
      "Epoch 347/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0500 - acc: 0.6283 - val_loss: 0.0622 - val_acc: 0.5445\n",
      "Epoch 348/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0502 - acc: 0.6325 - val_loss: 0.0621 - val_acc: 0.5420\n",
      "Epoch 349/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0504 - acc: 0.6239 - val_loss: 0.0626 - val_acc: 0.5370\n",
      "Epoch 350/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0498 - acc: 0.6294 - val_loss: 0.0618 - val_acc: 0.5461\n",
      "Epoch 351/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0504 - acc: 0.6242 - val_loss: 0.0632 - val_acc: 0.5278\n",
      "Epoch 352/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0503 - acc: 0.6283 - val_loss: 0.0618 - val_acc: 0.5495\n",
      "Epoch 353/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0501 - acc: 0.6256 - val_loss: 0.0618 - val_acc: 0.5470\n",
      "Epoch 354/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0501 - acc: 0.6261 - val_loss: 0.0616 - val_acc: 0.5420\n",
      "Epoch 355/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0500 - acc: 0.6344 - val_loss: 0.0624 - val_acc: 0.5461\n",
      "Epoch 356/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0505 - acc: 0.6192 - val_loss: 0.0623 - val_acc: 0.5403\n",
      "Epoch 357/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0500 - acc: 0.6269 - val_loss: 0.0616 - val_acc: 0.5578\n",
      "Epoch 358/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0500 - acc: 0.6364 - val_loss: 0.0615 - val_acc: 0.5445\n",
      "Epoch 359/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0503 - acc: 0.6253 - val_loss: 0.0612 - val_acc: 0.5528\n",
      "Epoch 360/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0495 - acc: 0.6361 - val_loss: 0.0617 - val_acc: 0.5553\n",
      "Epoch 361/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0501 - acc: 0.6325 - val_loss: 0.0616 - val_acc: 0.5611\n",
      "Epoch 362/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0500 - acc: 0.6322 - val_loss: 0.0617 - val_acc: 0.5436\n",
      "Epoch 363/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0500 - acc: 0.6280 - val_loss: 0.0627 - val_acc: 0.5420\n",
      "Epoch 364/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0502 - acc: 0.6283 - val_loss: 0.0616 - val_acc: 0.5470\n",
      "Epoch 365/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0495 - acc: 0.6416 - val_loss: 0.0622 - val_acc: 0.5503\n",
      "Epoch 366/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0499 - acc: 0.6336 - val_loss: 0.0613 - val_acc: 0.5461\n",
      "Epoch 367/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0499 - acc: 0.6258 - val_loss: 0.0612 - val_acc: 0.5528\n",
      "Epoch 368/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0496 - acc: 0.6319 - val_loss: 0.0620 - val_acc: 0.5495\n",
      "Epoch 369/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0499 - acc: 0.6283 - val_loss: 0.0624 - val_acc: 0.5378\n",
      "Epoch 370/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0507 - acc: 0.6236 - val_loss: 0.0618 - val_acc: 0.5536\n",
      "Epoch 371/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0504 - acc: 0.6303 - val_loss: 0.0614 - val_acc: 0.5511\n",
      "Epoch 372/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0506 - acc: 0.6275 - val_loss: 0.0614 - val_acc: 0.5445\n",
      "Epoch 373/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0500 - acc: 0.6322 - val_loss: 0.0618 - val_acc: 0.5586\n",
      "Epoch 374/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0499 - acc: 0.6317 - val_loss: 0.0648 - val_acc: 0.5254\n",
      "Epoch 375/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0501 - acc: 0.6283 - val_loss: 0.0624 - val_acc: 0.5561\n",
      "Epoch 376/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0510 - acc: 0.6233 - val_loss: 0.0617 - val_acc: 0.5503\n",
      "Epoch 377/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0503 - acc: 0.6272 - val_loss: 0.0620 - val_acc: 0.5536\n",
      "Epoch 378/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0497 - acc: 0.6339 - val_loss: 0.0621 - val_acc: 0.5461\n",
      "Epoch 379/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0499 - acc: 0.6297 - val_loss: 0.0614 - val_acc: 0.5553\n",
      "Epoch 380/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0497 - acc: 0.6269 - val_loss: 0.0621 - val_acc: 0.5470\n",
      "Epoch 381/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0501 - acc: 0.6292 - val_loss: 0.0622 - val_acc: 0.5503\n",
      "Epoch 382/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0494 - acc: 0.6339 - val_loss: 0.0622 - val_acc: 0.5453\n",
      "Epoch 383/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0499 - acc: 0.6344 - val_loss: 0.0624 - val_acc: 0.5411\n",
      "Epoch 384/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0502 - acc: 0.6203 - val_loss: 0.0615 - val_acc: 0.5553\n",
      "Epoch 385/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0499 - acc: 0.6314 - val_loss: 0.0620 - val_acc: 0.5586\n",
      "Epoch 386/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0500 - acc: 0.6297 - val_loss: 0.0631 - val_acc: 0.5303\n",
      "Epoch 387/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0499 - acc: 0.6308 - val_loss: 0.0622 - val_acc: 0.5528\n",
      "Epoch 388/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0503 - acc: 0.6278 - val_loss: 0.0618 - val_acc: 0.5411\n",
      "Epoch 389/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0499 - acc: 0.6311 - val_loss: 0.0626 - val_acc: 0.5470\n",
      "Epoch 390/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0498 - acc: 0.6322 - val_loss: 0.0615 - val_acc: 0.5403\n",
      "Epoch 391/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0493 - acc: 0.6386 - val_loss: 0.0617 - val_acc: 0.5453\n",
      "Epoch 392/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0497 - acc: 0.6350 - val_loss: 0.0623 - val_acc: 0.5495\n",
      "Epoch 393/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0497 - acc: 0.6303 - val_loss: 0.0626 - val_acc: 0.5378\n",
      "Epoch 394/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0496 - acc: 0.6366 - val_loss: 0.0623 - val_acc: 0.5586\n",
      "Epoch 395/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0500 - acc: 0.6322 - val_loss: 0.0627 - val_acc: 0.5495\n",
      "Epoch 396/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0497 - acc: 0.6328 - val_loss: 0.0627 - val_acc: 0.5511\n",
      "Epoch 397/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0503 - acc: 0.6325 - val_loss: 0.0619 - val_acc: 0.5520\n",
      "Epoch 398/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0495 - acc: 0.6369 - val_loss: 0.0626 - val_acc: 0.5411\n",
      "Epoch 399/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0499 - acc: 0.6297 - val_loss: 0.0619 - val_acc: 0.5470\n",
      "Epoch 400/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0496 - acc: 0.6341 - val_loss: 0.0622 - val_acc: 0.5528\n",
      "Epoch 401/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0501 - acc: 0.6244 - val_loss: 0.0626 - val_acc: 0.5461\n",
      "Epoch 402/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0503 - acc: 0.6280 - val_loss: 0.0624 - val_acc: 0.5436\n",
      "Epoch 403/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0494 - acc: 0.6369 - val_loss: 0.0629 - val_acc: 0.5287\n",
      "Epoch 404/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0491 - acc: 0.6358 - val_loss: 0.0624 - val_acc: 0.5495\n",
      "Epoch 405/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0497 - acc: 0.6308 - val_loss: 0.0623 - val_acc: 0.5428\n",
      "Epoch 406/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0493 - acc: 0.6322 - val_loss: 0.0623 - val_acc: 0.5420\n",
      "Epoch 407/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0493 - acc: 0.6366 - val_loss: 0.0624 - val_acc: 0.5453\n",
      "Epoch 408/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0496 - acc: 0.6364 - val_loss: 0.0624 - val_acc: 0.5470\n",
      "Epoch 409/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0496 - acc: 0.6389 - val_loss: 0.0625 - val_acc: 0.5520\n",
      "Epoch 410/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0494 - acc: 0.6425 - val_loss: 0.0629 - val_acc: 0.5395\n",
      "Epoch 411/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0496 - acc: 0.6308 - val_loss: 0.0629 - val_acc: 0.5445\n",
      "Epoch 412/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0501 - acc: 0.6328 - val_loss: 0.0623 - val_acc: 0.5553\n",
      "Epoch 413/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0499 - acc: 0.6303 - val_loss: 0.0624 - val_acc: 0.5470\n",
      "Epoch 414/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0498 - acc: 0.6322 - val_loss: 0.0637 - val_acc: 0.5436\n",
      "Epoch 415/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0495 - acc: 0.6372 - val_loss: 0.0624 - val_acc: 0.5520\n",
      "Epoch 416/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0495 - acc: 0.6336 - val_loss: 0.0628 - val_acc: 0.5520\n",
      "Epoch 417/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0495 - acc: 0.6375 - val_loss: 0.0632 - val_acc: 0.5495\n",
      "Epoch 418/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0500 - acc: 0.6361 - val_loss: 0.0627 - val_acc: 0.5436\n",
      "Epoch 419/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0494 - acc: 0.6358 - val_loss: 0.0615 - val_acc: 0.5503\n",
      "Epoch 420/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0503 - acc: 0.6325 - val_loss: 0.0618 - val_acc: 0.5503\n",
      "Epoch 421/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0497 - acc: 0.6330 - val_loss: 0.0618 - val_acc: 0.5611\n",
      "Epoch 422/3000\n",
      "3608/3608 [==============================] - 1s 205us/step - loss: 0.0498 - acc: 0.6353 - val_loss: 0.0627 - val_acc: 0.5520\n",
      "Epoch 423/3000\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0506 - acc: 0.6311 - val_loss: 0.0633 - val_acc: 0.5461\n",
      "Epoch 424/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0506 - acc: 0.6275 - val_loss: 0.0623 - val_acc: 0.5470\n",
      "Epoch 425/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0499 - acc: 0.6358 - val_loss: 0.0621 - val_acc: 0.5486\n",
      "Epoch 426/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0501 - acc: 0.6308 - val_loss: 0.0622 - val_acc: 0.5569\n",
      "Epoch 427/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0497 - acc: 0.6355 - val_loss: 0.0626 - val_acc: 0.5503\n",
      "Epoch 428/3000\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0498 - acc: 0.6286 - val_loss: 0.0631 - val_acc: 0.5453\n",
      "Epoch 429/3000\n",
      "3608/3608 [==============================] - 1s 218us/step - loss: 0.0499 - acc: 0.6330 - val_loss: 0.0622 - val_acc: 0.5461\n",
      "Epoch 430/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0495 - acc: 0.6339 - val_loss: 0.0622 - val_acc: 0.5420\n",
      "Epoch 431/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0493 - acc: 0.6341 - val_loss: 0.0632 - val_acc: 0.5495\n",
      "Epoch 432/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0493 - acc: 0.6400 - val_loss: 0.0630 - val_acc: 0.5312\n",
      "Epoch 433/3000\n",
      "3608/3608 [==============================] - 1s 219us/step - loss: 0.0500 - acc: 0.6305 - val_loss: 0.0633 - val_acc: 0.5428\n",
      "Epoch 434/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0494 - acc: 0.6333 - val_loss: 0.0622 - val_acc: 0.5470\n",
      "Epoch 435/3000\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0500 - acc: 0.6314 - val_loss: 0.0620 - val_acc: 0.5470\n",
      "Epoch 436/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0491 - acc: 0.6383 - val_loss: 0.0626 - val_acc: 0.5503\n",
      "Epoch 437/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0492 - acc: 0.6389 - val_loss: 0.0623 - val_acc: 0.5461\n",
      "Epoch 438/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0495 - acc: 0.6358 - val_loss: 0.0622 - val_acc: 0.5503\n",
      "Epoch 439/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0496 - acc: 0.6336 - val_loss: 0.0627 - val_acc: 0.5520\n",
      "Epoch 440/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0492 - acc: 0.6364 - val_loss: 0.0624 - val_acc: 0.5511\n",
      "Epoch 441/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0493 - acc: 0.6430 - val_loss: 0.0627 - val_acc: 0.5594\n",
      "Epoch 442/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0496 - acc: 0.6328 - val_loss: 0.0624 - val_acc: 0.5520\n",
      "Epoch 443/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0491 - acc: 0.6361 - val_loss: 0.0629 - val_acc: 0.5470\n",
      "Epoch 444/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0497 - acc: 0.6416 - val_loss: 0.0633 - val_acc: 0.5445\n",
      "Epoch 445/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0498 - acc: 0.6292 - val_loss: 0.0631 - val_acc: 0.5320\n",
      "Epoch 446/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0496 - acc: 0.6350 - val_loss: 0.0634 - val_acc: 0.5461\n",
      "Epoch 447/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0496 - acc: 0.6344 - val_loss: 0.0628 - val_acc: 0.5445\n",
      "Epoch 448/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0494 - acc: 0.6355 - val_loss: 0.0623 - val_acc: 0.5495\n",
      "Epoch 449/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0499 - acc: 0.6286 - val_loss: 0.0627 - val_acc: 0.5362\n",
      "Epoch 450/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0497 - acc: 0.6330 - val_loss: 0.0621 - val_acc: 0.5461\n",
      "Epoch 451/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0491 - acc: 0.6358 - val_loss: 0.0620 - val_acc: 0.5536\n",
      "Epoch 452/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0494 - acc: 0.6355 - val_loss: 0.0629 - val_acc: 0.5320\n",
      "Epoch 453/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0504 - acc: 0.6250 - val_loss: 0.0638 - val_acc: 0.5320\n",
      "Epoch 454/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0502 - acc: 0.6280 - val_loss: 0.0626 - val_acc: 0.5520\n",
      "Epoch 455/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0499 - acc: 0.6336 - val_loss: 0.0637 - val_acc: 0.5387\n",
      "Epoch 456/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0494 - acc: 0.6347 - val_loss: 0.0631 - val_acc: 0.5528\n",
      "Epoch 457/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0499 - acc: 0.6350 - val_loss: 0.0620 - val_acc: 0.5503\n",
      "Epoch 458/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0498 - acc: 0.6328 - val_loss: 0.0625 - val_acc: 0.5436\n",
      "Epoch 459/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0498 - acc: 0.6330 - val_loss: 0.0620 - val_acc: 0.5553\n",
      "Epoch 460/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0495 - acc: 0.6328 - val_loss: 0.0633 - val_acc: 0.5395\n",
      "Epoch 461/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0493 - acc: 0.6386 - val_loss: 0.0635 - val_acc: 0.5536\n",
      "Epoch 462/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0495 - acc: 0.6344 - val_loss: 0.0627 - val_acc: 0.5478\n",
      "Epoch 463/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0493 - acc: 0.6336 - val_loss: 0.0634 - val_acc: 0.5486\n",
      "Epoch 464/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0501 - acc: 0.6305 - val_loss: 0.0631 - val_acc: 0.5486\n",
      "Epoch 465/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0500 - acc: 0.6317 - val_loss: 0.0629 - val_acc: 0.5470\n",
      "Epoch 466/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0500 - acc: 0.6308 - val_loss: 0.0625 - val_acc: 0.5511\n",
      "Epoch 467/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0502 - acc: 0.6311 - val_loss: 0.0623 - val_acc: 0.5536\n",
      "Epoch 468/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0499 - acc: 0.6319 - val_loss: 0.0629 - val_acc: 0.5561\n",
      "Epoch 469/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0493 - acc: 0.6380 - val_loss: 0.0648 - val_acc: 0.5254\n",
      "Epoch 470/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0495 - acc: 0.6389 - val_loss: 0.0631 - val_acc: 0.5445\n",
      "Epoch 471/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0493 - acc: 0.6402 - val_loss: 0.0619 - val_acc: 0.5544\n",
      "Epoch 472/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0491 - acc: 0.6389 - val_loss: 0.0626 - val_acc: 0.5520\n",
      "Epoch 473/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0490 - acc: 0.6355 - val_loss: 0.0628 - val_acc: 0.5453\n",
      "Epoch 474/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0494 - acc: 0.6394 - val_loss: 0.0620 - val_acc: 0.5528\n",
      "Epoch 475/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0498 - acc: 0.6311 - val_loss: 0.0628 - val_acc: 0.5428\n",
      "Epoch 476/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0494 - acc: 0.6366 - val_loss: 0.0632 - val_acc: 0.5436\n",
      "Epoch 477/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0493 - acc: 0.6394 - val_loss: 0.0629 - val_acc: 0.5387\n",
      "Epoch 478/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0499 - acc: 0.6308 - val_loss: 0.0627 - val_acc: 0.5495\n",
      "Epoch 479/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0494 - acc: 0.6386 - val_loss: 0.0624 - val_acc: 0.5436\n",
      "Epoch 480/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0499 - acc: 0.6308 - val_loss: 0.0632 - val_acc: 0.5503\n",
      "Epoch 481/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0501 - acc: 0.6305 - val_loss: 0.0625 - val_acc: 0.5478\n",
      "Epoch 482/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0495 - acc: 0.6283 - val_loss: 0.0624 - val_acc: 0.5461\n",
      "Epoch 483/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0494 - acc: 0.6355 - val_loss: 0.0622 - val_acc: 0.5536\n",
      "Epoch 484/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0495 - acc: 0.6364 - val_loss: 0.0620 - val_acc: 0.5569\n",
      "Epoch 485/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0490 - acc: 0.6377 - val_loss: 0.0629 - val_acc: 0.5578\n",
      "Epoch 486/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0494 - acc: 0.6375 - val_loss: 0.0626 - val_acc: 0.5486\n",
      "Epoch 487/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0496 - acc: 0.6350 - val_loss: 0.0636 - val_acc: 0.5362\n",
      "Epoch 488/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0497 - acc: 0.6350 - val_loss: 0.0624 - val_acc: 0.5653\n",
      "Epoch 489/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0492 - acc: 0.6322 - val_loss: 0.0626 - val_acc: 0.5436\n",
      "Epoch 490/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0493 - acc: 0.6405 - val_loss: 0.0625 - val_acc: 0.5611\n",
      "Epoch 491/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0499 - acc: 0.6333 - val_loss: 0.0619 - val_acc: 0.5586\n",
      "Epoch 492/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0509 - acc: 0.6256 - val_loss: 0.0622 - val_acc: 0.5628\n",
      "Epoch 493/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0495 - acc: 0.6305 - val_loss: 0.0627 - val_acc: 0.5495\n",
      "Epoch 494/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0491 - acc: 0.6361 - val_loss: 0.0627 - val_acc: 0.5495\n",
      "Epoch 495/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0489 - acc: 0.6389 - val_loss: 0.0624 - val_acc: 0.5528\n",
      "Epoch 496/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0498 - acc: 0.6305 - val_loss: 0.0616 - val_acc: 0.5586\n",
      "Epoch 497/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0500 - acc: 0.6361 - val_loss: 0.0624 - val_acc: 0.5486\n",
      "Epoch 498/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0495 - acc: 0.6389 - val_loss: 0.0631 - val_acc: 0.5536\n",
      "Epoch 499/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0492 - acc: 0.6402 - val_loss: 0.0638 - val_acc: 0.5453\n",
      "Epoch 500/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0488 - acc: 0.6427 - val_loss: 0.0632 - val_acc: 0.5445\n",
      "Epoch 501/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0490 - acc: 0.6447 - val_loss: 0.0624 - val_acc: 0.5528\n",
      "Epoch 502/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0497 - acc: 0.6317 - val_loss: 0.0638 - val_acc: 0.5395\n",
      "Epoch 503/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0498 - acc: 0.6325 - val_loss: 0.0625 - val_acc: 0.5528\n",
      "Epoch 504/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0500 - acc: 0.6305 - val_loss: 0.0623 - val_acc: 0.5578\n",
      "Epoch 505/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0494 - acc: 0.6322 - val_loss: 0.0625 - val_acc: 0.5644\n",
      "Epoch 506/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0492 - acc: 0.6408 - val_loss: 0.0638 - val_acc: 0.5470\n",
      "Epoch 507/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0495 - acc: 0.6353 - val_loss: 0.0639 - val_acc: 0.5395\n",
      "Epoch 508/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0497 - acc: 0.6311 - val_loss: 0.0620 - val_acc: 0.5520\n",
      "Epoch 509/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0488 - acc: 0.6441 - val_loss: 0.0638 - val_acc: 0.5320\n",
      "Epoch 510/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0494 - acc: 0.6353 - val_loss: 0.0619 - val_acc: 0.5594\n",
      "Epoch 511/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0491 - acc: 0.6391 - val_loss: 0.0634 - val_acc: 0.5453\n",
      "Epoch 512/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0492 - acc: 0.6425 - val_loss: 0.0624 - val_acc: 0.5578\n",
      "Epoch 513/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0489 - acc: 0.6394 - val_loss: 0.0636 - val_acc: 0.5536\n",
      "Epoch 514/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0494 - acc: 0.6369 - val_loss: 0.0629 - val_acc: 0.5470\n",
      "Epoch 515/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0497 - acc: 0.6380 - val_loss: 0.0634 - val_acc: 0.5511\n",
      "Epoch 516/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0496 - acc: 0.6350 - val_loss: 0.0634 - val_acc: 0.5486\n",
      "Epoch 517/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0501 - acc: 0.6355 - val_loss: 0.0639 - val_acc: 0.5495\n",
      "Epoch 518/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0489 - acc: 0.6389 - val_loss: 0.0624 - val_acc: 0.5536\n",
      "Epoch 519/3000\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0491 - acc: 0.6405 - val_loss: 0.0633 - val_acc: 0.5486\n",
      "Epoch 520/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0490 - acc: 0.6377 - val_loss: 0.0622 - val_acc: 0.5478\n",
      "Epoch 521/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0490 - acc: 0.6425 - val_loss: 0.0622 - val_acc: 0.5536\n",
      "Epoch 522/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0488 - acc: 0.6380 - val_loss: 0.0633 - val_acc: 0.5445\n",
      "Epoch 523/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0493 - acc: 0.6328 - val_loss: 0.0622 - val_acc: 0.5411\n",
      "Epoch 524/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0487 - acc: 0.6447 - val_loss: 0.0626 - val_acc: 0.5486\n",
      "Epoch 525/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0490 - acc: 0.6427 - val_loss: 0.0625 - val_acc: 0.5611\n",
      "Epoch 526/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0492 - acc: 0.6372 - val_loss: 0.0613 - val_acc: 0.5536\n",
      "Epoch 527/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0489 - acc: 0.6369 - val_loss: 0.0636 - val_acc: 0.5470\n",
      "Epoch 528/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0495 - acc: 0.6358 - val_loss: 0.0621 - val_acc: 0.5520\n",
      "Epoch 529/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0491 - acc: 0.6411 - val_loss: 0.0631 - val_acc: 0.5495\n",
      "Epoch 530/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0488 - acc: 0.6469 - val_loss: 0.0634 - val_acc: 0.5486\n",
      "Epoch 531/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0492 - acc: 0.6416 - val_loss: 0.0623 - val_acc: 0.5536\n",
      "Epoch 532/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0488 - acc: 0.6455 - val_loss: 0.0621 - val_acc: 0.5520\n",
      "Epoch 533/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0489 - acc: 0.6402 - val_loss: 0.0628 - val_acc: 0.5561\n",
      "Epoch 534/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0488 - acc: 0.6427 - val_loss: 0.0638 - val_acc: 0.5395\n",
      "Epoch 535/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0491 - acc: 0.6386 - val_loss: 0.0630 - val_acc: 0.5520\n",
      "Epoch 536/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0486 - acc: 0.6441 - val_loss: 0.0635 - val_acc: 0.5569\n",
      "Epoch 537/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0487 - acc: 0.6408 - val_loss: 0.0628 - val_acc: 0.5536\n",
      "Epoch 538/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0490 - acc: 0.6369 - val_loss: 0.0629 - val_acc: 0.5561\n",
      "Epoch 539/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0491 - acc: 0.6408 - val_loss: 0.0626 - val_acc: 0.5536\n",
      "Epoch 540/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0488 - acc: 0.6450 - val_loss: 0.0623 - val_acc: 0.5536\n",
      "Epoch 541/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0490 - acc: 0.6386 - val_loss: 0.0624 - val_acc: 0.5520\n",
      "Epoch 542/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0493 - acc: 0.6380 - val_loss: 0.0620 - val_acc: 0.5486\n",
      "Epoch 543/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0491 - acc: 0.6397 - val_loss: 0.0631 - val_acc: 0.5528\n",
      "Epoch 544/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0496 - acc: 0.6400 - val_loss: 0.0621 - val_acc: 0.5586\n",
      "Epoch 545/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0492 - acc: 0.6369 - val_loss: 0.0629 - val_acc: 0.5470\n",
      "Epoch 546/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0485 - acc: 0.6422 - val_loss: 0.0632 - val_acc: 0.5528\n",
      "Epoch 547/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0486 - acc: 0.6466 - val_loss: 0.0627 - val_acc: 0.5470\n",
      "Epoch 548/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0490 - acc: 0.6416 - val_loss: 0.0637 - val_acc: 0.5320\n",
      "Epoch 549/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0484 - acc: 0.6483 - val_loss: 0.0633 - val_acc: 0.5445\n",
      "Epoch 550/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0485 - acc: 0.6425 - val_loss: 0.0627 - val_acc: 0.5495\n",
      "Epoch 551/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0488 - acc: 0.6383 - val_loss: 0.0624 - val_acc: 0.5553\n",
      "Epoch 552/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0490 - acc: 0.6397 - val_loss: 0.0628 - val_acc: 0.5536\n",
      "Epoch 553/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0490 - acc: 0.6433 - val_loss: 0.0637 - val_acc: 0.5403\n",
      "Epoch 554/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0499 - acc: 0.6297 - val_loss: 0.0629 - val_acc: 0.5503\n",
      "Epoch 555/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0495 - acc: 0.6355 - val_loss: 0.0629 - val_acc: 0.5445\n",
      "Epoch 556/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0494 - acc: 0.6339 - val_loss: 0.0629 - val_acc: 0.5436\n",
      "Epoch 557/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0488 - acc: 0.6411 - val_loss: 0.0629 - val_acc: 0.5511\n",
      "Epoch 558/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0489 - acc: 0.6372 - val_loss: 0.0629 - val_acc: 0.5503\n",
      "Epoch 559/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0487 - acc: 0.6408 - val_loss: 0.0621 - val_acc: 0.5586\n",
      "Epoch 560/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0486 - acc: 0.6419 - val_loss: 0.0636 - val_acc: 0.5387\n",
      "Epoch 561/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0496 - acc: 0.6383 - val_loss: 0.0635 - val_acc: 0.5503\n",
      "Epoch 562/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0491 - acc: 0.6402 - val_loss: 0.0634 - val_acc: 0.5428\n",
      "Epoch 563/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0490 - acc: 0.6366 - val_loss: 0.0633 - val_acc: 0.5495\n",
      "Epoch 564/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0490 - acc: 0.6389 - val_loss: 0.0625 - val_acc: 0.5495\n",
      "Epoch 565/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0491 - acc: 0.6427 - val_loss: 0.0632 - val_acc: 0.5395\n",
      "Epoch 566/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0497 - acc: 0.6372 - val_loss: 0.0628 - val_acc: 0.5511\n",
      "Epoch 567/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0488 - acc: 0.6416 - val_loss: 0.0626 - val_acc: 0.5428\n",
      "Epoch 568/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0490 - acc: 0.6386 - val_loss: 0.0628 - val_acc: 0.5436\n",
      "Epoch 569/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0491 - acc: 0.6419 - val_loss: 0.0634 - val_acc: 0.5420\n",
      "Epoch 570/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0488 - acc: 0.6444 - val_loss: 0.0622 - val_acc: 0.5478\n",
      "Epoch 571/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0487 - acc: 0.6466 - val_loss: 0.0634 - val_acc: 0.5461\n",
      "Epoch 572/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0488 - acc: 0.6366 - val_loss: 0.0635 - val_acc: 0.5478\n",
      "Epoch 573/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0488 - acc: 0.6411 - val_loss: 0.0628 - val_acc: 0.5503\n",
      "Epoch 574/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0487 - acc: 0.6458 - val_loss: 0.0635 - val_acc: 0.5411\n",
      "Epoch 575/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0489 - acc: 0.6414 - val_loss: 0.0637 - val_acc: 0.5411\n",
      "Epoch 576/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0492 - acc: 0.6383 - val_loss: 0.0626 - val_acc: 0.5561\n",
      "Epoch 577/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0493 - acc: 0.6328 - val_loss: 0.0651 - val_acc: 0.5170\n",
      "Epoch 578/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0490 - acc: 0.6383 - val_loss: 0.0622 - val_acc: 0.5578\n",
      "Epoch 579/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0492 - acc: 0.6391 - val_loss: 0.0623 - val_acc: 0.5544\n",
      "Epoch 580/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0493 - acc: 0.6328 - val_loss: 0.0626 - val_acc: 0.5495\n",
      "Epoch 581/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0489 - acc: 0.6377 - val_loss: 0.0622 - val_acc: 0.5495\n",
      "Epoch 582/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0488 - acc: 0.6355 - val_loss: 0.0630 - val_acc: 0.5461\n",
      "Epoch 583/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0492 - acc: 0.6397 - val_loss: 0.0649 - val_acc: 0.5345\n",
      "Epoch 584/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0490 - acc: 0.6380 - val_loss: 0.0633 - val_acc: 0.5511\n",
      "Epoch 585/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0487 - acc: 0.6380 - val_loss: 0.0645 - val_acc: 0.5370\n",
      "Epoch 586/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0487 - acc: 0.6444 - val_loss: 0.0626 - val_acc: 0.5511\n",
      "Epoch 587/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0482 - acc: 0.6499 - val_loss: 0.0635 - val_acc: 0.5536\n",
      "Epoch 588/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0481 - acc: 0.6425 - val_loss: 0.0620 - val_acc: 0.5511\n",
      "Epoch 589/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0490 - acc: 0.6353 - val_loss: 0.0630 - val_acc: 0.5553\n",
      "Epoch 590/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0489 - acc: 0.6394 - val_loss: 0.0634 - val_acc: 0.5387\n",
      "Epoch 591/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0487 - acc: 0.6414 - val_loss: 0.0636 - val_acc: 0.5445\n",
      "Epoch 592/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0494 - acc: 0.6394 - val_loss: 0.0626 - val_acc: 0.5486\n",
      "Epoch 593/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0487 - acc: 0.6405 - val_loss: 0.0625 - val_acc: 0.5478\n",
      "Epoch 594/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0493 - acc: 0.6336 - val_loss: 0.0645 - val_acc: 0.5370\n",
      "Epoch 595/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0489 - acc: 0.6425 - val_loss: 0.0632 - val_acc: 0.5495\n",
      "Epoch 596/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0492 - acc: 0.6355 - val_loss: 0.0637 - val_acc: 0.5395\n",
      "Epoch 597/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0490 - acc: 0.6419 - val_loss: 0.0628 - val_acc: 0.5486\n",
      "Epoch 598/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0492 - acc: 0.6355 - val_loss: 0.0629 - val_acc: 0.5486\n",
      "Epoch 599/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0490 - acc: 0.6369 - val_loss: 0.0632 - val_acc: 0.5411\n",
      "Epoch 600/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0491 - acc: 0.6369 - val_loss: 0.0625 - val_acc: 0.5470\n",
      "Epoch 601/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0492 - acc: 0.6322 - val_loss: 0.0631 - val_acc: 0.5536\n",
      "Epoch 602/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0490 - acc: 0.6402 - val_loss: 0.0627 - val_acc: 0.5578\n",
      "Epoch 603/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0485 - acc: 0.6441 - val_loss: 0.0634 - val_acc: 0.5470\n",
      "Epoch 604/3000\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0486 - acc: 0.6411 - val_loss: 0.0644 - val_acc: 0.5486\n",
      "Epoch 605/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0489 - acc: 0.6408 - val_loss: 0.0625 - val_acc: 0.5511\n",
      "Epoch 606/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0488 - acc: 0.6422 - val_loss: 0.0627 - val_acc: 0.5486\n",
      "Epoch 607/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0485 - acc: 0.6430 - val_loss: 0.0626 - val_acc: 0.5594\n",
      "Epoch 608/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0490 - acc: 0.6400 - val_loss: 0.0627 - val_acc: 0.5486\n",
      "Epoch 609/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0490 - acc: 0.6391 - val_loss: 0.0631 - val_acc: 0.5453\n",
      "Epoch 610/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0486 - acc: 0.6441 - val_loss: 0.0631 - val_acc: 0.5495\n",
      "Epoch 611/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0490 - acc: 0.6391 - val_loss: 0.0638 - val_acc: 0.5370\n",
      "Epoch 612/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0490 - acc: 0.6436 - val_loss: 0.0630 - val_acc: 0.5520\n",
      "Epoch 613/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0487 - acc: 0.6444 - val_loss: 0.0632 - val_acc: 0.5436\n",
      "Epoch 614/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0486 - acc: 0.6441 - val_loss: 0.0637 - val_acc: 0.5503\n",
      "Epoch 615/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0488 - acc: 0.6425 - val_loss: 0.0624 - val_acc: 0.5511\n",
      "Epoch 616/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0489 - acc: 0.6383 - val_loss: 0.0631 - val_acc: 0.5411\n",
      "Epoch 617/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0491 - acc: 0.6372 - val_loss: 0.0626 - val_acc: 0.5536\n",
      "Epoch 618/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0490 - acc: 0.6394 - val_loss: 0.0630 - val_acc: 0.5370\n",
      "Epoch 619/3000\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0493 - acc: 0.6414 - val_loss: 0.0630 - val_acc: 0.5528\n",
      "Epoch 620/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0491 - acc: 0.6358 - val_loss: 0.0637 - val_acc: 0.5453\n",
      "Epoch 621/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0490 - acc: 0.6411 - val_loss: 0.0636 - val_acc: 0.5445\n",
      "Epoch 622/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0495 - acc: 0.6463 - val_loss: 0.0630 - val_acc: 0.5544\n",
      "Epoch 623/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0494 - acc: 0.6369 - val_loss: 0.0627 - val_acc: 0.5603\n",
      "Epoch 624/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0488 - acc: 0.6402 - val_loss: 0.0636 - val_acc: 0.5453\n",
      "Epoch 625/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0489 - acc: 0.6441 - val_loss: 0.0632 - val_acc: 0.5511\n",
      "Epoch 626/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0486 - acc: 0.6441 - val_loss: 0.0635 - val_acc: 0.5420\n",
      "Epoch 627/3000\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0488 - acc: 0.6416 - val_loss: 0.0641 - val_acc: 0.5544\n",
      "Epoch 628/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0485 - acc: 0.6402 - val_loss: 0.0632 - val_acc: 0.5478\n",
      "Epoch 629/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0487 - acc: 0.6400 - val_loss: 0.0636 - val_acc: 0.5436\n",
      "Epoch 630/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0495 - acc: 0.6311 - val_loss: 0.0627 - val_acc: 0.5486\n",
      "Epoch 631/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0490 - acc: 0.6386 - val_loss: 0.0630 - val_acc: 0.5478\n",
      "Epoch 632/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0484 - acc: 0.6402 - val_loss: 0.0628 - val_acc: 0.5578\n",
      "Epoch 633/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0485 - acc: 0.6447 - val_loss: 0.0644 - val_acc: 0.5387\n",
      "Epoch 634/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0487 - acc: 0.6336 - val_loss: 0.0624 - val_acc: 0.5578\n",
      "Epoch 635/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0490 - acc: 0.6408 - val_loss: 0.0650 - val_acc: 0.5353\n",
      "Epoch 636/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0486 - acc: 0.6452 - val_loss: 0.0635 - val_acc: 0.5428\n",
      "Epoch 637/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0494 - acc: 0.6375 - val_loss: 0.0631 - val_acc: 0.5495\n",
      "Epoch 638/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0486 - acc: 0.6394 - val_loss: 0.0638 - val_acc: 0.5536\n",
      "Epoch 639/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0487 - acc: 0.6472 - val_loss: 0.0628 - val_acc: 0.5453\n",
      "Epoch 640/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0486 - acc: 0.6458 - val_loss: 0.0633 - val_acc: 0.5553\n",
      "Epoch 641/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0487 - acc: 0.6411 - val_loss: 0.0633 - val_acc: 0.5411\n",
      "Epoch 642/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0489 - acc: 0.6380 - val_loss: 0.0644 - val_acc: 0.5212\n",
      "Epoch 643/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0489 - acc: 0.6391 - val_loss: 0.0636 - val_acc: 0.5428\n",
      "Epoch 644/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0490 - acc: 0.6366 - val_loss: 0.0628 - val_acc: 0.5486\n",
      "Epoch 645/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0491 - acc: 0.6416 - val_loss: 0.0634 - val_acc: 0.5461\n",
      "Epoch 646/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0491 - acc: 0.6405 - val_loss: 0.0626 - val_acc: 0.5478\n",
      "Epoch 647/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0486 - acc: 0.6461 - val_loss: 0.0622 - val_acc: 0.5528\n",
      "Epoch 648/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0478 - acc: 0.6475 - val_loss: 0.0630 - val_acc: 0.5511\n",
      "Epoch 649/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0483 - acc: 0.6414 - val_loss: 0.0647 - val_acc: 0.5278\n",
      "Epoch 650/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0488 - acc: 0.6380 - val_loss: 0.0632 - val_acc: 0.5528\n",
      "Epoch 651/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0496 - acc: 0.6350 - val_loss: 0.0638 - val_acc: 0.5445\n",
      "Epoch 652/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0489 - acc: 0.6411 - val_loss: 0.0640 - val_acc: 0.5312\n",
      "Epoch 653/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0488 - acc: 0.6414 - val_loss: 0.0632 - val_acc: 0.5520\n",
      "Epoch 654/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0488 - acc: 0.6441 - val_loss: 0.0647 - val_acc: 0.5411\n",
      "Epoch 655/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0485 - acc: 0.6458 - val_loss: 0.0636 - val_acc: 0.5453\n",
      "Epoch 656/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0483 - acc: 0.6455 - val_loss: 0.0634 - val_acc: 0.5495\n",
      "Epoch 657/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0488 - acc: 0.6397 - val_loss: 0.0638 - val_acc: 0.5503\n",
      "Epoch 658/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0488 - acc: 0.6472 - val_loss: 0.0638 - val_acc: 0.5478\n",
      "Epoch 659/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0493 - acc: 0.6402 - val_loss: 0.0634 - val_acc: 0.5603\n",
      "Epoch 660/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0491 - acc: 0.6405 - val_loss: 0.0636 - val_acc: 0.5453\n",
      "Epoch 661/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0493 - acc: 0.6394 - val_loss: 0.0645 - val_acc: 0.5287\n",
      "Epoch 662/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0495 - acc: 0.6347 - val_loss: 0.0639 - val_acc: 0.5470\n",
      "Epoch 663/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0494 - acc: 0.6358 - val_loss: 0.0641 - val_acc: 0.5428\n",
      "Epoch 664/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0495 - acc: 0.6411 - val_loss: 0.0637 - val_acc: 0.5478\n",
      "Epoch 665/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0491 - acc: 0.6408 - val_loss: 0.0654 - val_acc: 0.5212\n",
      "Epoch 666/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0492 - acc: 0.6375 - val_loss: 0.0648 - val_acc: 0.5345\n",
      "Epoch 667/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0493 - acc: 0.6361 - val_loss: 0.0639 - val_acc: 0.5478\n",
      "Epoch 668/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0493 - acc: 0.6361 - val_loss: 0.0632 - val_acc: 0.5445\n",
      "Epoch 669/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0486 - acc: 0.6463 - val_loss: 0.0634 - val_acc: 0.5470\n",
      "Epoch 670/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0490 - acc: 0.6438 - val_loss: 0.0643 - val_acc: 0.5362\n",
      "Epoch 671/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0494 - acc: 0.6344 - val_loss: 0.0626 - val_acc: 0.5619\n",
      "Epoch 672/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0488 - acc: 0.6436 - val_loss: 0.0625 - val_acc: 0.5544\n",
      "Epoch 673/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0493 - acc: 0.6389 - val_loss: 0.0644 - val_acc: 0.5370\n",
      "Epoch 674/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0489 - acc: 0.6375 - val_loss: 0.0630 - val_acc: 0.5470\n",
      "Epoch 675/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0485 - acc: 0.6436 - val_loss: 0.0642 - val_acc: 0.5420\n",
      "Epoch 676/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0486 - acc: 0.6405 - val_loss: 0.0629 - val_acc: 0.5561\n",
      "Epoch 677/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0484 - acc: 0.6508 - val_loss: 0.0635 - val_acc: 0.5503\n",
      "Epoch 678/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0484 - acc: 0.6491 - val_loss: 0.0640 - val_acc: 0.5461\n",
      "Epoch 679/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0486 - acc: 0.6444 - val_loss: 0.0641 - val_acc: 0.5403\n",
      "Epoch 680/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0489 - acc: 0.6380 - val_loss: 0.0635 - val_acc: 0.5478\n",
      "Epoch 681/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0486 - acc: 0.6472 - val_loss: 0.0636 - val_acc: 0.5503\n",
      "Epoch 682/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0484 - acc: 0.6466 - val_loss: 0.0628 - val_acc: 0.5420\n",
      "Epoch 683/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0491 - acc: 0.6458 - val_loss: 0.0630 - val_acc: 0.5536\n",
      "Epoch 684/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0492 - acc: 0.6353 - val_loss: 0.0643 - val_acc: 0.5337\n",
      "Epoch 685/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0495 - acc: 0.6344 - val_loss: 0.0633 - val_acc: 0.5395\n",
      "Epoch 686/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0487 - acc: 0.6444 - val_loss: 0.0650 - val_acc: 0.5121\n",
      "Epoch 687/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0484 - acc: 0.6461 - val_loss: 0.0629 - val_acc: 0.5520\n",
      "Epoch 688/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0487 - acc: 0.6402 - val_loss: 0.0633 - val_acc: 0.5503\n",
      "Epoch 689/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0480 - acc: 0.6511 - val_loss: 0.0625 - val_acc: 0.5594\n",
      "Epoch 690/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0482 - acc: 0.6491 - val_loss: 0.0635 - val_acc: 0.5495\n",
      "Epoch 691/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0478 - acc: 0.6555 - val_loss: 0.0633 - val_acc: 0.5486\n",
      "Epoch 692/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0482 - acc: 0.6486 - val_loss: 0.0637 - val_acc: 0.5428\n",
      "Epoch 693/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0480 - acc: 0.6508 - val_loss: 0.0640 - val_acc: 0.5345\n",
      "Epoch 694/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0485 - acc: 0.6416 - val_loss: 0.0631 - val_acc: 0.5569\n",
      "Epoch 695/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0483 - acc: 0.6472 - val_loss: 0.0639 - val_acc: 0.5461\n",
      "Epoch 696/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0493 - acc: 0.6386 - val_loss: 0.0639 - val_acc: 0.5461\n",
      "Epoch 697/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0490 - acc: 0.6433 - val_loss: 0.0632 - val_acc: 0.5528\n",
      "Epoch 698/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0493 - acc: 0.6402 - val_loss: 0.0625 - val_acc: 0.5511\n",
      "Epoch 699/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0484 - acc: 0.6427 - val_loss: 0.0632 - val_acc: 0.5553\n",
      "Epoch 700/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0487 - acc: 0.6405 - val_loss: 0.0624 - val_acc: 0.5578\n",
      "Epoch 701/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0483 - acc: 0.6461 - val_loss: 0.0625 - val_acc: 0.5586\n",
      "Epoch 702/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0483 - acc: 0.6455 - val_loss: 0.0634 - val_acc: 0.5561\n",
      "Epoch 703/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0487 - acc: 0.6438 - val_loss: 0.0637 - val_acc: 0.5428\n",
      "Epoch 704/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0484 - acc: 0.6488 - val_loss: 0.0634 - val_acc: 0.5470\n",
      "Epoch 705/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0487 - acc: 0.6425 - val_loss: 0.0644 - val_acc: 0.5395\n",
      "Epoch 706/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0489 - acc: 0.6477 - val_loss: 0.0645 - val_acc: 0.5370\n",
      "Epoch 707/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0490 - acc: 0.6400 - val_loss: 0.0644 - val_acc: 0.5395\n",
      "Epoch 708/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0493 - acc: 0.6380 - val_loss: 0.0631 - val_acc: 0.5611\n",
      "Epoch 709/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0491 - acc: 0.6422 - val_loss: 0.0639 - val_acc: 0.5561\n",
      "Epoch 710/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0490 - acc: 0.6444 - val_loss: 0.0639 - val_acc: 0.5470\n",
      "Epoch 711/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0485 - acc: 0.6477 - val_loss: 0.0636 - val_acc: 0.5578\n",
      "Epoch 712/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0482 - acc: 0.6466 - val_loss: 0.0635 - val_acc: 0.5503\n",
      "Epoch 713/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0482 - acc: 0.6486 - val_loss: 0.0640 - val_acc: 0.5436\n",
      "Epoch 714/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0488 - acc: 0.6444 - val_loss: 0.0641 - val_acc: 0.5436\n",
      "Epoch 715/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0489 - acc: 0.6444 - val_loss: 0.0641 - val_acc: 0.5370\n",
      "Epoch 716/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0487 - acc: 0.6419 - val_loss: 0.0637 - val_acc: 0.5436\n",
      "Epoch 717/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0483 - acc: 0.6436 - val_loss: 0.0633 - val_acc: 0.5486\n",
      "Epoch 718/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0486 - acc: 0.6377 - val_loss: 0.0641 - val_acc: 0.5378\n",
      "Epoch 719/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0490 - acc: 0.6386 - val_loss: 0.0636 - val_acc: 0.5445\n",
      "Epoch 720/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0488 - acc: 0.6455 - val_loss: 0.0653 - val_acc: 0.5428\n",
      "Epoch 721/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0486 - acc: 0.6419 - val_loss: 0.0632 - val_acc: 0.5503\n",
      "Epoch 722/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0486 - acc: 0.6469 - val_loss: 0.0654 - val_acc: 0.5387\n",
      "Epoch 723/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0492 - acc: 0.6402 - val_loss: 0.0637 - val_acc: 0.5561\n",
      "Epoch 724/3000\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0486 - acc: 0.6438 - val_loss: 0.0641 - val_acc: 0.5503\n",
      "Epoch 725/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0487 - acc: 0.6438 - val_loss: 0.0642 - val_acc: 0.5461\n",
      "Epoch 726/3000\n",
      "3608/3608 [==============================] - 1s 195us/step - loss: 0.0483 - acc: 0.6447 - val_loss: 0.0635 - val_acc: 0.5486\n",
      "Epoch 727/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0488 - acc: 0.6433 - val_loss: 0.0634 - val_acc: 0.5436\n",
      "Epoch 728/3000\n",
      "3608/3608 [==============================] - 1s 220us/step - loss: 0.0484 - acc: 0.6483 - val_loss: 0.0636 - val_acc: 0.5503\n",
      "Epoch 729/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0484 - acc: 0.6416 - val_loss: 0.0652 - val_acc: 0.5370\n",
      "Epoch 730/3000\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0487 - acc: 0.6458 - val_loss: 0.0627 - val_acc: 0.5544\n",
      "Epoch 731/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0486 - acc: 0.6438 - val_loss: 0.0633 - val_acc: 0.5503\n",
      "Epoch 732/3000\n",
      "3608/3608 [==============================] - 1s 219us/step - loss: 0.0485 - acc: 0.6419 - val_loss: 0.0638 - val_acc: 0.5445\n",
      "Epoch 733/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0496 - acc: 0.6383 - val_loss: 0.0643 - val_acc: 0.5428\n",
      "Epoch 734/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0492 - acc: 0.6364 - val_loss: 0.0635 - val_acc: 0.5411\n",
      "Epoch 735/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0487 - acc: 0.6433 - val_loss: 0.0646 - val_acc: 0.5395\n",
      "Epoch 736/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0491 - acc: 0.6375 - val_loss: 0.0630 - val_acc: 0.5445\n",
      "Epoch 737/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0488 - acc: 0.6364 - val_loss: 0.0638 - val_acc: 0.5403\n",
      "Epoch 738/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0488 - acc: 0.6433 - val_loss: 0.0638 - val_acc: 0.5387\n",
      "Epoch 739/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0481 - acc: 0.6447 - val_loss: 0.0632 - val_acc: 0.5503\n",
      "Epoch 740/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0484 - acc: 0.6430 - val_loss: 0.0638 - val_acc: 0.5445\n",
      "Epoch 741/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0485 - acc: 0.6377 - val_loss: 0.0636 - val_acc: 0.5503\n",
      "Epoch 742/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0484 - acc: 0.6436 - val_loss: 0.0633 - val_acc: 0.5561\n",
      "Epoch 743/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0486 - acc: 0.6430 - val_loss: 0.0634 - val_acc: 0.5553\n",
      "Epoch 744/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0486 - acc: 0.6452 - val_loss: 0.0641 - val_acc: 0.5403\n",
      "Epoch 745/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0488 - acc: 0.6405 - val_loss: 0.0632 - val_acc: 0.5470\n",
      "Epoch 746/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0484 - acc: 0.6466 - val_loss: 0.0632 - val_acc: 0.5478\n",
      "Epoch 747/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0488 - acc: 0.6394 - val_loss: 0.0630 - val_acc: 0.5520\n",
      "Epoch 748/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0487 - acc: 0.6425 - val_loss: 0.0644 - val_acc: 0.5320\n",
      "Epoch 749/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0485 - acc: 0.6488 - val_loss: 0.0628 - val_acc: 0.5478\n",
      "Epoch 750/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0486 - acc: 0.6452 - val_loss: 0.0633 - val_acc: 0.5470\n",
      "Epoch 751/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0485 - acc: 0.6455 - val_loss: 0.0635 - val_acc: 0.5370\n",
      "Epoch 752/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0490 - acc: 0.6386 - val_loss: 0.0640 - val_acc: 0.5461\n",
      "Epoch 753/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0491 - acc: 0.6375 - val_loss: 0.0652 - val_acc: 0.5428\n",
      "Epoch 754/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0492 - acc: 0.6389 - val_loss: 0.0637 - val_acc: 0.5428\n",
      "Epoch 755/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0485 - acc: 0.6461 - val_loss: 0.0647 - val_acc: 0.5287\n",
      "Epoch 756/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0492 - acc: 0.6391 - val_loss: 0.0629 - val_acc: 0.5553\n",
      "Epoch 757/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0484 - acc: 0.6483 - val_loss: 0.0641 - val_acc: 0.5436\n",
      "Epoch 758/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0479 - acc: 0.6458 - val_loss: 0.0636 - val_acc: 0.5470\n",
      "Epoch 759/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0481 - acc: 0.6480 - val_loss: 0.0641 - val_acc: 0.5420\n",
      "Epoch 760/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0484 - acc: 0.6488 - val_loss: 0.0634 - val_acc: 0.5453\n",
      "Epoch 761/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0491 - acc: 0.6419 - val_loss: 0.0634 - val_acc: 0.5470\n",
      "Epoch 762/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0484 - acc: 0.6491 - val_loss: 0.0640 - val_acc: 0.5436\n",
      "Epoch 763/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0494 - acc: 0.6416 - val_loss: 0.0636 - val_acc: 0.5544\n",
      "Epoch 764/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0487 - acc: 0.6438 - val_loss: 0.0641 - val_acc: 0.5445\n",
      "Epoch 765/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0492 - acc: 0.6405 - val_loss: 0.0642 - val_acc: 0.5403\n",
      "Epoch 766/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0491 - acc: 0.6438 - val_loss: 0.0641 - val_acc: 0.5403\n",
      "Epoch 767/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0489 - acc: 0.6394 - val_loss: 0.0642 - val_acc: 0.5387\n",
      "Epoch 768/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0485 - acc: 0.6447 - val_loss: 0.0631 - val_acc: 0.5586\n",
      "Epoch 769/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0494 - acc: 0.6408 - val_loss: 0.0649 - val_acc: 0.5411\n",
      "Epoch 770/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0486 - acc: 0.6469 - val_loss: 0.0642 - val_acc: 0.5478\n",
      "Epoch 771/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0489 - acc: 0.6389 - val_loss: 0.0634 - val_acc: 0.5428\n",
      "Epoch 772/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0484 - acc: 0.6375 - val_loss: 0.0633 - val_acc: 0.5495\n",
      "Epoch 773/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0483 - acc: 0.6483 - val_loss: 0.0653 - val_acc: 0.5237\n",
      "Epoch 774/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0484 - acc: 0.6475 - val_loss: 0.0640 - val_acc: 0.5403\n",
      "Epoch 775/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0483 - acc: 0.6441 - val_loss: 0.0632 - val_acc: 0.5536\n",
      "Epoch 776/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0484 - acc: 0.6522 - val_loss: 0.0640 - val_acc: 0.5461\n",
      "Epoch 777/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0487 - acc: 0.6400 - val_loss: 0.0654 - val_acc: 0.5395\n",
      "Epoch 778/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0485 - acc: 0.6463 - val_loss: 0.0636 - val_acc: 0.5436\n",
      "Epoch 779/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0486 - acc: 0.6433 - val_loss: 0.0648 - val_acc: 0.5520\n",
      "Epoch 780/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0490 - acc: 0.6441 - val_loss: 0.0640 - val_acc: 0.5453\n",
      "Epoch 781/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0486 - acc: 0.6436 - val_loss: 0.0643 - val_acc: 0.5470\n",
      "Epoch 782/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0483 - acc: 0.6441 - val_loss: 0.0638 - val_acc: 0.5428\n",
      "Epoch 783/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0490 - acc: 0.6416 - val_loss: 0.0644 - val_acc: 0.5362\n",
      "Epoch 784/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0489 - acc: 0.6427 - val_loss: 0.0646 - val_acc: 0.5387\n",
      "Epoch 785/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0484 - acc: 0.6438 - val_loss: 0.0646 - val_acc: 0.5378\n",
      "Epoch 786/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0480 - acc: 0.6483 - val_loss: 0.0640 - val_acc: 0.5486\n",
      "Epoch 787/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0481 - acc: 0.6491 - val_loss: 0.0646 - val_acc: 0.5453\n",
      "Epoch 788/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0480 - acc: 0.6477 - val_loss: 0.0642 - val_acc: 0.5495\n",
      "Epoch 789/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0482 - acc: 0.6494 - val_loss: 0.0637 - val_acc: 0.5495\n",
      "Epoch 790/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0483 - acc: 0.6436 - val_loss: 0.0635 - val_acc: 0.5461\n",
      "Epoch 791/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0480 - acc: 0.6513 - val_loss: 0.0637 - val_acc: 0.5528\n",
      "Epoch 792/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0478 - acc: 0.6477 - val_loss: 0.0639 - val_acc: 0.5453\n",
      "Epoch 793/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0479 - acc: 0.6511 - val_loss: 0.0634 - val_acc: 0.5503\n",
      "Epoch 794/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0484 - acc: 0.6483 - val_loss: 0.0634 - val_acc: 0.5478\n",
      "Epoch 795/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0483 - acc: 0.6452 - val_loss: 0.0638 - val_acc: 0.5503\n",
      "Epoch 796/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0488 - acc: 0.6419 - val_loss: 0.0635 - val_acc: 0.5536\n",
      "Epoch 797/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0494 - acc: 0.6366 - val_loss: 0.0640 - val_acc: 0.5370\n",
      "Epoch 798/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0488 - acc: 0.6416 - val_loss: 0.0656 - val_acc: 0.5295\n",
      "Epoch 799/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0484 - acc: 0.6461 - val_loss: 0.0631 - val_acc: 0.5503\n",
      "Epoch 800/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0482 - acc: 0.6466 - val_loss: 0.0634 - val_acc: 0.5420\n",
      "Epoch 801/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0481 - acc: 0.6461 - val_loss: 0.0629 - val_acc: 0.5511\n",
      "Epoch 802/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0483 - acc: 0.6499 - val_loss: 0.0641 - val_acc: 0.5445\n",
      "Epoch 803/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0482 - acc: 0.6472 - val_loss: 0.0640 - val_acc: 0.5411\n",
      "Epoch 804/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0483 - acc: 0.6516 - val_loss: 0.0637 - val_acc: 0.5553\n",
      "Epoch 805/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0481 - acc: 0.6461 - val_loss: 0.0640 - val_acc: 0.5453\n",
      "Epoch 806/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0489 - acc: 0.6469 - val_loss: 0.0636 - val_acc: 0.5511\n",
      "Epoch 807/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0483 - acc: 0.6486 - val_loss: 0.0637 - val_acc: 0.5528\n",
      "Epoch 808/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0485 - acc: 0.6433 - val_loss: 0.0640 - val_acc: 0.5495\n",
      "Epoch 809/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0485 - acc: 0.6472 - val_loss: 0.0641 - val_acc: 0.5461\n",
      "Epoch 810/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0485 - acc: 0.6422 - val_loss: 0.0650 - val_acc: 0.5378\n",
      "Epoch 811/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0481 - acc: 0.6491 - val_loss: 0.0635 - val_acc: 0.5461\n",
      "Epoch 812/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0487 - acc: 0.6419 - val_loss: 0.0634 - val_acc: 0.5436\n",
      "Epoch 813/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0485 - acc: 0.6447 - val_loss: 0.0635 - val_acc: 0.5428\n",
      "Epoch 814/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0491 - acc: 0.6444 - val_loss: 0.0633 - val_acc: 0.5486\n",
      "Epoch 815/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0482 - acc: 0.6455 - val_loss: 0.0633 - val_acc: 0.5495\n",
      "Epoch 816/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0481 - acc: 0.6502 - val_loss: 0.0636 - val_acc: 0.5486\n",
      "Epoch 817/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6522 - val_loss: 0.0650 - val_acc: 0.5461\n",
      "Epoch 818/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0481 - acc: 0.6519 - val_loss: 0.0646 - val_acc: 0.5470\n",
      "Epoch 819/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0482 - acc: 0.6466 - val_loss: 0.0638 - val_acc: 0.5420\n",
      "Epoch 820/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0486 - acc: 0.6450 - val_loss: 0.0632 - val_acc: 0.5553\n",
      "Epoch 821/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0483 - acc: 0.6469 - val_loss: 0.0646 - val_acc: 0.5486\n",
      "Epoch 822/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0486 - acc: 0.6425 - val_loss: 0.0641 - val_acc: 0.5453\n",
      "Epoch 823/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0482 - acc: 0.6516 - val_loss: 0.0631 - val_acc: 0.5470\n",
      "Epoch 824/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0485 - acc: 0.6486 - val_loss: 0.0643 - val_acc: 0.5486\n",
      "Epoch 825/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0484 - acc: 0.6466 - val_loss: 0.0634 - val_acc: 0.5578\n",
      "Epoch 826/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0483 - acc: 0.6483 - val_loss: 0.0639 - val_acc: 0.5495\n",
      "Epoch 827/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0480 - acc: 0.6533 - val_loss: 0.0641 - val_acc: 0.5478\n",
      "Epoch 828/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0479 - acc: 0.6461 - val_loss: 0.0641 - val_acc: 0.5353\n",
      "Epoch 829/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0482 - acc: 0.6497 - val_loss: 0.0649 - val_acc: 0.5470\n",
      "Epoch 830/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0484 - acc: 0.6427 - val_loss: 0.0642 - val_acc: 0.5395\n",
      "Epoch 831/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0486 - acc: 0.6411 - val_loss: 0.0639 - val_acc: 0.5495\n",
      "Epoch 832/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0487 - acc: 0.6472 - val_loss: 0.0641 - val_acc: 0.5428\n",
      "Epoch 833/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0483 - acc: 0.6461 - val_loss: 0.0646 - val_acc: 0.5387\n",
      "Epoch 834/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0481 - acc: 0.6502 - val_loss: 0.0635 - val_acc: 0.5544\n",
      "Epoch 835/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0483 - acc: 0.6513 - val_loss: 0.0640 - val_acc: 0.5395\n",
      "Epoch 836/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0481 - acc: 0.6519 - val_loss: 0.0641 - val_acc: 0.5411\n",
      "Epoch 837/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0481 - acc: 0.6494 - val_loss: 0.0650 - val_acc: 0.5362\n",
      "Epoch 838/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0482 - acc: 0.6472 - val_loss: 0.0639 - val_acc: 0.5478\n",
      "Epoch 839/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0481 - acc: 0.6486 - val_loss: 0.0641 - val_acc: 0.5536\n",
      "Epoch 840/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6477 - val_loss: 0.0638 - val_acc: 0.5520\n",
      "Epoch 841/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0483 - acc: 0.6486 - val_loss: 0.0649 - val_acc: 0.5345\n",
      "Epoch 842/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0486 - acc: 0.6472 - val_loss: 0.0643 - val_acc: 0.5461\n",
      "Epoch 843/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0480 - acc: 0.6508 - val_loss: 0.0644 - val_acc: 0.5312\n",
      "Epoch 844/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6505 - val_loss: 0.0641 - val_acc: 0.5370\n",
      "Epoch 845/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0480 - acc: 0.6533 - val_loss: 0.0639 - val_acc: 0.5470\n",
      "Epoch 846/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0480 - acc: 0.6491 - val_loss: 0.0637 - val_acc: 0.5528\n",
      "Epoch 847/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0481 - acc: 0.6511 - val_loss: 0.0648 - val_acc: 0.5511\n",
      "Epoch 848/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0485 - acc: 0.6491 - val_loss: 0.0629 - val_acc: 0.5470\n",
      "Epoch 849/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0484 - acc: 0.6524 - val_loss: 0.0638 - val_acc: 0.5470\n",
      "Epoch 850/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0480 - acc: 0.6533 - val_loss: 0.0641 - val_acc: 0.5428\n",
      "Epoch 851/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0478 - acc: 0.6541 - val_loss: 0.0634 - val_acc: 0.5520\n",
      "Epoch 852/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0478 - acc: 0.6477 - val_loss: 0.0642 - val_acc: 0.5511\n",
      "Epoch 853/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0480 - acc: 0.6494 - val_loss: 0.0635 - val_acc: 0.5561\n",
      "Epoch 854/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0490 - acc: 0.6400 - val_loss: 0.0647 - val_acc: 0.5387\n",
      "Epoch 855/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0485 - acc: 0.6458 - val_loss: 0.0636 - val_acc: 0.5578\n",
      "Epoch 856/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0487 - acc: 0.6372 - val_loss: 0.0631 - val_acc: 0.5478\n",
      "Epoch 857/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0486 - acc: 0.6414 - val_loss: 0.0641 - val_acc: 0.5387\n",
      "Epoch 858/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0481 - acc: 0.6519 - val_loss: 0.0654 - val_acc: 0.5395\n",
      "Epoch 859/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0483 - acc: 0.6472 - val_loss: 0.0640 - val_acc: 0.5486\n",
      "Epoch 860/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0486 - acc: 0.6402 - val_loss: 0.0631 - val_acc: 0.5578\n",
      "Epoch 861/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0484 - acc: 0.6436 - val_loss: 0.0638 - val_acc: 0.5520\n",
      "Epoch 862/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0489 - acc: 0.6389 - val_loss: 0.0636 - val_acc: 0.5378\n",
      "Epoch 863/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0482 - acc: 0.6488 - val_loss: 0.0645 - val_acc: 0.5470\n",
      "Epoch 864/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0480 - acc: 0.6502 - val_loss: 0.0640 - val_acc: 0.5411\n",
      "Epoch 865/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0481 - acc: 0.6461 - val_loss: 0.0648 - val_acc: 0.5262\n",
      "Epoch 866/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0485 - acc: 0.6450 - val_loss: 0.0631 - val_acc: 0.5478\n",
      "Epoch 867/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0481 - acc: 0.6477 - val_loss: 0.0635 - val_acc: 0.5436\n",
      "Epoch 868/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0479 - acc: 0.6502 - val_loss: 0.0628 - val_acc: 0.5486\n",
      "Epoch 869/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0484 - acc: 0.6486 - val_loss: 0.0641 - val_acc: 0.5370\n",
      "Epoch 870/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0485 - acc: 0.6458 - val_loss: 0.0635 - val_acc: 0.5436\n",
      "Epoch 871/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0478 - acc: 0.6527 - val_loss: 0.0637 - val_acc: 0.5486\n",
      "Epoch 872/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0476 - acc: 0.6549 - val_loss: 0.0630 - val_acc: 0.5478\n",
      "Epoch 873/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0477 - acc: 0.6488 - val_loss: 0.0645 - val_acc: 0.5436\n",
      "Epoch 874/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0480 - acc: 0.6547 - val_loss: 0.0641 - val_acc: 0.5403\n",
      "Epoch 875/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0475 - acc: 0.6563 - val_loss: 0.0640 - val_acc: 0.5428\n",
      "Epoch 876/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0480 - acc: 0.6463 - val_loss: 0.0647 - val_acc: 0.5436\n",
      "Epoch 877/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0488 - acc: 0.6433 - val_loss: 0.0644 - val_acc: 0.5436\n",
      "Epoch 878/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0481 - acc: 0.6524 - val_loss: 0.0639 - val_acc: 0.5428\n",
      "Epoch 879/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0481 - acc: 0.6535 - val_loss: 0.0648 - val_acc: 0.5295\n",
      "Epoch 880/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0477 - acc: 0.6502 - val_loss: 0.0643 - val_acc: 0.5461\n",
      "Epoch 881/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0487 - acc: 0.6483 - val_loss: 0.0639 - val_acc: 0.5411\n",
      "Epoch 882/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0485 - acc: 0.6472 - val_loss: 0.0647 - val_acc: 0.5378\n",
      "Epoch 883/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0482 - acc: 0.6480 - val_loss: 0.0631 - val_acc: 0.5486\n",
      "Epoch 884/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0488 - acc: 0.6461 - val_loss: 0.0640 - val_acc: 0.5420\n",
      "Epoch 885/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0484 - acc: 0.6497 - val_loss: 0.0652 - val_acc: 0.5428\n",
      "Epoch 886/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0483 - acc: 0.6461 - val_loss: 0.0644 - val_acc: 0.5403\n",
      "Epoch 887/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0481 - acc: 0.6522 - val_loss: 0.0636 - val_acc: 0.5478\n",
      "Epoch 888/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0485 - acc: 0.6491 - val_loss: 0.0642 - val_acc: 0.5403\n",
      "Epoch 889/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6513 - val_loss: 0.0630 - val_acc: 0.5511\n",
      "Epoch 890/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0481 - acc: 0.6499 - val_loss: 0.0642 - val_acc: 0.5411\n",
      "Epoch 891/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0482 - acc: 0.6433 - val_loss: 0.0644 - val_acc: 0.5395\n",
      "Epoch 892/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0485 - acc: 0.6477 - val_loss: 0.0644 - val_acc: 0.5428\n",
      "Epoch 893/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0480 - acc: 0.6522 - val_loss: 0.0643 - val_acc: 0.5403\n",
      "Epoch 894/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0476 - acc: 0.6527 - val_loss: 0.0639 - val_acc: 0.5528\n",
      "Epoch 895/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0476 - acc: 0.6555 - val_loss: 0.0640 - val_acc: 0.5445\n",
      "Epoch 896/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0482 - acc: 0.6499 - val_loss: 0.0640 - val_acc: 0.5436\n",
      "Epoch 897/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0478 - acc: 0.6508 - val_loss: 0.0643 - val_acc: 0.5478\n",
      "Epoch 898/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0480 - acc: 0.6547 - val_loss: 0.0643 - val_acc: 0.5403\n",
      "Epoch 899/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0488 - acc: 0.6458 - val_loss: 0.0638 - val_acc: 0.5362\n",
      "Epoch 900/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0482 - acc: 0.6450 - val_loss: 0.0642 - val_acc: 0.5553\n",
      "Epoch 901/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0483 - acc: 0.6530 - val_loss: 0.0641 - val_acc: 0.5536\n",
      "Epoch 902/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0481 - acc: 0.6494 - val_loss: 0.0653 - val_acc: 0.5436\n",
      "Epoch 903/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0478 - acc: 0.6483 - val_loss: 0.0639 - val_acc: 0.5461\n",
      "Epoch 904/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0486 - acc: 0.6497 - val_loss: 0.0641 - val_acc: 0.5520\n",
      "Epoch 905/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0488 - acc: 0.6452 - val_loss: 0.0656 - val_acc: 0.5403\n",
      "Epoch 906/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0484 - acc: 0.6433 - val_loss: 0.0636 - val_acc: 0.5428\n",
      "Epoch 907/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0479 - acc: 0.6438 - val_loss: 0.0641 - val_acc: 0.5461\n",
      "Epoch 908/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0485 - acc: 0.6444 - val_loss: 0.0637 - val_acc: 0.5486\n",
      "Epoch 909/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0480 - acc: 0.6486 - val_loss: 0.0642 - val_acc: 0.5478\n",
      "Epoch 910/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0486 - acc: 0.6366 - val_loss: 0.0635 - val_acc: 0.5411\n",
      "Epoch 911/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0483 - acc: 0.6400 - val_loss: 0.0636 - val_acc: 0.5436\n",
      "Epoch 912/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0480 - acc: 0.6494 - val_loss: 0.0636 - val_acc: 0.5495\n",
      "Epoch 913/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0476 - acc: 0.6572 - val_loss: 0.0645 - val_acc: 0.5303\n",
      "Epoch 914/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6486 - val_loss: 0.0636 - val_acc: 0.5453\n",
      "Epoch 915/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0475 - acc: 0.6533 - val_loss: 0.0636 - val_acc: 0.5544\n",
      "Epoch 916/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0480 - acc: 0.6475 - val_loss: 0.0649 - val_acc: 0.5411\n",
      "Epoch 917/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0486 - acc: 0.6488 - val_loss: 0.0651 - val_acc: 0.5353\n",
      "Epoch 918/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6511 - val_loss: 0.0649 - val_acc: 0.5387\n",
      "Epoch 919/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0480 - acc: 0.6494 - val_loss: 0.0649 - val_acc: 0.5411\n",
      "Epoch 920/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0478 - acc: 0.6524 - val_loss: 0.0644 - val_acc: 0.5387\n",
      "Epoch 921/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0477 - acc: 0.6511 - val_loss: 0.0640 - val_acc: 0.5453\n",
      "Epoch 922/3000\n",
      "3608/3608 [==============================] - 1s 205us/step - loss: 0.0481 - acc: 0.6486 - val_loss: 0.0655 - val_acc: 0.5312\n",
      "Epoch 923/3000\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0476 - acc: 0.6555 - val_loss: 0.0647 - val_acc: 0.5370\n",
      "Epoch 924/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0479 - acc: 0.6560 - val_loss: 0.0637 - val_acc: 0.5478\n",
      "Epoch 925/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0478 - acc: 0.6535 - val_loss: 0.0637 - val_acc: 0.5478\n",
      "Epoch 926/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0478 - acc: 0.6505 - val_loss: 0.0644 - val_acc: 0.5586\n",
      "Epoch 927/3000\n",
      "3608/3608 [==============================] - 1s 206us/step - loss: 0.0482 - acc: 0.6497 - val_loss: 0.0636 - val_acc: 0.5478\n",
      "Epoch 928/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0483 - acc: 0.6450 - val_loss: 0.0645 - val_acc: 0.5387\n",
      "Epoch 929/3000\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0496 - acc: 0.6364 - val_loss: 0.0644 - val_acc: 0.5420\n",
      "Epoch 930/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0486 - acc: 0.6444 - val_loss: 0.0637 - val_acc: 0.5461\n",
      "Epoch 931/3000\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0484 - acc: 0.6450 - val_loss: 0.0645 - val_acc: 0.5378\n",
      "Epoch 932/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0481 - acc: 0.6524 - val_loss: 0.0637 - val_acc: 0.5511\n",
      "Epoch 933/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0481 - acc: 0.6452 - val_loss: 0.0640 - val_acc: 0.5420\n",
      "Epoch 934/3000\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0475 - acc: 0.6527 - val_loss: 0.0641 - val_acc: 0.5453\n",
      "Epoch 935/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0474 - acc: 0.6566 - val_loss: 0.0643 - val_acc: 0.5453\n",
      "Epoch 936/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0478 - acc: 0.6461 - val_loss: 0.0640 - val_acc: 0.5387\n",
      "Epoch 937/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0478 - acc: 0.6533 - val_loss: 0.0647 - val_acc: 0.5411\n",
      "Epoch 938/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0482 - acc: 0.6499 - val_loss: 0.0636 - val_acc: 0.5536\n",
      "Epoch 939/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0480 - acc: 0.6483 - val_loss: 0.0646 - val_acc: 0.5561\n",
      "Epoch 940/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0483 - acc: 0.6455 - val_loss: 0.0649 - val_acc: 0.5312\n",
      "Epoch 941/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0478 - acc: 0.6513 - val_loss: 0.0640 - val_acc: 0.5470\n",
      "Epoch 942/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0480 - acc: 0.6450 - val_loss: 0.0639 - val_acc: 0.5511\n",
      "Epoch 943/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0478 - acc: 0.6486 - val_loss: 0.0641 - val_acc: 0.5387\n",
      "Epoch 944/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0481 - acc: 0.6530 - val_loss: 0.0641 - val_acc: 0.5395\n",
      "Epoch 945/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0486 - acc: 0.6427 - val_loss: 0.0637 - val_acc: 0.5536\n",
      "Epoch 946/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0479 - acc: 0.6458 - val_loss: 0.0636 - val_acc: 0.5503\n",
      "Epoch 947/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0477 - acc: 0.6522 - val_loss: 0.0640 - val_acc: 0.5511\n",
      "Epoch 948/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0481 - acc: 0.6511 - val_loss: 0.0639 - val_acc: 0.5495\n",
      "Epoch 949/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0478 - acc: 0.6580 - val_loss: 0.0645 - val_acc: 0.5387\n",
      "Epoch 950/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0478 - acc: 0.6483 - val_loss: 0.0640 - val_acc: 0.5411\n",
      "Epoch 951/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0479 - acc: 0.6480 - val_loss: 0.0637 - val_acc: 0.5503\n",
      "Epoch 952/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0487 - acc: 0.6452 - val_loss: 0.0636 - val_acc: 0.5520\n",
      "Epoch 953/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0482 - acc: 0.6494 - val_loss: 0.0645 - val_acc: 0.5428\n",
      "Epoch 954/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0478 - acc: 0.6486 - val_loss: 0.0638 - val_acc: 0.5395\n",
      "Epoch 955/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0482 - acc: 0.6427 - val_loss: 0.0650 - val_acc: 0.5337\n",
      "Epoch 956/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0482 - acc: 0.6455 - val_loss: 0.0639 - val_acc: 0.5486\n",
      "Epoch 957/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0480 - acc: 0.6486 - val_loss: 0.0639 - val_acc: 0.5420\n",
      "Epoch 958/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0491 - acc: 0.6372 - val_loss: 0.0633 - val_acc: 0.5553\n",
      "Epoch 959/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0480 - acc: 0.6511 - val_loss: 0.0654 - val_acc: 0.5461\n",
      "Epoch 960/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0479 - acc: 0.6519 - val_loss: 0.0648 - val_acc: 0.5403\n",
      "Epoch 961/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0485 - acc: 0.6447 - val_loss: 0.0640 - val_acc: 0.5403\n",
      "Epoch 962/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0483 - acc: 0.6494 - val_loss: 0.0644 - val_acc: 0.5495\n",
      "Epoch 963/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0480 - acc: 0.6524 - val_loss: 0.0669 - val_acc: 0.5179\n",
      "Epoch 964/3000\n",
      "3608/3608 [==============================] - 1s 159us/step - loss: 0.0480 - acc: 0.6447 - val_loss: 0.0638 - val_acc: 0.5362\n",
      "Epoch 965/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6497 - val_loss: 0.0636 - val_acc: 0.5495\n",
      "Epoch 966/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0480 - acc: 0.6483 - val_loss: 0.0648 - val_acc: 0.5445\n",
      "Epoch 967/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0480 - acc: 0.6477 - val_loss: 0.0642 - val_acc: 0.5436\n",
      "Epoch 968/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0475 - acc: 0.6516 - val_loss: 0.0651 - val_acc: 0.5453\n",
      "Epoch 969/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0485 - acc: 0.6499 - val_loss: 0.0643 - val_acc: 0.5445\n",
      "Epoch 970/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0484 - acc: 0.6480 - val_loss: 0.0643 - val_acc: 0.5428\n",
      "Epoch 971/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0479 - acc: 0.6530 - val_loss: 0.0643 - val_acc: 0.5478\n",
      "Epoch 972/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0482 - acc: 0.6516 - val_loss: 0.0641 - val_acc: 0.5345\n",
      "Epoch 973/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0477 - acc: 0.6522 - val_loss: 0.0642 - val_acc: 0.5378\n",
      "Epoch 974/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0492 - acc: 0.6400 - val_loss: 0.0652 - val_acc: 0.5478\n",
      "Epoch 975/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0489 - acc: 0.6386 - val_loss: 0.0643 - val_acc: 0.5453\n",
      "Epoch 976/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0477 - acc: 0.6508 - val_loss: 0.0642 - val_acc: 0.5478\n",
      "Epoch 977/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0477 - acc: 0.6466 - val_loss: 0.0640 - val_acc: 0.5461\n",
      "Epoch 978/3000\n",
      "3608/3608 [==============================] - 1s 158us/step - loss: 0.0478 - acc: 0.6527 - val_loss: 0.0635 - val_acc: 0.5478\n",
      "Epoch 979/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0485 - acc: 0.6491 - val_loss: 0.0643 - val_acc: 0.5378\n",
      "Epoch 980/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0478 - acc: 0.6544 - val_loss: 0.0648 - val_acc: 0.5461\n",
      "Epoch 981/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6572 - val_loss: 0.0647 - val_acc: 0.5403\n",
      "Epoch 982/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0475 - acc: 0.6505 - val_loss: 0.0642 - val_acc: 0.5403\n",
      "Epoch 983/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0479 - acc: 0.6486 - val_loss: 0.0634 - val_acc: 0.5495\n",
      "Epoch 984/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6524 - val_loss: 0.0644 - val_acc: 0.5445\n",
      "Epoch 985/3000\n",
      "3608/3608 [==============================] - 1s 159us/step - loss: 0.0475 - acc: 0.6535 - val_loss: 0.0639 - val_acc: 0.5478\n",
      "Epoch 986/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0478 - acc: 0.6488 - val_loss: 0.0641 - val_acc: 0.5436\n",
      "Epoch 987/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0476 - acc: 0.6522 - val_loss: 0.0654 - val_acc: 0.5295\n",
      "Epoch 988/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0485 - acc: 0.6450 - val_loss: 0.0637 - val_acc: 0.5486\n",
      "Epoch 989/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0481 - acc: 0.6477 - val_loss: 0.0638 - val_acc: 0.5528\n",
      "Epoch 990/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0477 - acc: 0.6513 - val_loss: 0.0661 - val_acc: 0.5245\n",
      "Epoch 991/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0476 - acc: 0.6524 - val_loss: 0.0648 - val_acc: 0.5362\n",
      "Epoch 992/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0485 - acc: 0.6458 - val_loss: 0.0643 - val_acc: 0.5544\n",
      "Epoch 993/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0482 - acc: 0.6477 - val_loss: 0.0653 - val_acc: 0.5420\n",
      "Epoch 994/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0476 - acc: 0.6535 - val_loss: 0.0651 - val_acc: 0.5395\n",
      "Epoch 995/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0477 - acc: 0.6502 - val_loss: 0.0637 - val_acc: 0.5528\n",
      "Epoch 996/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0480 - acc: 0.6511 - val_loss: 0.0647 - val_acc: 0.5478\n",
      "Epoch 997/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0476 - acc: 0.6541 - val_loss: 0.0639 - val_acc: 0.5470\n",
      "Epoch 998/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0476 - acc: 0.6549 - val_loss: 0.0640 - val_acc: 0.5453\n",
      "Epoch 999/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0475 - acc: 0.6516 - val_loss: 0.0636 - val_acc: 0.5445\n",
      "Epoch 1000/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6569 - val_loss: 0.0644 - val_acc: 0.5370\n",
      "Epoch 1001/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0477 - acc: 0.6533 - val_loss: 0.0644 - val_acc: 0.5511\n",
      "Epoch 1002/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0482 - acc: 0.6486 - val_loss: 0.0639 - val_acc: 0.5461\n",
      "Epoch 1003/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0486 - acc: 0.6472 - val_loss: 0.0636 - val_acc: 0.5461\n",
      "Epoch 1004/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0483 - acc: 0.6475 - val_loss: 0.0642 - val_acc: 0.5428\n",
      "Epoch 1005/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0504 - acc: 0.6300 - val_loss: 0.0640 - val_acc: 0.5470\n",
      "Epoch 1006/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0497 - acc: 0.6383 - val_loss: 0.0646 - val_acc: 0.5470\n",
      "Epoch 1007/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0483 - acc: 0.6472 - val_loss: 0.0642 - val_acc: 0.5486\n",
      "Epoch 1008/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0485 - acc: 0.6400 - val_loss: 0.0648 - val_acc: 0.5520\n",
      "Epoch 1009/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0482 - acc: 0.6505 - val_loss: 0.0645 - val_acc: 0.5470\n",
      "Epoch 1010/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0478 - acc: 0.6505 - val_loss: 0.0642 - val_acc: 0.5461\n",
      "Epoch 1011/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0474 - acc: 0.6494 - val_loss: 0.0636 - val_acc: 0.5411\n",
      "Epoch 1012/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0476 - acc: 0.6527 - val_loss: 0.0642 - val_acc: 0.5478\n",
      "Epoch 1013/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0475 - acc: 0.6560 - val_loss: 0.0649 - val_acc: 0.5411\n",
      "Epoch 1014/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0478 - acc: 0.6486 - val_loss: 0.0637 - val_acc: 0.5503\n",
      "Epoch 1015/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0482 - acc: 0.6477 - val_loss: 0.0641 - val_acc: 0.5486\n",
      "Epoch 1016/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0489 - acc: 0.6386 - val_loss: 0.0652 - val_acc: 0.5420\n",
      "Epoch 1017/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0481 - acc: 0.6502 - val_loss: 0.0638 - val_acc: 0.5503\n",
      "Epoch 1018/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0475 - acc: 0.6513 - val_loss: 0.0647 - val_acc: 0.5495\n",
      "Epoch 1019/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0478 - acc: 0.6477 - val_loss: 0.0644 - val_acc: 0.5520\n",
      "Epoch 1020/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0472 - acc: 0.6560 - val_loss: 0.0638 - val_acc: 0.5503\n",
      "Epoch 1021/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6535 - val_loss: 0.0641 - val_acc: 0.5544\n",
      "Epoch 1022/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0474 - acc: 0.6522 - val_loss: 0.0635 - val_acc: 0.5553\n",
      "Epoch 1023/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0482 - acc: 0.6452 - val_loss: 0.0652 - val_acc: 0.5212\n",
      "Epoch 1024/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0480 - acc: 0.6505 - val_loss: 0.0642 - val_acc: 0.5478\n",
      "Epoch 1025/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0483 - acc: 0.6477 - val_loss: 0.0642 - val_acc: 0.5428\n",
      "Epoch 1026/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0480 - acc: 0.6463 - val_loss: 0.0647 - val_acc: 0.5370\n",
      "Epoch 1027/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0476 - acc: 0.6463 - val_loss: 0.0648 - val_acc: 0.5486\n",
      "Epoch 1028/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0475 - acc: 0.6552 - val_loss: 0.0637 - val_acc: 0.5403\n",
      "Epoch 1029/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6516 - val_loss: 0.0642 - val_acc: 0.5378\n",
      "Epoch 1030/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0476 - acc: 0.6491 - val_loss: 0.0643 - val_acc: 0.5445\n",
      "Epoch 1031/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6569 - val_loss: 0.0640 - val_acc: 0.5461\n",
      "Epoch 1032/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0479 - acc: 0.6541 - val_loss: 0.0642 - val_acc: 0.5470\n",
      "Epoch 1033/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0478 - acc: 0.6469 - val_loss: 0.0639 - val_acc: 0.5486\n",
      "Epoch 1034/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0490 - acc: 0.6438 - val_loss: 0.0647 - val_acc: 0.5395\n",
      "Epoch 1035/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0482 - acc: 0.6458 - val_loss: 0.0638 - val_acc: 0.5503\n",
      "Epoch 1036/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0482 - acc: 0.6444 - val_loss: 0.0649 - val_acc: 0.5403\n",
      "Epoch 1037/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0481 - acc: 0.6494 - val_loss: 0.0638 - val_acc: 0.5528\n",
      "Epoch 1038/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0478 - acc: 0.6466 - val_loss: 0.0646 - val_acc: 0.5337\n",
      "Epoch 1039/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0479 - acc: 0.6461 - val_loss: 0.0639 - val_acc: 0.5445\n",
      "Epoch 1040/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6502 - val_loss: 0.0645 - val_acc: 0.5436\n",
      "Epoch 1041/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0492 - acc: 0.6372 - val_loss: 0.0633 - val_acc: 0.5528\n",
      "Epoch 1042/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0477 - acc: 0.6522 - val_loss: 0.0635 - val_acc: 0.5453\n",
      "Epoch 1043/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6522 - val_loss: 0.0643 - val_acc: 0.5445\n",
      "Epoch 1044/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0485 - acc: 0.6397 - val_loss: 0.0640 - val_acc: 0.5428\n",
      "Epoch 1045/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0476 - acc: 0.6508 - val_loss: 0.0644 - val_acc: 0.5436\n",
      "Epoch 1046/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0480 - acc: 0.6466 - val_loss: 0.0646 - val_acc: 0.5403\n",
      "Epoch 1047/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6513 - val_loss: 0.0646 - val_acc: 0.5387\n",
      "Epoch 1048/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0478 - acc: 0.6513 - val_loss: 0.0646 - val_acc: 0.5370\n",
      "Epoch 1049/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0475 - acc: 0.6519 - val_loss: 0.0644 - val_acc: 0.5495\n",
      "Epoch 1050/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0482 - acc: 0.6480 - val_loss: 0.0644 - val_acc: 0.5428\n",
      "Epoch 1051/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0476 - acc: 0.6530 - val_loss: 0.0650 - val_acc: 0.5387\n",
      "Epoch 1052/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6480 - val_loss: 0.0640 - val_acc: 0.5478\n",
      "Epoch 1053/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6621 - val_loss: 0.0640 - val_acc: 0.5478\n",
      "Epoch 1054/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0474 - acc: 0.6486 - val_loss: 0.0646 - val_acc: 0.5528\n",
      "Epoch 1055/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0481 - acc: 0.6461 - val_loss: 0.0655 - val_acc: 0.5337\n",
      "Epoch 1056/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0479 - acc: 0.6494 - val_loss: 0.0671 - val_acc: 0.5303\n",
      "Epoch 1057/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0486 - acc: 0.6477 - val_loss: 0.0643 - val_acc: 0.5478\n",
      "Epoch 1058/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0484 - acc: 0.6436 - val_loss: 0.0639 - val_acc: 0.5511\n",
      "Epoch 1059/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0472 - acc: 0.6533 - val_loss: 0.0645 - val_acc: 0.5461\n",
      "Epoch 1060/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0476 - acc: 0.6519 - val_loss: 0.0645 - val_acc: 0.5511\n",
      "Epoch 1061/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0475 - acc: 0.6555 - val_loss: 0.0642 - val_acc: 0.5312\n",
      "Epoch 1062/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0476 - acc: 0.6524 - val_loss: 0.0647 - val_acc: 0.5420\n",
      "Epoch 1063/3000\n",
      "3608/3608 [==============================] - 1s 159us/step - loss: 0.0478 - acc: 0.6450 - val_loss: 0.0648 - val_acc: 0.5395\n",
      "Epoch 1064/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0477 - acc: 0.6477 - val_loss: 0.0666 - val_acc: 0.5395\n",
      "Epoch 1065/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0481 - acc: 0.6505 - val_loss: 0.0642 - val_acc: 0.5536\n",
      "Epoch 1066/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0475 - acc: 0.6555 - val_loss: 0.0646 - val_acc: 0.5420\n",
      "Epoch 1067/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0481 - acc: 0.6508 - val_loss: 0.0643 - val_acc: 0.5428\n",
      "Epoch 1068/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0476 - acc: 0.6535 - val_loss: 0.0641 - val_acc: 0.5387\n",
      "Epoch 1069/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0475 - acc: 0.6530 - val_loss: 0.0645 - val_acc: 0.5395\n",
      "Epoch 1070/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0479 - acc: 0.6461 - val_loss: 0.0651 - val_acc: 0.5295\n",
      "Epoch 1071/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0470 - acc: 0.6497 - val_loss: 0.0649 - val_acc: 0.5453\n",
      "Epoch 1072/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0477 - acc: 0.6497 - val_loss: 0.0657 - val_acc: 0.5337\n",
      "Epoch 1073/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0482 - acc: 0.6463 - val_loss: 0.0642 - val_acc: 0.5428\n",
      "Epoch 1074/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0474 - acc: 0.6527 - val_loss: 0.0650 - val_acc: 0.5436\n",
      "Epoch 1075/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0478 - acc: 0.6527 - val_loss: 0.0655 - val_acc: 0.5453\n",
      "Epoch 1076/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0477 - acc: 0.6513 - val_loss: 0.0637 - val_acc: 0.5436\n",
      "Epoch 1077/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0483 - acc: 0.6455 - val_loss: 0.0648 - val_acc: 0.5420\n",
      "Epoch 1078/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0487 - acc: 0.6425 - val_loss: 0.0648 - val_acc: 0.5378\n",
      "Epoch 1079/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0479 - acc: 0.6505 - val_loss: 0.0634 - val_acc: 0.5486\n",
      "Epoch 1080/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0475 - acc: 0.6538 - val_loss: 0.0648 - val_acc: 0.5436\n",
      "Epoch 1081/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0474 - acc: 0.6541 - val_loss: 0.0647 - val_acc: 0.5511\n",
      "Epoch 1082/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0482 - acc: 0.6455 - val_loss: 0.0636 - val_acc: 0.5520\n",
      "Epoch 1083/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0476 - acc: 0.6541 - val_loss: 0.0646 - val_acc: 0.5395\n",
      "Epoch 1084/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0485 - acc: 0.6477 - val_loss: 0.0653 - val_acc: 0.5320\n",
      "Epoch 1085/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0480 - acc: 0.6511 - val_loss: 0.0643 - val_acc: 0.5436\n",
      "Epoch 1086/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0486 - acc: 0.6447 - val_loss: 0.0648 - val_acc: 0.5478\n",
      "Epoch 1087/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0479 - acc: 0.6480 - val_loss: 0.0643 - val_acc: 0.5445\n",
      "Epoch 1088/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0482 - acc: 0.6475 - val_loss: 0.0645 - val_acc: 0.5520\n",
      "Epoch 1089/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0477 - acc: 0.6508 - val_loss: 0.0648 - val_acc: 0.5495\n",
      "Epoch 1090/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0477 - acc: 0.6555 - val_loss: 0.0640 - val_acc: 0.5420\n",
      "Epoch 1091/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0473 - acc: 0.6538 - val_loss: 0.0643 - val_acc: 0.5445\n",
      "Epoch 1092/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6544 - val_loss: 0.0650 - val_acc: 0.5353\n",
      "Epoch 1093/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0478 - acc: 0.6519 - val_loss: 0.0653 - val_acc: 0.5486\n",
      "Epoch 1094/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0476 - acc: 0.6547 - val_loss: 0.0646 - val_acc: 0.5511\n",
      "Epoch 1095/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6569 - val_loss: 0.0642 - val_acc: 0.5453\n",
      "Epoch 1096/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0474 - acc: 0.6574 - val_loss: 0.0646 - val_acc: 0.5470\n",
      "Epoch 1097/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0473 - acc: 0.6549 - val_loss: 0.0649 - val_acc: 0.5395\n",
      "Epoch 1098/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0487 - acc: 0.6466 - val_loss: 0.0641 - val_acc: 0.5461\n",
      "Epoch 1099/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0476 - acc: 0.6508 - val_loss: 0.0644 - val_acc: 0.5428\n",
      "Epoch 1100/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6477 - val_loss: 0.0647 - val_acc: 0.5411\n",
      "Epoch 1101/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0478 - acc: 0.6516 - val_loss: 0.0642 - val_acc: 0.5453\n",
      "Epoch 1102/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6558 - val_loss: 0.0649 - val_acc: 0.5403\n",
      "Epoch 1103/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0472 - acc: 0.6547 - val_loss: 0.0636 - val_acc: 0.5486\n",
      "Epoch 1104/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6527 - val_loss: 0.0640 - val_acc: 0.5553\n",
      "Epoch 1105/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0475 - acc: 0.6511 - val_loss: 0.0653 - val_acc: 0.5445\n",
      "Epoch 1106/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6499 - val_loss: 0.0652 - val_acc: 0.5528\n",
      "Epoch 1107/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0472 - acc: 0.6530 - val_loss: 0.0662 - val_acc: 0.5337\n",
      "Epoch 1108/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0471 - acc: 0.6572 - val_loss: 0.0645 - val_acc: 0.5461\n",
      "Epoch 1109/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0474 - acc: 0.6566 - val_loss: 0.0647 - val_acc: 0.5403\n",
      "Epoch 1110/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0474 - acc: 0.6555 - val_loss: 0.0653 - val_acc: 0.5420\n",
      "Epoch 1111/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0468 - acc: 0.6566 - val_loss: 0.0650 - val_acc: 0.5370\n",
      "Epoch 1112/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6513 - val_loss: 0.0643 - val_acc: 0.5461\n",
      "Epoch 1113/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0479 - acc: 0.6560 - val_loss: 0.0644 - val_acc: 0.5461\n",
      "Epoch 1114/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0481 - acc: 0.6516 - val_loss: 0.0650 - val_acc: 0.5445\n",
      "Epoch 1115/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0476 - acc: 0.6558 - val_loss: 0.0652 - val_acc: 0.5403\n",
      "Epoch 1116/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0470 - acc: 0.6583 - val_loss: 0.0654 - val_acc: 0.5395\n",
      "Epoch 1117/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0474 - acc: 0.6530 - val_loss: 0.0650 - val_acc: 0.5295\n",
      "Epoch 1118/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0472 - acc: 0.6552 - val_loss: 0.0644 - val_acc: 0.5453\n",
      "Epoch 1119/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6563 - val_loss: 0.0646 - val_acc: 0.5486\n",
      "Epoch 1120/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0472 - acc: 0.6549 - val_loss: 0.0649 - val_acc: 0.5495\n",
      "Epoch 1121/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0474 - acc: 0.6547 - val_loss: 0.0652 - val_acc: 0.5453\n",
      "Epoch 1122/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0483 - acc: 0.6472 - val_loss: 0.0643 - val_acc: 0.5453\n",
      "Epoch 1123/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0469 - acc: 0.6583 - val_loss: 0.0642 - val_acc: 0.5544\n",
      "Epoch 1124/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0476 - acc: 0.6572 - val_loss: 0.0648 - val_acc: 0.5411\n",
      "Epoch 1125/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0477 - acc: 0.6513 - val_loss: 0.0642 - val_acc: 0.5503\n",
      "Epoch 1126/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0473 - acc: 0.6533 - val_loss: 0.0644 - val_acc: 0.5445\n",
      "Epoch 1127/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6549 - val_loss: 0.0647 - val_acc: 0.5503\n",
      "Epoch 1128/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0483 - acc: 0.6483 - val_loss: 0.0644 - val_acc: 0.5461\n",
      "Epoch 1129/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0476 - acc: 0.6511 - val_loss: 0.0644 - val_acc: 0.5370\n",
      "Epoch 1130/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6599 - val_loss: 0.0638 - val_acc: 0.5511\n",
      "Epoch 1131/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0483 - acc: 0.6522 - val_loss: 0.0641 - val_acc: 0.5420\n",
      "Epoch 1132/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0480 - acc: 0.6494 - val_loss: 0.0655 - val_acc: 0.5403\n",
      "Epoch 1133/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0497 - acc: 0.6358 - val_loss: 0.0645 - val_acc: 0.5503\n",
      "Epoch 1134/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0485 - acc: 0.6486 - val_loss: 0.0643 - val_acc: 0.5520\n",
      "Epoch 1135/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0480 - acc: 0.6544 - val_loss: 0.0647 - val_acc: 0.5436\n",
      "Epoch 1136/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0474 - acc: 0.6522 - val_loss: 0.0651 - val_acc: 0.5428\n",
      "Epoch 1137/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0475 - acc: 0.6535 - val_loss: 0.0647 - val_acc: 0.5436\n",
      "Epoch 1138/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0479 - acc: 0.6505 - val_loss: 0.0648 - val_acc: 0.5486\n",
      "Epoch 1139/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0476 - acc: 0.6502 - val_loss: 0.0644 - val_acc: 0.5420\n",
      "Epoch 1140/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0477 - acc: 0.6558 - val_loss: 0.0660 - val_acc: 0.5295\n",
      "Epoch 1141/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0476 - acc: 0.6530 - val_loss: 0.0647 - val_acc: 0.5461\n",
      "Epoch 1142/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0473 - acc: 0.6547 - val_loss: 0.0637 - val_acc: 0.5436\n",
      "Epoch 1143/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0477 - acc: 0.6497 - val_loss: 0.0635 - val_acc: 0.5478\n",
      "Epoch 1144/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0479 - acc: 0.6516 - val_loss: 0.0643 - val_acc: 0.5470\n",
      "Epoch 1145/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0475 - acc: 0.6535 - val_loss: 0.0650 - val_acc: 0.5486\n",
      "Epoch 1146/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0485 - acc: 0.6463 - val_loss: 0.0649 - val_acc: 0.5420\n",
      "Epoch 1147/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0485 - acc: 0.6502 - val_loss: 0.0641 - val_acc: 0.5453\n",
      "Epoch 1148/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0480 - acc: 0.6491 - val_loss: 0.0640 - val_acc: 0.5453\n",
      "Epoch 1149/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0481 - acc: 0.6480 - val_loss: 0.0659 - val_acc: 0.5362\n",
      "Epoch 1150/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0482 - acc: 0.6480 - val_loss: 0.0638 - val_acc: 0.5470\n",
      "Epoch 1151/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0473 - acc: 0.6552 - val_loss: 0.0652 - val_acc: 0.5445\n",
      "Epoch 1152/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6499 - val_loss: 0.0652 - val_acc: 0.5453\n",
      "Epoch 1153/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0483 - acc: 0.6469 - val_loss: 0.0651 - val_acc: 0.5320\n",
      "Epoch 1154/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0485 - acc: 0.6438 - val_loss: 0.0650 - val_acc: 0.5403\n",
      "Epoch 1155/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0480 - acc: 0.6450 - val_loss: 0.0643 - val_acc: 0.5445\n",
      "Epoch 1156/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0471 - acc: 0.6563 - val_loss: 0.0650 - val_acc: 0.5486\n",
      "Epoch 1157/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0475 - acc: 0.6558 - val_loss: 0.0660 - val_acc: 0.5353\n",
      "Epoch 1158/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0477 - acc: 0.6497 - val_loss: 0.0652 - val_acc: 0.5436\n",
      "Epoch 1159/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6574 - val_loss: 0.0651 - val_acc: 0.5436\n",
      "Epoch 1160/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0489 - acc: 0.6461 - val_loss: 0.0645 - val_acc: 0.5320\n",
      "Epoch 1161/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0480 - acc: 0.6497 - val_loss: 0.0644 - val_acc: 0.5495\n",
      "Epoch 1162/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0475 - acc: 0.6530 - val_loss: 0.0643 - val_acc: 0.5395\n",
      "Epoch 1163/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0476 - acc: 0.6502 - val_loss: 0.0640 - val_acc: 0.5395\n",
      "Epoch 1164/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0475 - acc: 0.6541 - val_loss: 0.0638 - val_acc: 0.5478\n",
      "Epoch 1165/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0471 - acc: 0.6574 - val_loss: 0.0644 - val_acc: 0.5470\n",
      "Epoch 1166/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0470 - acc: 0.6588 - val_loss: 0.0641 - val_acc: 0.5461\n",
      "Epoch 1167/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0474 - acc: 0.6524 - val_loss: 0.0649 - val_acc: 0.5453\n",
      "Epoch 1168/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0477 - acc: 0.6516 - val_loss: 0.0642 - val_acc: 0.5411\n",
      "Epoch 1169/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0474 - acc: 0.6547 - val_loss: 0.0652 - val_acc: 0.5461\n",
      "Epoch 1170/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0475 - acc: 0.6558 - val_loss: 0.0649 - val_acc: 0.5495\n",
      "Epoch 1171/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0480 - acc: 0.6516 - val_loss: 0.0651 - val_acc: 0.5511\n",
      "Epoch 1172/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0479 - acc: 0.6522 - val_loss: 0.0649 - val_acc: 0.5495\n",
      "Epoch 1173/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0481 - acc: 0.6405 - val_loss: 0.0647 - val_acc: 0.5486\n",
      "Epoch 1174/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0471 - acc: 0.6572 - val_loss: 0.0651 - val_acc: 0.5453\n",
      "Epoch 1175/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0475 - acc: 0.6563 - val_loss: 0.0659 - val_acc: 0.5370\n",
      "Epoch 1176/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0479 - acc: 0.6527 - val_loss: 0.0650 - val_acc: 0.5445\n",
      "Epoch 1177/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0472 - acc: 0.6569 - val_loss: 0.0648 - val_acc: 0.5470\n",
      "Epoch 1178/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0481 - acc: 0.6477 - val_loss: 0.0656 - val_acc: 0.5445\n",
      "Epoch 1179/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0477 - acc: 0.6541 - val_loss: 0.0661 - val_acc: 0.5436\n",
      "Epoch 1180/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0477 - acc: 0.6522 - val_loss: 0.0644 - val_acc: 0.5561\n",
      "Epoch 1181/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0474 - acc: 0.6513 - val_loss: 0.0667 - val_acc: 0.5328\n",
      "Epoch 1182/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6547 - val_loss: 0.0654 - val_acc: 0.5378\n",
      "Epoch 1183/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0482 - acc: 0.6480 - val_loss: 0.0654 - val_acc: 0.5411\n",
      "Epoch 1184/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0480 - acc: 0.6513 - val_loss: 0.0649 - val_acc: 0.5420\n",
      "Epoch 1185/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0478 - acc: 0.6488 - val_loss: 0.0656 - val_acc: 0.5445\n",
      "Epoch 1186/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0481 - acc: 0.6505 - val_loss: 0.0646 - val_acc: 0.5370\n",
      "Epoch 1187/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0473 - acc: 0.6516 - val_loss: 0.0647 - val_acc: 0.5345\n",
      "Epoch 1188/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0473 - acc: 0.6533 - val_loss: 0.0652 - val_acc: 0.5370\n",
      "Epoch 1189/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0472 - acc: 0.6566 - val_loss: 0.0643 - val_acc: 0.5478\n",
      "Epoch 1190/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0472 - acc: 0.6566 - val_loss: 0.0647 - val_acc: 0.5370\n",
      "Epoch 1191/3000\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0471 - acc: 0.6555 - val_loss: 0.0649 - val_acc: 0.5403\n",
      "Epoch 1192/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0470 - acc: 0.6566 - val_loss: 0.0638 - val_acc: 0.5470\n",
      "Epoch 1193/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0478 - acc: 0.6488 - val_loss: 0.0644 - val_acc: 0.5411\n",
      "Epoch 1194/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0471 - acc: 0.6555 - val_loss: 0.0652 - val_acc: 0.5478\n",
      "Epoch 1195/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0477 - acc: 0.6524 - val_loss: 0.0641 - val_acc: 0.5544\n",
      "Epoch 1196/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0480 - acc: 0.6499 - val_loss: 0.0650 - val_acc: 0.5436\n",
      "Epoch 1197/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0475 - acc: 0.6494 - val_loss: 0.0655 - val_acc: 0.5303\n",
      "Epoch 1198/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0476 - acc: 0.6552 - val_loss: 0.0649 - val_acc: 0.5428\n",
      "Epoch 1199/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0469 - acc: 0.6560 - val_loss: 0.0652 - val_acc: 0.5511\n",
      "Epoch 1200/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0481 - acc: 0.6430 - val_loss: 0.0655 - val_acc: 0.5387\n",
      "Epoch 1201/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0480 - acc: 0.6530 - val_loss: 0.0653 - val_acc: 0.5378\n",
      "Epoch 1202/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0477 - acc: 0.6505 - val_loss: 0.0669 - val_acc: 0.5345\n",
      "Epoch 1203/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6505 - val_loss: 0.0649 - val_acc: 0.5403\n",
      "Epoch 1204/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0472 - acc: 0.6569 - val_loss: 0.0638 - val_acc: 0.5503\n",
      "Epoch 1205/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0473 - acc: 0.6552 - val_loss: 0.0653 - val_acc: 0.5395\n",
      "Epoch 1206/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0469 - acc: 0.6577 - val_loss: 0.0649 - val_acc: 0.5453\n",
      "Epoch 1207/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0483 - acc: 0.6463 - val_loss: 0.0665 - val_acc: 0.5170\n",
      "Epoch 1208/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0498 - acc: 0.6322 - val_loss: 0.0664 - val_acc: 0.5378\n",
      "Epoch 1209/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0483 - acc: 0.6494 - val_loss: 0.0648 - val_acc: 0.5470\n",
      "Epoch 1210/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0479 - acc: 0.6552 - val_loss: 0.0649 - val_acc: 0.5387\n",
      "Epoch 1211/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0476 - acc: 0.6463 - val_loss: 0.0653 - val_acc: 0.5420\n",
      "Epoch 1212/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0475 - acc: 0.6541 - val_loss: 0.0642 - val_acc: 0.5453\n",
      "Epoch 1213/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0474 - acc: 0.6535 - val_loss: 0.0651 - val_acc: 0.5387\n",
      "Epoch 1214/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0473 - acc: 0.6522 - val_loss: 0.0645 - val_acc: 0.5320\n",
      "Epoch 1215/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0481 - acc: 0.6477 - val_loss: 0.0643 - val_acc: 0.5520\n",
      "Epoch 1216/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0482 - acc: 0.6466 - val_loss: 0.0652 - val_acc: 0.5503\n",
      "Epoch 1217/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0483 - acc: 0.6447 - val_loss: 0.0642 - val_acc: 0.5520\n",
      "Epoch 1218/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0474 - acc: 0.6513 - val_loss: 0.0640 - val_acc: 0.5511\n",
      "Epoch 1219/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6533 - val_loss: 0.0641 - val_acc: 0.5428\n",
      "Epoch 1220/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0468 - acc: 0.6569 - val_loss: 0.0651 - val_acc: 0.5445\n",
      "Epoch 1221/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6549 - val_loss: 0.0645 - val_acc: 0.5420\n",
      "Epoch 1222/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0476 - acc: 0.6466 - val_loss: 0.0650 - val_acc: 0.5428\n",
      "Epoch 1223/3000\n",
      "3608/3608 [==============================] - 1s 185us/step - loss: 0.0467 - acc: 0.6569 - val_loss: 0.0647 - val_acc: 0.5486\n",
      "Epoch 1224/3000\n",
      "3608/3608 [==============================] - 1s 220us/step - loss: 0.0471 - acc: 0.6524 - val_loss: 0.0642 - val_acc: 0.5395\n",
      "Epoch 1225/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0471 - acc: 0.6577 - val_loss: 0.0654 - val_acc: 0.5453\n",
      "Epoch 1226/3000\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0478 - acc: 0.6541 - val_loss: 0.0647 - val_acc: 0.5453\n",
      "Epoch 1227/3000\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0474 - acc: 0.6580 - val_loss: 0.0641 - val_acc: 0.5578\n",
      "Epoch 1228/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0470 - acc: 0.6594 - val_loss: 0.0643 - val_acc: 0.5428\n",
      "Epoch 1229/3000\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0477 - acc: 0.6483 - val_loss: 0.0650 - val_acc: 0.5445\n",
      "Epoch 1230/3000\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0473 - acc: 0.6508 - val_loss: 0.0648 - val_acc: 0.5478\n",
      "Epoch 1231/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0472 - acc: 0.6522 - val_loss: 0.0645 - val_acc: 0.5461\n",
      "Epoch 1232/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6552 - val_loss: 0.0646 - val_acc: 0.5436\n",
      "Epoch 1233/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0478 - acc: 0.6544 - val_loss: 0.0645 - val_acc: 0.5470\n",
      "Epoch 1234/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0489 - acc: 0.6475 - val_loss: 0.0692 - val_acc: 0.5054\n",
      "Epoch 1235/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0492 - acc: 0.6425 - val_loss: 0.0649 - val_acc: 0.5403\n",
      "Epoch 1236/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0473 - acc: 0.6591 - val_loss: 0.0641 - val_acc: 0.5536\n",
      "Epoch 1237/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0479 - acc: 0.6541 - val_loss: 0.0653 - val_acc: 0.5403\n",
      "Epoch 1238/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6558 - val_loss: 0.0649 - val_acc: 0.5470\n",
      "Epoch 1239/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0468 - acc: 0.6588 - val_loss: 0.0645 - val_acc: 0.5486\n",
      "Epoch 1240/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0476 - acc: 0.6544 - val_loss: 0.0654 - val_acc: 0.5403\n",
      "Epoch 1241/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0479 - acc: 0.6522 - val_loss: 0.0656 - val_acc: 0.5428\n",
      "Epoch 1242/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0478 - acc: 0.6522 - val_loss: 0.0649 - val_acc: 0.5544\n",
      "Epoch 1243/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0474 - acc: 0.6547 - val_loss: 0.0642 - val_acc: 0.5486\n",
      "Epoch 1244/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0475 - acc: 0.6544 - val_loss: 0.0648 - val_acc: 0.5387\n",
      "Epoch 1245/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6547 - val_loss: 0.0646 - val_acc: 0.5420\n",
      "Epoch 1246/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0477 - acc: 0.6488 - val_loss: 0.0648 - val_acc: 0.5461\n",
      "Epoch 1247/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6522 - val_loss: 0.0661 - val_acc: 0.5328\n",
      "Epoch 1248/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6560 - val_loss: 0.0650 - val_acc: 0.5478\n",
      "Epoch 1249/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6608 - val_loss: 0.0651 - val_acc: 0.5478\n",
      "Epoch 1250/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6566 - val_loss: 0.0647 - val_acc: 0.5461\n",
      "Epoch 1251/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0476 - acc: 0.6516 - val_loss: 0.0656 - val_acc: 0.5403\n",
      "Epoch 1252/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0474 - acc: 0.6524 - val_loss: 0.0661 - val_acc: 0.5470\n",
      "Epoch 1253/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0475 - acc: 0.6555 - val_loss: 0.0653 - val_acc: 0.5353\n",
      "Epoch 1254/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6483 - val_loss: 0.0648 - val_acc: 0.5420\n",
      "Epoch 1255/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0471 - acc: 0.6563 - val_loss: 0.0650 - val_acc: 0.5387\n",
      "Epoch 1256/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0477 - acc: 0.6555 - val_loss: 0.0651 - val_acc: 0.5403\n",
      "Epoch 1257/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0474 - acc: 0.6541 - val_loss: 0.0643 - val_acc: 0.5395\n",
      "Epoch 1258/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0472 - acc: 0.6569 - val_loss: 0.0644 - val_acc: 0.5478\n",
      "Epoch 1259/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0470 - acc: 0.6533 - val_loss: 0.0648 - val_acc: 0.5461\n",
      "Epoch 1260/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6632 - val_loss: 0.0650 - val_acc: 0.5411\n",
      "Epoch 1261/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0470 - acc: 0.6541 - val_loss: 0.0664 - val_acc: 0.5370\n",
      "Epoch 1262/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6519 - val_loss: 0.0661 - val_acc: 0.5328\n",
      "Epoch 1263/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6588 - val_loss: 0.0649 - val_acc: 0.5511\n",
      "Epoch 1264/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0470 - acc: 0.6605 - val_loss: 0.0644 - val_acc: 0.5470\n",
      "Epoch 1265/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0475 - acc: 0.6560 - val_loss: 0.0644 - val_acc: 0.5453\n",
      "Epoch 1266/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0465 - acc: 0.6588 - val_loss: 0.0646 - val_acc: 0.5411\n",
      "Epoch 1267/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0478 - acc: 0.6533 - val_loss: 0.0644 - val_acc: 0.5411\n",
      "Epoch 1268/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6599 - val_loss: 0.0652 - val_acc: 0.5337\n",
      "Epoch 1269/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0476 - acc: 0.6524 - val_loss: 0.0651 - val_acc: 0.5337\n",
      "Epoch 1270/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0473 - acc: 0.6547 - val_loss: 0.0651 - val_acc: 0.5486\n",
      "Epoch 1271/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0466 - acc: 0.6566 - val_loss: 0.0647 - val_acc: 0.5486\n",
      "Epoch 1272/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6572 - val_loss: 0.0643 - val_acc: 0.5520\n",
      "Epoch 1273/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6524 - val_loss: 0.0655 - val_acc: 0.5254\n",
      "Epoch 1274/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0483 - acc: 0.6455 - val_loss: 0.0645 - val_acc: 0.5536\n",
      "Epoch 1275/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0479 - acc: 0.6502 - val_loss: 0.0650 - val_acc: 0.5445\n",
      "Epoch 1276/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0481 - acc: 0.6480 - val_loss: 0.0647 - val_acc: 0.5486\n",
      "Epoch 1277/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6508 - val_loss: 0.0671 - val_acc: 0.5387\n",
      "Epoch 1278/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0489 - acc: 0.6386 - val_loss: 0.0643 - val_acc: 0.5486\n",
      "Epoch 1279/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6535 - val_loss: 0.0637 - val_acc: 0.5636\n",
      "Epoch 1280/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0477 - acc: 0.6538 - val_loss: 0.0654 - val_acc: 0.5387\n",
      "Epoch 1281/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0479 - acc: 0.6491 - val_loss: 0.0641 - val_acc: 0.5495\n",
      "Epoch 1282/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0477 - acc: 0.6466 - val_loss: 0.0653 - val_acc: 0.5420\n",
      "Epoch 1283/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6538 - val_loss: 0.0650 - val_acc: 0.5445\n",
      "Epoch 1284/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0471 - acc: 0.6513 - val_loss: 0.0649 - val_acc: 0.5528\n",
      "Epoch 1285/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6552 - val_loss: 0.0654 - val_acc: 0.5445\n",
      "Epoch 1286/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6494 - val_loss: 0.0655 - val_acc: 0.5395\n",
      "Epoch 1287/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0474 - acc: 0.6552 - val_loss: 0.0655 - val_acc: 0.5503\n",
      "Epoch 1288/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0476 - acc: 0.6505 - val_loss: 0.0652 - val_acc: 0.5403\n",
      "Epoch 1289/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6505 - val_loss: 0.0652 - val_acc: 0.5362\n",
      "Epoch 1290/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6511 - val_loss: 0.0653 - val_acc: 0.5370\n",
      "Epoch 1291/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0472 - acc: 0.6552 - val_loss: 0.0648 - val_acc: 0.5478\n",
      "Epoch 1292/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0474 - acc: 0.6569 - val_loss: 0.0650 - val_acc: 0.5411\n",
      "Epoch 1293/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0481 - acc: 0.6441 - val_loss: 0.0640 - val_acc: 0.5453\n",
      "Epoch 1294/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6511 - val_loss: 0.0638 - val_acc: 0.5503\n",
      "Epoch 1295/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0478 - acc: 0.6508 - val_loss: 0.0641 - val_acc: 0.5445\n",
      "Epoch 1296/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6530 - val_loss: 0.0652 - val_acc: 0.5520\n",
      "Epoch 1297/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0473 - acc: 0.6602 - val_loss: 0.0650 - val_acc: 0.5436\n",
      "Epoch 1298/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6591 - val_loss: 0.0648 - val_acc: 0.5362\n",
      "Epoch 1299/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0474 - acc: 0.6585 - val_loss: 0.0646 - val_acc: 0.5536\n",
      "Epoch 1300/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0470 - acc: 0.6591 - val_loss: 0.0652 - val_acc: 0.5486\n",
      "Epoch 1301/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6572 - val_loss: 0.0648 - val_acc: 0.5403\n",
      "Epoch 1302/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0470 - acc: 0.6599 - val_loss: 0.0665 - val_acc: 0.5370\n",
      "Epoch 1303/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0500 - acc: 0.6311 - val_loss: 0.0639 - val_acc: 0.5445\n",
      "Epoch 1304/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0489 - acc: 0.6400 - val_loss: 0.0640 - val_acc: 0.5528\n",
      "Epoch 1305/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0482 - acc: 0.6483 - val_loss: 0.0656 - val_acc: 0.5411\n",
      "Epoch 1306/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0479 - acc: 0.6466 - val_loss: 0.0660 - val_acc: 0.5378\n",
      "Epoch 1307/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0479 - acc: 0.6499 - val_loss: 0.0646 - val_acc: 0.5503\n",
      "Epoch 1308/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6547 - val_loss: 0.0644 - val_acc: 0.5520\n",
      "Epoch 1309/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0472 - acc: 0.6563 - val_loss: 0.0652 - val_acc: 0.5428\n",
      "Epoch 1310/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0477 - acc: 0.6538 - val_loss: 0.0658 - val_acc: 0.5403\n",
      "Epoch 1311/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0477 - acc: 0.6505 - val_loss: 0.0653 - val_acc: 0.5470\n",
      "Epoch 1312/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0483 - acc: 0.6516 - val_loss: 0.0656 - val_acc: 0.5387\n",
      "Epoch 1313/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0483 - acc: 0.6414 - val_loss: 0.0642 - val_acc: 0.5553\n",
      "Epoch 1314/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0481 - acc: 0.6491 - val_loss: 0.0640 - val_acc: 0.5536\n",
      "Epoch 1315/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0482 - acc: 0.6458 - val_loss: 0.0642 - val_acc: 0.5428\n",
      "Epoch 1316/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0470 - acc: 0.6585 - val_loss: 0.0642 - val_acc: 0.5503\n",
      "Epoch 1317/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6569 - val_loss: 0.0645 - val_acc: 0.5478\n",
      "Epoch 1318/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0472 - acc: 0.6527 - val_loss: 0.0664 - val_acc: 0.5411\n",
      "Epoch 1319/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6533 - val_loss: 0.0647 - val_acc: 0.5387\n",
      "Epoch 1320/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0485 - acc: 0.6491 - val_loss: 0.0655 - val_acc: 0.5337\n",
      "Epoch 1321/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6488 - val_loss: 0.0639 - val_acc: 0.5478\n",
      "Epoch 1322/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0472 - acc: 0.6549 - val_loss: 0.0660 - val_acc: 0.5395\n",
      "Epoch 1323/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6599 - val_loss: 0.0649 - val_acc: 0.5411\n",
      "Epoch 1324/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0478 - acc: 0.6585 - val_loss: 0.0655 - val_acc: 0.5420\n",
      "Epoch 1325/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0469 - acc: 0.6630 - val_loss: 0.0666 - val_acc: 0.5345\n",
      "Epoch 1326/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0475 - acc: 0.6486 - val_loss: 0.0651 - val_acc: 0.5353\n",
      "Epoch 1327/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0480 - acc: 0.6494 - val_loss: 0.0651 - val_acc: 0.5461\n",
      "Epoch 1328/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0485 - acc: 0.6477 - val_loss: 0.0653 - val_acc: 0.5495\n",
      "Epoch 1329/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6544 - val_loss: 0.0657 - val_acc: 0.5378\n",
      "Epoch 1330/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0477 - acc: 0.6508 - val_loss: 0.0657 - val_acc: 0.5395\n",
      "Epoch 1331/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0479 - acc: 0.6491 - val_loss: 0.0659 - val_acc: 0.5245\n",
      "Epoch 1332/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0481 - acc: 0.6497 - val_loss: 0.0648 - val_acc: 0.5411\n",
      "Epoch 1333/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6574 - val_loss: 0.0657 - val_acc: 0.5411\n",
      "Epoch 1334/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0472 - acc: 0.6580 - val_loss: 0.0649 - val_acc: 0.5445\n",
      "Epoch 1335/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0471 - acc: 0.6511 - val_loss: 0.0653 - val_acc: 0.5461\n",
      "Epoch 1336/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0475 - acc: 0.6572 - val_loss: 0.0659 - val_acc: 0.5503\n",
      "Epoch 1337/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0472 - acc: 0.6544 - val_loss: 0.0647 - val_acc: 0.5411\n",
      "Epoch 1338/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0473 - acc: 0.6558 - val_loss: 0.0652 - val_acc: 0.5378\n",
      "Epoch 1339/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0472 - acc: 0.6569 - val_loss: 0.0645 - val_acc: 0.5353\n",
      "Epoch 1340/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0470 - acc: 0.6596 - val_loss: 0.0651 - val_acc: 0.5362\n",
      "Epoch 1341/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0473 - acc: 0.6566 - val_loss: 0.0648 - val_acc: 0.5445\n",
      "Epoch 1342/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0480 - acc: 0.6502 - val_loss: 0.0659 - val_acc: 0.5436\n",
      "Epoch 1343/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0484 - acc: 0.6430 - val_loss: 0.0648 - val_acc: 0.5470\n",
      "Epoch 1344/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0470 - acc: 0.6533 - val_loss: 0.0653 - val_acc: 0.5544\n",
      "Epoch 1345/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0476 - acc: 0.6513 - val_loss: 0.0656 - val_acc: 0.5445\n",
      "Epoch 1346/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0474 - acc: 0.6566 - val_loss: 0.0651 - val_acc: 0.5370\n",
      "Epoch 1347/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6605 - val_loss: 0.0645 - val_acc: 0.5453\n",
      "Epoch 1348/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0469 - acc: 0.6621 - val_loss: 0.0651 - val_acc: 0.5478\n",
      "Epoch 1349/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0498 - acc: 0.6394 - val_loss: 0.0653 - val_acc: 0.5486\n",
      "Epoch 1350/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0481 - acc: 0.6475 - val_loss: 0.0648 - val_acc: 0.5403\n",
      "Epoch 1351/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0471 - acc: 0.6577 - val_loss: 0.0651 - val_acc: 0.5470\n",
      "Epoch 1352/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0481 - acc: 0.6397 - val_loss: 0.0642 - val_acc: 0.5511\n",
      "Epoch 1353/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0477 - acc: 0.6499 - val_loss: 0.0647 - val_acc: 0.5536\n",
      "Epoch 1354/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0474 - acc: 0.6588 - val_loss: 0.0646 - val_acc: 0.5420\n",
      "Epoch 1355/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6563 - val_loss: 0.0648 - val_acc: 0.5411\n",
      "Epoch 1356/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0471 - acc: 0.6572 - val_loss: 0.0644 - val_acc: 0.5495\n",
      "Epoch 1357/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6585 - val_loss: 0.0647 - val_acc: 0.5453\n",
      "Epoch 1358/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0478 - acc: 0.6560 - val_loss: 0.0657 - val_acc: 0.5403\n",
      "Epoch 1359/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0478 - acc: 0.6469 - val_loss: 0.0665 - val_acc: 0.5245\n",
      "Epoch 1360/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0477 - acc: 0.6463 - val_loss: 0.0649 - val_acc: 0.5420\n",
      "Epoch 1361/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0469 - acc: 0.6583 - val_loss: 0.0656 - val_acc: 0.5403\n",
      "Epoch 1362/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6555 - val_loss: 0.0648 - val_acc: 0.5411\n",
      "Epoch 1363/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6563 - val_loss: 0.0664 - val_acc: 0.5345\n",
      "Epoch 1364/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.6596 - val_loss: 0.0649 - val_acc: 0.5445\n",
      "Epoch 1365/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0466 - acc: 0.6619 - val_loss: 0.0650 - val_acc: 0.5428\n",
      "Epoch 1366/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6563 - val_loss: 0.0662 - val_acc: 0.5395\n",
      "Epoch 1367/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6544 - val_loss: 0.0654 - val_acc: 0.5328\n",
      "Epoch 1368/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0470 - acc: 0.6522 - val_loss: 0.0665 - val_acc: 0.5337\n",
      "Epoch 1369/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6560 - val_loss: 0.0659 - val_acc: 0.5320\n",
      "Epoch 1370/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0475 - acc: 0.6572 - val_loss: 0.0659 - val_acc: 0.5395\n",
      "Epoch 1371/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0476 - acc: 0.6524 - val_loss: 0.0660 - val_acc: 0.5378\n",
      "Epoch 1372/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0471 - acc: 0.6560 - val_loss: 0.0649 - val_acc: 0.5503\n",
      "Epoch 1373/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6535 - val_loss: 0.0656 - val_acc: 0.5370\n",
      "Epoch 1374/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6599 - val_loss: 0.0658 - val_acc: 0.5453\n",
      "Epoch 1375/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0469 - acc: 0.6588 - val_loss: 0.0656 - val_acc: 0.5428\n",
      "Epoch 1376/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6602 - val_loss: 0.0657 - val_acc: 0.5428\n",
      "Epoch 1377/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0475 - acc: 0.6547 - val_loss: 0.0667 - val_acc: 0.5353\n",
      "Epoch 1378/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0478 - acc: 0.6541 - val_loss: 0.0649 - val_acc: 0.5470\n",
      "Epoch 1379/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0474 - acc: 0.6516 - val_loss: 0.0664 - val_acc: 0.5362\n",
      "Epoch 1380/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0476 - acc: 0.6502 - val_loss: 0.0657 - val_acc: 0.5553\n",
      "Epoch 1381/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0478 - acc: 0.6505 - val_loss: 0.0682 - val_acc: 0.5187\n",
      "Epoch 1382/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0487 - acc: 0.6483 - val_loss: 0.0656 - val_acc: 0.5353\n",
      "Epoch 1383/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0477 - acc: 0.6513 - val_loss: 0.0657 - val_acc: 0.5353\n",
      "Epoch 1384/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0476 - acc: 0.6538 - val_loss: 0.0675 - val_acc: 0.5229\n",
      "Epoch 1385/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0476 - acc: 0.6530 - val_loss: 0.0643 - val_acc: 0.5503\n",
      "Epoch 1386/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0477 - acc: 0.6549 - val_loss: 0.0647 - val_acc: 0.5495\n",
      "Epoch 1387/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0469 - acc: 0.6577 - val_loss: 0.0646 - val_acc: 0.5478\n",
      "Epoch 1388/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0469 - acc: 0.6594 - val_loss: 0.0651 - val_acc: 0.5353\n",
      "Epoch 1389/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0479 - acc: 0.6502 - val_loss: 0.0650 - val_acc: 0.5428\n",
      "Epoch 1390/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0479 - acc: 0.6494 - val_loss: 0.0658 - val_acc: 0.5337\n",
      "Epoch 1391/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0469 - acc: 0.6591 - val_loss: 0.0655 - val_acc: 0.5411\n",
      "Epoch 1392/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6549 - val_loss: 0.0656 - val_acc: 0.5461\n",
      "Epoch 1393/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6560 - val_loss: 0.0659 - val_acc: 0.5287\n",
      "Epoch 1394/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0484 - acc: 0.6472 - val_loss: 0.0653 - val_acc: 0.5370\n",
      "Epoch 1395/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0478 - acc: 0.6477 - val_loss: 0.0654 - val_acc: 0.5428\n",
      "Epoch 1396/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0469 - acc: 0.6608 - val_loss: 0.0652 - val_acc: 0.5378\n",
      "Epoch 1397/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.6577 - val_loss: 0.0652 - val_acc: 0.5362\n",
      "Epoch 1398/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0465 - acc: 0.6632 - val_loss: 0.0647 - val_acc: 0.5503\n",
      "Epoch 1399/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6560 - val_loss: 0.0653 - val_acc: 0.5411\n",
      "Epoch 1400/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6616 - val_loss: 0.0662 - val_acc: 0.5420\n",
      "Epoch 1401/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0466 - acc: 0.6627 - val_loss: 0.0650 - val_acc: 0.5395\n",
      "Epoch 1402/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6574 - val_loss: 0.0652 - val_acc: 0.5411\n",
      "Epoch 1403/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0467 - acc: 0.6596 - val_loss: 0.0651 - val_acc: 0.5453\n",
      "Epoch 1404/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0469 - acc: 0.6569 - val_loss: 0.0647 - val_acc: 0.5453\n",
      "Epoch 1405/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6652 - val_loss: 0.0652 - val_acc: 0.5461\n",
      "Epoch 1406/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0473 - acc: 0.6572 - val_loss: 0.0660 - val_acc: 0.5445\n",
      "Epoch 1407/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0505 - acc: 0.6350 - val_loss: 0.0656 - val_acc: 0.5420\n",
      "Epoch 1408/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0476 - acc: 0.6569 - val_loss: 0.0651 - val_acc: 0.5378\n",
      "Epoch 1409/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0476 - acc: 0.6533 - val_loss: 0.0668 - val_acc: 0.5212\n",
      "Epoch 1410/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0477 - acc: 0.6547 - val_loss: 0.0647 - val_acc: 0.5378\n",
      "Epoch 1411/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6566 - val_loss: 0.0653 - val_acc: 0.5578\n",
      "Epoch 1412/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0474 - acc: 0.6522 - val_loss: 0.0663 - val_acc: 0.5420\n",
      "Epoch 1413/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0473 - acc: 0.6535 - val_loss: 0.0654 - val_acc: 0.5461\n",
      "Epoch 1414/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0469 - acc: 0.6591 - val_loss: 0.0648 - val_acc: 0.5453\n",
      "Epoch 1415/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0468 - acc: 0.6613 - val_loss: 0.0660 - val_acc: 0.5445\n",
      "Epoch 1416/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6608 - val_loss: 0.0649 - val_acc: 0.5461\n",
      "Epoch 1417/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6566 - val_loss: 0.0650 - val_acc: 0.5420\n",
      "Epoch 1418/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0473 - acc: 0.6522 - val_loss: 0.0647 - val_acc: 0.5470\n",
      "Epoch 1419/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.6605 - val_loss: 0.0651 - val_acc: 0.5403\n",
      "Epoch 1420/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0465 - acc: 0.6630 - val_loss: 0.0646 - val_acc: 0.5470\n",
      "Epoch 1421/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0473 - acc: 0.6591 - val_loss: 0.0657 - val_acc: 0.5370\n",
      "Epoch 1422/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6605 - val_loss: 0.0654 - val_acc: 0.5445\n",
      "Epoch 1423/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6655 - val_loss: 0.0655 - val_acc: 0.5428\n",
      "Epoch 1424/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6591 - val_loss: 0.0647 - val_acc: 0.5395\n",
      "Epoch 1425/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0465 - acc: 0.6619 - val_loss: 0.0659 - val_acc: 0.5411\n",
      "Epoch 1426/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6530 - val_loss: 0.0660 - val_acc: 0.5362\n",
      "Epoch 1427/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0473 - acc: 0.6552 - val_loss: 0.0649 - val_acc: 0.5395\n",
      "Epoch 1428/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0477 - acc: 0.6544 - val_loss: 0.0654 - val_acc: 0.5345\n",
      "Epoch 1429/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0475 - acc: 0.6547 - val_loss: 0.0671 - val_acc: 0.5287\n",
      "Epoch 1430/3000\n",
      "3608/3608 [==============================] - 1s 192us/step - loss: 0.0474 - acc: 0.6566 - val_loss: 0.0648 - val_acc: 0.5436\n",
      "Epoch 1431/3000\n",
      "3608/3608 [==============================] - 1s 221us/step - loss: 0.0475 - acc: 0.6516 - val_loss: 0.0659 - val_acc: 0.5303\n",
      "Epoch 1432/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0480 - acc: 0.6516 - val_loss: 0.0654 - val_acc: 0.5428\n",
      "Epoch 1433/3000\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0470 - acc: 0.6594 - val_loss: 0.0659 - val_acc: 0.5387\n",
      "Epoch 1434/3000\n",
      "3608/3608 [==============================] - 1s 208us/step - loss: 0.0478 - acc: 0.6522 - val_loss: 0.0662 - val_acc: 0.5345\n",
      "Epoch 1435/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0474 - acc: 0.6513 - val_loss: 0.0658 - val_acc: 0.5411\n",
      "Epoch 1436/3000\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0474 - acc: 0.6577 - val_loss: 0.0656 - val_acc: 0.5428\n",
      "Epoch 1437/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0468 - acc: 0.6555 - val_loss: 0.0650 - val_acc: 0.5403\n",
      "Epoch 1438/3000\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0474 - acc: 0.6508 - val_loss: 0.0662 - val_acc: 0.5403\n",
      "Epoch 1439/3000\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0488 - acc: 0.6463 - val_loss: 0.0653 - val_acc: 0.5503\n",
      "Epoch 1440/3000\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0502 - acc: 0.6339 - val_loss: 0.0636 - val_acc: 0.5495\n",
      "Epoch 1441/3000\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0488 - acc: 0.6383 - val_loss: 0.0640 - val_acc: 0.5403\n",
      "Epoch 1442/3000\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0483 - acc: 0.6508 - val_loss: 0.0644 - val_acc: 0.5486\n",
      "Epoch 1443/3000\n",
      "3608/3608 [==============================] - 1s 198us/step - loss: 0.0477 - acc: 0.6511 - val_loss: 0.0645 - val_acc: 0.5362\n",
      "Epoch 1444/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0489 - acc: 0.6375 - val_loss: 0.0650 - val_acc: 0.5428\n",
      "Epoch 1445/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0488 - acc: 0.6455 - val_loss: 0.0652 - val_acc: 0.5362\n",
      "Epoch 1446/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0471 - acc: 0.6488 - val_loss: 0.0655 - val_acc: 0.5387\n",
      "Epoch 1447/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6544 - val_loss: 0.0655 - val_acc: 0.5428\n",
      "Epoch 1448/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0465 - acc: 0.6605 - val_loss: 0.0672 - val_acc: 0.5287\n",
      "Epoch 1449/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0475 - acc: 0.6549 - val_loss: 0.0659 - val_acc: 0.5362\n",
      "Epoch 1450/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0495 - acc: 0.6369 - val_loss: 0.0655 - val_acc: 0.5411\n",
      "Epoch 1451/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0477 - acc: 0.6522 - val_loss: 0.0649 - val_acc: 0.5411\n",
      "Epoch 1452/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0471 - acc: 0.6563 - val_loss: 0.0655 - val_acc: 0.5428\n",
      "Epoch 1453/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6569 - val_loss: 0.0649 - val_acc: 0.5436\n",
      "Epoch 1454/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0470 - acc: 0.6558 - val_loss: 0.0660 - val_acc: 0.5370\n",
      "Epoch 1455/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0480 - acc: 0.6491 - val_loss: 0.0650 - val_acc: 0.5411\n",
      "Epoch 1456/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0474 - acc: 0.6613 - val_loss: 0.0657 - val_acc: 0.5362\n",
      "Epoch 1457/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0471 - acc: 0.6519 - val_loss: 0.0651 - val_acc: 0.5420\n",
      "Epoch 1458/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6635 - val_loss: 0.0650 - val_acc: 0.5312\n",
      "Epoch 1459/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0470 - acc: 0.6541 - val_loss: 0.0645 - val_acc: 0.5495\n",
      "Epoch 1460/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0477 - acc: 0.6533 - val_loss: 0.0656 - val_acc: 0.5295\n",
      "Epoch 1461/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0479 - acc: 0.6535 - val_loss: 0.0665 - val_acc: 0.5353\n",
      "Epoch 1462/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0481 - acc: 0.6511 - val_loss: 0.0649 - val_acc: 0.5453\n",
      "Epoch 1463/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0475 - acc: 0.6516 - val_loss: 0.0646 - val_acc: 0.5461\n",
      "Epoch 1464/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0475 - acc: 0.6511 - val_loss: 0.0650 - val_acc: 0.5478\n",
      "Epoch 1465/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0469 - acc: 0.6594 - val_loss: 0.0652 - val_acc: 0.5420\n",
      "Epoch 1466/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6605 - val_loss: 0.0666 - val_acc: 0.5312\n",
      "Epoch 1467/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0479 - acc: 0.6527 - val_loss: 0.0674 - val_acc: 0.5278\n",
      "Epoch 1468/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0478 - acc: 0.6511 - val_loss: 0.0660 - val_acc: 0.5328\n",
      "Epoch 1469/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0474 - acc: 0.6610 - val_loss: 0.0669 - val_acc: 0.5387\n",
      "Epoch 1470/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0469 - acc: 0.6577 - val_loss: 0.0664 - val_acc: 0.5395\n",
      "Epoch 1471/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0472 - acc: 0.6508 - val_loss: 0.0658 - val_acc: 0.5378\n",
      "Epoch 1472/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0469 - acc: 0.6602 - val_loss: 0.0653 - val_acc: 0.5436\n",
      "Epoch 1473/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0467 - acc: 0.6602 - val_loss: 0.0656 - val_acc: 0.5461\n",
      "Epoch 1474/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6608 - val_loss: 0.0654 - val_acc: 0.5428\n",
      "Epoch 1475/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6602 - val_loss: 0.0679 - val_acc: 0.5370\n",
      "Epoch 1476/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0468 - acc: 0.6580 - val_loss: 0.0665 - val_acc: 0.5370\n",
      "Epoch 1477/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6585 - val_loss: 0.0653 - val_acc: 0.5395\n",
      "Epoch 1478/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0465 - acc: 0.6608 - val_loss: 0.0654 - val_acc: 0.5420\n",
      "Epoch 1479/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0468 - acc: 0.6588 - val_loss: 0.0657 - val_acc: 0.5362\n",
      "Epoch 1480/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6574 - val_loss: 0.0651 - val_acc: 0.5420\n",
      "Epoch 1481/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6591 - val_loss: 0.0669 - val_acc: 0.5345\n",
      "Epoch 1482/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6627 - val_loss: 0.0667 - val_acc: 0.5337\n",
      "Epoch 1483/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0476 - acc: 0.6535 - val_loss: 0.0674 - val_acc: 0.5303\n",
      "Epoch 1484/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0482 - acc: 0.6458 - val_loss: 0.0661 - val_acc: 0.5411\n",
      "Epoch 1485/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0469 - acc: 0.6596 - val_loss: 0.0668 - val_acc: 0.5420\n",
      "Epoch 1486/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0488 - acc: 0.6405 - val_loss: 0.0658 - val_acc: 0.5428\n",
      "Epoch 1487/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6602 - val_loss: 0.0656 - val_acc: 0.5445\n",
      "Epoch 1488/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0469 - acc: 0.6580 - val_loss: 0.0652 - val_acc: 0.5453\n",
      "Epoch 1489/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6547 - val_loss: 0.0664 - val_acc: 0.5320\n",
      "Epoch 1490/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0469 - acc: 0.6580 - val_loss: 0.0661 - val_acc: 0.5378\n",
      "Epoch 1491/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6596 - val_loss: 0.0660 - val_acc: 0.5278\n",
      "Epoch 1492/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0466 - acc: 0.6616 - val_loss: 0.0669 - val_acc: 0.5387\n",
      "Epoch 1493/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6552 - val_loss: 0.0662 - val_acc: 0.5237\n",
      "Epoch 1494/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6569 - val_loss: 0.0652 - val_acc: 0.5395\n",
      "Epoch 1495/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0468 - acc: 0.6577 - val_loss: 0.0653 - val_acc: 0.5428\n",
      "Epoch 1496/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6560 - val_loss: 0.0659 - val_acc: 0.5245\n",
      "Epoch 1497/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0468 - acc: 0.6577 - val_loss: 0.0657 - val_acc: 0.5428\n",
      "Epoch 1498/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0470 - acc: 0.6558 - val_loss: 0.0666 - val_acc: 0.5453\n",
      "Epoch 1499/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0475 - acc: 0.6535 - val_loss: 0.0657 - val_acc: 0.5378\n",
      "Epoch 1500/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0473 - acc: 0.6560 - val_loss: 0.0660 - val_acc: 0.5486\n",
      "Epoch 1501/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0476 - acc: 0.6566 - val_loss: 0.0658 - val_acc: 0.5470\n",
      "Epoch 1502/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0463 - acc: 0.6627 - val_loss: 0.0660 - val_acc: 0.5378\n",
      "Epoch 1503/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0475 - acc: 0.6519 - val_loss: 0.0651 - val_acc: 0.5495\n",
      "Epoch 1504/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0473 - acc: 0.6549 - val_loss: 0.0658 - val_acc: 0.5453\n",
      "Epoch 1505/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6624 - val_loss: 0.0649 - val_acc: 0.5478\n",
      "Epoch 1506/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0472 - acc: 0.6621 - val_loss: 0.0668 - val_acc: 0.5403\n",
      "Epoch 1507/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0470 - acc: 0.6621 - val_loss: 0.0674 - val_acc: 0.5411\n",
      "Epoch 1508/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0484 - acc: 0.6555 - val_loss: 0.0657 - val_acc: 0.5353\n",
      "Epoch 1509/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0475 - acc: 0.6494 - val_loss: 0.0660 - val_acc: 0.5370\n",
      "Epoch 1510/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6533 - val_loss: 0.0669 - val_acc: 0.5337\n",
      "Epoch 1511/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0477 - acc: 0.6560 - val_loss: 0.0655 - val_acc: 0.5378\n",
      "Epoch 1512/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6547 - val_loss: 0.0656 - val_acc: 0.5428\n",
      "Epoch 1513/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0474 - acc: 0.6511 - val_loss: 0.0661 - val_acc: 0.5411\n",
      "Epoch 1514/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0466 - acc: 0.6572 - val_loss: 0.0660 - val_acc: 0.5362\n",
      "Epoch 1515/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6574 - val_loss: 0.0655 - val_acc: 0.5445\n",
      "Epoch 1516/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6608 - val_loss: 0.0649 - val_acc: 0.5470\n",
      "Epoch 1517/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6569 - val_loss: 0.0658 - val_acc: 0.5503\n",
      "Epoch 1518/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.6533 - val_loss: 0.0673 - val_acc: 0.5295\n",
      "Epoch 1519/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0466 - acc: 0.6585 - val_loss: 0.0660 - val_acc: 0.5362\n",
      "Epoch 1520/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0476 - acc: 0.6591 - val_loss: 0.0652 - val_acc: 0.5461\n",
      "Epoch 1521/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6574 - val_loss: 0.0652 - val_acc: 0.5345\n",
      "Epoch 1522/3000\n",
      "3608/3608 [==============================] - 1s 159us/step - loss: 0.0471 - acc: 0.6580 - val_loss: 0.0659 - val_acc: 0.5453\n",
      "Epoch 1523/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0472 - acc: 0.6566 - val_loss: 0.0657 - val_acc: 0.5461\n",
      "Epoch 1524/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0478 - acc: 0.6491 - val_loss: 0.0660 - val_acc: 0.5378\n",
      "Epoch 1525/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0468 - acc: 0.6544 - val_loss: 0.0658 - val_acc: 0.5395\n",
      "Epoch 1526/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6610 - val_loss: 0.0673 - val_acc: 0.5320\n",
      "Epoch 1527/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6566 - val_loss: 0.0667 - val_acc: 0.5370\n",
      "Epoch 1528/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0472 - acc: 0.6569 - val_loss: 0.0656 - val_acc: 0.5345\n",
      "Epoch 1529/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6599 - val_loss: 0.0660 - val_acc: 0.5420\n",
      "Epoch 1530/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0472 - acc: 0.6563 - val_loss: 0.0654 - val_acc: 0.5478\n",
      "Epoch 1531/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0471 - acc: 0.6555 - val_loss: 0.0654 - val_acc: 0.5420\n",
      "Epoch 1532/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0484 - acc: 0.6441 - val_loss: 0.0679 - val_acc: 0.5262\n",
      "Epoch 1533/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0488 - acc: 0.6427 - val_loss: 0.0666 - val_acc: 0.5378\n",
      "Epoch 1534/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0469 - acc: 0.6577 - val_loss: 0.0657 - val_acc: 0.5461\n",
      "Epoch 1535/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0466 - acc: 0.6555 - val_loss: 0.0667 - val_acc: 0.5320\n",
      "Epoch 1536/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0468 - acc: 0.6558 - val_loss: 0.0651 - val_acc: 0.5420\n",
      "Epoch 1537/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0467 - acc: 0.6627 - val_loss: 0.0662 - val_acc: 0.5362\n",
      "Epoch 1538/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0474 - acc: 0.6505 - val_loss: 0.0654 - val_acc: 0.5378\n",
      "Epoch 1539/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0472 - acc: 0.6583 - val_loss: 0.0659 - val_acc: 0.5337\n",
      "Epoch 1540/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0468 - acc: 0.6577 - val_loss: 0.0660 - val_acc: 0.5345\n",
      "Epoch 1541/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0474 - acc: 0.6574 - val_loss: 0.0674 - val_acc: 0.5220\n",
      "Epoch 1542/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0477 - acc: 0.6497 - val_loss: 0.0653 - val_acc: 0.5478\n",
      "Epoch 1543/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0477 - acc: 0.6527 - val_loss: 0.0652 - val_acc: 0.5470\n",
      "Epoch 1544/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6572 - val_loss: 0.0656 - val_acc: 0.5353\n",
      "Epoch 1545/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0468 - acc: 0.6594 - val_loss: 0.0660 - val_acc: 0.5445\n",
      "Epoch 1546/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0469 - acc: 0.6619 - val_loss: 0.0647 - val_acc: 0.5403\n",
      "Epoch 1547/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0473 - acc: 0.6577 - val_loss: 0.0671 - val_acc: 0.5262\n",
      "Epoch 1548/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6585 - val_loss: 0.0656 - val_acc: 0.5387\n",
      "Epoch 1549/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0474 - acc: 0.6508 - val_loss: 0.0652 - val_acc: 0.5403\n",
      "Epoch 1550/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6608 - val_loss: 0.0650 - val_acc: 0.5420\n",
      "Epoch 1551/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6641 - val_loss: 0.0666 - val_acc: 0.5328\n",
      "Epoch 1552/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6619 - val_loss: 0.0660 - val_acc: 0.5320\n",
      "Epoch 1553/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6616 - val_loss: 0.0657 - val_acc: 0.5378\n",
      "Epoch 1554/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0464 - acc: 0.6627 - val_loss: 0.0656 - val_acc: 0.5511\n",
      "Epoch 1555/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0472 - acc: 0.6516 - val_loss: 0.0657 - val_acc: 0.5495\n",
      "Epoch 1556/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0470 - acc: 0.6588 - val_loss: 0.0663 - val_acc: 0.5287\n",
      "Epoch 1557/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0472 - acc: 0.6549 - val_loss: 0.0651 - val_acc: 0.5503\n",
      "Epoch 1558/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6558 - val_loss: 0.0658 - val_acc: 0.5370\n",
      "Epoch 1559/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0473 - acc: 0.6530 - val_loss: 0.0661 - val_acc: 0.5411\n",
      "Epoch 1560/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0479 - acc: 0.6527 - val_loss: 0.0645 - val_acc: 0.5478\n",
      "Epoch 1561/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0472 - acc: 0.6524 - val_loss: 0.0653 - val_acc: 0.5345\n",
      "Epoch 1562/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6563 - val_loss: 0.0655 - val_acc: 0.5387\n",
      "Epoch 1563/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0476 - acc: 0.6513 - val_loss: 0.0663 - val_acc: 0.5420\n",
      "Epoch 1564/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0473 - acc: 0.6583 - val_loss: 0.0673 - val_acc: 0.5320\n",
      "Epoch 1565/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0493 - acc: 0.6422 - val_loss: 0.0660 - val_acc: 0.5420\n",
      "Epoch 1566/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0505 - acc: 0.6366 - val_loss: 0.0651 - val_acc: 0.5536\n",
      "Epoch 1567/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0495 - acc: 0.6400 - val_loss: 0.0650 - val_acc: 0.5378\n",
      "Epoch 1568/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0477 - acc: 0.6502 - val_loss: 0.0656 - val_acc: 0.5411\n",
      "Epoch 1569/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6569 - val_loss: 0.0652 - val_acc: 0.5403\n",
      "Epoch 1570/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6569 - val_loss: 0.0653 - val_acc: 0.5436\n",
      "Epoch 1571/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6610 - val_loss: 0.0652 - val_acc: 0.5378\n",
      "Epoch 1572/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0477 - acc: 0.6524 - val_loss: 0.0667 - val_acc: 0.5345\n",
      "Epoch 1573/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0468 - acc: 0.6594 - val_loss: 0.0658 - val_acc: 0.5403\n",
      "Epoch 1574/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6572 - val_loss: 0.0651 - val_acc: 0.5520\n",
      "Epoch 1575/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6608 - val_loss: 0.0663 - val_acc: 0.5337\n",
      "Epoch 1576/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0480 - acc: 0.6511 - val_loss: 0.0653 - val_acc: 0.5411\n",
      "Epoch 1577/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0477 - acc: 0.6491 - val_loss: 0.0659 - val_acc: 0.5395\n",
      "Epoch 1578/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0474 - acc: 0.6488 - val_loss: 0.0650 - val_acc: 0.5378\n",
      "Epoch 1579/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0469 - acc: 0.6560 - val_loss: 0.0661 - val_acc: 0.5387\n",
      "Epoch 1580/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6608 - val_loss: 0.0658 - val_acc: 0.5387\n",
      "Epoch 1581/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0467 - acc: 0.6566 - val_loss: 0.0656 - val_acc: 0.5353\n",
      "Epoch 1582/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0469 - acc: 0.6569 - val_loss: 0.0658 - val_acc: 0.5428\n",
      "Epoch 1583/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0466 - acc: 0.6616 - val_loss: 0.0653 - val_acc: 0.5428\n",
      "Epoch 1584/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0466 - acc: 0.6641 - val_loss: 0.0667 - val_acc: 0.5370\n",
      "Epoch 1585/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6580 - val_loss: 0.0660 - val_acc: 0.5470\n",
      "Epoch 1586/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0472 - acc: 0.6555 - val_loss: 0.0658 - val_acc: 0.5420\n",
      "Epoch 1587/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0466 - acc: 0.6619 - val_loss: 0.0655 - val_acc: 0.5478\n",
      "Epoch 1588/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6563 - val_loss: 0.0662 - val_acc: 0.5486\n",
      "Epoch 1589/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0474 - acc: 0.6547 - val_loss: 0.0652 - val_acc: 0.5420\n",
      "Epoch 1590/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0477 - acc: 0.6522 - val_loss: 0.0657 - val_acc: 0.5395\n",
      "Epoch 1591/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0468 - acc: 0.6572 - val_loss: 0.0652 - val_acc: 0.5328\n",
      "Epoch 1592/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0468 - acc: 0.6627 - val_loss: 0.0665 - val_acc: 0.5353\n",
      "Epoch 1593/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0466 - acc: 0.6610 - val_loss: 0.0660 - val_acc: 0.5461\n",
      "Epoch 1594/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0468 - acc: 0.6547 - val_loss: 0.0657 - val_acc: 0.5420\n",
      "Epoch 1595/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0475 - acc: 0.6530 - val_loss: 0.0656 - val_acc: 0.5320\n",
      "Epoch 1596/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0473 - acc: 0.6574 - val_loss: 0.0661 - val_acc: 0.5470\n",
      "Epoch 1597/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0481 - acc: 0.6516 - val_loss: 0.0652 - val_acc: 0.5495\n",
      "Epoch 1598/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0470 - acc: 0.6588 - val_loss: 0.0659 - val_acc: 0.5387\n",
      "Epoch 1599/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0467 - acc: 0.6605 - val_loss: 0.0663 - val_acc: 0.5403\n",
      "Epoch 1600/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0469 - acc: 0.6574 - val_loss: 0.0662 - val_acc: 0.5328\n",
      "Epoch 1601/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0471 - acc: 0.6591 - val_loss: 0.0670 - val_acc: 0.5370\n",
      "Epoch 1602/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0472 - acc: 0.6538 - val_loss: 0.0714 - val_acc: 0.4904\n",
      "Epoch 1603/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0488 - acc: 0.6475 - val_loss: 0.0651 - val_acc: 0.5470\n",
      "Epoch 1604/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0480 - acc: 0.6527 - val_loss: 0.0657 - val_acc: 0.5320\n",
      "Epoch 1605/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0473 - acc: 0.6547 - val_loss: 0.0666 - val_acc: 0.5345\n",
      "Epoch 1606/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0485 - acc: 0.6427 - val_loss: 0.0660 - val_acc: 0.5353\n",
      "Epoch 1607/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0468 - acc: 0.6585 - val_loss: 0.0655 - val_acc: 0.5370\n",
      "Epoch 1608/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0467 - acc: 0.6574 - val_loss: 0.0651 - val_acc: 0.5478\n",
      "Epoch 1609/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0471 - acc: 0.6574 - val_loss: 0.0659 - val_acc: 0.5337\n",
      "Epoch 1610/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0464 - acc: 0.6605 - val_loss: 0.0659 - val_acc: 0.5411\n",
      "Epoch 1611/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0464 - acc: 0.6666 - val_loss: 0.0655 - val_acc: 0.5403\n",
      "Epoch 1612/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0469 - acc: 0.6608 - val_loss: 0.0668 - val_acc: 0.5328\n",
      "Epoch 1613/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0475 - acc: 0.6530 - val_loss: 0.0662 - val_acc: 0.5395\n",
      "Epoch 1614/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0468 - acc: 0.6621 - val_loss: 0.0666 - val_acc: 0.5320\n",
      "Epoch 1615/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0469 - acc: 0.6577 - val_loss: 0.0651 - val_acc: 0.5478\n",
      "Epoch 1616/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0471 - acc: 0.6580 - val_loss: 0.0659 - val_acc: 0.5262\n",
      "Epoch 1617/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0467 - acc: 0.6580 - val_loss: 0.0655 - val_acc: 0.5445\n",
      "Epoch 1618/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0465 - acc: 0.6610 - val_loss: 0.0664 - val_acc: 0.5345\n",
      "Epoch 1619/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0467 - acc: 0.6613 - val_loss: 0.0661 - val_acc: 0.5428\n",
      "Epoch 1620/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0477 - acc: 0.6513 - val_loss: 0.0657 - val_acc: 0.5453\n",
      "Epoch 1621/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0470 - acc: 0.6591 - val_loss: 0.0655 - val_acc: 0.5495\n",
      "Epoch 1622/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0470 - acc: 0.6585 - val_loss: 0.0660 - val_acc: 0.5461\n",
      "Epoch 1623/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0481 - acc: 0.6475 - val_loss: 0.0665 - val_acc: 0.5387\n",
      "Epoch 1624/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6505 - val_loss: 0.0659 - val_acc: 0.5362\n",
      "Epoch 1625/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0478 - acc: 0.6524 - val_loss: 0.0669 - val_acc: 0.5353\n",
      "Epoch 1626/3000\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0477 - acc: 0.6544 - val_loss: 0.0658 - val_acc: 0.5378\n",
      "Epoch 1627/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0478 - acc: 0.6508 - val_loss: 0.0658 - val_acc: 0.5470\n",
      "Epoch 1628/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0471 - acc: 0.6552 - val_loss: 0.0655 - val_acc: 0.5445\n",
      "Epoch 1629/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0465 - acc: 0.6649 - val_loss: 0.0667 - val_acc: 0.5345\n",
      "Epoch 1630/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0468 - acc: 0.6580 - val_loss: 0.0671 - val_acc: 0.5345\n",
      "Epoch 1631/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0471 - acc: 0.6569 - val_loss: 0.0673 - val_acc: 0.5278\n",
      "Epoch 1632/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0471 - acc: 0.6569 - val_loss: 0.0656 - val_acc: 0.5453\n",
      "Epoch 1633/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6616 - val_loss: 0.0660 - val_acc: 0.5436\n",
      "Epoch 1634/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6591 - val_loss: 0.0651 - val_acc: 0.5478\n",
      "Epoch 1635/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0467 - acc: 0.6646 - val_loss: 0.0662 - val_acc: 0.5303\n",
      "Epoch 1636/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0465 - acc: 0.6596 - val_loss: 0.0652 - val_acc: 0.5461\n",
      "Epoch 1637/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0473 - acc: 0.6602 - val_loss: 0.0664 - val_acc: 0.5387\n",
      "Epoch 1638/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0468 - acc: 0.6566 - val_loss: 0.0658 - val_acc: 0.5362\n",
      "Epoch 1639/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0474 - acc: 0.6535 - val_loss: 0.0668 - val_acc: 0.5378\n",
      "Epoch 1640/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6508 - val_loss: 0.0668 - val_acc: 0.5387\n",
      "Epoch 1641/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.6594 - val_loss: 0.0664 - val_acc: 0.5453\n",
      "Epoch 1642/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0470 - acc: 0.6563 - val_loss: 0.0660 - val_acc: 0.5378\n",
      "Epoch 1643/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0471 - acc: 0.6563 - val_loss: 0.0664 - val_acc: 0.5328\n",
      "Epoch 1644/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0470 - acc: 0.6563 - val_loss: 0.0671 - val_acc: 0.5145\n",
      "Epoch 1645/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0472 - acc: 0.6569 - val_loss: 0.0654 - val_acc: 0.5353\n",
      "Epoch 1646/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0474 - acc: 0.6560 - val_loss: 0.0662 - val_acc: 0.5278\n",
      "Epoch 1647/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0469 - acc: 0.6596 - val_loss: 0.0658 - val_acc: 0.5486\n",
      "Epoch 1648/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6555 - val_loss: 0.0658 - val_acc: 0.5403\n",
      "Epoch 1649/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0473 - acc: 0.6580 - val_loss: 0.0657 - val_acc: 0.5353\n",
      "Epoch 1650/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0467 - acc: 0.6630 - val_loss: 0.0667 - val_acc: 0.5403\n",
      "Epoch 1651/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6552 - val_loss: 0.0662 - val_acc: 0.5411\n",
      "Epoch 1652/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0474 - acc: 0.6574 - val_loss: 0.0661 - val_acc: 0.5312\n",
      "Epoch 1653/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0464 - acc: 0.6616 - val_loss: 0.0667 - val_acc: 0.5320\n",
      "Epoch 1654/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0479 - acc: 0.6475 - val_loss: 0.0654 - val_acc: 0.5453\n",
      "Epoch 1655/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0469 - acc: 0.6574 - val_loss: 0.0655 - val_acc: 0.5503\n",
      "Epoch 1656/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0468 - acc: 0.6605 - val_loss: 0.0663 - val_acc: 0.5395\n",
      "Epoch 1657/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0466 - acc: 0.6583 - val_loss: 0.0671 - val_acc: 0.5387\n",
      "Epoch 1658/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0469 - acc: 0.6577 - val_loss: 0.0661 - val_acc: 0.5420\n",
      "Epoch 1659/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0475 - acc: 0.6513 - val_loss: 0.0657 - val_acc: 0.5345\n",
      "Epoch 1660/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0464 - acc: 0.6646 - val_loss: 0.0668 - val_acc: 0.5245\n",
      "Epoch 1661/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0474 - acc: 0.6566 - val_loss: 0.0650 - val_acc: 0.5420\n",
      "Epoch 1662/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0478 - acc: 0.6505 - val_loss: 0.0660 - val_acc: 0.5420\n",
      "Epoch 1663/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6569 - val_loss: 0.0668 - val_acc: 0.5345\n",
      "Epoch 1664/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6563 - val_loss: 0.0653 - val_acc: 0.5420\n",
      "Epoch 1665/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6572 - val_loss: 0.0666 - val_acc: 0.5262\n",
      "Epoch 1666/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0471 - acc: 0.6544 - val_loss: 0.0652 - val_acc: 0.5428\n",
      "Epoch 1667/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6602 - val_loss: 0.0666 - val_acc: 0.5403\n",
      "Epoch 1668/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0470 - acc: 0.6544 - val_loss: 0.0658 - val_acc: 0.5411\n",
      "Epoch 1669/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6563 - val_loss: 0.0650 - val_acc: 0.5445\n",
      "Epoch 1670/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0469 - acc: 0.6619 - val_loss: 0.0662 - val_acc: 0.5345\n",
      "Epoch 1671/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0472 - acc: 0.6535 - val_loss: 0.0660 - val_acc: 0.5486\n",
      "Epoch 1672/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6591 - val_loss: 0.0648 - val_acc: 0.5520\n",
      "Epoch 1673/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0471 - acc: 0.6544 - val_loss: 0.0664 - val_acc: 0.5303\n",
      "Epoch 1674/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6572 - val_loss: 0.0658 - val_acc: 0.5461\n",
      "Epoch 1675/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6599 - val_loss: 0.0668 - val_acc: 0.5295\n",
      "Epoch 1676/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0465 - acc: 0.6574 - val_loss: 0.0663 - val_acc: 0.5353\n",
      "Epoch 1677/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0469 - acc: 0.6577 - val_loss: 0.0653 - val_acc: 0.5411\n",
      "Epoch 1678/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0465 - acc: 0.6583 - val_loss: 0.0667 - val_acc: 0.5362\n",
      "Epoch 1679/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6605 - val_loss: 0.0659 - val_acc: 0.5387\n",
      "Epoch 1680/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6569 - val_loss: 0.0656 - val_acc: 0.5478\n",
      "Epoch 1681/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0474 - acc: 0.6535 - val_loss: 0.0664 - val_acc: 0.5353\n",
      "Epoch 1682/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0472 - acc: 0.6549 - val_loss: 0.0676 - val_acc: 0.5287\n",
      "Epoch 1683/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0488 - acc: 0.6458 - val_loss: 0.0673 - val_acc: 0.5403\n",
      "Epoch 1684/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0483 - acc: 0.6458 - val_loss: 0.0652 - val_acc: 0.5428\n",
      "Epoch 1685/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0477 - acc: 0.6499 - val_loss: 0.0663 - val_acc: 0.5362\n",
      "Epoch 1686/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0476 - acc: 0.6555 - val_loss: 0.0667 - val_acc: 0.5320\n",
      "Epoch 1687/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0472 - acc: 0.6530 - val_loss: 0.0657 - val_acc: 0.5312\n",
      "Epoch 1688/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0465 - acc: 0.6624 - val_loss: 0.0663 - val_acc: 0.5436\n",
      "Epoch 1689/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0466 - acc: 0.6602 - val_loss: 0.0663 - val_acc: 0.5278\n",
      "Epoch 1690/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0464 - acc: 0.6596 - val_loss: 0.0660 - val_acc: 0.5511\n",
      "Epoch 1691/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0476 - acc: 0.6583 - val_loss: 0.0656 - val_acc: 0.5395\n",
      "Epoch 1692/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0470 - acc: 0.6596 - val_loss: 0.0664 - val_acc: 0.5287\n",
      "Epoch 1693/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6535 - val_loss: 0.0661 - val_acc: 0.5486\n",
      "Epoch 1694/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0468 - acc: 0.6577 - val_loss: 0.0664 - val_acc: 0.5387\n",
      "Epoch 1695/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6519 - val_loss: 0.0670 - val_acc: 0.5353\n",
      "Epoch 1696/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0469 - acc: 0.6574 - val_loss: 0.0654 - val_acc: 0.5411\n",
      "Epoch 1697/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0469 - acc: 0.6585 - val_loss: 0.0665 - val_acc: 0.5328\n",
      "Epoch 1698/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6613 - val_loss: 0.0662 - val_acc: 0.5378\n",
      "Epoch 1699/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0468 - acc: 0.6610 - val_loss: 0.0669 - val_acc: 0.5237\n",
      "Epoch 1700/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0471 - acc: 0.6547 - val_loss: 0.0663 - val_acc: 0.5345\n",
      "Epoch 1701/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0467 - acc: 0.6591 - val_loss: 0.0676 - val_acc: 0.5387\n",
      "Epoch 1702/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6663 - val_loss: 0.0666 - val_acc: 0.5295\n",
      "Epoch 1703/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0470 - acc: 0.6572 - val_loss: 0.0669 - val_acc: 0.5320\n",
      "Epoch 1704/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0483 - acc: 0.6494 - val_loss: 0.0660 - val_acc: 0.5362\n",
      "Epoch 1705/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6583 - val_loss: 0.0663 - val_acc: 0.5370\n",
      "Epoch 1706/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0477 - acc: 0.6477 - val_loss: 0.0665 - val_acc: 0.5337\n",
      "Epoch 1707/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0479 - acc: 0.6541 - val_loss: 0.0661 - val_acc: 0.5387\n",
      "Epoch 1708/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0469 - acc: 0.6566 - val_loss: 0.0661 - val_acc: 0.5387\n",
      "Epoch 1709/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0475 - acc: 0.6555 - val_loss: 0.0677 - val_acc: 0.5295\n",
      "Epoch 1710/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6544 - val_loss: 0.0666 - val_acc: 0.5378\n",
      "Epoch 1711/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0469 - acc: 0.6563 - val_loss: 0.0652 - val_acc: 0.5420\n",
      "Epoch 1712/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0472 - acc: 0.6558 - val_loss: 0.0656 - val_acc: 0.5461\n",
      "Epoch 1713/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0468 - acc: 0.6605 - val_loss: 0.0659 - val_acc: 0.5403\n",
      "Epoch 1714/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0465 - acc: 0.6632 - val_loss: 0.0664 - val_acc: 0.5436\n",
      "Epoch 1715/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6599 - val_loss: 0.0652 - val_acc: 0.5495\n",
      "Epoch 1716/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0469 - acc: 0.6558 - val_loss: 0.0661 - val_acc: 0.5395\n",
      "Epoch 1717/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0470 - acc: 0.6599 - val_loss: 0.0654 - val_acc: 0.5411\n",
      "Epoch 1718/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0471 - acc: 0.6572 - val_loss: 0.0648 - val_acc: 0.5403\n",
      "Epoch 1719/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6544 - val_loss: 0.0659 - val_acc: 0.5403\n",
      "Epoch 1720/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0470 - acc: 0.6616 - val_loss: 0.0666 - val_acc: 0.5337\n",
      "Epoch 1721/3000\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0479 - acc: 0.6511 - val_loss: 0.0664 - val_acc: 0.5370\n",
      "Epoch 1722/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0474 - acc: 0.6533 - val_loss: 0.0675 - val_acc: 0.5295\n",
      "Epoch 1723/3000\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0470 - acc: 0.6555 - val_loss: 0.0659 - val_acc: 0.5428\n",
      "Epoch 1724/3000\n",
      "3608/3608 [==============================] - 1s 224us/step - loss: 0.0473 - acc: 0.6535 - val_loss: 0.0663 - val_acc: 0.5337\n",
      "Epoch 1725/3000\n",
      "3608/3608 [==============================] - 1s 219us/step - loss: 0.0466 - acc: 0.6596 - val_loss: 0.0665 - val_acc: 0.5295\n",
      "Epoch 1726/3000\n",
      "3608/3608 [==============================] - 1s 220us/step - loss: 0.0470 - acc: 0.6572 - val_loss: 0.0658 - val_acc: 0.5320\n",
      "Epoch 1727/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0468 - acc: 0.6588 - val_loss: 0.0665 - val_acc: 0.5387\n",
      "Epoch 1728/3000\n",
      "3608/3608 [==============================] - 1s 189us/step - loss: 0.0468 - acc: 0.6610 - val_loss: 0.0665 - val_acc: 0.5411\n",
      "Epoch 1729/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0477 - acc: 0.6516 - val_loss: 0.0668 - val_acc: 0.5312\n",
      "Epoch 1730/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6566 - val_loss: 0.0670 - val_acc: 0.5353\n",
      "Epoch 1731/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6519 - val_loss: 0.0657 - val_acc: 0.5511\n",
      "Epoch 1732/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6613 - val_loss: 0.0664 - val_acc: 0.5387\n",
      "Epoch 1733/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6560 - val_loss: 0.0654 - val_acc: 0.5378\n",
      "Epoch 1734/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6585 - val_loss: 0.0657 - val_acc: 0.5387\n",
      "Epoch 1735/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0469 - acc: 0.6585 - val_loss: 0.0665 - val_acc: 0.5370\n",
      "Epoch 1736/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6558 - val_loss: 0.0654 - val_acc: 0.5461\n",
      "Epoch 1737/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0466 - acc: 0.6583 - val_loss: 0.0660 - val_acc: 0.5403\n",
      "Epoch 1738/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0487 - acc: 0.6441 - val_loss: 0.0664 - val_acc: 0.5229\n",
      "Epoch 1739/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0482 - acc: 0.6430 - val_loss: 0.0659 - val_acc: 0.5370\n",
      "Epoch 1740/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0470 - acc: 0.6533 - val_loss: 0.0661 - val_acc: 0.5320\n",
      "Epoch 1741/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6552 - val_loss: 0.0669 - val_acc: 0.5245\n",
      "Epoch 1742/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0472 - acc: 0.6605 - val_loss: 0.0657 - val_acc: 0.5320\n",
      "Epoch 1743/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6558 - val_loss: 0.0652 - val_acc: 0.5495\n",
      "Epoch 1744/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6585 - val_loss: 0.0659 - val_acc: 0.5328\n",
      "Epoch 1745/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6580 - val_loss: 0.0653 - val_acc: 0.5436\n",
      "Epoch 1746/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6613 - val_loss: 0.0658 - val_acc: 0.5478\n",
      "Epoch 1747/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0483 - acc: 0.6494 - val_loss: 0.0655 - val_acc: 0.5387\n",
      "Epoch 1748/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0474 - acc: 0.6530 - val_loss: 0.0660 - val_acc: 0.5395\n",
      "Epoch 1749/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0476 - acc: 0.6558 - val_loss: 0.0667 - val_acc: 0.5387\n",
      "Epoch 1750/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0480 - acc: 0.6499 - val_loss: 0.0653 - val_acc: 0.5387\n",
      "Epoch 1751/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0480 - acc: 0.6527 - val_loss: 0.0658 - val_acc: 0.5486\n",
      "Epoch 1752/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0471 - acc: 0.6613 - val_loss: 0.0664 - val_acc: 0.5362\n",
      "Epoch 1753/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6538 - val_loss: 0.0665 - val_acc: 0.5411\n",
      "Epoch 1754/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0477 - acc: 0.6513 - val_loss: 0.0672 - val_acc: 0.5345\n",
      "Epoch 1755/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0483 - acc: 0.6502 - val_loss: 0.0653 - val_acc: 0.5312\n",
      "Epoch 1756/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0476 - acc: 0.6538 - val_loss: 0.0652 - val_acc: 0.5403\n",
      "Epoch 1757/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6491 - val_loss: 0.0650 - val_acc: 0.5511\n",
      "Epoch 1758/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0473 - acc: 0.6572 - val_loss: 0.0662 - val_acc: 0.5270\n",
      "Epoch 1759/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0513 - acc: 0.6297 - val_loss: 0.0669 - val_acc: 0.5287\n",
      "Epoch 1760/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0480 - acc: 0.6505 - val_loss: 0.0662 - val_acc: 0.5337\n",
      "Epoch 1761/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6541 - val_loss: 0.0654 - val_acc: 0.5436\n",
      "Epoch 1762/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6558 - val_loss: 0.0662 - val_acc: 0.5370\n",
      "Epoch 1763/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6566 - val_loss: 0.0657 - val_acc: 0.5387\n",
      "Epoch 1764/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0484 - acc: 0.6483 - val_loss: 0.0664 - val_acc: 0.5353\n",
      "Epoch 1765/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6580 - val_loss: 0.0657 - val_acc: 0.5486\n",
      "Epoch 1766/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0470 - acc: 0.6560 - val_loss: 0.0660 - val_acc: 0.5345\n",
      "Epoch 1767/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6522 - val_loss: 0.0654 - val_acc: 0.5353\n",
      "Epoch 1768/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6602 - val_loss: 0.0652 - val_acc: 0.5594\n",
      "Epoch 1769/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0464 - acc: 0.6591 - val_loss: 0.0656 - val_acc: 0.5445\n",
      "Epoch 1770/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0464 - acc: 0.6663 - val_loss: 0.0662 - val_acc: 0.5403\n",
      "Epoch 1771/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0463 - acc: 0.6641 - val_loss: 0.0650 - val_acc: 0.5420\n",
      "Epoch 1772/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6632 - val_loss: 0.0646 - val_acc: 0.5520\n",
      "Epoch 1773/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0463 - acc: 0.6632 - val_loss: 0.0654 - val_acc: 0.5436\n",
      "Epoch 1774/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0469 - acc: 0.6572 - val_loss: 0.0652 - val_acc: 0.5536\n",
      "Epoch 1775/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6560 - val_loss: 0.0656 - val_acc: 0.5478\n",
      "Epoch 1776/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0485 - acc: 0.6511 - val_loss: 0.0672 - val_acc: 0.5328\n",
      "Epoch 1777/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0480 - acc: 0.6477 - val_loss: 0.0651 - val_acc: 0.5486\n",
      "Epoch 1778/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0475 - acc: 0.6566 - val_loss: 0.0666 - val_acc: 0.5544\n",
      "Epoch 1779/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6591 - val_loss: 0.0666 - val_acc: 0.5312\n",
      "Epoch 1780/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0467 - acc: 0.6588 - val_loss: 0.0673 - val_acc: 0.5345\n",
      "Epoch 1781/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0477 - acc: 0.6508 - val_loss: 0.0652 - val_acc: 0.5486\n",
      "Epoch 1782/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0467 - acc: 0.6572 - val_loss: 0.0658 - val_acc: 0.5403\n",
      "Epoch 1783/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0466 - acc: 0.6608 - val_loss: 0.0654 - val_acc: 0.5411\n",
      "Epoch 1784/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0465 - acc: 0.6580 - val_loss: 0.0652 - val_acc: 0.5453\n",
      "Epoch 1785/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6577 - val_loss: 0.0663 - val_acc: 0.5370\n",
      "Epoch 1786/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0478 - acc: 0.6527 - val_loss: 0.0658 - val_acc: 0.5528\n",
      "Epoch 1787/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0476 - acc: 0.6524 - val_loss: 0.0668 - val_acc: 0.5387\n",
      "Epoch 1788/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0472 - acc: 0.6569 - val_loss: 0.0656 - val_acc: 0.5470\n",
      "Epoch 1789/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0469 - acc: 0.6644 - val_loss: 0.0660 - val_acc: 0.5362\n",
      "Epoch 1790/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0468 - acc: 0.6572 - val_loss: 0.0660 - val_acc: 0.5420\n",
      "Epoch 1791/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6558 - val_loss: 0.0652 - val_acc: 0.5528\n",
      "Epoch 1792/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0479 - acc: 0.6558 - val_loss: 0.0660 - val_acc: 0.5378\n",
      "Epoch 1793/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0468 - acc: 0.6585 - val_loss: 0.0665 - val_acc: 0.5378\n",
      "Epoch 1794/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0477 - acc: 0.6533 - val_loss: 0.0661 - val_acc: 0.5353\n",
      "Epoch 1795/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6594 - val_loss: 0.0661 - val_acc: 0.5287\n",
      "Epoch 1796/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6566 - val_loss: 0.0662 - val_acc: 0.5345\n",
      "Epoch 1797/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0465 - acc: 0.6588 - val_loss: 0.0679 - val_acc: 0.5270\n",
      "Epoch 1798/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0475 - acc: 0.6522 - val_loss: 0.0663 - val_acc: 0.5378\n",
      "Epoch 1799/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6574 - val_loss: 0.0670 - val_acc: 0.5378\n",
      "Epoch 1800/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6558 - val_loss: 0.0657 - val_acc: 0.5436\n",
      "Epoch 1801/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0470 - acc: 0.6630 - val_loss: 0.0664 - val_acc: 0.5387\n",
      "Epoch 1802/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0473 - acc: 0.6560 - val_loss: 0.0670 - val_acc: 0.5353\n",
      "Epoch 1803/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6580 - val_loss: 0.0673 - val_acc: 0.5328\n",
      "Epoch 1804/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0470 - acc: 0.6569 - val_loss: 0.0662 - val_acc: 0.5395\n",
      "Epoch 1805/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0468 - acc: 0.6583 - val_loss: 0.0657 - val_acc: 0.5445\n",
      "Epoch 1806/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0475 - acc: 0.6530 - val_loss: 0.0664 - val_acc: 0.5436\n",
      "Epoch 1807/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0472 - acc: 0.6583 - val_loss: 0.0666 - val_acc: 0.5420\n",
      "Epoch 1808/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6555 - val_loss: 0.0666 - val_acc: 0.5362\n",
      "Epoch 1809/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0480 - acc: 0.6516 - val_loss: 0.0668 - val_acc: 0.5387\n",
      "Epoch 1810/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0485 - acc: 0.6441 - val_loss: 0.0664 - val_acc: 0.5378\n",
      "Epoch 1811/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0478 - acc: 0.6477 - val_loss: 0.0653 - val_acc: 0.5370\n",
      "Epoch 1812/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0468 - acc: 0.6560 - val_loss: 0.0651 - val_acc: 0.5461\n",
      "Epoch 1813/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6635 - val_loss: 0.0663 - val_acc: 0.5353\n",
      "Epoch 1814/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0466 - acc: 0.6591 - val_loss: 0.0666 - val_acc: 0.5245\n",
      "Epoch 1815/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6652 - val_loss: 0.0654 - val_acc: 0.5428\n",
      "Epoch 1816/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6563 - val_loss: 0.0658 - val_acc: 0.5303\n",
      "Epoch 1817/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0469 - acc: 0.6574 - val_loss: 0.0672 - val_acc: 0.5295\n",
      "Epoch 1818/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0471 - acc: 0.6572 - val_loss: 0.0650 - val_acc: 0.5420\n",
      "Epoch 1819/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0463 - acc: 0.6621 - val_loss: 0.0655 - val_acc: 0.5403\n",
      "Epoch 1820/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0472 - acc: 0.6552 - val_loss: 0.0659 - val_acc: 0.5453\n",
      "Epoch 1821/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0463 - acc: 0.6663 - val_loss: 0.0670 - val_acc: 0.5395\n",
      "Epoch 1822/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0475 - acc: 0.6585 - val_loss: 0.0657 - val_acc: 0.5436\n",
      "Epoch 1823/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0464 - acc: 0.6619 - val_loss: 0.0659 - val_acc: 0.5403\n",
      "Epoch 1824/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0466 - acc: 0.6608 - val_loss: 0.0659 - val_acc: 0.5328\n",
      "Epoch 1825/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6535 - val_loss: 0.0657 - val_acc: 0.5436\n",
      "Epoch 1826/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0471 - acc: 0.6566 - val_loss: 0.0666 - val_acc: 0.5328\n",
      "Epoch 1827/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0470 - acc: 0.6594 - val_loss: 0.0663 - val_acc: 0.5428\n",
      "Epoch 1828/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0473 - acc: 0.6577 - val_loss: 0.0650 - val_acc: 0.5528\n",
      "Epoch 1829/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0470 - acc: 0.6563 - val_loss: 0.0660 - val_acc: 0.5395\n",
      "Epoch 1830/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0468 - acc: 0.6585 - val_loss: 0.0667 - val_acc: 0.5395\n",
      "Epoch 1831/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0462 - acc: 0.6646 - val_loss: 0.0666 - val_acc: 0.5387\n",
      "Epoch 1832/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0470 - acc: 0.6585 - val_loss: 0.0661 - val_acc: 0.5395\n",
      "Epoch 1833/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0474 - acc: 0.6505 - val_loss: 0.0657 - val_acc: 0.5353\n",
      "Epoch 1834/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0475 - acc: 0.6524 - val_loss: 0.0656 - val_acc: 0.5436\n",
      "Epoch 1835/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0470 - acc: 0.6602 - val_loss: 0.0657 - val_acc: 0.5428\n",
      "Epoch 1836/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6558 - val_loss: 0.0655 - val_acc: 0.5403\n",
      "Epoch 1837/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6624 - val_loss: 0.0654 - val_acc: 0.5495\n",
      "Epoch 1838/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0464 - acc: 0.6627 - val_loss: 0.0656 - val_acc: 0.5478\n",
      "Epoch 1839/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0461 - acc: 0.6660 - val_loss: 0.0662 - val_acc: 0.5370\n",
      "Epoch 1840/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0463 - acc: 0.6641 - val_loss: 0.0662 - val_acc: 0.5387\n",
      "Epoch 1841/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0464 - acc: 0.6646 - val_loss: 0.0661 - val_acc: 0.5520\n",
      "Epoch 1842/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0463 - acc: 0.6655 - val_loss: 0.0669 - val_acc: 0.5362\n",
      "Epoch 1843/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0465 - acc: 0.6619 - val_loss: 0.0657 - val_acc: 0.5445\n",
      "Epoch 1844/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0479 - acc: 0.6466 - val_loss: 0.0661 - val_acc: 0.5436\n",
      "Epoch 1845/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0480 - acc: 0.6513 - val_loss: 0.0659 - val_acc: 0.5378\n",
      "Epoch 1846/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0488 - acc: 0.6427 - val_loss: 0.0662 - val_acc: 0.5387\n",
      "Epoch 1847/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0480 - acc: 0.6486 - val_loss: 0.0650 - val_acc: 0.5503\n",
      "Epoch 1848/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0473 - acc: 0.6563 - val_loss: 0.0666 - val_acc: 0.5362\n",
      "Epoch 1849/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0470 - acc: 0.6533 - val_loss: 0.0662 - val_acc: 0.5370\n",
      "Epoch 1850/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0474 - acc: 0.6524 - val_loss: 0.0663 - val_acc: 0.5362\n",
      "Epoch 1851/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0479 - acc: 0.6530 - val_loss: 0.0668 - val_acc: 0.5387\n",
      "Epoch 1852/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0473 - acc: 0.6547 - val_loss: 0.0663 - val_acc: 0.5420\n",
      "Epoch 1853/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6602 - val_loss: 0.0667 - val_acc: 0.5362\n",
      "Epoch 1854/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0467 - acc: 0.6588 - val_loss: 0.0660 - val_acc: 0.5378\n",
      "Epoch 1855/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0475 - acc: 0.6558 - val_loss: 0.0666 - val_acc: 0.5428\n",
      "Epoch 1856/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0480 - acc: 0.6530 - val_loss: 0.0655 - val_acc: 0.5436\n",
      "Epoch 1857/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0470 - acc: 0.6588 - val_loss: 0.0660 - val_acc: 0.5461\n",
      "Epoch 1858/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0466 - acc: 0.6624 - val_loss: 0.0668 - val_acc: 0.5295\n",
      "Epoch 1859/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0466 - acc: 0.6585 - val_loss: 0.0670 - val_acc: 0.5212\n",
      "Epoch 1860/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0468 - acc: 0.6580 - val_loss: 0.0662 - val_acc: 0.5436\n",
      "Epoch 1861/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0466 - acc: 0.6596 - val_loss: 0.0657 - val_acc: 0.5436\n",
      "Epoch 1862/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0466 - acc: 0.6599 - val_loss: 0.0658 - val_acc: 0.5353\n",
      "Epoch 1863/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0466 - acc: 0.6605 - val_loss: 0.0672 - val_acc: 0.5287\n",
      "Epoch 1864/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0465 - acc: 0.6630 - val_loss: 0.0656 - val_acc: 0.5470\n",
      "Epoch 1865/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0467 - acc: 0.6630 - val_loss: 0.0660 - val_acc: 0.5453\n",
      "Epoch 1866/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0465 - acc: 0.6632 - val_loss: 0.0667 - val_acc: 0.5337\n",
      "Epoch 1867/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0465 - acc: 0.6627 - val_loss: 0.0665 - val_acc: 0.5395\n",
      "Epoch 1868/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0466 - acc: 0.6638 - val_loss: 0.0655 - val_acc: 0.5511\n",
      "Epoch 1869/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0465 - acc: 0.6608 - val_loss: 0.0666 - val_acc: 0.5436\n",
      "Epoch 1870/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0470 - acc: 0.6563 - val_loss: 0.0662 - val_acc: 0.5362\n",
      "Epoch 1871/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0468 - acc: 0.6608 - val_loss: 0.0666 - val_acc: 0.5345\n",
      "Epoch 1872/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0463 - acc: 0.6627 - val_loss: 0.0657 - val_acc: 0.5428\n",
      "Epoch 1873/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0469 - acc: 0.6596 - val_loss: 0.0659 - val_acc: 0.5478\n",
      "Epoch 1874/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0467 - acc: 0.6569 - val_loss: 0.0668 - val_acc: 0.5420\n",
      "Epoch 1875/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0476 - acc: 0.6555 - val_loss: 0.0655 - val_acc: 0.5453\n",
      "Epoch 1876/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0470 - acc: 0.6616 - val_loss: 0.0662 - val_acc: 0.5420\n",
      "Epoch 1877/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0469 - acc: 0.6566 - val_loss: 0.0656 - val_acc: 0.5503\n",
      "Epoch 1878/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0471 - acc: 0.6527 - val_loss: 0.0673 - val_acc: 0.5378\n",
      "Epoch 1879/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0499 - acc: 0.6389 - val_loss: 0.0689 - val_acc: 0.5187\n",
      "Epoch 1880/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0486 - acc: 0.6452 - val_loss: 0.0657 - val_acc: 0.5445\n",
      "Epoch 1881/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0471 - acc: 0.6563 - val_loss: 0.0667 - val_acc: 0.5345\n",
      "Epoch 1882/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0469 - acc: 0.6566 - val_loss: 0.0657 - val_acc: 0.5411\n",
      "Epoch 1883/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0474 - acc: 0.6552 - val_loss: 0.0659 - val_acc: 0.5520\n",
      "Epoch 1884/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0467 - acc: 0.6605 - val_loss: 0.0657 - val_acc: 0.5445\n",
      "Epoch 1885/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0471 - acc: 0.6563 - val_loss: 0.0669 - val_acc: 0.5403\n",
      "Epoch 1886/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0465 - acc: 0.6569 - val_loss: 0.0667 - val_acc: 0.5287\n",
      "Epoch 1887/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6535 - val_loss: 0.0662 - val_acc: 0.5370\n",
      "Epoch 1888/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0466 - acc: 0.6585 - val_loss: 0.0671 - val_acc: 0.5245\n",
      "Epoch 1889/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0463 - acc: 0.6657 - val_loss: 0.0664 - val_acc: 0.5362\n",
      "Epoch 1890/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0462 - acc: 0.6680 - val_loss: 0.0659 - val_acc: 0.5378\n",
      "Epoch 1891/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0461 - acc: 0.6644 - val_loss: 0.0659 - val_acc: 0.5420\n",
      "Epoch 1892/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6599 - val_loss: 0.0663 - val_acc: 0.5420\n",
      "Epoch 1893/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0465 - acc: 0.6596 - val_loss: 0.0667 - val_acc: 0.5362\n",
      "Epoch 1894/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0462 - acc: 0.6663 - val_loss: 0.0660 - val_acc: 0.5387\n",
      "Epoch 1895/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0465 - acc: 0.6627 - val_loss: 0.0676 - val_acc: 0.5320\n",
      "Epoch 1896/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0463 - acc: 0.6627 - val_loss: 0.0667 - val_acc: 0.5395\n",
      "Epoch 1897/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0468 - acc: 0.6574 - val_loss: 0.0665 - val_acc: 0.5370\n",
      "Epoch 1898/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6547 - val_loss: 0.0667 - val_acc: 0.5378\n",
      "Epoch 1899/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0463 - acc: 0.6646 - val_loss: 0.0671 - val_acc: 0.5362\n",
      "Epoch 1900/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0465 - acc: 0.6652 - val_loss: 0.0666 - val_acc: 0.5436\n",
      "Epoch 1901/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0465 - acc: 0.6638 - val_loss: 0.0660 - val_acc: 0.5378\n",
      "Epoch 1902/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0469 - acc: 0.6541 - val_loss: 0.0653 - val_acc: 0.5511\n",
      "Epoch 1903/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0465 - acc: 0.6610 - val_loss: 0.0676 - val_acc: 0.5362\n",
      "Epoch 1904/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0465 - acc: 0.6674 - val_loss: 0.0666 - val_acc: 0.5395\n",
      "Epoch 1905/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0464 - acc: 0.6624 - val_loss: 0.0666 - val_acc: 0.5420\n",
      "Epoch 1906/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0489 - acc: 0.6438 - val_loss: 0.0672 - val_acc: 0.5395\n",
      "Epoch 1907/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0476 - acc: 0.6555 - val_loss: 0.0672 - val_acc: 0.5387\n",
      "Epoch 1908/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0465 - acc: 0.6685 - val_loss: 0.0669 - val_acc: 0.5295\n",
      "Epoch 1909/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6599 - val_loss: 0.0675 - val_acc: 0.5295\n",
      "Epoch 1910/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6641 - val_loss: 0.0668 - val_acc: 0.5378\n",
      "Epoch 1911/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0471 - acc: 0.6580 - val_loss: 0.0657 - val_acc: 0.5312\n",
      "Epoch 1912/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0469 - acc: 0.6608 - val_loss: 0.0665 - val_acc: 0.5461\n",
      "Epoch 1913/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0472 - acc: 0.6608 - val_loss: 0.0681 - val_acc: 0.5162\n",
      "Epoch 1914/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.6599 - val_loss: 0.0670 - val_acc: 0.5362\n",
      "Epoch 1915/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6649 - val_loss: 0.0667 - val_acc: 0.5312\n",
      "Epoch 1916/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6580 - val_loss: 0.0665 - val_acc: 0.5320\n",
      "Epoch 1917/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0466 - acc: 0.6657 - val_loss: 0.0666 - val_acc: 0.5420\n",
      "Epoch 1918/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0468 - acc: 0.6591 - val_loss: 0.0670 - val_acc: 0.5295\n",
      "Epoch 1919/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6541 - val_loss: 0.0665 - val_acc: 0.5320\n",
      "Epoch 1920/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0465 - acc: 0.6644 - val_loss: 0.0663 - val_acc: 0.5362\n",
      "Epoch 1921/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0480 - acc: 0.6524 - val_loss: 0.0676 - val_acc: 0.5362\n",
      "Epoch 1922/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0470 - acc: 0.6569 - val_loss: 0.0669 - val_acc: 0.5387\n",
      "Epoch 1923/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6630 - val_loss: 0.0672 - val_acc: 0.5345\n",
      "Epoch 1924/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6519 - val_loss: 0.0661 - val_acc: 0.5411\n",
      "Epoch 1925/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0465 - acc: 0.6608 - val_loss: 0.0674 - val_acc: 0.5162\n",
      "Epoch 1926/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0464 - acc: 0.6591 - val_loss: 0.0660 - val_acc: 0.5378\n",
      "Epoch 1927/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0470 - acc: 0.6638 - val_loss: 0.0673 - val_acc: 0.5270\n",
      "Epoch 1928/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0471 - acc: 0.6544 - val_loss: 0.0656 - val_acc: 0.5420\n",
      "Epoch 1929/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0477 - acc: 0.6530 - val_loss: 0.0666 - val_acc: 0.5353\n",
      "Epoch 1930/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0470 - acc: 0.6588 - val_loss: 0.0668 - val_acc: 0.5370\n",
      "Epoch 1931/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0466 - acc: 0.6660 - val_loss: 0.0662 - val_acc: 0.5395\n",
      "Epoch 1932/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0486 - acc: 0.6463 - val_loss: 0.0668 - val_acc: 0.5387\n",
      "Epoch 1933/3000\n",
      "3608/3608 [==============================] - 1s 201us/step - loss: 0.0481 - acc: 0.6488 - val_loss: 0.0660 - val_acc: 0.5428\n",
      "Epoch 1934/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0467 - acc: 0.6627 - val_loss: 0.0659 - val_acc: 0.5428\n",
      "Epoch 1935/3000\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0467 - acc: 0.6602 - val_loss: 0.0657 - val_acc: 0.5436\n",
      "Epoch 1936/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0460 - acc: 0.6619 - val_loss: 0.0675 - val_acc: 0.5312\n",
      "Epoch 1937/3000\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0469 - acc: 0.6627 - val_loss: 0.0661 - val_acc: 0.5436\n",
      "Epoch 1938/3000\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0465 - acc: 0.6644 - val_loss: 0.0669 - val_acc: 0.5295\n",
      "Epoch 1939/3000\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0464 - acc: 0.6608 - val_loss: 0.0684 - val_acc: 0.5229\n",
      "Epoch 1940/3000\n",
      "3608/3608 [==============================] - 1s 218us/step - loss: 0.0464 - acc: 0.6638 - val_loss: 0.0664 - val_acc: 0.5387\n",
      "Epoch 1941/3000\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0465 - acc: 0.6610 - val_loss: 0.0664 - val_acc: 0.5428\n",
      "Epoch 1942/3000\n",
      "3608/3608 [==============================] - 1s 220us/step - loss: 0.0468 - acc: 0.6583 - val_loss: 0.0666 - val_acc: 0.5362\n",
      "Epoch 1943/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0470 - acc: 0.6627 - val_loss: 0.0675 - val_acc: 0.5328\n",
      "Epoch 1944/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0465 - acc: 0.6646 - val_loss: 0.0664 - val_acc: 0.5353\n",
      "Epoch 1945/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0463 - acc: 0.6677 - val_loss: 0.0659 - val_acc: 0.5428\n",
      "Epoch 1946/3000\n",
      "3608/3608 [==============================] - 1s 188us/step - loss: 0.0479 - acc: 0.6527 - val_loss: 0.0659 - val_acc: 0.5420\n",
      "Epoch 1947/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0470 - acc: 0.6599 - val_loss: 0.0669 - val_acc: 0.5337\n",
      "Epoch 1948/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0468 - acc: 0.6599 - val_loss: 0.0678 - val_acc: 0.5254\n",
      "Epoch 1949/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0476 - acc: 0.6527 - val_loss: 0.0668 - val_acc: 0.5478\n",
      "Epoch 1950/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0479 - acc: 0.6524 - val_loss: 0.0662 - val_acc: 0.5345\n",
      "Epoch 1951/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0470 - acc: 0.6541 - val_loss: 0.0669 - val_acc: 0.5353\n",
      "Epoch 1952/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0466 - acc: 0.6619 - val_loss: 0.0657 - val_acc: 0.5478\n",
      "Epoch 1953/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6610 - val_loss: 0.0662 - val_acc: 0.5387\n",
      "Epoch 1954/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0471 - acc: 0.6580 - val_loss: 0.0661 - val_acc: 0.5403\n",
      "Epoch 1955/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0471 - acc: 0.6594 - val_loss: 0.0670 - val_acc: 0.5411\n",
      "Epoch 1956/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0466 - acc: 0.6594 - val_loss: 0.0672 - val_acc: 0.5370\n",
      "Epoch 1957/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6596 - val_loss: 0.0670 - val_acc: 0.5370\n",
      "Epoch 1958/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0462 - acc: 0.6577 - val_loss: 0.0675 - val_acc: 0.5436\n",
      "Epoch 1959/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0460 - acc: 0.6630 - val_loss: 0.0671 - val_acc: 0.5370\n",
      "Epoch 1960/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0468 - acc: 0.6596 - val_loss: 0.0666 - val_acc: 0.5295\n",
      "Epoch 1961/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0468 - acc: 0.6560 - val_loss: 0.0668 - val_acc: 0.5312\n",
      "Epoch 1962/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0466 - acc: 0.6630 - val_loss: 0.0661 - val_acc: 0.5445\n",
      "Epoch 1963/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0465 - acc: 0.6577 - val_loss: 0.0663 - val_acc: 0.5445\n",
      "Epoch 1964/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6572 - val_loss: 0.0663 - val_acc: 0.5387\n",
      "Epoch 1965/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0463 - acc: 0.6602 - val_loss: 0.0670 - val_acc: 0.5387\n",
      "Epoch 1966/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0470 - acc: 0.6602 - val_loss: 0.0656 - val_acc: 0.5495\n",
      "Epoch 1967/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0463 - acc: 0.6627 - val_loss: 0.0665 - val_acc: 0.5445\n",
      "Epoch 1968/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0468 - acc: 0.6605 - val_loss: 0.0664 - val_acc: 0.5453\n",
      "Epoch 1969/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6585 - val_loss: 0.0678 - val_acc: 0.5395\n",
      "Epoch 1970/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0468 - acc: 0.6610 - val_loss: 0.0666 - val_acc: 0.5453\n",
      "Epoch 1971/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6632 - val_loss: 0.0664 - val_acc: 0.5370\n",
      "Epoch 1972/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6605 - val_loss: 0.0661 - val_acc: 0.5486\n",
      "Epoch 1973/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6547 - val_loss: 0.0672 - val_acc: 0.5403\n",
      "Epoch 1974/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0464 - acc: 0.6627 - val_loss: 0.0673 - val_acc: 0.5337\n",
      "Epoch 1975/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6638 - val_loss: 0.0665 - val_acc: 0.5362\n",
      "Epoch 1976/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6605 - val_loss: 0.0679 - val_acc: 0.5337\n",
      "Epoch 1977/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6583 - val_loss: 0.0660 - val_acc: 0.5478\n",
      "Epoch 1978/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6583 - val_loss: 0.0666 - val_acc: 0.5353\n",
      "Epoch 1979/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6491 - val_loss: 0.0657 - val_acc: 0.5486\n",
      "Epoch 1980/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0468 - acc: 0.6619 - val_loss: 0.0667 - val_acc: 0.5320\n",
      "Epoch 1981/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6591 - val_loss: 0.0659 - val_acc: 0.5353\n",
      "Epoch 1982/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6610 - val_loss: 0.0659 - val_acc: 0.5395\n",
      "Epoch 1983/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6580 - val_loss: 0.0666 - val_acc: 0.5328\n",
      "Epoch 1984/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6505 - val_loss: 0.0655 - val_acc: 0.5503\n",
      "Epoch 1985/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6632 - val_loss: 0.0660 - val_acc: 0.5387\n",
      "Epoch 1986/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6588 - val_loss: 0.0656 - val_acc: 0.5445\n",
      "Epoch 1987/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0468 - acc: 0.6583 - val_loss: 0.0662 - val_acc: 0.5486\n",
      "Epoch 1988/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6580 - val_loss: 0.0665 - val_acc: 0.5411\n",
      "Epoch 1989/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0460 - acc: 0.6663 - val_loss: 0.0669 - val_acc: 0.5411\n",
      "Epoch 1990/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0461 - acc: 0.6627 - val_loss: 0.0673 - val_acc: 0.5303\n",
      "Epoch 1991/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0474 - acc: 0.6544 - val_loss: 0.0662 - val_acc: 0.5387\n",
      "Epoch 1992/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0461 - acc: 0.6655 - val_loss: 0.0670 - val_acc: 0.5287\n",
      "Epoch 1993/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0464 - acc: 0.6621 - val_loss: 0.0669 - val_acc: 0.5337\n",
      "Epoch 1994/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0460 - acc: 0.6641 - val_loss: 0.0672 - val_acc: 0.5362\n",
      "Epoch 1995/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6569 - val_loss: 0.0667 - val_acc: 0.5362\n",
      "Epoch 1996/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6572 - val_loss: 0.0660 - val_acc: 0.5470\n",
      "Epoch 1997/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0469 - acc: 0.6632 - val_loss: 0.0667 - val_acc: 0.5370\n",
      "Epoch 1998/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0463 - acc: 0.6608 - val_loss: 0.0658 - val_acc: 0.5453\n",
      "Epoch 1999/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.6513 - val_loss: 0.0665 - val_acc: 0.5395\n",
      "Epoch 2000/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6541 - val_loss: 0.0660 - val_acc: 0.5478\n",
      "Epoch 2001/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6621 - val_loss: 0.0680 - val_acc: 0.5303\n",
      "Epoch 2002/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0467 - acc: 0.6605 - val_loss: 0.0667 - val_acc: 0.5387\n",
      "Epoch 2003/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0463 - acc: 0.6663 - val_loss: 0.0661 - val_acc: 0.5362\n",
      "Epoch 2004/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6588 - val_loss: 0.0662 - val_acc: 0.5436\n",
      "Epoch 2005/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0467 - acc: 0.6644 - val_loss: 0.0657 - val_acc: 0.5470\n",
      "Epoch 2006/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0461 - acc: 0.6646 - val_loss: 0.0663 - val_acc: 0.5411\n",
      "Epoch 2007/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0463 - acc: 0.6627 - val_loss: 0.0674 - val_acc: 0.5278\n",
      "Epoch 2008/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6613 - val_loss: 0.0669 - val_acc: 0.5403\n",
      "Epoch 2009/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6630 - val_loss: 0.0674 - val_acc: 0.5320\n",
      "Epoch 2010/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6616 - val_loss: 0.0674 - val_acc: 0.5345\n",
      "Epoch 2011/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0467 - acc: 0.6632 - val_loss: 0.0672 - val_acc: 0.5287\n",
      "Epoch 2012/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0462 - acc: 0.6621 - val_loss: 0.0665 - val_acc: 0.5353\n",
      "Epoch 2013/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0460 - acc: 0.6638 - val_loss: 0.0675 - val_acc: 0.5278\n",
      "Epoch 2014/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6624 - val_loss: 0.0658 - val_acc: 0.5478\n",
      "Epoch 2015/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0463 - acc: 0.6660 - val_loss: 0.0670 - val_acc: 0.5362\n",
      "Epoch 2016/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0470 - acc: 0.6605 - val_loss: 0.0680 - val_acc: 0.5204\n",
      "Epoch 2017/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6638 - val_loss: 0.0677 - val_acc: 0.5262\n",
      "Epoch 2018/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0465 - acc: 0.6591 - val_loss: 0.0673 - val_acc: 0.5237\n",
      "Epoch 2019/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0513 - acc: 0.6283 - val_loss: 0.0682 - val_acc: 0.5337\n",
      "Epoch 2020/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0481 - acc: 0.6516 - val_loss: 0.0661 - val_acc: 0.5420\n",
      "Epoch 2021/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0469 - acc: 0.6566 - val_loss: 0.0664 - val_acc: 0.5387\n",
      "Epoch 2022/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0466 - acc: 0.6605 - val_loss: 0.0664 - val_acc: 0.5353\n",
      "Epoch 2023/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0468 - acc: 0.6616 - val_loss: 0.0670 - val_acc: 0.5320\n",
      "Epoch 2024/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0463 - acc: 0.6610 - val_loss: 0.0657 - val_acc: 0.5411\n",
      "Epoch 2025/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0464 - acc: 0.6627 - val_loss: 0.0666 - val_acc: 0.5353\n",
      "Epoch 2026/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0468 - acc: 0.6585 - val_loss: 0.0663 - val_acc: 0.5328\n",
      "Epoch 2027/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0471 - acc: 0.6549 - val_loss: 0.0679 - val_acc: 0.5337\n",
      "Epoch 2028/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0470 - acc: 0.6560 - val_loss: 0.0662 - val_acc: 0.5478\n",
      "Epoch 2029/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0465 - acc: 0.6696 - val_loss: 0.0668 - val_acc: 0.5403\n",
      "Epoch 2030/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0460 - acc: 0.6671 - val_loss: 0.0666 - val_acc: 0.5395\n",
      "Epoch 2031/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0461 - acc: 0.6627 - val_loss: 0.0671 - val_acc: 0.5387\n",
      "Epoch 2032/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6641 - val_loss: 0.0664 - val_acc: 0.5470\n",
      "Epoch 2033/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0462 - acc: 0.6616 - val_loss: 0.0675 - val_acc: 0.5345\n",
      "Epoch 2034/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0474 - acc: 0.6560 - val_loss: 0.0653 - val_acc: 0.5445\n",
      "Epoch 2035/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0464 - acc: 0.6583 - val_loss: 0.0670 - val_acc: 0.5370\n",
      "Epoch 2036/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0470 - acc: 0.6610 - val_loss: 0.0675 - val_acc: 0.5370\n",
      "Epoch 2037/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6569 - val_loss: 0.0669 - val_acc: 0.5403\n",
      "Epoch 2038/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0471 - acc: 0.6591 - val_loss: 0.0671 - val_acc: 0.5495\n",
      "Epoch 2039/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0486 - acc: 0.6414 - val_loss: 0.0665 - val_acc: 0.5353\n",
      "Epoch 2040/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0473 - acc: 0.6583 - val_loss: 0.0658 - val_acc: 0.5370\n",
      "Epoch 2041/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6619 - val_loss: 0.0674 - val_acc: 0.5337\n",
      "Epoch 2042/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0469 - acc: 0.6574 - val_loss: 0.0665 - val_acc: 0.5478\n",
      "Epoch 2043/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0469 - acc: 0.6588 - val_loss: 0.0668 - val_acc: 0.5378\n",
      "Epoch 2044/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6621 - val_loss: 0.0662 - val_acc: 0.5428\n",
      "Epoch 2045/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6649 - val_loss: 0.0662 - val_acc: 0.5461\n",
      "Epoch 2046/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0495 - acc: 0.6419 - val_loss: 0.0674 - val_acc: 0.5395\n",
      "Epoch 2047/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0478 - acc: 0.6530 - val_loss: 0.0667 - val_acc: 0.5411\n",
      "Epoch 2048/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0476 - acc: 0.6494 - val_loss: 0.0664 - val_acc: 0.5362\n",
      "Epoch 2049/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0470 - acc: 0.6580 - val_loss: 0.0666 - val_acc: 0.5362\n",
      "Epoch 2050/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0466 - acc: 0.6632 - val_loss: 0.0663 - val_acc: 0.5378\n",
      "Epoch 2051/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6655 - val_loss: 0.0662 - val_acc: 0.5362\n",
      "Epoch 2052/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0474 - acc: 0.6574 - val_loss: 0.0675 - val_acc: 0.5345\n",
      "Epoch 2053/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0467 - acc: 0.6616 - val_loss: 0.0669 - val_acc: 0.5420\n",
      "Epoch 2054/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0462 - acc: 0.6660 - val_loss: 0.0672 - val_acc: 0.5362\n",
      "Epoch 2055/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0463 - acc: 0.6671 - val_loss: 0.0673 - val_acc: 0.5453\n",
      "Epoch 2056/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6585 - val_loss: 0.0661 - val_acc: 0.5428\n",
      "Epoch 2057/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0467 - acc: 0.6566 - val_loss: 0.0662 - val_acc: 0.5453\n",
      "Epoch 2058/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0465 - acc: 0.6666 - val_loss: 0.0674 - val_acc: 0.5328\n",
      "Epoch 2059/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6555 - val_loss: 0.0662 - val_acc: 0.5511\n",
      "Epoch 2060/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0471 - acc: 0.6558 - val_loss: 0.0665 - val_acc: 0.5420\n",
      "Epoch 2061/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6624 - val_loss: 0.0658 - val_acc: 0.5445\n",
      "Epoch 2062/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0469 - acc: 0.6588 - val_loss: 0.0667 - val_acc: 0.5378\n",
      "Epoch 2063/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6566 - val_loss: 0.0670 - val_acc: 0.5403\n",
      "Epoch 2064/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6613 - val_loss: 0.0670 - val_acc: 0.5320\n",
      "Epoch 2065/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6599 - val_loss: 0.0673 - val_acc: 0.5378\n",
      "Epoch 2066/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0463 - acc: 0.6610 - val_loss: 0.0670 - val_acc: 0.5353\n",
      "Epoch 2067/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6619 - val_loss: 0.0668 - val_acc: 0.5436\n",
      "Epoch 2068/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0465 - acc: 0.6652 - val_loss: 0.0685 - val_acc: 0.5237\n",
      "Epoch 2069/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0471 - acc: 0.6535 - val_loss: 0.0667 - val_acc: 0.5320\n",
      "Epoch 2070/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6641 - val_loss: 0.0668 - val_acc: 0.5378\n",
      "Epoch 2071/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6632 - val_loss: 0.0672 - val_acc: 0.5345\n",
      "Epoch 2072/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0465 - acc: 0.6602 - val_loss: 0.0672 - val_acc: 0.5370\n",
      "Epoch 2073/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6596 - val_loss: 0.0665 - val_acc: 0.5353\n",
      "Epoch 2074/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6583 - val_loss: 0.0671 - val_acc: 0.5370\n",
      "Epoch 2075/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0474 - acc: 0.6569 - val_loss: 0.0705 - val_acc: 0.5154\n",
      "Epoch 2076/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0489 - acc: 0.6475 - val_loss: 0.0669 - val_acc: 0.5387\n",
      "Epoch 2077/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0472 - acc: 0.6566 - val_loss: 0.0666 - val_acc: 0.5370\n",
      "Epoch 2078/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0475 - acc: 0.6552 - val_loss: 0.0675 - val_acc: 0.5362\n",
      "Epoch 2079/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0472 - acc: 0.6596 - val_loss: 0.0667 - val_acc: 0.5387\n",
      "Epoch 2080/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0476 - acc: 0.6552 - val_loss: 0.0670 - val_acc: 0.5387\n",
      "Epoch 2081/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0472 - acc: 0.6549 - val_loss: 0.0669 - val_acc: 0.5387\n",
      "Epoch 2082/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0461 - acc: 0.6666 - val_loss: 0.0667 - val_acc: 0.5362\n",
      "Epoch 2083/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0462 - acc: 0.6610 - val_loss: 0.0659 - val_acc: 0.5303\n",
      "Epoch 2084/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0465 - acc: 0.6619 - val_loss: 0.0664 - val_acc: 0.5337\n",
      "Epoch 2085/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0461 - acc: 0.6627 - val_loss: 0.0663 - val_acc: 0.5428\n",
      "Epoch 2086/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6638 - val_loss: 0.0666 - val_acc: 0.5378\n",
      "Epoch 2087/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6588 - val_loss: 0.0696 - val_acc: 0.5154\n",
      "Epoch 2088/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0472 - acc: 0.6555 - val_loss: 0.0682 - val_acc: 0.5204\n",
      "Epoch 2089/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0464 - acc: 0.6641 - val_loss: 0.0667 - val_acc: 0.5312\n",
      "Epoch 2090/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0472 - acc: 0.6602 - val_loss: 0.0670 - val_acc: 0.5478\n",
      "Epoch 2091/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6585 - val_loss: 0.0684 - val_acc: 0.5312\n",
      "Epoch 2092/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0471 - acc: 0.6572 - val_loss: 0.0674 - val_acc: 0.5353\n",
      "Epoch 2093/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0488 - acc: 0.6450 - val_loss: 0.0674 - val_acc: 0.5320\n",
      "Epoch 2094/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0472 - acc: 0.6544 - val_loss: 0.0668 - val_acc: 0.5370\n",
      "Epoch 2095/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0469 - acc: 0.6610 - val_loss: 0.0687 - val_acc: 0.5195\n",
      "Epoch 2096/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0477 - acc: 0.6566 - val_loss: 0.0664 - val_acc: 0.5411\n",
      "Epoch 2097/3000\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0477 - acc: 0.6530 - val_loss: 0.0672 - val_acc: 0.5436\n",
      "Epoch 2098/3000\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0480 - acc: 0.6469 - val_loss: 0.0662 - val_acc: 0.5362\n",
      "Epoch 2099/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0466 - acc: 0.6630 - val_loss: 0.0666 - val_acc: 0.5403\n",
      "Epoch 2100/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0470 - acc: 0.6610 - val_loss: 0.0668 - val_acc: 0.5337\n",
      "Epoch 2101/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0465 - acc: 0.6644 - val_loss: 0.0670 - val_acc: 0.5395\n",
      "Epoch 2102/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6644 - val_loss: 0.0662 - val_acc: 0.5362\n",
      "Epoch 2103/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0466 - acc: 0.6627 - val_loss: 0.0680 - val_acc: 0.5295\n",
      "Epoch 2104/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6580 - val_loss: 0.0670 - val_acc: 0.5154\n",
      "Epoch 2105/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0469 - acc: 0.6652 - val_loss: 0.0665 - val_acc: 0.5445\n",
      "Epoch 2106/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0464 - acc: 0.6655 - val_loss: 0.0664 - val_acc: 0.5395\n",
      "Epoch 2107/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6588 - val_loss: 0.0665 - val_acc: 0.5403\n",
      "Epoch 2108/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0465 - acc: 0.6613 - val_loss: 0.0664 - val_acc: 0.5411\n",
      "Epoch 2109/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0466 - acc: 0.6613 - val_loss: 0.0660 - val_acc: 0.5378\n",
      "Epoch 2110/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0468 - acc: 0.6594 - val_loss: 0.0665 - val_acc: 0.5420\n",
      "Epoch 2111/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0464 - acc: 0.6602 - val_loss: 0.0663 - val_acc: 0.5328\n",
      "Epoch 2112/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0470 - acc: 0.6613 - val_loss: 0.0658 - val_acc: 0.5387\n",
      "Epoch 2113/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6574 - val_loss: 0.0659 - val_acc: 0.5503\n",
      "Epoch 2114/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0468 - acc: 0.6572 - val_loss: 0.0677 - val_acc: 0.5353\n",
      "Epoch 2115/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0465 - acc: 0.6627 - val_loss: 0.0677 - val_acc: 0.5362\n",
      "Epoch 2116/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0462 - acc: 0.6655 - val_loss: 0.0662 - val_acc: 0.5387\n",
      "Epoch 2117/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0466 - acc: 0.6627 - val_loss: 0.0662 - val_acc: 0.5370\n",
      "Epoch 2118/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0471 - acc: 0.6610 - val_loss: 0.0662 - val_acc: 0.5353\n",
      "Epoch 2119/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0465 - acc: 0.6624 - val_loss: 0.0662 - val_acc: 0.5445\n",
      "Epoch 2120/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0462 - acc: 0.6671 - val_loss: 0.0674 - val_acc: 0.5345\n",
      "Epoch 2121/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0472 - acc: 0.6591 - val_loss: 0.0683 - val_acc: 0.5287\n",
      "Epoch 2122/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0469 - acc: 0.6599 - val_loss: 0.0661 - val_acc: 0.5387\n",
      "Epoch 2123/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0482 - acc: 0.6527 - val_loss: 0.0659 - val_acc: 0.5420\n",
      "Epoch 2124/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0484 - acc: 0.6516 - val_loss: 0.0667 - val_acc: 0.5278\n",
      "Epoch 2125/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0476 - acc: 0.6594 - val_loss: 0.0661 - val_acc: 0.5436\n",
      "Epoch 2126/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0483 - acc: 0.6499 - val_loss: 0.0653 - val_acc: 0.5436\n",
      "Epoch 2127/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6566 - val_loss: 0.0665 - val_acc: 0.5453\n",
      "Epoch 2128/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0471 - acc: 0.6577 - val_loss: 0.0661 - val_acc: 0.5486\n",
      "Epoch 2129/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0468 - acc: 0.6602 - val_loss: 0.0663 - val_acc: 0.5387\n",
      "Epoch 2130/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6671 - val_loss: 0.0662 - val_acc: 0.5328\n",
      "Epoch 2131/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0463 - acc: 0.6641 - val_loss: 0.0659 - val_acc: 0.5420\n",
      "Epoch 2132/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0475 - acc: 0.6541 - val_loss: 0.0662 - val_acc: 0.5328\n",
      "Epoch 2133/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0466 - acc: 0.6621 - val_loss: 0.0663 - val_acc: 0.5337\n",
      "Epoch 2134/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6585 - val_loss: 0.0660 - val_acc: 0.5428\n",
      "Epoch 2135/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0465 - acc: 0.6616 - val_loss: 0.0655 - val_acc: 0.5511\n",
      "Epoch 2136/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0457 - acc: 0.6707 - val_loss: 0.0666 - val_acc: 0.5378\n",
      "Epoch 2137/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0466 - acc: 0.6608 - val_loss: 0.0668 - val_acc: 0.5378\n",
      "Epoch 2138/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0465 - acc: 0.6646 - val_loss: 0.0663 - val_acc: 0.5420\n",
      "Epoch 2139/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6644 - val_loss: 0.0665 - val_acc: 0.5453\n",
      "Epoch 2140/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0475 - acc: 0.6558 - val_loss: 0.0668 - val_acc: 0.5403\n",
      "Epoch 2141/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6558 - val_loss: 0.0662 - val_acc: 0.5461\n",
      "Epoch 2142/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0467 - acc: 0.6632 - val_loss: 0.0659 - val_acc: 0.5328\n",
      "Epoch 2143/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6613 - val_loss: 0.0660 - val_acc: 0.5436\n",
      "Epoch 2144/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0463 - acc: 0.6613 - val_loss: 0.0656 - val_acc: 0.5453\n",
      "Epoch 2145/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0473 - acc: 0.6560 - val_loss: 0.0668 - val_acc: 0.5345\n",
      "Epoch 2146/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6572 - val_loss: 0.0661 - val_acc: 0.5420\n",
      "Epoch 2147/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0464 - acc: 0.6619 - val_loss: 0.0672 - val_acc: 0.5353\n",
      "Epoch 2148/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0459 - acc: 0.6669 - val_loss: 0.0687 - val_acc: 0.5337\n",
      "Epoch 2149/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6635 - val_loss: 0.0661 - val_acc: 0.5470\n",
      "Epoch 2150/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0465 - acc: 0.6685 - val_loss: 0.0665 - val_acc: 0.5370\n",
      "Epoch 2151/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0463 - acc: 0.6641 - val_loss: 0.0669 - val_acc: 0.5436\n",
      "Epoch 2152/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0468 - acc: 0.6594 - val_loss: 0.0659 - val_acc: 0.5495\n",
      "Epoch 2153/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6641 - val_loss: 0.0662 - val_acc: 0.5436\n",
      "Epoch 2154/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6616 - val_loss: 0.0667 - val_acc: 0.5411\n",
      "Epoch 2155/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6613 - val_loss: 0.0662 - val_acc: 0.5478\n",
      "Epoch 2156/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0463 - acc: 0.6632 - val_loss: 0.0665 - val_acc: 0.5411\n",
      "Epoch 2157/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0469 - acc: 0.6569 - val_loss: 0.0666 - val_acc: 0.5387\n",
      "Epoch 2158/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6585 - val_loss: 0.0663 - val_acc: 0.5436\n",
      "Epoch 2159/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6544 - val_loss: 0.0670 - val_acc: 0.5511\n",
      "Epoch 2160/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0478 - acc: 0.6511 - val_loss: 0.0682 - val_acc: 0.5170\n",
      "Epoch 2161/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6632 - val_loss: 0.0661 - val_acc: 0.5486\n",
      "Epoch 2162/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6563 - val_loss: 0.0667 - val_acc: 0.5287\n",
      "Epoch 2163/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6621 - val_loss: 0.0654 - val_acc: 0.5461\n",
      "Epoch 2164/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6630 - val_loss: 0.0669 - val_acc: 0.5428\n",
      "Epoch 2165/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0459 - acc: 0.6691 - val_loss: 0.0664 - val_acc: 0.5378\n",
      "Epoch 2166/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0460 - acc: 0.6677 - val_loss: 0.0664 - val_acc: 0.5495\n",
      "Epoch 2167/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0466 - acc: 0.6627 - val_loss: 0.0670 - val_acc: 0.5378\n",
      "Epoch 2168/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0461 - acc: 0.6677 - val_loss: 0.0667 - val_acc: 0.5353\n",
      "Epoch 2169/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6630 - val_loss: 0.0667 - val_acc: 0.5453\n",
      "Epoch 2170/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0464 - acc: 0.6657 - val_loss: 0.0681 - val_acc: 0.5245\n",
      "Epoch 2171/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0469 - acc: 0.6624 - val_loss: 0.0674 - val_acc: 0.5428\n",
      "Epoch 2172/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0482 - acc: 0.6552 - val_loss: 0.0667 - val_acc: 0.5403\n",
      "Epoch 2173/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6596 - val_loss: 0.0679 - val_acc: 0.5345\n",
      "Epoch 2174/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0488 - acc: 0.6463 - val_loss: 0.0663 - val_acc: 0.5436\n",
      "Epoch 2175/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0469 - acc: 0.6627 - val_loss: 0.0669 - val_acc: 0.5378\n",
      "Epoch 2176/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0460 - acc: 0.6671 - val_loss: 0.0671 - val_acc: 0.5420\n",
      "Epoch 2177/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0459 - acc: 0.6674 - val_loss: 0.0667 - val_acc: 0.5403\n",
      "Epoch 2178/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0463 - acc: 0.6688 - val_loss: 0.0669 - val_acc: 0.5428\n",
      "Epoch 2179/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0464 - acc: 0.6613 - val_loss: 0.0668 - val_acc: 0.5478\n",
      "Epoch 2180/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6610 - val_loss: 0.0671 - val_acc: 0.5420\n",
      "Epoch 2181/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0491 - acc: 0.6458 - val_loss: 0.0662 - val_acc: 0.5387\n",
      "Epoch 2182/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0468 - acc: 0.6563 - val_loss: 0.0677 - val_acc: 0.5287\n",
      "Epoch 2183/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0477 - acc: 0.6574 - val_loss: 0.0660 - val_acc: 0.5445\n",
      "Epoch 2184/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0475 - acc: 0.6580 - val_loss: 0.0666 - val_acc: 0.5470\n",
      "Epoch 2185/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6663 - val_loss: 0.0667 - val_acc: 0.5328\n",
      "Epoch 2186/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0465 - acc: 0.6624 - val_loss: 0.0673 - val_acc: 0.5328\n",
      "Epoch 2187/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0464 - acc: 0.6608 - val_loss: 0.0668 - val_acc: 0.5411\n",
      "Epoch 2188/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0467 - acc: 0.6649 - val_loss: 0.0672 - val_acc: 0.5420\n",
      "Epoch 2189/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6635 - val_loss: 0.0667 - val_acc: 0.5470\n",
      "Epoch 2190/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0476 - acc: 0.6527 - val_loss: 0.0665 - val_acc: 0.5436\n",
      "Epoch 2191/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0465 - acc: 0.6649 - val_loss: 0.0666 - val_acc: 0.5387\n",
      "Epoch 2192/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6602 - val_loss: 0.0663 - val_acc: 0.5486\n",
      "Epoch 2193/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0468 - acc: 0.6619 - val_loss: 0.0671 - val_acc: 0.5303\n",
      "Epoch 2194/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0494 - acc: 0.6433 - val_loss: 0.0657 - val_acc: 0.5486\n",
      "Epoch 2195/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0483 - acc: 0.6488 - val_loss: 0.0664 - val_acc: 0.5395\n",
      "Epoch 2196/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0469 - acc: 0.6583 - val_loss: 0.0666 - val_acc: 0.5395\n",
      "Epoch 2197/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0463 - acc: 0.6657 - val_loss: 0.0668 - val_acc: 0.5362\n",
      "Epoch 2198/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0462 - acc: 0.6669 - val_loss: 0.0669 - val_acc: 0.5403\n",
      "Epoch 2199/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0470 - acc: 0.6671 - val_loss: 0.0666 - val_acc: 0.5461\n",
      "Epoch 2200/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0477 - acc: 0.6583 - val_loss: 0.0670 - val_acc: 0.5362\n",
      "Epoch 2201/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6547 - val_loss: 0.0662 - val_acc: 0.5470\n",
      "Epoch 2202/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6563 - val_loss: 0.0663 - val_acc: 0.5370\n",
      "Epoch 2203/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.6599 - val_loss: 0.0674 - val_acc: 0.5237\n",
      "Epoch 2204/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0465 - acc: 0.6616 - val_loss: 0.0654 - val_acc: 0.5486\n",
      "Epoch 2205/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0469 - acc: 0.6627 - val_loss: 0.0669 - val_acc: 0.5337\n",
      "Epoch 2206/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0468 - acc: 0.6599 - val_loss: 0.0668 - val_acc: 0.5362\n",
      "Epoch 2207/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6596 - val_loss: 0.0670 - val_acc: 0.5287\n",
      "Epoch 2208/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0470 - acc: 0.6596 - val_loss: 0.0669 - val_acc: 0.5328\n",
      "Epoch 2209/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0463 - acc: 0.6630 - val_loss: 0.0661 - val_acc: 0.5461\n",
      "Epoch 2210/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0464 - acc: 0.6630 - val_loss: 0.0666 - val_acc: 0.5453\n",
      "Epoch 2211/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6602 - val_loss: 0.0677 - val_acc: 0.5428\n",
      "Epoch 2212/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6572 - val_loss: 0.0677 - val_acc: 0.5287\n",
      "Epoch 2213/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0468 - acc: 0.6558 - val_loss: 0.0666 - val_acc: 0.5436\n",
      "Epoch 2214/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0463 - acc: 0.6649 - val_loss: 0.0677 - val_acc: 0.5362\n",
      "Epoch 2215/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0466 - acc: 0.6635 - val_loss: 0.0669 - val_acc: 0.5403\n",
      "Epoch 2216/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0469 - acc: 0.6632 - val_loss: 0.0674 - val_acc: 0.5320\n",
      "Epoch 2217/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6613 - val_loss: 0.0679 - val_acc: 0.5328\n",
      "Epoch 2218/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0462 - acc: 0.6652 - val_loss: 0.0667 - val_acc: 0.5411\n",
      "Epoch 2219/3000\n",
      "3608/3608 [==============================] - 1s 200us/step - loss: 0.0466 - acc: 0.6594 - val_loss: 0.0672 - val_acc: 0.5328\n",
      "Epoch 2220/3000\n",
      "3608/3608 [==============================] - 1s 219us/step - loss: 0.0465 - acc: 0.6621 - val_loss: 0.0675 - val_acc: 0.5287\n",
      "Epoch 2221/3000\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0471 - acc: 0.6599 - val_loss: 0.0662 - val_acc: 0.5428\n",
      "Epoch 2222/3000\n",
      "3608/3608 [==============================] - 1s 218us/step - loss: 0.0464 - acc: 0.6646 - val_loss: 0.0671 - val_acc: 0.5395\n",
      "Epoch 2223/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0468 - acc: 0.6624 - val_loss: 0.0669 - val_acc: 0.5353\n",
      "Epoch 2224/3000\n",
      "3608/3608 [==============================] - 1s 218us/step - loss: 0.0464 - acc: 0.6674 - val_loss: 0.0671 - val_acc: 0.5337\n",
      "Epoch 2225/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0472 - acc: 0.6635 - val_loss: 0.0669 - val_acc: 0.5486\n",
      "Epoch 2226/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0465 - acc: 0.6638 - val_loss: 0.0669 - val_acc: 0.5387\n",
      "Epoch 2227/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0466 - acc: 0.6657 - val_loss: 0.0670 - val_acc: 0.5328\n",
      "Epoch 2228/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0468 - acc: 0.6599 - val_loss: 0.0656 - val_acc: 0.5520\n",
      "Epoch 2229/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6671 - val_loss: 0.0670 - val_acc: 0.5270\n",
      "Epoch 2230/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0476 - acc: 0.6522 - val_loss: 0.0657 - val_acc: 0.5461\n",
      "Epoch 2231/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0487 - acc: 0.6511 - val_loss: 0.0693 - val_acc: 0.5162\n",
      "Epoch 2232/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0485 - acc: 0.6519 - val_loss: 0.0679 - val_acc: 0.5229\n",
      "Epoch 2233/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0468 - acc: 0.6632 - val_loss: 0.0667 - val_acc: 0.5486\n",
      "Epoch 2234/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6580 - val_loss: 0.0661 - val_acc: 0.5328\n",
      "Epoch 2235/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0463 - acc: 0.6619 - val_loss: 0.0668 - val_acc: 0.5387\n",
      "Epoch 2236/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0464 - acc: 0.6616 - val_loss: 0.0669 - val_acc: 0.5420\n",
      "Epoch 2237/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0459 - acc: 0.6669 - val_loss: 0.0667 - val_acc: 0.5470\n",
      "Epoch 2238/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0462 - acc: 0.6649 - val_loss: 0.0664 - val_acc: 0.5370\n",
      "Epoch 2239/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0462 - acc: 0.6608 - val_loss: 0.0660 - val_acc: 0.5428\n",
      "Epoch 2240/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0462 - acc: 0.6677 - val_loss: 0.0666 - val_acc: 0.5370\n",
      "Epoch 2241/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0460 - acc: 0.6688 - val_loss: 0.0675 - val_acc: 0.5378\n",
      "Epoch 2242/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0457 - acc: 0.6671 - val_loss: 0.0663 - val_acc: 0.5395\n",
      "Epoch 2243/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0463 - acc: 0.6608 - val_loss: 0.0669 - val_acc: 0.5403\n",
      "Epoch 2244/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6644 - val_loss: 0.0667 - val_acc: 0.5403\n",
      "Epoch 2245/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0465 - acc: 0.6630 - val_loss: 0.0663 - val_acc: 0.5345\n",
      "Epoch 2246/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0462 - acc: 0.6677 - val_loss: 0.0669 - val_acc: 0.5428\n",
      "Epoch 2247/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0466 - acc: 0.6655 - val_loss: 0.0666 - val_acc: 0.5403\n",
      "Epoch 2248/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0461 - acc: 0.6666 - val_loss: 0.0676 - val_acc: 0.5420\n",
      "Epoch 2249/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0465 - acc: 0.6630 - val_loss: 0.0678 - val_acc: 0.5345\n",
      "Epoch 2250/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0471 - acc: 0.6580 - val_loss: 0.0670 - val_acc: 0.5378\n",
      "Epoch 2251/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0492 - acc: 0.6463 - val_loss: 0.0674 - val_acc: 0.5353\n",
      "Epoch 2252/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6572 - val_loss: 0.0670 - val_acc: 0.5420\n",
      "Epoch 2253/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0485 - acc: 0.6486 - val_loss: 0.0669 - val_acc: 0.5287\n",
      "Epoch 2254/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0473 - acc: 0.6596 - val_loss: 0.0666 - val_acc: 0.5312\n",
      "Epoch 2255/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0469 - acc: 0.6594 - val_loss: 0.0665 - val_acc: 0.5378\n",
      "Epoch 2256/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6619 - val_loss: 0.0671 - val_acc: 0.5378\n",
      "Epoch 2257/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0469 - acc: 0.6613 - val_loss: 0.0660 - val_acc: 0.5461\n",
      "Epoch 2258/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0465 - acc: 0.6624 - val_loss: 0.0679 - val_acc: 0.5328\n",
      "Epoch 2259/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6646 - val_loss: 0.0665 - val_acc: 0.5378\n",
      "Epoch 2260/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0463 - acc: 0.6630 - val_loss: 0.0668 - val_acc: 0.5353\n",
      "Epoch 2261/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0462 - acc: 0.6635 - val_loss: 0.0671 - val_acc: 0.5453\n",
      "Epoch 2262/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0465 - acc: 0.6599 - val_loss: 0.0673 - val_acc: 0.5353\n",
      "Epoch 2263/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6608 - val_loss: 0.0664 - val_acc: 0.5445\n",
      "Epoch 2264/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6566 - val_loss: 0.0674 - val_acc: 0.5337\n",
      "Epoch 2265/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6580 - val_loss: 0.0666 - val_acc: 0.5461\n",
      "Epoch 2266/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0471 - acc: 0.6574 - val_loss: 0.0667 - val_acc: 0.5245\n",
      "Epoch 2267/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0464 - acc: 0.6635 - val_loss: 0.0663 - val_acc: 0.5461\n",
      "Epoch 2268/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0473 - acc: 0.6544 - val_loss: 0.0666 - val_acc: 0.5544\n",
      "Epoch 2269/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0480 - acc: 0.6508 - val_loss: 0.0662 - val_acc: 0.5445\n",
      "Epoch 2270/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0481 - acc: 0.6508 - val_loss: 0.0661 - val_acc: 0.5445\n",
      "Epoch 2271/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6605 - val_loss: 0.0666 - val_acc: 0.5411\n",
      "Epoch 2272/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0474 - acc: 0.6599 - val_loss: 0.0674 - val_acc: 0.5345\n",
      "Epoch 2273/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6619 - val_loss: 0.0663 - val_acc: 0.5520\n",
      "Epoch 2274/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0463 - acc: 0.6666 - val_loss: 0.0669 - val_acc: 0.5378\n",
      "Epoch 2275/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0462 - acc: 0.6624 - val_loss: 0.0664 - val_acc: 0.5436\n",
      "Epoch 2276/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0458 - acc: 0.6693 - val_loss: 0.0669 - val_acc: 0.5387\n",
      "Epoch 2277/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0458 - acc: 0.6669 - val_loss: 0.0675 - val_acc: 0.5403\n",
      "Epoch 2278/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6605 - val_loss: 0.0662 - val_acc: 0.5486\n",
      "Epoch 2279/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0461 - acc: 0.6663 - val_loss: 0.0682 - val_acc: 0.5287\n",
      "Epoch 2280/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0469 - acc: 0.6549 - val_loss: 0.0673 - val_acc: 0.5345\n",
      "Epoch 2281/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0466 - acc: 0.6602 - val_loss: 0.0674 - val_acc: 0.5370\n",
      "Epoch 2282/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0458 - acc: 0.6691 - val_loss: 0.0663 - val_acc: 0.5428\n",
      "Epoch 2283/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0462 - acc: 0.6660 - val_loss: 0.0674 - val_acc: 0.5353\n",
      "Epoch 2284/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0461 - acc: 0.6621 - val_loss: 0.0665 - val_acc: 0.5420\n",
      "Epoch 2285/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0457 - acc: 0.6646 - val_loss: 0.0656 - val_acc: 0.5428\n",
      "Epoch 2286/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0462 - acc: 0.6682 - val_loss: 0.0677 - val_acc: 0.5387\n",
      "Epoch 2287/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0467 - acc: 0.6613 - val_loss: 0.0669 - val_acc: 0.5395\n",
      "Epoch 2288/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0460 - acc: 0.6663 - val_loss: 0.0669 - val_acc: 0.5370\n",
      "Epoch 2289/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0463 - acc: 0.6632 - val_loss: 0.0662 - val_acc: 0.5470\n",
      "Epoch 2290/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0482 - acc: 0.6483 - val_loss: 0.0663 - val_acc: 0.5378\n",
      "Epoch 2291/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0468 - acc: 0.6619 - val_loss: 0.0662 - val_acc: 0.5486\n",
      "Epoch 2292/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6663 - val_loss: 0.0695 - val_acc: 0.5270\n",
      "Epoch 2293/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0498 - acc: 0.6414 - val_loss: 0.0658 - val_acc: 0.5411\n",
      "Epoch 2294/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0472 - acc: 0.6591 - val_loss: 0.0659 - val_acc: 0.5395\n",
      "Epoch 2295/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6641 - val_loss: 0.0661 - val_acc: 0.5387\n",
      "Epoch 2296/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0478 - acc: 0.6547 - val_loss: 0.0664 - val_acc: 0.5395\n",
      "Epoch 2297/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0480 - acc: 0.6497 - val_loss: 0.0654 - val_acc: 0.5478\n",
      "Epoch 2298/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0478 - acc: 0.6552 - val_loss: 0.0668 - val_acc: 0.5436\n",
      "Epoch 2299/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6588 - val_loss: 0.0672 - val_acc: 0.5470\n",
      "Epoch 2300/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0478 - acc: 0.6549 - val_loss: 0.0698 - val_acc: 0.5054\n",
      "Epoch 2301/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0476 - acc: 0.6511 - val_loss: 0.0677 - val_acc: 0.5287\n",
      "Epoch 2302/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0465 - acc: 0.6608 - val_loss: 0.0668 - val_acc: 0.5362\n",
      "Epoch 2303/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0464 - acc: 0.6610 - val_loss: 0.0669 - val_acc: 0.5212\n",
      "Epoch 2304/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0461 - acc: 0.6727 - val_loss: 0.0669 - val_acc: 0.5387\n",
      "Epoch 2305/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0463 - acc: 0.6669 - val_loss: 0.0672 - val_acc: 0.5362\n",
      "Epoch 2306/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0461 - acc: 0.6641 - val_loss: 0.0671 - val_acc: 0.5387\n",
      "Epoch 2307/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0459 - acc: 0.6702 - val_loss: 0.0666 - val_acc: 0.5445\n",
      "Epoch 2308/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0459 - acc: 0.6691 - val_loss: 0.0668 - val_acc: 0.5403\n",
      "Epoch 2309/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0461 - acc: 0.6680 - val_loss: 0.0664 - val_acc: 0.5362\n",
      "Epoch 2310/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0459 - acc: 0.6682 - val_loss: 0.0678 - val_acc: 0.5278\n",
      "Epoch 2311/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0460 - acc: 0.6688 - val_loss: 0.0672 - val_acc: 0.5370\n",
      "Epoch 2312/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0462 - acc: 0.6635 - val_loss: 0.0667 - val_acc: 0.5453\n",
      "Epoch 2313/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0466 - acc: 0.6646 - val_loss: 0.0679 - val_acc: 0.5337\n",
      "Epoch 2314/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0463 - acc: 0.6605 - val_loss: 0.0668 - val_acc: 0.5461\n",
      "Epoch 2315/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0470 - acc: 0.6569 - val_loss: 0.0679 - val_acc: 0.5353\n",
      "Epoch 2316/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0465 - acc: 0.6630 - val_loss: 0.0677 - val_acc: 0.5428\n",
      "Epoch 2317/3000\n",
      "3608/3608 [==============================] - 1s 158us/step - loss: 0.0466 - acc: 0.6624 - val_loss: 0.0671 - val_acc: 0.5403\n",
      "Epoch 2318/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0476 - acc: 0.6588 - val_loss: 0.0680 - val_acc: 0.5245\n",
      "Epoch 2319/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0474 - acc: 0.6602 - val_loss: 0.0676 - val_acc: 0.5362\n",
      "Epoch 2320/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0467 - acc: 0.6588 - val_loss: 0.0673 - val_acc: 0.5403\n",
      "Epoch 2321/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0469 - acc: 0.6638 - val_loss: 0.0675 - val_acc: 0.5320\n",
      "Epoch 2322/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0461 - acc: 0.6641 - val_loss: 0.0675 - val_acc: 0.5353\n",
      "Epoch 2323/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0487 - acc: 0.6477 - val_loss: 0.0661 - val_acc: 0.5503\n",
      "Epoch 2324/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0470 - acc: 0.6624 - val_loss: 0.0672 - val_acc: 0.5362\n",
      "Epoch 2325/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0465 - acc: 0.6635 - val_loss: 0.0671 - val_acc: 0.5436\n",
      "Epoch 2326/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.6558 - val_loss: 0.0660 - val_acc: 0.5470\n",
      "Epoch 2327/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0460 - acc: 0.6682 - val_loss: 0.0666 - val_acc: 0.5453\n",
      "Epoch 2328/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6649 - val_loss: 0.0663 - val_acc: 0.5520\n",
      "Epoch 2329/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0462 - acc: 0.6666 - val_loss: 0.0677 - val_acc: 0.5403\n",
      "Epoch 2330/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0468 - acc: 0.6599 - val_loss: 0.0669 - val_acc: 0.5420\n",
      "Epoch 2331/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0460 - acc: 0.6691 - val_loss: 0.0681 - val_acc: 0.5403\n",
      "Epoch 2332/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0468 - acc: 0.6624 - val_loss: 0.0665 - val_acc: 0.5453\n",
      "Epoch 2333/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6610 - val_loss: 0.0669 - val_acc: 0.5428\n",
      "Epoch 2334/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0464 - acc: 0.6616 - val_loss: 0.0672 - val_acc: 0.5428\n",
      "Epoch 2335/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0472 - acc: 0.6663 - val_loss: 0.0681 - val_acc: 0.5453\n",
      "Epoch 2336/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0465 - acc: 0.6599 - val_loss: 0.0676 - val_acc: 0.5345\n",
      "Epoch 2337/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0467 - acc: 0.6591 - val_loss: 0.0675 - val_acc: 0.5262\n",
      "Epoch 2338/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0473 - acc: 0.6547 - val_loss: 0.0669 - val_acc: 0.5453\n",
      "Epoch 2339/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0480 - acc: 0.6519 - val_loss: 0.0665 - val_acc: 0.5436\n",
      "Epoch 2340/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0471 - acc: 0.6552 - val_loss: 0.0665 - val_acc: 0.5461\n",
      "Epoch 2341/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0469 - acc: 0.6638 - val_loss: 0.0661 - val_acc: 0.5411\n",
      "Epoch 2342/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6655 - val_loss: 0.0665 - val_acc: 0.5328\n",
      "Epoch 2343/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0483 - acc: 0.6552 - val_loss: 0.0661 - val_acc: 0.5486\n",
      "Epoch 2344/3000\n",
      "3608/3608 [==============================] - 1s 159us/step - loss: 0.0465 - acc: 0.6619 - val_loss: 0.0664 - val_acc: 0.5337\n",
      "Epoch 2345/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0464 - acc: 0.6563 - val_loss: 0.0662 - val_acc: 0.5411\n",
      "Epoch 2346/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0462 - acc: 0.6674 - val_loss: 0.0666 - val_acc: 0.5411\n",
      "Epoch 2347/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0474 - acc: 0.6641 - val_loss: 0.0675 - val_acc: 0.5378\n",
      "Epoch 2348/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0472 - acc: 0.6572 - val_loss: 0.0663 - val_acc: 0.5445\n",
      "Epoch 2349/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0464 - acc: 0.6688 - val_loss: 0.0658 - val_acc: 0.5503\n",
      "Epoch 2350/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6588 - val_loss: 0.0667 - val_acc: 0.5486\n",
      "Epoch 2351/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0462 - acc: 0.6616 - val_loss: 0.0653 - val_acc: 0.5536\n",
      "Epoch 2352/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0460 - acc: 0.6660 - val_loss: 0.0675 - val_acc: 0.5378\n",
      "Epoch 2353/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0456 - acc: 0.6696 - val_loss: 0.0662 - val_acc: 0.5395\n",
      "Epoch 2354/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0457 - acc: 0.6652 - val_loss: 0.0659 - val_acc: 0.5453\n",
      "Epoch 2355/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0455 - acc: 0.6680 - val_loss: 0.0662 - val_acc: 0.5478\n",
      "Epoch 2356/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0464 - acc: 0.6674 - val_loss: 0.0678 - val_acc: 0.5345\n",
      "Epoch 2357/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0462 - acc: 0.6641 - val_loss: 0.0668 - val_acc: 0.5445\n",
      "Epoch 2358/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0462 - acc: 0.6638 - val_loss: 0.0663 - val_acc: 0.5411\n",
      "Epoch 2359/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0460 - acc: 0.6649 - val_loss: 0.0661 - val_acc: 0.5478\n",
      "Epoch 2360/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0465 - acc: 0.6652 - val_loss: 0.0669 - val_acc: 0.5428\n",
      "Epoch 2361/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0466 - acc: 0.6605 - val_loss: 0.0652 - val_acc: 0.5544\n",
      "Epoch 2362/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6674 - val_loss: 0.0674 - val_acc: 0.5395\n",
      "Epoch 2363/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0462 - acc: 0.6649 - val_loss: 0.0663 - val_acc: 0.5453\n",
      "Epoch 2364/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0470 - acc: 0.6596 - val_loss: 0.0663 - val_acc: 0.5395\n",
      "Epoch 2365/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0463 - acc: 0.6635 - val_loss: 0.0657 - val_acc: 0.5486\n",
      "Epoch 2366/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0463 - acc: 0.6638 - val_loss: 0.0662 - val_acc: 0.5495\n",
      "Epoch 2367/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0465 - acc: 0.6674 - val_loss: 0.0665 - val_acc: 0.5470\n",
      "Epoch 2368/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0461 - acc: 0.6682 - val_loss: 0.0658 - val_acc: 0.5528\n",
      "Epoch 2369/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0461 - acc: 0.6641 - val_loss: 0.0668 - val_acc: 0.5287\n",
      "Epoch 2370/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0463 - acc: 0.6655 - val_loss: 0.0659 - val_acc: 0.5461\n",
      "Epoch 2371/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6535 - val_loss: 0.0672 - val_acc: 0.5436\n",
      "Epoch 2372/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0464 - acc: 0.6641 - val_loss: 0.0662 - val_acc: 0.5486\n",
      "Epoch 2373/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6599 - val_loss: 0.0656 - val_acc: 0.5420\n",
      "Epoch 2374/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0475 - acc: 0.6572 - val_loss: 0.0661 - val_acc: 0.5411\n",
      "Epoch 2375/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0477 - acc: 0.6552 - val_loss: 0.0660 - val_acc: 0.5411\n",
      "Epoch 2376/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6558 - val_loss: 0.0661 - val_acc: 0.5470\n",
      "Epoch 2377/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0465 - acc: 0.6610 - val_loss: 0.0659 - val_acc: 0.5528\n",
      "Epoch 2378/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6627 - val_loss: 0.0660 - val_acc: 0.5428\n",
      "Epoch 2379/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0472 - acc: 0.6574 - val_loss: 0.0671 - val_acc: 0.5320\n",
      "Epoch 2380/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0466 - acc: 0.6660 - val_loss: 0.0672 - val_acc: 0.5353\n",
      "Epoch 2381/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0466 - acc: 0.6591 - val_loss: 0.0680 - val_acc: 0.5312\n",
      "Epoch 2382/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6596 - val_loss: 0.0664 - val_acc: 0.5420\n",
      "Epoch 2383/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0459 - acc: 0.6674 - val_loss: 0.0657 - val_acc: 0.5470\n",
      "Epoch 2384/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0459 - acc: 0.6705 - val_loss: 0.0668 - val_acc: 0.5544\n",
      "Epoch 2385/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0464 - acc: 0.6632 - val_loss: 0.0666 - val_acc: 0.5411\n",
      "Epoch 2386/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0464 - acc: 0.6613 - val_loss: 0.0663 - val_acc: 0.5445\n",
      "Epoch 2387/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0462 - acc: 0.6630 - val_loss: 0.0692 - val_acc: 0.5229\n",
      "Epoch 2388/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0465 - acc: 0.6666 - val_loss: 0.0681 - val_acc: 0.5312\n",
      "Epoch 2389/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6660 - val_loss: 0.0671 - val_acc: 0.5362\n",
      "Epoch 2390/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0462 - acc: 0.6646 - val_loss: 0.0683 - val_acc: 0.5378\n",
      "Epoch 2391/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0464 - acc: 0.6685 - val_loss: 0.0669 - val_acc: 0.5428\n",
      "Epoch 2392/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0463 - acc: 0.6635 - val_loss: 0.0671 - val_acc: 0.5461\n",
      "Epoch 2393/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0462 - acc: 0.6621 - val_loss: 0.0662 - val_acc: 0.5503\n",
      "Epoch 2394/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0461 - acc: 0.6641 - val_loss: 0.0665 - val_acc: 0.5403\n",
      "Epoch 2395/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0465 - acc: 0.6671 - val_loss: 0.0673 - val_acc: 0.5478\n",
      "Epoch 2396/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0466 - acc: 0.6619 - val_loss: 0.0659 - val_acc: 0.5395\n",
      "Epoch 2397/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0459 - acc: 0.6660 - val_loss: 0.0670 - val_acc: 0.5445\n",
      "Epoch 2398/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0480 - acc: 0.6502 - val_loss: 0.0671 - val_acc: 0.5353\n",
      "Epoch 2399/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0477 - acc: 0.6572 - val_loss: 0.0668 - val_acc: 0.5436\n",
      "Epoch 2400/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6572 - val_loss: 0.0663 - val_acc: 0.5420\n",
      "Epoch 2401/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0463 - acc: 0.6655 - val_loss: 0.0664 - val_acc: 0.5395\n",
      "Epoch 2402/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6646 - val_loss: 0.0666 - val_acc: 0.5411\n",
      "Epoch 2403/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0470 - acc: 0.6619 - val_loss: 0.0683 - val_acc: 0.5328\n",
      "Epoch 2404/3000\n",
      "3608/3608 [==============================] - 1s 159us/step - loss: 0.0466 - acc: 0.6627 - val_loss: 0.0665 - val_acc: 0.5353\n",
      "Epoch 2405/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0465 - acc: 0.6630 - val_loss: 0.0682 - val_acc: 0.5312\n",
      "Epoch 2406/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0464 - acc: 0.6671 - val_loss: 0.0668 - val_acc: 0.5428\n",
      "Epoch 2407/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0459 - acc: 0.6721 - val_loss: 0.0665 - val_acc: 0.5328\n",
      "Epoch 2408/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0457 - acc: 0.6705 - val_loss: 0.0669 - val_acc: 0.5478\n",
      "Epoch 2409/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6649 - val_loss: 0.0673 - val_acc: 0.5453\n",
      "Epoch 2410/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6585 - val_loss: 0.0688 - val_acc: 0.5328\n",
      "Epoch 2411/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0476 - acc: 0.6541 - val_loss: 0.0671 - val_acc: 0.5387\n",
      "Epoch 2412/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0486 - acc: 0.6499 - val_loss: 0.0668 - val_acc: 0.5378\n",
      "Epoch 2413/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6624 - val_loss: 0.0669 - val_acc: 0.5428\n",
      "Epoch 2414/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6610 - val_loss: 0.0674 - val_acc: 0.5461\n",
      "Epoch 2415/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6649 - val_loss: 0.0668 - val_acc: 0.5403\n",
      "Epoch 2416/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6569 - val_loss: 0.0660 - val_acc: 0.5486\n",
      "Epoch 2417/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0462 - acc: 0.6624 - val_loss: 0.0676 - val_acc: 0.5387\n",
      "Epoch 2418/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6591 - val_loss: 0.0676 - val_acc: 0.5337\n",
      "Epoch 2419/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6630 - val_loss: 0.0662 - val_acc: 0.5478\n",
      "Epoch 2420/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6624 - val_loss: 0.0661 - val_acc: 0.5486\n",
      "Epoch 2421/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0465 - acc: 0.6621 - val_loss: 0.0676 - val_acc: 0.5378\n",
      "Epoch 2422/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0464 - acc: 0.6660 - val_loss: 0.0665 - val_acc: 0.5503\n",
      "Epoch 2423/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6655 - val_loss: 0.0681 - val_acc: 0.5345\n",
      "Epoch 2424/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0473 - acc: 0.6574 - val_loss: 0.0678 - val_acc: 0.5403\n",
      "Epoch 2425/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6613 - val_loss: 0.0662 - val_acc: 0.5470\n",
      "Epoch 2426/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0464 - acc: 0.6657 - val_loss: 0.0665 - val_acc: 0.5362\n",
      "Epoch 2427/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.6588 - val_loss: 0.0678 - val_acc: 0.5229\n",
      "Epoch 2428/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0464 - acc: 0.6663 - val_loss: 0.0661 - val_acc: 0.5478\n",
      "Epoch 2429/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0466 - acc: 0.6596 - val_loss: 0.0689 - val_acc: 0.5370\n",
      "Epoch 2430/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0476 - acc: 0.6566 - val_loss: 0.0684 - val_acc: 0.5312\n",
      "Epoch 2431/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0469 - acc: 0.6599 - val_loss: 0.0680 - val_acc: 0.5337\n",
      "Epoch 2432/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0474 - acc: 0.6585 - val_loss: 0.0681 - val_acc: 0.5254\n",
      "Epoch 2433/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0463 - acc: 0.6619 - val_loss: 0.0680 - val_acc: 0.5337\n",
      "Epoch 2434/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0469 - acc: 0.6599 - val_loss: 0.0671 - val_acc: 0.5345\n",
      "Epoch 2435/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0481 - acc: 0.6497 - val_loss: 0.0668 - val_acc: 0.5370\n",
      "Epoch 2436/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6572 - val_loss: 0.0681 - val_acc: 0.5270\n",
      "Epoch 2437/3000\n",
      "3608/3608 [==============================] - 1s 159us/step - loss: 0.0468 - acc: 0.6652 - val_loss: 0.0675 - val_acc: 0.5295\n",
      "Epoch 2438/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6583 - val_loss: 0.0679 - val_acc: 0.5320\n",
      "Epoch 2439/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6574 - val_loss: 0.0684 - val_acc: 0.5270\n",
      "Epoch 2440/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0466 - acc: 0.6619 - val_loss: 0.0669 - val_acc: 0.5411\n",
      "Epoch 2441/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0463 - acc: 0.6693 - val_loss: 0.0682 - val_acc: 0.5320\n",
      "Epoch 2442/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0464 - acc: 0.6632 - val_loss: 0.0675 - val_acc: 0.5387\n",
      "Epoch 2443/3000\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0459 - acc: 0.6685 - val_loss: 0.0669 - val_acc: 0.5445\n",
      "Epoch 2444/3000\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0469 - acc: 0.6591 - val_loss: 0.0663 - val_acc: 0.5436\n",
      "Epoch 2445/3000\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0460 - acc: 0.6644 - val_loss: 0.0663 - val_acc: 0.5553\n",
      "Epoch 2446/3000\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0464 - acc: 0.6666 - val_loss: 0.0674 - val_acc: 0.5453\n",
      "Epoch 2447/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0463 - acc: 0.6666 - val_loss: 0.0674 - val_acc: 0.5403\n",
      "Epoch 2448/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0459 - acc: 0.6682 - val_loss: 0.0674 - val_acc: 0.5270\n",
      "Epoch 2449/3000\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0458 - acc: 0.6702 - val_loss: 0.0670 - val_acc: 0.5328\n",
      "Epoch 2450/3000\n",
      "3608/3608 [==============================] - 1s 208us/step - loss: 0.0486 - acc: 0.6497 - val_loss: 0.0671 - val_acc: 0.5312\n",
      "Epoch 2451/3000\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0472 - acc: 0.6558 - val_loss: 0.0687 - val_acc: 0.5395\n",
      "Epoch 2452/3000\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0467 - acc: 0.6641 - val_loss: 0.0675 - val_acc: 0.5411\n",
      "Epoch 2453/3000\n",
      "3608/3608 [==============================] - 1s 215us/step - loss: 0.0462 - acc: 0.6680 - val_loss: 0.0667 - val_acc: 0.5420\n",
      "Epoch 2454/3000\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0465 - acc: 0.6624 - val_loss: 0.0667 - val_acc: 0.5370\n",
      "Epoch 2455/3000\n",
      "3608/3608 [==============================] - 1s 210us/step - loss: 0.0466 - acc: 0.6574 - val_loss: 0.0670 - val_acc: 0.5303\n",
      "Epoch 2456/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0458 - acc: 0.6657 - val_loss: 0.0677 - val_acc: 0.5303\n",
      "Epoch 2457/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6574 - val_loss: 0.0665 - val_acc: 0.5395\n",
      "Epoch 2458/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6580 - val_loss: 0.0673 - val_acc: 0.5395\n",
      "Epoch 2459/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0466 - acc: 0.6641 - val_loss: 0.0674 - val_acc: 0.5362\n",
      "Epoch 2460/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6619 - val_loss: 0.0666 - val_acc: 0.5511\n",
      "Epoch 2461/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0459 - acc: 0.6693 - val_loss: 0.0661 - val_acc: 0.5387\n",
      "Epoch 2462/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0462 - acc: 0.6646 - val_loss: 0.0657 - val_acc: 0.5486\n",
      "Epoch 2463/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0467 - acc: 0.6627 - val_loss: 0.0678 - val_acc: 0.5295\n",
      "Epoch 2464/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6630 - val_loss: 0.0668 - val_acc: 0.5370\n",
      "Epoch 2465/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0464 - acc: 0.6641 - val_loss: 0.0672 - val_acc: 0.5320\n",
      "Epoch 2466/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0466 - acc: 0.6646 - val_loss: 0.0674 - val_acc: 0.5345\n",
      "Epoch 2467/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6649 - val_loss: 0.0675 - val_acc: 0.5395\n",
      "Epoch 2468/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6655 - val_loss: 0.0682 - val_acc: 0.5436\n",
      "Epoch 2469/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0467 - acc: 0.6655 - val_loss: 0.0665 - val_acc: 0.5353\n",
      "Epoch 2470/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0467 - acc: 0.6616 - val_loss: 0.0669 - val_acc: 0.5378\n",
      "Epoch 2471/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0462 - acc: 0.6657 - val_loss: 0.0671 - val_acc: 0.5362\n",
      "Epoch 2472/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0462 - acc: 0.6660 - val_loss: 0.0675 - val_acc: 0.5312\n",
      "Epoch 2473/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0466 - acc: 0.6649 - val_loss: 0.0673 - val_acc: 0.5420\n",
      "Epoch 2474/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0463 - acc: 0.6594 - val_loss: 0.0674 - val_acc: 0.5395\n",
      "Epoch 2475/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0462 - acc: 0.6674 - val_loss: 0.0690 - val_acc: 0.5270\n",
      "Epoch 2476/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0464 - acc: 0.6632 - val_loss: 0.0668 - val_acc: 0.5445\n",
      "Epoch 2477/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0457 - acc: 0.6688 - val_loss: 0.0662 - val_acc: 0.5495\n",
      "Epoch 2478/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0462 - acc: 0.6663 - val_loss: 0.0671 - val_acc: 0.5353\n",
      "Epoch 2479/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0460 - acc: 0.6655 - val_loss: 0.0669 - val_acc: 0.5420\n",
      "Epoch 2480/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0462 - acc: 0.6663 - val_loss: 0.0678 - val_acc: 0.5387\n",
      "Epoch 2481/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0466 - acc: 0.6652 - val_loss: 0.0674 - val_acc: 0.5353\n",
      "Epoch 2482/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0472 - acc: 0.6674 - val_loss: 0.0672 - val_acc: 0.5420\n",
      "Epoch 2483/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0467 - acc: 0.6583 - val_loss: 0.0682 - val_acc: 0.5303\n",
      "Epoch 2484/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0473 - acc: 0.6535 - val_loss: 0.0675 - val_acc: 0.5345\n",
      "Epoch 2485/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0468 - acc: 0.6610 - val_loss: 0.0668 - val_acc: 0.5411\n",
      "Epoch 2486/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0465 - acc: 0.6588 - val_loss: 0.0671 - val_acc: 0.5320\n",
      "Epoch 2487/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0474 - acc: 0.6555 - val_loss: 0.0676 - val_acc: 0.5254\n",
      "Epoch 2488/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0472 - acc: 0.6585 - val_loss: 0.0672 - val_acc: 0.5362\n",
      "Epoch 2489/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0462 - acc: 0.6685 - val_loss: 0.0664 - val_acc: 0.5411\n",
      "Epoch 2490/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0469 - acc: 0.6580 - val_loss: 0.0661 - val_acc: 0.5428\n",
      "Epoch 2491/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0460 - acc: 0.6657 - val_loss: 0.0674 - val_acc: 0.5420\n",
      "Epoch 2492/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6660 - val_loss: 0.0666 - val_acc: 0.5445\n",
      "Epoch 2493/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0469 - acc: 0.6599 - val_loss: 0.0675 - val_acc: 0.5353\n",
      "Epoch 2494/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0473 - acc: 0.6533 - val_loss: 0.0663 - val_acc: 0.5370\n",
      "Epoch 2495/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0472 - acc: 0.6613 - val_loss: 0.0679 - val_acc: 0.5436\n",
      "Epoch 2496/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0491 - acc: 0.6450 - val_loss: 0.0669 - val_acc: 0.5345\n",
      "Epoch 2497/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6560 - val_loss: 0.0666 - val_acc: 0.5378\n",
      "Epoch 2498/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0474 - acc: 0.6541 - val_loss: 0.0678 - val_acc: 0.5170\n",
      "Epoch 2499/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0474 - acc: 0.6583 - val_loss: 0.0676 - val_acc: 0.5237\n",
      "Epoch 2500/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6549 - val_loss: 0.0674 - val_acc: 0.5345\n",
      "Epoch 2501/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0465 - acc: 0.6638 - val_loss: 0.0669 - val_acc: 0.5387\n",
      "Epoch 2502/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6608 - val_loss: 0.0679 - val_acc: 0.5295\n",
      "Epoch 2503/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0465 - acc: 0.6627 - val_loss: 0.0663 - val_acc: 0.5436\n",
      "Epoch 2504/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0473 - acc: 0.6599 - val_loss: 0.0682 - val_acc: 0.5295\n",
      "Epoch 2505/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0462 - acc: 0.6655 - val_loss: 0.0669 - val_acc: 0.5395\n",
      "Epoch 2506/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0463 - acc: 0.6646 - val_loss: 0.0675 - val_acc: 0.5353\n",
      "Epoch 2507/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6583 - val_loss: 0.0672 - val_acc: 0.5370\n",
      "Epoch 2508/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0469 - acc: 0.6627 - val_loss: 0.0666 - val_acc: 0.5353\n",
      "Epoch 2509/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0463 - acc: 0.6657 - val_loss: 0.0672 - val_acc: 0.5287\n",
      "Epoch 2510/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6641 - val_loss: 0.0672 - val_acc: 0.5337\n",
      "Epoch 2511/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0476 - acc: 0.6555 - val_loss: 0.0662 - val_acc: 0.5520\n",
      "Epoch 2512/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0461 - acc: 0.6655 - val_loss: 0.0672 - val_acc: 0.5395\n",
      "Epoch 2513/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0464 - acc: 0.6624 - val_loss: 0.0666 - val_acc: 0.5353\n",
      "Epoch 2514/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0462 - acc: 0.6674 - val_loss: 0.0678 - val_acc: 0.5303\n",
      "Epoch 2515/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0463 - acc: 0.6632 - val_loss: 0.0672 - val_acc: 0.5395\n",
      "Epoch 2516/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6585 - val_loss: 0.0676 - val_acc: 0.5378\n",
      "Epoch 2517/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0461 - acc: 0.6699 - val_loss: 0.0680 - val_acc: 0.5445\n",
      "Epoch 2518/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0461 - acc: 0.6657 - val_loss: 0.0663 - val_acc: 0.5461\n",
      "Epoch 2519/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0460 - acc: 0.6669 - val_loss: 0.0671 - val_acc: 0.5312\n",
      "Epoch 2520/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0458 - acc: 0.6649 - val_loss: 0.0684 - val_acc: 0.5353\n",
      "Epoch 2521/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0462 - acc: 0.6635 - val_loss: 0.0669 - val_acc: 0.5520\n",
      "Epoch 2522/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0458 - acc: 0.6702 - val_loss: 0.0665 - val_acc: 0.5403\n",
      "Epoch 2523/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0463 - acc: 0.6616 - val_loss: 0.0666 - val_acc: 0.5420\n",
      "Epoch 2524/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0460 - acc: 0.6646 - val_loss: 0.0674 - val_acc: 0.5312\n",
      "Epoch 2525/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0464 - acc: 0.6644 - val_loss: 0.0668 - val_acc: 0.5411\n",
      "Epoch 2526/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0461 - acc: 0.6688 - val_loss: 0.0679 - val_acc: 0.5195\n",
      "Epoch 2527/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0460 - acc: 0.6674 - val_loss: 0.0665 - val_acc: 0.5478\n",
      "Epoch 2528/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0459 - acc: 0.6608 - val_loss: 0.0666 - val_acc: 0.5445\n",
      "Epoch 2529/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0461 - acc: 0.6657 - val_loss: 0.0684 - val_acc: 0.5370\n",
      "Epoch 2530/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6563 - val_loss: 0.0667 - val_acc: 0.5411\n",
      "Epoch 2531/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0469 - acc: 0.6602 - val_loss: 0.0678 - val_acc: 0.5287\n",
      "Epoch 2532/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0460 - acc: 0.6671 - val_loss: 0.0678 - val_acc: 0.5420\n",
      "Epoch 2533/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0460 - acc: 0.6682 - val_loss: 0.0675 - val_acc: 0.5411\n",
      "Epoch 2534/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0462 - acc: 0.6660 - val_loss: 0.0682 - val_acc: 0.5395\n",
      "Epoch 2535/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0466 - acc: 0.6594 - val_loss: 0.0678 - val_acc: 0.5395\n",
      "Epoch 2536/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0462 - acc: 0.6691 - val_loss: 0.0672 - val_acc: 0.5420\n",
      "Epoch 2537/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0462 - acc: 0.6677 - val_loss: 0.0678 - val_acc: 0.5378\n",
      "Epoch 2538/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0468 - acc: 0.6616 - val_loss: 0.0685 - val_acc: 0.5245\n",
      "Epoch 2539/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0464 - acc: 0.6644 - val_loss: 0.0668 - val_acc: 0.5478\n",
      "Epoch 2540/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6594 - val_loss: 0.0668 - val_acc: 0.5362\n",
      "Epoch 2541/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0463 - acc: 0.6666 - val_loss: 0.0680 - val_acc: 0.5395\n",
      "Epoch 2542/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0469 - acc: 0.6616 - val_loss: 0.0692 - val_acc: 0.5303\n",
      "Epoch 2543/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0474 - acc: 0.6596 - val_loss: 0.0667 - val_acc: 0.5387\n",
      "Epoch 2544/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0461 - acc: 0.6649 - val_loss: 0.0654 - val_acc: 0.5569\n",
      "Epoch 2545/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0458 - acc: 0.6663 - val_loss: 0.0664 - val_acc: 0.5495\n",
      "Epoch 2546/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6702 - val_loss: 0.0673 - val_acc: 0.5387\n",
      "Epoch 2547/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0467 - acc: 0.6624 - val_loss: 0.0674 - val_acc: 0.5420\n",
      "Epoch 2548/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0459 - acc: 0.6693 - val_loss: 0.0670 - val_acc: 0.5470\n",
      "Epoch 2549/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0459 - acc: 0.6688 - val_loss: 0.0677 - val_acc: 0.5403\n",
      "Epoch 2550/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0456 - acc: 0.6749 - val_loss: 0.0674 - val_acc: 0.5436\n",
      "Epoch 2551/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0462 - acc: 0.6644 - val_loss: 0.0673 - val_acc: 0.5403\n",
      "Epoch 2552/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0462 - acc: 0.6641 - val_loss: 0.0673 - val_acc: 0.5362\n",
      "Epoch 2553/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0464 - acc: 0.6649 - val_loss: 0.0681 - val_acc: 0.5337\n",
      "Epoch 2554/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6699 - val_loss: 0.0681 - val_acc: 0.5370\n",
      "Epoch 2555/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0461 - acc: 0.6627 - val_loss: 0.0673 - val_acc: 0.5428\n",
      "Epoch 2556/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0459 - acc: 0.6685 - val_loss: 0.0675 - val_acc: 0.5486\n",
      "Epoch 2557/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0462 - acc: 0.6644 - val_loss: 0.0672 - val_acc: 0.5420\n",
      "Epoch 2558/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0461 - acc: 0.6641 - val_loss: 0.0676 - val_acc: 0.5320\n",
      "Epoch 2559/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0467 - acc: 0.6596 - val_loss: 0.0667 - val_acc: 0.5370\n",
      "Epoch 2560/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6657 - val_loss: 0.0672 - val_acc: 0.5362\n",
      "Epoch 2561/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6594 - val_loss: 0.0668 - val_acc: 0.5370\n",
      "Epoch 2562/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0463 - acc: 0.6635 - val_loss: 0.0667 - val_acc: 0.5387\n",
      "Epoch 2563/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0471 - acc: 0.6632 - val_loss: 0.0671 - val_acc: 0.5403\n",
      "Epoch 2564/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0469 - acc: 0.6588 - val_loss: 0.0669 - val_acc: 0.5411\n",
      "Epoch 2565/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0459 - acc: 0.6705 - val_loss: 0.0685 - val_acc: 0.5278\n",
      "Epoch 2566/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0456 - acc: 0.6693 - val_loss: 0.0681 - val_acc: 0.5303\n",
      "Epoch 2567/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6638 - val_loss: 0.0676 - val_acc: 0.5320\n",
      "Epoch 2568/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0461 - acc: 0.6671 - val_loss: 0.0682 - val_acc: 0.5378\n",
      "Epoch 2569/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0460 - acc: 0.6721 - val_loss: 0.0669 - val_acc: 0.5453\n",
      "Epoch 2570/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0462 - acc: 0.6663 - val_loss: 0.0681 - val_acc: 0.5345\n",
      "Epoch 2571/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0480 - acc: 0.6508 - val_loss: 0.0669 - val_acc: 0.5420\n",
      "Epoch 2572/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0465 - acc: 0.6655 - val_loss: 0.0669 - val_acc: 0.5370\n",
      "Epoch 2573/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0464 - acc: 0.6594 - val_loss: 0.0671 - val_acc: 0.5411\n",
      "Epoch 2574/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0464 - acc: 0.6644 - val_loss: 0.0678 - val_acc: 0.5353\n",
      "Epoch 2575/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6613 - val_loss: 0.0666 - val_acc: 0.5478\n",
      "Epoch 2576/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6655 - val_loss: 0.0676 - val_acc: 0.5395\n",
      "Epoch 2577/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0462 - acc: 0.6635 - val_loss: 0.0671 - val_acc: 0.5428\n",
      "Epoch 2578/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6608 - val_loss: 0.0669 - val_acc: 0.5436\n",
      "Epoch 2579/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0460 - acc: 0.6669 - val_loss: 0.0685 - val_acc: 0.5270\n",
      "Epoch 2580/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0470 - acc: 0.6627 - val_loss: 0.0685 - val_acc: 0.5362\n",
      "Epoch 2581/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0462 - acc: 0.6685 - val_loss: 0.0672 - val_acc: 0.5378\n",
      "Epoch 2582/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0460 - acc: 0.6649 - val_loss: 0.0670 - val_acc: 0.5362\n",
      "Epoch 2583/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0462 - acc: 0.6663 - val_loss: 0.0687 - val_acc: 0.5212\n",
      "Epoch 2584/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0470 - acc: 0.6605 - val_loss: 0.0678 - val_acc: 0.5287\n",
      "Epoch 2585/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0463 - acc: 0.6627 - val_loss: 0.0676 - val_acc: 0.5337\n",
      "Epoch 2586/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0477 - acc: 0.6530 - val_loss: 0.0671 - val_acc: 0.5328\n",
      "Epoch 2587/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6646 - val_loss: 0.0671 - val_acc: 0.5353\n",
      "Epoch 2588/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0455 - acc: 0.6693 - val_loss: 0.0669 - val_acc: 0.5395\n",
      "Epoch 2589/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0458 - acc: 0.6688 - val_loss: 0.0671 - val_acc: 0.5362\n",
      "Epoch 2590/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6630 - val_loss: 0.0676 - val_acc: 0.5387\n",
      "Epoch 2591/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0459 - acc: 0.6674 - val_loss: 0.0671 - val_acc: 0.5436\n",
      "Epoch 2592/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6610 - val_loss: 0.0685 - val_acc: 0.5362\n",
      "Epoch 2593/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0462 - acc: 0.6635 - val_loss: 0.0676 - val_acc: 0.5362\n",
      "Epoch 2594/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0466 - acc: 0.6630 - val_loss: 0.0668 - val_acc: 0.5436\n",
      "Epoch 2595/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0468 - acc: 0.6610 - val_loss: 0.0674 - val_acc: 0.5370\n",
      "Epoch 2596/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0473 - acc: 0.6549 - val_loss: 0.0672 - val_acc: 0.5328\n",
      "Epoch 2597/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0470 - acc: 0.6638 - val_loss: 0.0666 - val_acc: 0.5420\n",
      "Epoch 2598/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6652 - val_loss: 0.0670 - val_acc: 0.5503\n",
      "Epoch 2599/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0468 - acc: 0.6588 - val_loss: 0.0672 - val_acc: 0.5345\n",
      "Epoch 2600/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0471 - acc: 0.6624 - val_loss: 0.0673 - val_acc: 0.5445\n",
      "Epoch 2601/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0463 - acc: 0.6649 - val_loss: 0.0682 - val_acc: 0.5278\n",
      "Epoch 2602/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0464 - acc: 0.6627 - val_loss: 0.0673 - val_acc: 0.5378\n",
      "Epoch 2603/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0461 - acc: 0.6638 - val_loss: 0.0678 - val_acc: 0.5295\n",
      "Epoch 2604/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0459 - acc: 0.6657 - val_loss: 0.0670 - val_acc: 0.5403\n",
      "Epoch 2605/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0462 - acc: 0.6652 - val_loss: 0.0670 - val_acc: 0.5362\n",
      "Epoch 2606/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0465 - acc: 0.6632 - val_loss: 0.0681 - val_acc: 0.5445\n",
      "Epoch 2607/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6608 - val_loss: 0.0676 - val_acc: 0.5362\n",
      "Epoch 2608/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0462 - acc: 0.6619 - val_loss: 0.0676 - val_acc: 0.5378\n",
      "Epoch 2609/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0459 - acc: 0.6624 - val_loss: 0.0670 - val_acc: 0.5387\n",
      "Epoch 2610/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0478 - acc: 0.6486 - val_loss: 0.0673 - val_acc: 0.5345\n",
      "Epoch 2611/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6652 - val_loss: 0.0670 - val_acc: 0.5362\n",
      "Epoch 2612/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0459 - acc: 0.6677 - val_loss: 0.0664 - val_acc: 0.5470\n",
      "Epoch 2613/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0464 - acc: 0.6621 - val_loss: 0.0660 - val_acc: 0.5486\n",
      "Epoch 2614/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0459 - acc: 0.6699 - val_loss: 0.0664 - val_acc: 0.5478\n",
      "Epoch 2615/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0461 - acc: 0.6702 - val_loss: 0.0667 - val_acc: 0.5470\n",
      "Epoch 2616/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0455 - acc: 0.6707 - val_loss: 0.0673 - val_acc: 0.5345\n",
      "Epoch 2617/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0460 - acc: 0.6657 - val_loss: 0.0675 - val_acc: 0.5362\n",
      "Epoch 2618/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0460 - acc: 0.6630 - val_loss: 0.0669 - val_acc: 0.5378\n",
      "Epoch 2619/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0475 - acc: 0.6605 - val_loss: 0.0675 - val_acc: 0.5262\n",
      "Epoch 2620/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0466 - acc: 0.6644 - val_loss: 0.0664 - val_acc: 0.5420\n",
      "Epoch 2621/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0462 - acc: 0.6674 - val_loss: 0.0665 - val_acc: 0.5420\n",
      "Epoch 2622/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0466 - acc: 0.6669 - val_loss: 0.0674 - val_acc: 0.5428\n",
      "Epoch 2623/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6641 - val_loss: 0.0674 - val_acc: 0.5403\n",
      "Epoch 2624/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0465 - acc: 0.6610 - val_loss: 0.0684 - val_acc: 0.5270\n",
      "Epoch 2625/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0472 - acc: 0.6558 - val_loss: 0.0669 - val_acc: 0.5411\n",
      "Epoch 2626/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0461 - acc: 0.6655 - val_loss: 0.0671 - val_acc: 0.5403\n",
      "Epoch 2627/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0470 - acc: 0.6541 - val_loss: 0.0674 - val_acc: 0.5378\n",
      "Epoch 2628/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0470 - acc: 0.6621 - val_loss: 0.0672 - val_acc: 0.5328\n",
      "Epoch 2629/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0465 - acc: 0.6638 - val_loss: 0.0671 - val_acc: 0.5378\n",
      "Epoch 2630/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0461 - acc: 0.6666 - val_loss: 0.0670 - val_acc: 0.5387\n",
      "Epoch 2631/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0468 - acc: 0.6599 - val_loss: 0.0670 - val_acc: 0.5387\n",
      "Epoch 2632/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0467 - acc: 0.6652 - val_loss: 0.0667 - val_acc: 0.5411\n",
      "Epoch 2633/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0460 - acc: 0.6693 - val_loss: 0.0676 - val_acc: 0.5387\n",
      "Epoch 2634/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0465 - acc: 0.6624 - val_loss: 0.0671 - val_acc: 0.5453\n",
      "Epoch 2635/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0461 - acc: 0.6699 - val_loss: 0.0673 - val_acc: 0.5395\n",
      "Epoch 2636/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0463 - acc: 0.6613 - val_loss: 0.0675 - val_acc: 0.5387\n",
      "Epoch 2637/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0461 - acc: 0.6663 - val_loss: 0.0670 - val_acc: 0.5403\n",
      "Epoch 2638/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0460 - acc: 0.6716 - val_loss: 0.0667 - val_acc: 0.5495\n",
      "Epoch 2639/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0462 - acc: 0.6641 - val_loss: 0.0679 - val_acc: 0.5411\n",
      "Epoch 2640/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0464 - acc: 0.6630 - val_loss: 0.0678 - val_acc: 0.5420\n",
      "Epoch 2641/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6652 - val_loss: 0.0668 - val_acc: 0.5453\n",
      "Epoch 2642/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0457 - acc: 0.6705 - val_loss: 0.0674 - val_acc: 0.5403\n",
      "Epoch 2643/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0461 - acc: 0.6696 - val_loss: 0.0675 - val_acc: 0.5362\n",
      "Epoch 2644/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0455 - acc: 0.6710 - val_loss: 0.0680 - val_acc: 0.5378\n",
      "Epoch 2645/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0462 - acc: 0.6608 - val_loss: 0.0667 - val_acc: 0.5387\n",
      "Epoch 2646/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0464 - acc: 0.6624 - val_loss: 0.0674 - val_acc: 0.5387\n",
      "Epoch 2647/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0460 - acc: 0.6649 - val_loss: 0.0687 - val_acc: 0.5237\n",
      "Epoch 2648/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0473 - acc: 0.6616 - val_loss: 0.0677 - val_acc: 0.5320\n",
      "Epoch 2649/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0477 - acc: 0.6547 - val_loss: 0.0662 - val_acc: 0.5470\n",
      "Epoch 2650/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6580 - val_loss: 0.0675 - val_acc: 0.5370\n",
      "Epoch 2651/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0464 - acc: 0.6580 - val_loss: 0.0668 - val_acc: 0.5403\n",
      "Epoch 2652/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0460 - acc: 0.6663 - val_loss: 0.0679 - val_acc: 0.5237\n",
      "Epoch 2653/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0473 - acc: 0.6608 - val_loss: 0.0676 - val_acc: 0.5303\n",
      "Epoch 2654/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0463 - acc: 0.6621 - val_loss: 0.0673 - val_acc: 0.5345\n",
      "Epoch 2655/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0463 - acc: 0.6666 - val_loss: 0.0666 - val_acc: 0.5370\n",
      "Epoch 2656/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0461 - acc: 0.6674 - val_loss: 0.0681 - val_acc: 0.5303\n",
      "Epoch 2657/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0467 - acc: 0.6566 - val_loss: 0.0685 - val_acc: 0.5353\n",
      "Epoch 2658/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0488 - acc: 0.6511 - val_loss: 0.0671 - val_acc: 0.5436\n",
      "Epoch 2659/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0463 - acc: 0.6624 - val_loss: 0.0670 - val_acc: 0.5403\n",
      "Epoch 2660/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0485 - acc: 0.6450 - val_loss: 0.0662 - val_acc: 0.5478\n",
      "Epoch 2661/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0517 - acc: 0.6319 - val_loss: 0.0666 - val_acc: 0.5362\n",
      "Epoch 2662/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0489 - acc: 0.6458 - val_loss: 0.0665 - val_acc: 0.5461\n",
      "Epoch 2663/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0481 - acc: 0.6530 - val_loss: 0.0668 - val_acc: 0.5345\n",
      "Epoch 2664/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0468 - acc: 0.6624 - val_loss: 0.0676 - val_acc: 0.5370\n",
      "Epoch 2665/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0463 - acc: 0.6627 - val_loss: 0.0671 - val_acc: 0.5353\n",
      "Epoch 2666/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0464 - acc: 0.6655 - val_loss: 0.0674 - val_acc: 0.5328\n",
      "Epoch 2667/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0464 - acc: 0.6657 - val_loss: 0.0665 - val_acc: 0.5461\n",
      "Epoch 2668/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0461 - acc: 0.6669 - val_loss: 0.0675 - val_acc: 0.5345\n",
      "Epoch 2669/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0462 - acc: 0.6616 - val_loss: 0.0672 - val_acc: 0.5395\n",
      "Epoch 2670/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0458 - acc: 0.6677 - val_loss: 0.0683 - val_acc: 0.5328\n",
      "Epoch 2671/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0467 - acc: 0.6657 - val_loss: 0.0687 - val_acc: 0.5303\n",
      "Epoch 2672/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0466 - acc: 0.6605 - val_loss: 0.0664 - val_acc: 0.5461\n",
      "Epoch 2673/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0461 - acc: 0.6605 - val_loss: 0.0666 - val_acc: 0.5362\n",
      "Epoch 2674/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0468 - acc: 0.6602 - val_loss: 0.0691 - val_acc: 0.5254\n",
      "Epoch 2675/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0465 - acc: 0.6674 - val_loss: 0.0682 - val_acc: 0.5353\n",
      "Epoch 2676/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0459 - acc: 0.6671 - val_loss: 0.0673 - val_acc: 0.5353\n",
      "Epoch 2677/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0457 - acc: 0.6682 - val_loss: 0.0675 - val_acc: 0.5353\n",
      "Epoch 2678/3000\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0460 - acc: 0.6646 - val_loss: 0.0666 - val_acc: 0.5478\n",
      "Epoch 2679/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0460 - acc: 0.6671 - val_loss: 0.0689 - val_acc: 0.5278\n",
      "Epoch 2680/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0459 - acc: 0.6677 - val_loss: 0.0663 - val_acc: 0.5453\n",
      "Epoch 2681/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0459 - acc: 0.6671 - val_loss: 0.0672 - val_acc: 0.5428\n",
      "Epoch 2682/3000\n",
      "3608/3608 [==============================] - 1s 183us/step - loss: 0.0464 - acc: 0.6646 - val_loss: 0.0672 - val_acc: 0.5362\n",
      "Epoch 2683/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0469 - acc: 0.6657 - val_loss: 0.0688 - val_acc: 0.5287\n",
      "Epoch 2684/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0466 - acc: 0.6602 - val_loss: 0.0673 - val_acc: 0.5337\n",
      "Epoch 2685/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0460 - acc: 0.6677 - val_loss: 0.0685 - val_acc: 0.5295\n",
      "Epoch 2686/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0467 - acc: 0.6591 - val_loss: 0.0674 - val_acc: 0.5378\n",
      "Epoch 2687/3000\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0463 - acc: 0.6649 - val_loss: 0.0675 - val_acc: 0.5278\n",
      "Epoch 2688/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0464 - acc: 0.6657 - val_loss: 0.0671 - val_acc: 0.5403\n",
      "Epoch 2689/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0460 - acc: 0.6674 - val_loss: 0.0669 - val_acc: 0.5378\n",
      "Epoch 2690/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0456 - acc: 0.6763 - val_loss: 0.0674 - val_acc: 0.5395\n",
      "Epoch 2691/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0465 - acc: 0.6591 - val_loss: 0.0670 - val_acc: 0.5362\n",
      "Epoch 2692/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0459 - acc: 0.6685 - val_loss: 0.0677 - val_acc: 0.5345\n",
      "Epoch 2693/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0468 - acc: 0.6619 - val_loss: 0.0671 - val_acc: 0.5378\n",
      "Epoch 2694/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0464 - acc: 0.6624 - val_loss: 0.0683 - val_acc: 0.5129\n",
      "Epoch 2695/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0467 - acc: 0.6638 - val_loss: 0.0671 - val_acc: 0.5362\n",
      "Epoch 2696/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0463 - acc: 0.6602 - val_loss: 0.0667 - val_acc: 0.5470\n",
      "Epoch 2697/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0466 - acc: 0.6649 - val_loss: 0.0672 - val_acc: 0.5387\n",
      "Epoch 2698/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0475 - acc: 0.6627 - val_loss: 0.0687 - val_acc: 0.5278\n",
      "Epoch 2699/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0479 - acc: 0.6475 - val_loss: 0.0670 - val_acc: 0.5436\n",
      "Epoch 2700/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0467 - acc: 0.6635 - val_loss: 0.0668 - val_acc: 0.5328\n",
      "Epoch 2701/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0469 - acc: 0.6594 - val_loss: 0.0670 - val_acc: 0.5445\n",
      "Epoch 2702/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6641 - val_loss: 0.0669 - val_acc: 0.5378\n",
      "Epoch 2703/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0465 - acc: 0.6693 - val_loss: 0.0666 - val_acc: 0.5345\n",
      "Epoch 2704/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0471 - acc: 0.6585 - val_loss: 0.0674 - val_acc: 0.5420\n",
      "Epoch 2705/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0471 - acc: 0.6627 - val_loss: 0.0669 - val_acc: 0.5345\n",
      "Epoch 2706/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0459 - acc: 0.6652 - val_loss: 0.0673 - val_acc: 0.5370\n",
      "Epoch 2707/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0465 - acc: 0.6655 - val_loss: 0.0676 - val_acc: 0.5370\n",
      "Epoch 2708/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0462 - acc: 0.6635 - val_loss: 0.0681 - val_acc: 0.5229\n",
      "Epoch 2709/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0462 - acc: 0.6624 - val_loss: 0.0661 - val_acc: 0.5353\n",
      "Epoch 2710/3000\n",
      "3608/3608 [==============================] - 1s 186us/step - loss: 0.0462 - acc: 0.6693 - val_loss: 0.0678 - val_acc: 0.5411\n",
      "Epoch 2711/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0465 - acc: 0.6616 - val_loss: 0.0674 - val_acc: 0.5328\n",
      "Epoch 2712/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0466 - acc: 0.6663 - val_loss: 0.0699 - val_acc: 0.5320\n",
      "Epoch 2713/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0464 - acc: 0.6610 - val_loss: 0.0690 - val_acc: 0.5278\n",
      "Epoch 2714/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0458 - acc: 0.6707 - val_loss: 0.0668 - val_acc: 0.5428\n",
      "Epoch 2715/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0473 - acc: 0.6588 - val_loss: 0.0675 - val_acc: 0.5370\n",
      "Epoch 2716/3000\n",
      "3608/3608 [==============================] - 1s 193us/step - loss: 0.0467 - acc: 0.6660 - val_loss: 0.0670 - val_acc: 0.5436\n",
      "Epoch 2717/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0464 - acc: 0.6649 - val_loss: 0.0691 - val_acc: 0.5245\n",
      "Epoch 2718/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0464 - acc: 0.6663 - val_loss: 0.0678 - val_acc: 0.5262\n",
      "Epoch 2719/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0461 - acc: 0.6655 - val_loss: 0.0674 - val_acc: 0.5403\n",
      "Epoch 2720/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0464 - acc: 0.6641 - val_loss: 0.0667 - val_acc: 0.5370\n",
      "Epoch 2721/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0465 - acc: 0.6596 - val_loss: 0.0676 - val_acc: 0.5370\n",
      "Epoch 2722/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0464 - acc: 0.6610 - val_loss: 0.0667 - val_acc: 0.5436\n",
      "Epoch 2723/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0460 - acc: 0.6663 - val_loss: 0.0675 - val_acc: 0.5345\n",
      "Epoch 2724/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0460 - acc: 0.6627 - val_loss: 0.0680 - val_acc: 0.5478\n",
      "Epoch 2725/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0462 - acc: 0.6685 - val_loss: 0.0676 - val_acc: 0.5328\n",
      "Epoch 2726/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0459 - acc: 0.6716 - val_loss: 0.0678 - val_acc: 0.5387\n",
      "Epoch 2727/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0453 - acc: 0.6707 - val_loss: 0.0675 - val_acc: 0.5411\n",
      "Epoch 2728/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0463 - acc: 0.6652 - val_loss: 0.0676 - val_acc: 0.5320\n",
      "Epoch 2729/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0458 - acc: 0.6685 - val_loss: 0.0673 - val_acc: 0.5370\n",
      "Epoch 2730/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0460 - acc: 0.6685 - val_loss: 0.0675 - val_acc: 0.5378\n",
      "Epoch 2731/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0466 - acc: 0.6671 - val_loss: 0.0670 - val_acc: 0.5387\n",
      "Epoch 2732/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0459 - acc: 0.6630 - val_loss: 0.0677 - val_acc: 0.5403\n",
      "Epoch 2733/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0465 - acc: 0.6646 - val_loss: 0.0665 - val_acc: 0.5370\n",
      "Epoch 2734/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0459 - acc: 0.6688 - val_loss: 0.0684 - val_acc: 0.5262\n",
      "Epoch 2735/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0462 - acc: 0.6641 - val_loss: 0.0671 - val_acc: 0.5428\n",
      "Epoch 2736/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0463 - acc: 0.6691 - val_loss: 0.0675 - val_acc: 0.5312\n",
      "Epoch 2737/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0466 - acc: 0.6632 - val_loss: 0.0671 - val_acc: 0.5411\n",
      "Epoch 2738/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6613 - val_loss: 0.0693 - val_acc: 0.5353\n",
      "Epoch 2739/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0487 - acc: 0.6499 - val_loss: 0.0689 - val_acc: 0.5287\n",
      "Epoch 2740/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0480 - acc: 0.6502 - val_loss: 0.0683 - val_acc: 0.5370\n",
      "Epoch 2741/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0472 - acc: 0.6572 - val_loss: 0.0669 - val_acc: 0.5403\n",
      "Epoch 2742/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0497 - acc: 0.6438 - val_loss: 0.0674 - val_acc: 0.5353\n",
      "Epoch 2743/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0483 - acc: 0.6508 - val_loss: 0.0673 - val_acc: 0.5420\n",
      "Epoch 2744/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0465 - acc: 0.6635 - val_loss: 0.0674 - val_acc: 0.5337\n",
      "Epoch 2745/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0470 - acc: 0.6569 - val_loss: 0.0694 - val_acc: 0.5179\n",
      "Epoch 2746/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0477 - acc: 0.6563 - val_loss: 0.0671 - val_acc: 0.5395\n",
      "Epoch 2747/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6619 - val_loss: 0.0670 - val_acc: 0.5387\n",
      "Epoch 2748/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0471 - acc: 0.6569 - val_loss: 0.0675 - val_acc: 0.5428\n",
      "Epoch 2749/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0460 - acc: 0.6671 - val_loss: 0.0673 - val_acc: 0.5328\n",
      "Epoch 2750/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0462 - acc: 0.6688 - val_loss: 0.0663 - val_acc: 0.5428\n",
      "Epoch 2751/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0459 - acc: 0.6682 - val_loss: 0.0668 - val_acc: 0.5436\n",
      "Epoch 2752/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6644 - val_loss: 0.0662 - val_acc: 0.5428\n",
      "Epoch 2753/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0458 - acc: 0.6691 - val_loss: 0.0668 - val_acc: 0.5461\n",
      "Epoch 2754/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0457 - acc: 0.6666 - val_loss: 0.0680 - val_acc: 0.5445\n",
      "Epoch 2755/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0453 - acc: 0.6724 - val_loss: 0.0695 - val_acc: 0.5187\n",
      "Epoch 2756/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0458 - acc: 0.6754 - val_loss: 0.0678 - val_acc: 0.5387\n",
      "Epoch 2757/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0460 - acc: 0.6688 - val_loss: 0.0682 - val_acc: 0.5345\n",
      "Epoch 2758/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0459 - acc: 0.6660 - val_loss: 0.0675 - val_acc: 0.5370\n",
      "Epoch 2759/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0458 - acc: 0.6710 - val_loss: 0.0674 - val_acc: 0.5403\n",
      "Epoch 2760/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0456 - acc: 0.6716 - val_loss: 0.0680 - val_acc: 0.5395\n",
      "Epoch 2761/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0456 - acc: 0.6718 - val_loss: 0.0675 - val_acc: 0.5337\n",
      "Epoch 2762/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6644 - val_loss: 0.0675 - val_acc: 0.5345\n",
      "Epoch 2763/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0475 - acc: 0.6549 - val_loss: 0.0669 - val_acc: 0.5387\n",
      "Epoch 2764/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0475 - acc: 0.6583 - val_loss: 0.0669 - val_acc: 0.5395\n",
      "Epoch 2765/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6541 - val_loss: 0.0686 - val_acc: 0.5270\n",
      "Epoch 2766/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0476 - acc: 0.6558 - val_loss: 0.0692 - val_acc: 0.5204\n",
      "Epoch 2767/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6560 - val_loss: 0.0674 - val_acc: 0.5345\n",
      "Epoch 2768/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0462 - acc: 0.6657 - val_loss: 0.0676 - val_acc: 0.5378\n",
      "Epoch 2769/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0456 - acc: 0.6693 - val_loss: 0.0674 - val_acc: 0.5328\n",
      "Epoch 2770/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0458 - acc: 0.6680 - val_loss: 0.0692 - val_acc: 0.5303\n",
      "Epoch 2771/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0463 - acc: 0.6635 - val_loss: 0.0674 - val_acc: 0.5436\n",
      "Epoch 2772/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0465 - acc: 0.6638 - val_loss: 0.0677 - val_acc: 0.5303\n",
      "Epoch 2773/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0471 - acc: 0.6585 - val_loss: 0.0671 - val_acc: 0.5328\n",
      "Epoch 2774/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0461 - acc: 0.6696 - val_loss: 0.0670 - val_acc: 0.5328\n",
      "Epoch 2775/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0457 - acc: 0.6680 - val_loss: 0.0676 - val_acc: 0.5353\n",
      "Epoch 2776/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0461 - acc: 0.6663 - val_loss: 0.0670 - val_acc: 0.5320\n",
      "Epoch 2777/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6632 - val_loss: 0.0679 - val_acc: 0.5278\n",
      "Epoch 2778/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0482 - acc: 0.6530 - val_loss: 0.0669 - val_acc: 0.5411\n",
      "Epoch 2779/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0469 - acc: 0.6624 - val_loss: 0.0668 - val_acc: 0.5353\n",
      "Epoch 2780/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6630 - val_loss: 0.0678 - val_acc: 0.5420\n",
      "Epoch 2781/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6588 - val_loss: 0.0671 - val_acc: 0.5362\n",
      "Epoch 2782/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6613 - val_loss: 0.0673 - val_acc: 0.5303\n",
      "Epoch 2783/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0473 - acc: 0.6580 - val_loss: 0.0664 - val_acc: 0.5445\n",
      "Epoch 2784/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0469 - acc: 0.6580 - val_loss: 0.0668 - val_acc: 0.5370\n",
      "Epoch 2785/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0470 - acc: 0.6641 - val_loss: 0.0669 - val_acc: 0.5337\n",
      "Epoch 2786/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0463 - acc: 0.6644 - val_loss: 0.0667 - val_acc: 0.5403\n",
      "Epoch 2787/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0461 - acc: 0.6671 - val_loss: 0.0679 - val_acc: 0.5254\n",
      "Epoch 2788/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0460 - acc: 0.6685 - val_loss: 0.0675 - val_acc: 0.5353\n",
      "Epoch 2789/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0466 - acc: 0.6657 - val_loss: 0.0668 - val_acc: 0.5378\n",
      "Epoch 2790/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0456 - acc: 0.6716 - val_loss: 0.0676 - val_acc: 0.5229\n",
      "Epoch 2791/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0460 - acc: 0.6702 - val_loss: 0.0682 - val_acc: 0.5328\n",
      "Epoch 2792/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0468 - acc: 0.6624 - val_loss: 0.0667 - val_acc: 0.5403\n",
      "Epoch 2793/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0463 - acc: 0.6663 - val_loss: 0.0670 - val_acc: 0.5411\n",
      "Epoch 2794/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0466 - acc: 0.6663 - val_loss: 0.0665 - val_acc: 0.5403\n",
      "Epoch 2795/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0461 - acc: 0.6688 - val_loss: 0.0681 - val_acc: 0.5204\n",
      "Epoch 2796/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0465 - acc: 0.6635 - val_loss: 0.0672 - val_acc: 0.5445\n",
      "Epoch 2797/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6630 - val_loss: 0.0666 - val_acc: 0.5328\n",
      "Epoch 2798/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0463 - acc: 0.6641 - val_loss: 0.0671 - val_acc: 0.5411\n",
      "Epoch 2799/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0465 - acc: 0.6638 - val_loss: 0.0689 - val_acc: 0.5370\n",
      "Epoch 2800/3000\n",
      "3608/3608 [==============================] - 1s 160us/step - loss: 0.0462 - acc: 0.6685 - val_loss: 0.0675 - val_acc: 0.5353\n",
      "Epoch 2801/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6649 - val_loss: 0.0674 - val_acc: 0.5320\n",
      "Epoch 2802/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0498 - acc: 0.6419 - val_loss: 0.0667 - val_acc: 0.5387\n",
      "Epoch 2803/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6533 - val_loss: 0.0670 - val_acc: 0.5362\n",
      "Epoch 2804/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0475 - acc: 0.6502 - val_loss: 0.0676 - val_acc: 0.5270\n",
      "Epoch 2805/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0471 - acc: 0.6563 - val_loss: 0.0684 - val_acc: 0.5337\n",
      "Epoch 2806/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0467 - acc: 0.6635 - val_loss: 0.0669 - val_acc: 0.5403\n",
      "Epoch 2807/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0459 - acc: 0.6741 - val_loss: 0.0671 - val_acc: 0.5345\n",
      "Epoch 2808/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0475 - acc: 0.6563 - val_loss: 0.0680 - val_acc: 0.5320\n",
      "Epoch 2809/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0473 - acc: 0.6591 - val_loss: 0.0669 - val_acc: 0.5436\n",
      "Epoch 2810/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0472 - acc: 0.6602 - val_loss: 0.0673 - val_acc: 0.5295\n",
      "Epoch 2811/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0463 - acc: 0.6655 - val_loss: 0.0674 - val_acc: 0.5254\n",
      "Epoch 2812/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0466 - acc: 0.6599 - val_loss: 0.0671 - val_acc: 0.5345\n",
      "Epoch 2813/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0462 - acc: 0.6718 - val_loss: 0.0666 - val_acc: 0.5428\n",
      "Epoch 2814/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0468 - acc: 0.6660 - val_loss: 0.0676 - val_acc: 0.5403\n",
      "Epoch 2815/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0475 - acc: 0.6558 - val_loss: 0.0665 - val_acc: 0.5387\n",
      "Epoch 2816/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0472 - acc: 0.6621 - val_loss: 0.0668 - val_acc: 0.5503\n",
      "Epoch 2817/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0467 - acc: 0.6627 - val_loss: 0.0677 - val_acc: 0.5237\n",
      "Epoch 2818/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0467 - acc: 0.6616 - val_loss: 0.0678 - val_acc: 0.5353\n",
      "Epoch 2819/3000\n",
      "3608/3608 [==============================] - 1s 162us/step - loss: 0.0466 - acc: 0.6621 - val_loss: 0.0693 - val_acc: 0.5295\n",
      "Epoch 2820/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0469 - acc: 0.6602 - val_loss: 0.0680 - val_acc: 0.5345\n",
      "Epoch 2821/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6652 - val_loss: 0.0702 - val_acc: 0.5262\n",
      "Epoch 2822/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0500 - acc: 0.6444 - val_loss: 0.0667 - val_acc: 0.5328\n",
      "Epoch 2823/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0474 - acc: 0.6533 - val_loss: 0.0671 - val_acc: 0.5353\n",
      "Epoch 2824/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0476 - acc: 0.6585 - val_loss: 0.0661 - val_acc: 0.5470\n",
      "Epoch 2825/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0467 - acc: 0.6644 - val_loss: 0.0673 - val_acc: 0.5370\n",
      "Epoch 2826/3000\n",
      "3608/3608 [==============================] - 1s 161us/step - loss: 0.0465 - acc: 0.6652 - val_loss: 0.0664 - val_acc: 0.5387\n",
      "Epoch 2827/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0461 - acc: 0.6685 - val_loss: 0.0671 - val_acc: 0.5378\n",
      "Epoch 2828/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0469 - acc: 0.6572 - val_loss: 0.0672 - val_acc: 0.5320\n",
      "Epoch 2829/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0467 - acc: 0.6555 - val_loss: 0.0668 - val_acc: 0.5370\n",
      "Epoch 2830/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6599 - val_loss: 0.0673 - val_acc: 0.5420\n",
      "Epoch 2831/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0469 - acc: 0.6621 - val_loss: 0.0663 - val_acc: 0.5470\n",
      "Epoch 2832/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6583 - val_loss: 0.0667 - val_acc: 0.5486\n",
      "Epoch 2833/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0470 - acc: 0.6594 - val_loss: 0.0675 - val_acc: 0.5312\n",
      "Epoch 2834/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0471 - acc: 0.6632 - val_loss: 0.0675 - val_acc: 0.5370\n",
      "Epoch 2835/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0460 - acc: 0.6693 - val_loss: 0.0668 - val_acc: 0.5453\n",
      "Epoch 2836/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0461 - acc: 0.6666 - val_loss: 0.0673 - val_acc: 0.5345\n",
      "Epoch 2837/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0461 - acc: 0.6652 - val_loss: 0.0673 - val_acc: 0.5436\n",
      "Epoch 2838/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0459 - acc: 0.6691 - val_loss: 0.0673 - val_acc: 0.5337\n",
      "Epoch 2839/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0460 - acc: 0.6660 - val_loss: 0.0673 - val_acc: 0.5378\n",
      "Epoch 2840/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0462 - acc: 0.6646 - val_loss: 0.0673 - val_acc: 0.5370\n",
      "Epoch 2841/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0460 - acc: 0.6685 - val_loss: 0.0670 - val_acc: 0.5370\n",
      "Epoch 2842/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0459 - acc: 0.6669 - val_loss: 0.0672 - val_acc: 0.5362\n",
      "Epoch 2843/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0462 - acc: 0.6621 - val_loss: 0.0679 - val_acc: 0.5353\n",
      "Epoch 2844/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0464 - acc: 0.6660 - val_loss: 0.0676 - val_acc: 0.5337\n",
      "Epoch 2845/3000\n",
      "3608/3608 [==============================] - 1s 163us/step - loss: 0.0462 - acc: 0.6682 - val_loss: 0.0691 - val_acc: 0.5187\n",
      "Epoch 2846/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0465 - acc: 0.6641 - val_loss: 0.0678 - val_acc: 0.5353\n",
      "Epoch 2847/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0471 - acc: 0.6613 - val_loss: 0.0663 - val_acc: 0.5478\n",
      "Epoch 2848/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0494 - acc: 0.6408 - val_loss: 0.0662 - val_acc: 0.5370\n",
      "Epoch 2849/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0465 - acc: 0.6602 - val_loss: 0.0677 - val_acc: 0.5370\n",
      "Epoch 2850/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0471 - acc: 0.6610 - val_loss: 0.0667 - val_acc: 0.5428\n",
      "Epoch 2851/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6641 - val_loss: 0.0682 - val_acc: 0.5328\n",
      "Epoch 2852/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0463 - acc: 0.6682 - val_loss: 0.0681 - val_acc: 0.5345\n",
      "Epoch 2853/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0462 - acc: 0.6696 - val_loss: 0.0673 - val_acc: 0.5312\n",
      "Epoch 2854/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0466 - acc: 0.6680 - val_loss: 0.0680 - val_acc: 0.5411\n",
      "Epoch 2855/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0466 - acc: 0.6616 - val_loss: 0.0683 - val_acc: 0.5295\n",
      "Epoch 2856/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0473 - acc: 0.6608 - val_loss: 0.0679 - val_acc: 0.5353\n",
      "Epoch 2857/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0475 - acc: 0.6569 - val_loss: 0.0673 - val_acc: 0.5320\n",
      "Epoch 2858/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0466 - acc: 0.6663 - val_loss: 0.0680 - val_acc: 0.5337\n",
      "Epoch 2859/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0469 - acc: 0.6638 - val_loss: 0.0672 - val_acc: 0.5395\n",
      "Epoch 2860/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0473 - acc: 0.6572 - val_loss: 0.0686 - val_acc: 0.5287\n",
      "Epoch 2861/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0479 - acc: 0.6527 - val_loss: 0.0691 - val_acc: 0.5170\n",
      "Epoch 2862/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0488 - acc: 0.6452 - val_loss: 0.0669 - val_acc: 0.5403\n",
      "Epoch 2863/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0468 - acc: 0.6635 - val_loss: 0.0679 - val_acc: 0.5303\n",
      "Epoch 2864/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0465 - acc: 0.6610 - val_loss: 0.0669 - val_acc: 0.5362\n",
      "Epoch 2865/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0459 - acc: 0.6682 - val_loss: 0.0671 - val_acc: 0.5403\n",
      "Epoch 2866/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0459 - acc: 0.6669 - val_loss: 0.0674 - val_acc: 0.5403\n",
      "Epoch 2867/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0460 - acc: 0.6657 - val_loss: 0.0682 - val_acc: 0.5287\n",
      "Epoch 2868/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0463 - acc: 0.6630 - val_loss: 0.0668 - val_acc: 0.5387\n",
      "Epoch 2869/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0463 - acc: 0.6666 - val_loss: 0.0667 - val_acc: 0.5362\n",
      "Epoch 2870/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0455 - acc: 0.6727 - val_loss: 0.0684 - val_acc: 0.5220\n",
      "Epoch 2871/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0469 - acc: 0.6602 - val_loss: 0.0682 - val_acc: 0.5370\n",
      "Epoch 2872/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6666 - val_loss: 0.0687 - val_acc: 0.5312\n",
      "Epoch 2873/3000\n",
      "3608/3608 [==============================] - 1s 177us/step - loss: 0.0469 - acc: 0.6616 - val_loss: 0.0684 - val_acc: 0.5287\n",
      "Epoch 2874/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0468 - acc: 0.6652 - val_loss: 0.0682 - val_acc: 0.5287\n",
      "Epoch 2875/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0465 - acc: 0.6657 - val_loss: 0.0676 - val_acc: 0.5328\n",
      "Epoch 2876/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0461 - acc: 0.6713 - val_loss: 0.0678 - val_acc: 0.5395\n",
      "Epoch 2877/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0478 - acc: 0.6596 - val_loss: 0.0669 - val_acc: 0.5445\n",
      "Epoch 2878/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0471 - acc: 0.6602 - val_loss: 0.0667 - val_acc: 0.5395\n",
      "Epoch 2879/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0466 - acc: 0.6649 - val_loss: 0.0671 - val_acc: 0.5403\n",
      "Epoch 2880/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0463 - acc: 0.6657 - val_loss: 0.0673 - val_acc: 0.5378\n",
      "Epoch 2881/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0460 - acc: 0.6693 - val_loss: 0.0678 - val_acc: 0.5378\n",
      "Epoch 2882/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0457 - acc: 0.6669 - val_loss: 0.0672 - val_acc: 0.5353\n",
      "Epoch 2883/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0460 - acc: 0.6674 - val_loss: 0.0678 - val_acc: 0.5420\n",
      "Epoch 2884/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0462 - acc: 0.6693 - val_loss: 0.0679 - val_acc: 0.5337\n",
      "Epoch 2885/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0460 - acc: 0.6699 - val_loss: 0.0681 - val_acc: 0.5395\n",
      "Epoch 2886/3000\n",
      "3608/3608 [==============================] - 1s 176us/step - loss: 0.0461 - acc: 0.6652 - val_loss: 0.0684 - val_acc: 0.5270\n",
      "Epoch 2887/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0455 - acc: 0.6710 - val_loss: 0.0676 - val_acc: 0.5353\n",
      "Epoch 2888/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0465 - acc: 0.6644 - val_loss: 0.0667 - val_acc: 0.5436\n",
      "Epoch 2889/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0459 - acc: 0.6685 - val_loss: 0.0702 - val_acc: 0.5071\n",
      "Epoch 2890/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0465 - acc: 0.6632 - val_loss: 0.0674 - val_acc: 0.5328\n",
      "Epoch 2891/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0476 - acc: 0.6549 - val_loss: 0.0673 - val_acc: 0.5378\n",
      "Epoch 2892/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0479 - acc: 0.6583 - val_loss: 0.0672 - val_acc: 0.5337\n",
      "Epoch 2893/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0474 - acc: 0.6580 - val_loss: 0.0672 - val_acc: 0.5303\n",
      "Epoch 2894/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6641 - val_loss: 0.0677 - val_acc: 0.5337\n",
      "Epoch 2895/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0496 - acc: 0.6317 - val_loss: 0.0656 - val_acc: 0.5503\n",
      "Epoch 2896/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0511 - acc: 0.6208 - val_loss: 0.0675 - val_acc: 0.5411\n",
      "Epoch 2897/3000\n",
      "3608/3608 [==============================] - 1s 181us/step - loss: 0.0490 - acc: 0.6422 - val_loss: 0.0663 - val_acc: 0.5486\n",
      "Epoch 2898/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0479 - acc: 0.6505 - val_loss: 0.0668 - val_acc: 0.5411\n",
      "Epoch 2899/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0471 - acc: 0.6580 - val_loss: 0.0673 - val_acc: 0.5353\n",
      "Epoch 2900/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6616 - val_loss: 0.0670 - val_acc: 0.5478\n",
      "Epoch 2901/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0464 - acc: 0.6641 - val_loss: 0.0673 - val_acc: 0.5370\n",
      "Epoch 2902/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0466 - acc: 0.6627 - val_loss: 0.0674 - val_acc: 0.5337\n",
      "Epoch 2903/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0461 - acc: 0.6605 - val_loss: 0.0672 - val_acc: 0.5436\n",
      "Epoch 2904/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0457 - acc: 0.6688 - val_loss: 0.0671 - val_acc: 0.5353\n",
      "Epoch 2905/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0462 - acc: 0.6660 - val_loss: 0.0682 - val_acc: 0.5320\n",
      "Epoch 2906/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0468 - acc: 0.6605 - val_loss: 0.0671 - val_acc: 0.5345\n",
      "Epoch 2907/3000\n",
      "3608/3608 [==============================] - 1s 179us/step - loss: 0.0472 - acc: 0.6572 - val_loss: 0.0674 - val_acc: 0.5403\n",
      "Epoch 2908/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0464 - acc: 0.6674 - val_loss: 0.0673 - val_acc: 0.5403\n",
      "Epoch 2909/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0467 - acc: 0.6669 - val_loss: 0.0672 - val_acc: 0.5345\n",
      "Epoch 2910/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0467 - acc: 0.6652 - val_loss: 0.0671 - val_acc: 0.5445\n",
      "Epoch 2911/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0466 - acc: 0.6657 - val_loss: 0.0682 - val_acc: 0.5420\n",
      "Epoch 2912/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0472 - acc: 0.6585 - val_loss: 0.0665 - val_acc: 0.5403\n",
      "Epoch 2913/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0466 - acc: 0.6608 - val_loss: 0.0665 - val_acc: 0.5445\n",
      "Epoch 2914/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0462 - acc: 0.6657 - val_loss: 0.0670 - val_acc: 0.5378\n",
      "Epoch 2915/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0456 - acc: 0.6696 - val_loss: 0.0691 - val_acc: 0.5195\n",
      "Epoch 2916/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0464 - acc: 0.6677 - val_loss: 0.0673 - val_acc: 0.5378\n",
      "Epoch 2917/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0460 - acc: 0.6655 - val_loss: 0.0683 - val_acc: 0.5320\n",
      "Epoch 2918/3000\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0470 - acc: 0.6608 - val_loss: 0.0674 - val_acc: 0.5337\n",
      "Epoch 2919/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0467 - acc: 0.6655 - val_loss: 0.0678 - val_acc: 0.5345\n",
      "Epoch 2920/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0467 - acc: 0.6619 - val_loss: 0.0676 - val_acc: 0.5287\n",
      "Epoch 2921/3000\n",
      "3608/3608 [==============================] - 1s 180us/step - loss: 0.0471 - acc: 0.6535 - val_loss: 0.0673 - val_acc: 0.5312\n",
      "Epoch 2922/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0471 - acc: 0.6608 - val_loss: 0.0677 - val_acc: 0.5278\n",
      "Epoch 2923/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0463 - acc: 0.6666 - val_loss: 0.0669 - val_acc: 0.5411\n",
      "Epoch 2924/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0461 - acc: 0.6677 - val_loss: 0.0680 - val_acc: 0.5337\n",
      "Epoch 2925/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0458 - acc: 0.6680 - val_loss: 0.0674 - val_acc: 0.5387\n",
      "Epoch 2926/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0459 - acc: 0.6674 - val_loss: 0.0674 - val_acc: 0.5387\n",
      "Epoch 2927/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0456 - acc: 0.6691 - val_loss: 0.0678 - val_acc: 0.5237\n",
      "Epoch 2928/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0460 - acc: 0.6705 - val_loss: 0.0676 - val_acc: 0.5345\n",
      "Epoch 2929/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0460 - acc: 0.6718 - val_loss: 0.0682 - val_acc: 0.5420\n",
      "Epoch 2930/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6671 - val_loss: 0.0676 - val_acc: 0.5328\n",
      "Epoch 2931/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0468 - acc: 0.6624 - val_loss: 0.0679 - val_acc: 0.5270\n",
      "Epoch 2932/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0460 - acc: 0.6721 - val_loss: 0.0668 - val_acc: 0.5303\n",
      "Epoch 2933/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0462 - acc: 0.6660 - val_loss: 0.0675 - val_acc: 0.5353\n",
      "Epoch 2934/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0462 - acc: 0.6669 - val_loss: 0.0673 - val_acc: 0.5320\n",
      "Epoch 2935/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0463 - acc: 0.6660 - val_loss: 0.0683 - val_acc: 0.5362\n",
      "Epoch 2936/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0460 - acc: 0.6699 - val_loss: 0.0666 - val_acc: 0.5320\n",
      "Epoch 2937/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0466 - acc: 0.6666 - val_loss: 0.0677 - val_acc: 0.5378\n",
      "Epoch 2938/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0465 - acc: 0.6657 - val_loss: 0.0691 - val_acc: 0.5237\n",
      "Epoch 2939/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0462 - acc: 0.6666 - val_loss: 0.0697 - val_acc: 0.5195\n",
      "Epoch 2940/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0489 - acc: 0.6547 - val_loss: 0.0675 - val_acc: 0.5370\n",
      "Epoch 2941/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0467 - acc: 0.6627 - val_loss: 0.0667 - val_acc: 0.5403\n",
      "Epoch 2942/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0459 - acc: 0.6669 - val_loss: 0.0676 - val_acc: 0.5428\n",
      "Epoch 2943/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0457 - acc: 0.6666 - val_loss: 0.0686 - val_acc: 0.5270\n",
      "Epoch 2944/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0461 - acc: 0.6660 - val_loss: 0.0675 - val_acc: 0.5337\n",
      "Epoch 2945/3000\n",
      "3608/3608 [==============================] - 1s 182us/step - loss: 0.0460 - acc: 0.6663 - val_loss: 0.0668 - val_acc: 0.5411\n",
      "Epoch 2946/3000\n",
      "3608/3608 [==============================] - 1s 217us/step - loss: 0.0456 - acc: 0.6735 - val_loss: 0.0679 - val_acc: 0.5353\n",
      "Epoch 2947/3000\n",
      "3608/3608 [==============================] - 1s 219us/step - loss: 0.0470 - acc: 0.6644 - val_loss: 0.0663 - val_acc: 0.5470\n",
      "Epoch 2948/3000\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0473 - acc: 0.6591 - val_loss: 0.0679 - val_acc: 0.5328\n",
      "Epoch 2949/3000\n",
      "3608/3608 [==============================] - 1s 212us/step - loss: 0.0467 - acc: 0.6635 - val_loss: 0.0664 - val_acc: 0.5370\n",
      "Epoch 2950/3000\n",
      "3608/3608 [==============================] - 1s 214us/step - loss: 0.0463 - acc: 0.6663 - val_loss: 0.0675 - val_acc: 0.5395\n",
      "Epoch 2951/3000\n",
      "3608/3608 [==============================] - 1s 211us/step - loss: 0.0464 - acc: 0.6649 - val_loss: 0.0673 - val_acc: 0.5370\n",
      "Epoch 2952/3000\n",
      "3608/3608 [==============================] - 1s 209us/step - loss: 0.0457 - acc: 0.6707 - val_loss: 0.0677 - val_acc: 0.5345\n",
      "Epoch 2953/3000\n",
      "3608/3608 [==============================] - 1s 216us/step - loss: 0.0466 - acc: 0.6613 - val_loss: 0.0673 - val_acc: 0.5337\n",
      "Epoch 2954/3000\n",
      "3608/3608 [==============================] - 1s 219us/step - loss: 0.0457 - acc: 0.6705 - val_loss: 0.0680 - val_acc: 0.5320\n",
      "Epoch 2955/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0462 - acc: 0.6693 - val_loss: 0.0673 - val_acc: 0.5362\n",
      "Epoch 2956/3000\n",
      "3608/3608 [==============================] - 1s 208us/step - loss: 0.0460 - acc: 0.6691 - val_loss: 0.0670 - val_acc: 0.5370\n",
      "Epoch 2957/3000\n",
      "3608/3608 [==============================] - 1s 213us/step - loss: 0.0466 - acc: 0.6632 - val_loss: 0.0669 - val_acc: 0.5362\n",
      "Epoch 2958/3000\n",
      "3608/3608 [==============================] - 1s 207us/step - loss: 0.0460 - acc: 0.6655 - val_loss: 0.0680 - val_acc: 0.5337\n",
      "Epoch 2959/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0464 - acc: 0.6655 - val_loss: 0.0672 - val_acc: 0.5328\n",
      "Epoch 2960/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0461 - acc: 0.6588 - val_loss: 0.0677 - val_acc: 0.5370\n",
      "Epoch 2961/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0458 - acc: 0.6749 - val_loss: 0.0679 - val_acc: 0.5436\n",
      "Epoch 2962/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0458 - acc: 0.6660 - val_loss: 0.0679 - val_acc: 0.5320\n",
      "Epoch 2963/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0462 - acc: 0.6641 - val_loss: 0.0684 - val_acc: 0.5378\n",
      "Epoch 2964/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0464 - acc: 0.6580 - val_loss: 0.0676 - val_acc: 0.5370\n",
      "Epoch 2965/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0465 - acc: 0.6610 - val_loss: 0.0675 - val_acc: 0.5345\n",
      "Epoch 2966/3000\n",
      "3608/3608 [==============================] - 1s 175us/step - loss: 0.0467 - acc: 0.6599 - val_loss: 0.0672 - val_acc: 0.5395\n",
      "Epoch 2967/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0471 - acc: 0.6569 - val_loss: 0.0671 - val_acc: 0.5411\n",
      "Epoch 2968/3000\n",
      "3608/3608 [==============================] - 1s 178us/step - loss: 0.0465 - acc: 0.6663 - val_loss: 0.0678 - val_acc: 0.5395\n",
      "Epoch 2969/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0468 - acc: 0.6610 - val_loss: 0.0685 - val_acc: 0.5262\n",
      "Epoch 2970/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0464 - acc: 0.6663 - val_loss: 0.0682 - val_acc: 0.5395\n",
      "Epoch 2971/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0462 - acc: 0.6641 - val_loss: 0.0673 - val_acc: 0.5337\n",
      "Epoch 2972/3000\n",
      "3608/3608 [==============================] - 1s 174us/step - loss: 0.0465 - acc: 0.6641 - val_loss: 0.0676 - val_acc: 0.5287\n",
      "Epoch 2973/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0479 - acc: 0.6491 - val_loss: 0.0679 - val_acc: 0.5229\n",
      "Epoch 2974/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0487 - acc: 0.6472 - val_loss: 0.0661 - val_acc: 0.5445\n",
      "Epoch 2975/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0471 - acc: 0.6563 - val_loss: 0.0682 - val_acc: 0.5353\n",
      "Epoch 2976/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0469 - acc: 0.6630 - val_loss: 0.0686 - val_acc: 0.5262\n",
      "Epoch 2977/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0460 - acc: 0.6677 - val_loss: 0.0676 - val_acc: 0.5387\n",
      "Epoch 2978/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0459 - acc: 0.6713 - val_loss: 0.0671 - val_acc: 0.5370\n",
      "Epoch 2979/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0468 - acc: 0.6655 - val_loss: 0.0681 - val_acc: 0.5328\n",
      "Epoch 2980/3000\n",
      "3608/3608 [==============================] - 1s 171us/step - loss: 0.0462 - acc: 0.6608 - val_loss: 0.0669 - val_acc: 0.5428\n",
      "Epoch 2981/3000\n",
      "3608/3608 [==============================] - 1s 173us/step - loss: 0.0460 - acc: 0.6688 - val_loss: 0.0675 - val_acc: 0.5312\n",
      "Epoch 2982/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0461 - acc: 0.6669 - val_loss: 0.0674 - val_acc: 0.5320\n",
      "Epoch 2983/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0459 - acc: 0.6680 - val_loss: 0.0676 - val_acc: 0.5320\n",
      "Epoch 2984/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0460 - acc: 0.6638 - val_loss: 0.0673 - val_acc: 0.5420\n",
      "Epoch 2985/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0458 - acc: 0.6685 - val_loss: 0.0672 - val_acc: 0.5362\n",
      "Epoch 2986/3000\n",
      "3608/3608 [==============================] - 1s 172us/step - loss: 0.0456 - acc: 0.6716 - val_loss: 0.0681 - val_acc: 0.5320\n",
      "Epoch 2987/3000\n",
      "3608/3608 [==============================] - 1s 166us/step - loss: 0.0462 - acc: 0.6705 - val_loss: 0.0667 - val_acc: 0.5453\n",
      "Epoch 2988/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6652 - val_loss: 0.0680 - val_acc: 0.5362\n",
      "Epoch 2989/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0480 - acc: 0.6522 - val_loss: 0.0680 - val_acc: 0.5362\n",
      "Epoch 2990/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0463 - acc: 0.6621 - val_loss: 0.0678 - val_acc: 0.5337\n",
      "Epoch 2991/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0465 - acc: 0.6646 - val_loss: 0.0676 - val_acc: 0.5378\n",
      "Epoch 2992/3000\n",
      "3608/3608 [==============================] - 1s 165us/step - loss: 0.0499 - acc: 0.6402 - val_loss: 0.0677 - val_acc: 0.5312\n",
      "Epoch 2993/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0479 - acc: 0.6502 - val_loss: 0.0678 - val_acc: 0.5420\n",
      "Epoch 2994/3000\n",
      "3608/3608 [==============================] - 1s 168us/step - loss: 0.0487 - acc: 0.6466 - val_loss: 0.0671 - val_acc: 0.5303\n",
      "Epoch 2995/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0464 - acc: 0.6608 - val_loss: 0.0665 - val_acc: 0.5486\n",
      "Epoch 2996/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0457 - acc: 0.6641 - val_loss: 0.0673 - val_acc: 0.5353\n",
      "Epoch 2997/3000\n",
      "3608/3608 [==============================] - 1s 169us/step - loss: 0.0465 - acc: 0.6669 - val_loss: 0.0678 - val_acc: 0.5212\n",
      "Epoch 2998/3000\n",
      "3608/3608 [==============================] - 1s 170us/step - loss: 0.0460 - acc: 0.6641 - val_loss: 0.0678 - val_acc: 0.5370\n",
      "Epoch 2999/3000\n",
      "3608/3608 [==============================] - 1s 164us/step - loss: 0.0462 - acc: 0.6666 - val_loss: 0.0682 - val_acc: 0.5403\n",
      "Epoch 3000/3000\n",
      "3608/3608 [==============================] - 1s 167us/step - loss: 0.0457 - acc: 0.6677 - val_loss: 0.0680 - val_acc: 0.5295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77febe2668>"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='relu'))\n",
    "nn_model.add(Dense(units=16, activation='relu'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "nn_model.fit(x=train_x, y=train_y, epochs=3000, validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102170
    },
    "colab_type": "code",
    "id": "eTofz1rvRvHu",
    "outputId": "ddcbadfd-2504-481f-95ef-f126418f2b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3608 samples, validate on 1203 samples\n",
      "Epoch 1/3000\n",
      "3608/3608 [==============================] - 2s 435us/step - loss: 0.0842 - acc: 0.2725 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 2/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 3/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 4/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 5/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 6/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 7/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 8/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 9/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 10/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 11/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 12/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 13/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 14/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 15/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 16/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 17/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 18/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 19/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 20/3000\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0805 - val_acc: 0.3076\n",
      "Epoch 21/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 22/3000\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 23/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 24/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 25/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 26/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 27/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 28/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 29/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 30/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 31/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 32/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 33/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 34/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 35/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 36/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 37/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 38/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 39/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 40/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 41/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 42/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 43/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 44/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 45/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 46/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 47/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 48/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 49/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 50/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 51/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 52/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 53/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 54/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 55/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 56/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 57/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 58/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 59/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 60/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 61/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 62/3000\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 63/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 64/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 65/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 66/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 67/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 68/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 69/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 70/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 71/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 72/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 73/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 74/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 75/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 76/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 77/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 78/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 79/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 80/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 81/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 82/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 83/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 84/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 85/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 86/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 87/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 88/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 89/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 90/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 91/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 92/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 93/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 94/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 95/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 96/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 97/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 98/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 99/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 100/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 101/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 102/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 103/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 104/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 105/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 106/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 107/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 108/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 109/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 110/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 111/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 112/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 113/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 114/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 115/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 116/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 117/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 118/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 119/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 120/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 121/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 122/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 123/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 124/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 125/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 126/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 127/3000\n",
      "3608/3608 [==============================] - 1s 339us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 128/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 129/3000\n",
      "3608/3608 [==============================] - 1s 360us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 130/3000\n",
      "3608/3608 [==============================] - 1s 361us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 131/3000\n",
      "3608/3608 [==============================] - 1s 308us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 132/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 133/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 134/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 135/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 136/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 137/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 138/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 139/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 140/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 141/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 142/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 143/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 144/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 145/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 146/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 147/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 148/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 149/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 150/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 151/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 152/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 153/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 154/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 155/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 156/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 157/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 158/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 159/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 160/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 161/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 162/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 163/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 164/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 165/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 166/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 167/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 168/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 169/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 170/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 171/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 172/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 173/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 174/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 175/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 176/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 177/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 178/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 179/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 180/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 181/3000\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 182/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 183/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 184/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 185/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 186/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 187/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 188/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 189/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 190/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 191/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 192/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 193/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 194/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 195/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 196/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 197/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 198/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 199/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 200/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 201/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 202/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 203/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 204/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 205/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 206/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 207/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 208/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 209/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 210/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 211/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 212/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 213/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 214/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 215/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 216/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 217/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 218/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 219/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 220/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 221/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 222/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 223/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 224/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 225/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 226/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 227/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 228/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 229/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 230/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 231/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 232/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 233/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 234/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 235/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 236/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 237/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 238/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 239/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 240/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 241/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 242/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 243/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 244/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 245/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 246/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 247/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 248/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 249/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 250/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 251/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 252/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 253/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 254/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 255/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 256/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 257/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 258/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 259/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 260/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 261/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 262/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 263/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 264/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 265/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 266/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 267/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 268/3000\n",
      "3608/3608 [==============================] - 1s 337us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 269/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 270/3000\n",
      "3608/3608 [==============================] - 1s 355us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 271/3000\n",
      "3608/3608 [==============================] - 1s 355us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 272/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 273/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 274/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 275/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 276/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 277/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 278/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 279/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 280/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 281/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 282/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 283/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 284/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 285/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 286/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 287/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 288/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 289/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 290/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 291/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 292/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 293/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 294/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 295/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 296/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 297/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 298/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 299/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 300/3000\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 301/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 302/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 303/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 304/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 305/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 306/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 307/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 308/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 309/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 310/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 311/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 312/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 313/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 314/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 315/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 316/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 317/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 318/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 319/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 320/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 321/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 322/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 323/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 324/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 325/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 326/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 327/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 328/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 329/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 330/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 331/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 332/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 333/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 334/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 335/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 336/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 337/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 338/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 339/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 340/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 341/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 342/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 343/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 344/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 345/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 346/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 347/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 348/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 349/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 350/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 351/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 352/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 353/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 354/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 355/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 356/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 357/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 358/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 359/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 360/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 361/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 362/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 363/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 364/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 365/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 366/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 367/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 368/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 369/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 370/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 371/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 372/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 373/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 374/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 375/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 376/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 377/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 378/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 379/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 380/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 381/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 382/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 383/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 384/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 385/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 386/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 387/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 388/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 389/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 390/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 391/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 392/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 393/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 394/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 395/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 396/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 397/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 398/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 399/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 400/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 401/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 402/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 403/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 404/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 405/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 406/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 407/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 408/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 409/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 410/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 411/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 412/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 413/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 414/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 415/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 416/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 417/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 418/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 419/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 420/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 421/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 422/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 423/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 424/3000\n",
      "3608/3608 [==============================] - 1s 361us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 425/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 426/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 427/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 428/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 429/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 430/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 431/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 432/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 433/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 434/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 435/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 436/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 437/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 438/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 439/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 440/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 441/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 442/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 443/3000\n",
      "3608/3608 [==============================] - 1s 304us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 444/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 445/3000\n",
      "3608/3608 [==============================] - 1s 308us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 446/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 447/3000\n",
      "3608/3608 [==============================] - 1s 311us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 448/3000\n",
      "3608/3608 [==============================] - 1s 304us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 449/3000\n",
      "3608/3608 [==============================] - 1s 311us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 450/3000\n",
      "3608/3608 [==============================] - 1s 308us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 451/3000\n",
      "3608/3608 [==============================] - 1s 313us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 452/3000\n",
      "3608/3608 [==============================] - 1s 303us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 453/3000\n",
      "3608/3608 [==============================] - 1s 305us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 454/3000\n",
      "3608/3608 [==============================] - 1s 310us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 455/3000\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 456/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 457/3000\n",
      "3608/3608 [==============================] - 1s 304us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 458/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 459/3000\n",
      "3608/3608 [==============================] - 1s 305us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 460/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 461/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 462/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 463/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 464/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 465/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 466/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 467/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 468/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 469/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 470/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 471/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 472/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 473/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 474/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 475/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 476/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 477/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 478/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 479/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 480/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 481/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 482/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 483/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 484/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 485/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 486/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 487/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 488/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 489/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 490/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 491/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 492/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 493/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 494/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 495/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 496/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 497/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 498/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 499/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 500/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 501/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 502/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 503/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 504/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 505/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 506/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 507/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 508/3000\n",
      "3608/3608 [==============================] - 1s 305us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 509/3000\n",
      "3608/3608 [==============================] - 1s 306us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 510/3000\n",
      "3608/3608 [==============================] - 1s 305us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 511/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 512/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 513/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 514/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 515/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 516/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 517/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 518/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 519/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 520/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 521/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 522/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 523/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 524/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 525/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 526/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 527/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 528/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 529/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 530/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 531/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 532/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 533/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 534/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 535/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 536/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 537/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 538/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 539/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 540/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 541/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 542/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 543/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 544/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 545/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 546/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 547/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 548/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 549/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 550/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 551/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 552/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 553/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 554/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 555/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 556/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 557/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 558/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 559/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 560/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 561/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 562/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 563/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 564/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 565/3000\n",
      "3608/3608 [==============================] - 1s 320us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 566/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 567/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 568/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 569/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 570/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 571/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 572/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 573/3000\n",
      "3608/3608 [==============================] - 1s 317us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 574/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 575/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 576/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 577/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 578/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 579/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 580/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 581/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 582/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 583/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 584/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 585/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 586/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 587/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 588/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 589/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 590/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 591/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 592/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 593/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 594/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 595/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 596/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 597/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 598/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 599/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 600/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 601/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 602/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 603/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 604/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 605/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 606/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 607/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 608/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 609/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 610/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 611/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 612/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 613/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 614/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 615/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 616/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 617/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 618/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 619/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 620/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 621/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 622/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 623/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 624/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 625/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 626/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 627/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 628/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 629/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 630/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 631/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 632/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 633/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 634/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 635/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 636/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 637/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 638/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 639/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 640/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 641/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 642/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 643/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 644/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 645/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 646/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 647/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 648/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 649/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 650/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 651/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 652/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 653/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 654/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 655/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 656/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 657/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 658/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 659/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 660/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 661/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 662/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 663/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 664/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 665/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 666/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 667/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 668/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 669/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 670/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 671/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 672/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 673/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 674/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 675/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 676/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 677/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 678/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 679/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 680/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 681/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 682/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 683/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 684/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 685/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 686/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 687/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 688/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 689/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 690/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 691/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 692/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 693/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 694/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 695/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 696/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 697/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 698/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 699/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 700/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 701/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 702/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 703/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 704/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 705/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 706/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 707/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 708/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 709/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 710/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 711/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 712/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 713/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 714/3000\n",
      "3608/3608 [==============================] - 1s 371us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 715/3000\n",
      "3608/3608 [==============================] - 1s 366us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 716/3000\n",
      "3608/3608 [==============================] - 1s 372us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 717/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 718/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 719/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 720/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 721/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 722/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 723/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 724/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 725/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 726/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 727/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 728/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 729/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 730/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 731/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 732/3000\n",
      "3608/3608 [==============================] - 1s 303us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 733/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 734/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 735/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 736/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 737/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 738/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 739/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 740/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 741/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 742/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 743/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 744/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 745/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 746/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 747/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 748/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 749/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 750/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 751/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 752/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 753/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 754/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 755/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 756/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 757/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 758/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 759/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 760/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 761/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 762/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 763/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 764/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 765/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 766/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 767/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 768/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 769/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 770/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 771/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 772/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 773/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 774/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 775/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 776/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 777/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 778/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 779/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 780/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 781/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 782/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 783/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 784/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 785/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 786/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 787/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 788/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 789/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 790/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 791/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 792/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 793/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 794/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 795/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 796/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 797/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 798/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 799/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 800/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 801/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 802/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 803/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 804/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 805/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 806/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 807/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 808/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 809/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 810/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 811/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 812/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 813/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 814/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 815/3000\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 816/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 817/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 818/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 819/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 820/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 821/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 822/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 823/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 824/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 825/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 826/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 827/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 828/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 829/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 830/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 831/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 832/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 833/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 834/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 835/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 836/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 837/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 838/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 839/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 840/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 841/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 842/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 843/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 844/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 845/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 846/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 847/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 848/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 849/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 850/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 851/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 852/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 853/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 854/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 855/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 856/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 857/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 858/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 859/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 860/3000\n",
      "3608/3608 [==============================] - 1s 355us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 861/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 862/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 863/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 864/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 865/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 866/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 867/3000\n",
      "3608/3608 [==============================] - 1s 344us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 868/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 869/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 870/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 871/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 872/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 873/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 874/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 875/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 876/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 877/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 878/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 879/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 880/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 881/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 882/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 883/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 884/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 885/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 886/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 887/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 888/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 889/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 890/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 891/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 892/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 893/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 894/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 895/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 896/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 897/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 898/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 899/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 900/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 901/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 902/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 903/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 904/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 905/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 906/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 907/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 908/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 909/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 910/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 911/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 912/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 913/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 914/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 915/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 916/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 917/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 918/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 919/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 920/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 921/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 922/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 923/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 924/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 925/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 926/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 927/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 928/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 929/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 930/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 931/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 932/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 933/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 934/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 935/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 936/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 937/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 938/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 939/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 940/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 941/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 942/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 943/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 944/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 945/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 946/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 947/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 948/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 949/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 950/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 951/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 952/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 953/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 954/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 955/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 956/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 957/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 958/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 959/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 960/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 961/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 962/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 963/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 964/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 965/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 966/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 967/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 968/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 969/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 970/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 971/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 972/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 973/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 974/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 975/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 976/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 977/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 978/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 979/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 980/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 981/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 982/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 983/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 984/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 985/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 986/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 987/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 988/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 989/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 990/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 991/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 992/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 993/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 994/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 995/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 996/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 997/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 998/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 999/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1000/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1001/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1002/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1003/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1004/3000\n",
      "3608/3608 [==============================] - 1s 337us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1005/3000\n",
      "3608/3608 [==============================] - 1s 359us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1006/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1007/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1008/3000\n",
      "3608/3608 [==============================] - 1s 310us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1009/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1010/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1011/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1012/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1013/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1014/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1015/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1016/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1017/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1018/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1019/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1020/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1021/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1022/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1023/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1024/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1025/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1026/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1027/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1028/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 1029/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1030/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1031/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1032/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1033/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1034/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1035/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1036/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1037/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1038/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1039/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1040/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1041/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1042/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1043/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1044/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1045/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1046/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1047/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1048/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1049/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1050/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1051/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1052/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1053/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1054/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1055/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1056/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1057/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1058/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1059/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1060/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1061/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1062/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1063/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1064/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1065/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1066/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1067/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1068/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1069/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1070/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1071/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1072/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1073/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1074/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1075/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1076/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1077/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1078/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1079/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1080/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1081/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1082/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1083/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1084/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1085/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1086/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1087/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1088/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1089/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1090/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1091/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1092/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1093/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1094/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1095/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1096/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1097/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1098/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1099/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1100/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1101/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1102/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1103/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1104/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1105/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1106/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1107/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1108/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1109/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1110/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1111/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1112/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1113/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1114/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1115/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1116/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1117/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1118/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1119/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1120/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1121/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1122/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1123/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1124/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1125/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1126/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1127/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1128/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1129/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1130/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1131/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1132/3000\n",
      "3608/3608 [==============================] - 1s 305us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1133/3000\n",
      "3608/3608 [==============================] - 1s 305us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1134/3000\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1135/3000\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1136/3000\n",
      "3608/3608 [==============================] - 1s 304us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1137/3000\n",
      "3608/3608 [==============================] - 1s 305us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1138/3000\n",
      "3608/3608 [==============================] - 1s 303us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1139/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1140/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1141/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1142/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1143/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1144/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1145/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1146/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1147/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1148/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1149/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1150/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1151/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1152/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1153/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1154/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1155/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1156/3000\n",
      "3608/3608 [==============================] - 1s 355us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1157/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1158/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1159/3000\n",
      "3608/3608 [==============================] - 1s 361us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1160/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1161/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1162/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1163/3000\n",
      "3608/3608 [==============================] - 1s 347us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1164/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1165/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1166/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1167/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1168/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1169/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1170/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1171/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1172/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1173/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1174/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1175/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1176/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1177/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1178/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1179/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1180/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1181/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1182/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1183/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1184/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1185/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1186/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1187/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1188/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1189/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1190/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1191/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1192/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1193/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1194/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1195/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1196/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1197/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1198/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1199/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1200/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1201/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1202/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1203/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1204/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1205/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1206/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1207/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1208/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1209/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1210/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1211/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1212/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1213/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1214/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1215/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1216/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1217/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1218/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1219/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1220/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1221/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1222/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1223/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1224/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1225/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1226/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1227/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1228/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1229/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1230/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1231/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1232/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1233/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1234/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1235/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1236/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1237/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1238/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1239/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1240/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1241/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1242/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1243/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1244/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1245/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1246/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1247/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1248/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1249/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1250/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1251/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1252/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1253/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1254/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1255/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1256/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1257/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1258/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1259/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1260/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1261/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1262/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1263/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1264/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1265/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1266/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1267/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1268/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1269/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1270/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1271/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1272/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1273/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1274/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1275/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1276/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1277/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1278/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1279/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1280/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1281/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1282/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1283/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1284/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1285/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1286/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1287/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1288/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1289/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1290/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1291/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1292/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1293/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1294/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1295/3000\n",
      "3608/3608 [==============================] - 1s 362us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1296/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1297/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1298/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1299/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1300/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1301/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1302/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1303/3000\n",
      "3608/3608 [==============================] - 1s 308us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1304/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1305/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1306/3000\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1307/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1308/3000\n",
      "3608/3608 [==============================] - 1s 303us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1309/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1310/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1311/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1312/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1313/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1314/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1315/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1316/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1317/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1318/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1319/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1320/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1321/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1322/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1323/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1324/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1325/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1326/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1327/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1328/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1329/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1330/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1331/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1332/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1333/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1334/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1335/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1336/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1337/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1338/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1339/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1340/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1341/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1342/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1343/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1344/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1345/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1346/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1347/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1348/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1349/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1350/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1351/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1352/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1353/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1354/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1355/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1356/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1357/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1358/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1359/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1360/3000\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1361/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1362/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1363/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1364/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1365/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1366/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1367/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1368/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1369/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1370/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1371/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1372/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1373/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1374/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1375/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1376/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1377/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1378/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1379/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1380/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1381/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1382/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1383/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1384/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1385/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1386/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1387/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1388/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1389/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1390/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1391/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1392/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1393/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1394/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1395/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1396/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1397/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1398/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1399/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1400/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1401/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1402/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1403/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1404/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1405/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1406/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1407/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1408/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1409/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1410/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1411/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1412/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1413/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1414/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1415/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1416/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1417/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1418/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1419/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1420/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1421/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1422/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1423/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1424/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1425/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1426/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1427/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1428/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1429/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1430/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1431/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1432/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1433/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1434/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1435/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1436/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1437/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1438/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1439/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1440/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1441/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1442/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1443/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1444/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1445/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1446/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1447/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1448/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1449/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1450/3000\n",
      "3608/3608 [==============================] - 1s 317us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1451/3000\n",
      "3608/3608 [==============================] - 1s 352us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1452/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1453/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1454/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1455/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1456/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1457/3000\n",
      "3608/3608 [==============================] - 1s 352us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1458/3000\n",
      "3608/3608 [==============================] - 1s 319us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1459/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1460/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1461/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1462/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1463/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1464/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1465/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1466/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1467/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1468/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1469/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1470/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1471/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1472/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1473/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1474/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1475/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1476/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1477/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1478/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1479/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1480/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1481/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1482/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1483/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1484/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1485/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1486/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1487/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1488/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1489/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1490/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1491/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1492/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1493/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1494/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1495/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1496/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1497/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1498/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1499/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1500/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1501/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1502/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1503/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1504/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1505/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1506/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1507/3000\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1508/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1509/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1510/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1511/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1512/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1513/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1514/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1515/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1516/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1517/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1518/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1519/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1520/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1521/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1522/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1523/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1524/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1525/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1526/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1527/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1528/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1529/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1530/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1531/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1532/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1533/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1534/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1535/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1536/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1537/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1538/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1539/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1540/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1541/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1542/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1543/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1544/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1545/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1546/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1547/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1548/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1549/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1550/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1551/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1552/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1553/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1554/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1555/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1556/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1557/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1558/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1559/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1560/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1561/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1562/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1563/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1564/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1565/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1566/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1567/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1568/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1569/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1570/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1571/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1572/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1573/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1574/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1575/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1576/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1577/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1578/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1579/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1580/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1581/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1582/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1583/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1584/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1585/3000\n",
      "3608/3608 [==============================] - 1s 338us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1586/3000\n",
      "3608/3608 [==============================] - 1s 362us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1587/3000\n",
      "3608/3608 [==============================] - 1s 359us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1588/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1589/3000\n",
      "3608/3608 [==============================] - 1s 313us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1590/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1591/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1592/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1593/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1594/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1595/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1596/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1597/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1598/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1599/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1600/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1601/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1602/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1603/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1604/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1605/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1606/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1607/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1608/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1609/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1610/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1611/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1612/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1613/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1614/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1615/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1616/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1617/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1618/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1619/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1620/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1621/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1622/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1623/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1624/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1625/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1626/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1627/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1628/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1629/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1630/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1631/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1632/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1633/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1634/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1635/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1636/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1637/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1638/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1639/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1640/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1641/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1642/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1643/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1644/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1645/3000\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1646/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1647/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1648/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1649/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1650/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1651/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1652/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1653/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1654/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1655/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1656/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1657/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1658/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1659/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1660/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1661/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1662/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1663/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1664/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1665/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1666/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1667/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1668/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1669/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1670/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1671/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1672/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1673/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1674/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1675/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1676/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1677/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1678/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1679/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1680/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1681/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1682/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1683/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1684/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1685/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1686/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1687/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1688/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1689/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1690/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1691/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1692/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1693/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1694/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1695/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1696/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1697/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1698/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1699/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1700/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1701/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1702/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1703/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1704/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1705/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1706/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1707/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1708/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1709/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1710/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1711/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1712/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1713/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1714/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1715/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1716/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1717/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1718/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1719/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1720/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1721/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1722/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1723/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1724/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1725/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1726/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1727/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1728/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1729/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1730/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1731/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1732/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1733/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1734/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1735/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1736/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1737/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1738/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1739/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1740/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1741/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1742/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1743/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1744/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1745/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1746/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1747/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1748/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1749/3000\n",
      "3608/3608 [==============================] - 1s 333us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1750/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1751/3000\n",
      "3608/3608 [==============================] - 1s 359us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1752/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1753/3000\n",
      "3608/3608 [==============================] - 1s 352us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1754/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1755/3000\n",
      "3608/3608 [==============================] - 1s 359us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1756/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1757/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1758/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1759/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1760/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1761/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1762/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1763/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1764/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1765/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1766/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1767/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1768/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1769/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1770/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1771/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1772/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1773/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1774/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1775/3000\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1776/3000\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1777/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1778/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1779/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1780/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1781/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1782/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1783/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1784/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1785/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1786/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1787/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1788/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1789/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1790/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1791/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1792/3000\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1793/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1794/3000\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1795/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1796/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1797/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1798/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1799/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1800/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1801/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1802/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1803/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1804/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1805/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1806/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1807/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1808/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1809/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1810/3000\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1811/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1812/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1813/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1814/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1815/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1816/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1817/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1818/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1819/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1820/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1821/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1822/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1823/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1824/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1825/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1826/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1827/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1828/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1829/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1830/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1831/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1832/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1833/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1834/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1835/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1836/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1837/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1838/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1839/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1840/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1841/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1842/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1843/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1844/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1845/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1846/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1847/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1848/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1849/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1850/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1851/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1852/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1853/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1854/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1855/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1856/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1857/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1858/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1859/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1860/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1861/3000\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1862/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1863/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1864/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1865/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1866/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1867/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1868/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1869/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1870/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1871/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1872/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1873/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1874/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1875/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1876/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1877/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1878/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1879/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1880/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1881/3000\n",
      "3608/3608 [==============================] - 1s 332us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1882/3000\n",
      "3608/3608 [==============================] - 1s 361us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1883/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1884/3000\n",
      "3608/3608 [==============================] - 1s 359us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1885/3000\n",
      "3608/3608 [==============================] - 1s 326us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1886/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1887/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1888/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1889/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1890/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1891/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1892/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1893/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1894/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1895/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1896/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1897/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1898/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1899/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1900/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1901/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1902/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1903/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1904/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1905/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1906/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1907/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1908/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1909/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1910/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1911/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1912/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1913/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1914/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1915/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1916/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1917/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1918/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1919/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1920/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1921/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1922/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1923/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1924/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1925/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1926/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1927/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1928/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1929/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1930/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1931/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1932/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1933/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1934/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1935/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1936/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1937/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1938/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1939/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1940/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1941/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1942/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1943/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1944/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1945/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1946/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1947/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1948/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1949/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1950/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1951/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1952/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1953/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1954/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1955/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1956/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1957/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1958/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1959/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1960/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1961/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1962/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1963/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1964/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1965/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1966/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1967/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1968/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1969/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1970/3000\n",
      "3608/3608 [==============================] - 1s 306us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1971/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1972/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1973/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1974/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1975/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1976/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1977/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 1978/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1979/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1980/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1981/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1982/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1983/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1984/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1985/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1986/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1987/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1988/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1989/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1990/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1991/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1992/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1993/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1994/3000\n",
      "3608/3608 [==============================] - 1s 307us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1995/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1996/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 1997/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1998/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 1999/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2000/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2001/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2002/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2003/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2004/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2005/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2006/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2007/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2008/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2009/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2010/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2011/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2012/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2013/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2014/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2015/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2016/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 2017/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2018/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2019/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2020/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2021/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2022/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2023/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2024/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2025/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2026/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2027/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2028/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2029/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2030/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2031/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2032/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2033/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2034/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2035/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2036/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2037/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2038/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2039/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2040/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2041/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2042/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2043/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2044/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2045/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2046/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2047/3000\n",
      "3608/3608 [==============================] - 1s 355us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2048/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2049/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2050/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2051/3000\n",
      "3608/3608 [==============================] - 1s 352us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2052/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2053/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2054/3000\n",
      "3608/3608 [==============================] - 1s 348us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2055/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2056/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2057/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2058/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2059/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2060/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2061/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2062/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2063/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2064/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2065/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2066/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2067/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2068/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2069/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2070/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2071/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2072/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2073/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2074/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2075/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2076/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2077/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2078/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2079/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2080/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2081/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2082/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2083/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2084/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2085/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2086/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2087/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2088/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2089/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2090/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2091/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2092/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2093/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2094/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2095/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2096/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2097/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2098/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2099/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2100/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2101/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 2102/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2103/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2104/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2105/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2106/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2107/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2108/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2109/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2110/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2111/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2112/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2113/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2114/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2115/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2116/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2117/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2118/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2119/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2120/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2121/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2122/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2123/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2124/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2125/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2126/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2127/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2128/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2129/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2130/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2131/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2132/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2133/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2134/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2135/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2136/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2137/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2138/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2139/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2140/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2141/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2142/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2143/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2144/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2145/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2146/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2147/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2148/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2149/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2150/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2151/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2152/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2153/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2154/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2155/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2156/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2157/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2158/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2159/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2160/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2161/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2162/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2163/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2164/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2165/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2166/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2167/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2168/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2169/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2170/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2171/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2172/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2173/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2174/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2175/3000\n",
      "3608/3608 [==============================] - 1s 359us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2176/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2177/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2178/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2179/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2180/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2181/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2182/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2183/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2184/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2185/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2186/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2187/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2188/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2189/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2190/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2191/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2192/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2193/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2194/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2195/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2196/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2197/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2198/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2199/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2200/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2201/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2202/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2203/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2204/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2205/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2206/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2207/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2208/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2209/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2210/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2211/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2212/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2213/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2214/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2215/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2216/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2217/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2218/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2219/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2220/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2221/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2222/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2223/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2224/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2225/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2226/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2227/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2228/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2229/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2230/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2231/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2232/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2233/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2234/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2235/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2236/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2237/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2238/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2239/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2240/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2241/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2242/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2243/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2244/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2245/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2246/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2247/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2248/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2249/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2250/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2251/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2252/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2253/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2254/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2255/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2256/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2257/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2258/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2259/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2260/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2261/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2262/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2263/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2264/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2265/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2266/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2267/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2268/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2269/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2270/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2271/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2272/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2273/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2274/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2275/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2276/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2277/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2278/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2279/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2280/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2281/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2282/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2283/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2284/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2285/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2286/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2287/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2288/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2289/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2290/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2291/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2292/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2293/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2294/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2295/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2296/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2297/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2298/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2299/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2300/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2301/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2302/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2303/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2304/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2305/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2306/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2307/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2308/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2309/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2310/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2311/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2312/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2313/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2314/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2315/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2316/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2317/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2318/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2319/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2320/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2321/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2322/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2323/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2324/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2325/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2326/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2327/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2328/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2329/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2330/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2331/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2332/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2333/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2334/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2335/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2336/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2337/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2338/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2339/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2340/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2341/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2342/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2343/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2344/3000\n",
      "3608/3608 [==============================] - 1s 352us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2345/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2346/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2347/3000\n",
      "3608/3608 [==============================] - 1s 355us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2348/3000\n",
      "3608/3608 [==============================] - 1s 355us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2349/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2350/3000\n",
      "3608/3608 [==============================] - 1s 355us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2351/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2352/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2353/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2354/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2355/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2356/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2357/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2358/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2359/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2360/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2361/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2362/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2363/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2364/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2365/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2366/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2367/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2368/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2369/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2370/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2371/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 2372/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2373/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2374/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2375/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2376/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2377/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2378/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2379/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2380/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2381/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2382/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2383/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2384/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2385/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2386/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2387/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2388/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2389/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2390/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2391/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2392/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2393/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2394/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2395/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2396/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2397/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2398/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2399/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2400/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2401/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2402/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2403/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2404/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2405/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2406/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2407/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2408/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2409/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2410/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2411/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2412/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2413/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2414/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2415/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2416/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2417/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2418/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2419/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2420/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2421/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2422/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2423/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2424/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2425/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2426/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2427/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2428/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2429/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2430/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2431/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2432/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2433/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2434/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2435/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2436/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2437/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2438/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2439/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2440/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2441/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2442/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2443/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2444/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2445/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2446/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2447/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2448/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2449/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2450/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2451/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2452/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2453/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 2454/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2455/3000\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2456/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2457/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2458/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2459/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2460/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2461/3000\n",
      "3608/3608 [==============================] - 1s 345us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2462/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2463/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2464/3000\n",
      "3608/3608 [==============================] - 1s 360us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2465/3000\n",
      "3608/3608 [==============================] - 1s 308us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2466/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2467/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2468/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2469/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2470/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2471/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2472/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2473/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2474/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2475/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2476/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2477/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2478/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2479/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2480/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2481/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2482/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2483/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2484/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2485/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2486/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2487/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2488/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2489/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2490/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2491/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2492/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2493/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2494/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2495/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2496/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2497/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2498/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2499/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2500/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2501/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2502/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2503/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2504/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2505/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2506/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2507/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2508/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2509/3000\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2510/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2511/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2512/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2513/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2514/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2515/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2516/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2517/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2518/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2519/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2520/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2521/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2522/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2523/3000\n",
      "3608/3608 [==============================] - 1s 306us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2524/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2525/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2526/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2527/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2528/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2529/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2530/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2531/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2532/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2533/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2534/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2535/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2536/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2537/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2538/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2539/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2540/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2541/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2542/3000\n",
      "3608/3608 [==============================] - 1s 303us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2543/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2544/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2545/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2546/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2547/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2548/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2549/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2550/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2551/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2552/3000\n",
      "3608/3608 [==============================] - 1s 306us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2553/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2554/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2555/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2556/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2557/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2558/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2559/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2560/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2561/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2562/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2563/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2564/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2565/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2566/3000\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2567/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2568/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2569/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2570/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2571/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2572/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2573/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2574/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2575/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2576/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2577/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2578/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2579/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2580/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2581/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2582/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2583/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2584/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2585/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2586/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2587/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2588/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2589/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2590/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2591/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2592/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2593/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2594/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2595/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2596/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2597/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 2598/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2599/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2600/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2601/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2602/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2603/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2604/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2605/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2606/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2607/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2608/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2609/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2610/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2611/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2612/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2613/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2614/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2615/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2616/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2617/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2618/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2619/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2620/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2621/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2622/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2623/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2624/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2625/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2626/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2627/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2628/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2629/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2630/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2631/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2632/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2633/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2634/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2635/3000\n",
      "3608/3608 [==============================] - 1s 325us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2636/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2637/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2638/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2639/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2640/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2641/3000\n",
      "3608/3608 [==============================] - 1s 353us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2642/3000\n",
      "3608/3608 [==============================] - 1s 351us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2643/3000\n",
      "3608/3608 [==============================] - 1s 312us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2644/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2645/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2646/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2647/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2648/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2649/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2650/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2651/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2652/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2653/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2654/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2655/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2656/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2657/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2658/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2659/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2660/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2661/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2662/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2663/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2664/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2665/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2666/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2667/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2668/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2669/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2670/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2671/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2672/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2673/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2674/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2675/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2676/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2677/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2678/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2679/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2680/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2681/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2682/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2683/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2684/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2685/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2686/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2687/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2688/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2689/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2690/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2691/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2692/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2693/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2694/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2695/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2696/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2697/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2698/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2699/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2700/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2701/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2702/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2703/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2704/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2705/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2706/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2707/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2708/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2709/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2710/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2711/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2712/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2713/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2714/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2715/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2716/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2717/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2718/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2719/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2720/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2721/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2722/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2723/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2724/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2725/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2726/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2727/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2728/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2729/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2730/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2731/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2732/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2733/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2734/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2735/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2736/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2737/3000\n",
      "3608/3608 [==============================] - 1s 297us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2738/3000\n",
      "3608/3608 [==============================] - 1s 298us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2739/3000\n",
      "3608/3608 [==============================] - 1s 308us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2740/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2741/3000\n",
      "3608/3608 [==============================] - 1s 312us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2742/3000\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2743/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2744/3000\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2745/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2746/3000\n",
      "3608/3608 [==============================] - 1s 303us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2747/3000\n",
      "3608/3608 [==============================] - 1s 307us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2748/3000\n",
      "3608/3608 [==============================] - 1s 299us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2749/3000\n",
      "3608/3608 [==============================] - 1s 333us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2750/3000\n",
      "3608/3608 [==============================] - 1s 369us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2751/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2752/3000\n",
      "3608/3608 [==============================] - 1s 367us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2753/3000\n",
      "3608/3608 [==============================] - 1s 333us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2754/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2755/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2756/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2757/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2758/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2759/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2760/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2761/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2762/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2763/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2764/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2765/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2766/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2767/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2768/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2769/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2770/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2771/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2772/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2773/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2774/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2775/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2776/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2777/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2778/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2779/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2780/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2781/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2782/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2783/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2784/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2785/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2786/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2787/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2788/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2789/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2790/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2791/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2792/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2793/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2794/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2795/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2796/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2797/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2798/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2799/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2800/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2801/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2802/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2803/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2804/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2805/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2806/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2807/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2808/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2809/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2810/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2811/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2812/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2813/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2814/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2815/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2816/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2817/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2818/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2819/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2820/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2821/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2822/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2823/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2824/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2825/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2826/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2827/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2828/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2829/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2830/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2831/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2832/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2833/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2834/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2835/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2836/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2837/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2838/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2839/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2840/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2841/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2842/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2843/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2844/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2845/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2846/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2847/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2848/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0804 - val_acc: 0.3076\n",
      "Epoch 2849/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2850/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2851/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2852/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2853/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2854/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2855/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2856/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2857/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2858/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2859/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2860/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2861/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2862/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2863/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2864/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2865/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2866/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2867/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2868/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2869/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2870/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2871/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2872/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2873/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2874/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2875/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2876/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2877/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2878/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2879/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2880/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2881/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2882/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2883/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2884/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2885/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2886/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2887/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2888/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2889/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2890/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2891/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2892/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2893/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2894/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2895/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2896/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2897/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2898/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2899/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2900/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2901/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2902/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2903/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2904/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2905/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2906/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2907/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2908/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2909/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2910/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2911/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2912/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2913/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2914/3000\n",
      "3608/3608 [==============================] - 1s 295us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2915/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2916/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2917/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2918/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2919/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2920/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2921/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2922/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2923/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2924/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2925/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2926/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2927/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2928/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2929/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2930/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2931/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2932/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2933/3000\n",
      "3608/3608 [==============================] - 1s 356us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2934/3000\n",
      "3608/3608 [==============================] - 1s 358us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2935/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2936/3000\n",
      "3608/3608 [==============================] - 1s 355us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2937/3000\n",
      "3608/3608 [==============================] - 1s 354us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2938/3000\n",
      "3608/3608 [==============================] - 1s 357us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2939/3000\n",
      "3608/3608 [==============================] - 1s 351us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2940/3000\n",
      "3608/3608 [==============================] - 1s 355us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2941/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2942/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2943/3000\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2944/3000\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2945/3000\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2946/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2947/3000\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2948/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2949/3000\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2950/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2951/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2952/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2953/3000\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2954/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2955/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2956/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2957/3000\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2958/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2959/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2960/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2961/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2962/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2963/3000\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2964/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2965/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2966/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2967/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2968/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2969/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2970/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2971/3000\n",
      "3608/3608 [==============================] - 1s 293us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2972/3000\n",
      "3608/3608 [==============================] - 1s 296us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2973/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2974/3000\n",
      "3608/3608 [==============================] - 1s 292us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2975/3000\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2976/3000\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2977/3000\n",
      "3608/3608 [==============================] - 1s 294us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2978/3000\n",
      "3608/3608 [==============================] - 1s 289us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2979/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2980/3000\n",
      "3608/3608 [==============================] - 1s 290us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2981/3000\n",
      "3608/3608 [==============================] - 1s 288us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2982/3000\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2983/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2984/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2985/3000\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0814 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2986/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2987/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2988/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2989/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2990/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2991/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2992/3000\n",
      "3608/3608 [==============================] - 1s 287us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2993/3000\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2994/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2995/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2996/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2997/3000\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 2998/3000\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0803 - val_acc: 0.3076\n",
      "Epoch 2999/3000\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n",
      "Epoch 3000/3000\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0813 - acc: 0.2905 - val_loss: 0.0802 - val_acc: 0.3076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77fe75ae10>"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.summary()\n",
    "nn_model.add(Dense(units=32, activation='relu'))\n",
    "nn_model.add(Dense(units=16, activation='relu'))\n",
    "nn_model.add(Dense(units=8, activation='relu'))\n",
    "nn_model.add(Dense(units=4, activation='relu'))\n",
    "nn_model.add(Dense(units=2, activation='relu'))\n",
    "nn_model.add(Dense(units=4, activation='relu'))\n",
    "nn_model.add(Dense(units=8, activation='relu'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "nn_model.fit(x=train_x, y=train_y, epochs=3000, validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXgKZRCvqWuf"
   },
   "source": [
    "Evne big epoches it seems like it goes to local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FHEAN36sSPSd"
   },
   "source": [
    "### 5) feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rc60jWubNRoX"
   },
   "source": [
    "I dropped genre features becuase it only considered as sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fiX-qlLPDOvu"
   },
   "outputs": [],
   "source": [
    "new_train_x = train_x.drop(columns=[\"genre_\"+str(int(i)) for i in range(0, 23)])\n",
    "new_valid_x = valid_x.drop(columns=[\"genre_\"+str(int(i)) for i in range(0, 23)])\n",
    "new_test_x = test_x.drop(columns=[\"genre_\"+str(int(i)) for i in range(0, 23)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zoAEaJpSeFG"
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(new_train_x, train_y)\n",
    "pred_y = dt.predict(new_valid_x)\n",
    "precision, recall, fscore, support = score(valid_y, pred_y)\n",
    "result = result.append({'model':'decision tree', 'accuracy':accuracy_score(valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17374
    },
    "colab_type": "code",
    "id": "z5WUgEM5TvWY",
    "outputId": "ec022255-c2ac-4dde-8a51-e5d5ac7071d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 9)                 90        \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 12)                120       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 15)                195       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 18)                288       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 12)                228       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 1,051\n",
      "Trainable params: 1,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3608 samples, validate on 1203 samples\n",
      "Epoch 1/500\n",
      "3608/3608 [==============================] - 1s 405us/step - loss: 0.0747 - acc: 0.3708 - val_loss: 0.0656 - val_acc: 0.4580\n",
      "Epoch 2/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0666 - acc: 0.4435 - val_loss: 0.0640 - val_acc: 0.4763\n",
      "Epoch 3/500\n",
      "3608/3608 [==============================] - 1s 237us/step - loss: 0.0654 - acc: 0.4498 - val_loss: 0.0630 - val_acc: 0.4763\n",
      "Epoch 4/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0646 - acc: 0.4562 - val_loss: 0.0629 - val_acc: 0.5162\n",
      "Epoch 5/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0643 - acc: 0.4593 - val_loss: 0.0625 - val_acc: 0.5245\n",
      "Epoch 6/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0630 - acc: 0.4859 - val_loss: 0.0612 - val_acc: 0.5312\n",
      "Epoch 7/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0626 - acc: 0.4906 - val_loss: 0.0630 - val_acc: 0.4796\n",
      "Epoch 8/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0626 - acc: 0.5022 - val_loss: 0.0593 - val_acc: 0.5337\n",
      "Epoch 9/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0608 - acc: 0.5213 - val_loss: 0.0578 - val_acc: 0.5736\n",
      "Epoch 10/500\n",
      "3608/3608 [==============================] - 1s 236us/step - loss: 0.0602 - acc: 0.5377 - val_loss: 0.0576 - val_acc: 0.5628\n",
      "Epoch 11/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0602 - acc: 0.5360 - val_loss: 0.0583 - val_acc: 0.5328\n",
      "Epoch 12/500\n",
      "3608/3608 [==============================] - 1s 237us/step - loss: 0.0599 - acc: 0.5374 - val_loss: 0.0566 - val_acc: 0.5752\n",
      "Epoch 13/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0595 - acc: 0.5374 - val_loss: 0.0608 - val_acc: 0.5145\n",
      "Epoch 14/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0598 - acc: 0.5335 - val_loss: 0.0570 - val_acc: 0.5736\n",
      "Epoch 15/500\n",
      "3608/3608 [==============================] - 1s 237us/step - loss: 0.0595 - acc: 0.5405 - val_loss: 0.0572 - val_acc: 0.5744\n",
      "Epoch 16/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0589 - acc: 0.5457 - val_loss: 0.0568 - val_acc: 0.5794\n",
      "Epoch 17/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0589 - acc: 0.5455 - val_loss: 0.0563 - val_acc: 0.5844\n",
      "Epoch 18/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0589 - acc: 0.5502 - val_loss: 0.0566 - val_acc: 0.5777\n",
      "Epoch 19/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0591 - acc: 0.5380 - val_loss: 0.0592 - val_acc: 0.5619\n",
      "Epoch 20/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0602 - acc: 0.5324 - val_loss: 0.0570 - val_acc: 0.5661\n",
      "Epoch 21/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0592 - acc: 0.5396 - val_loss: 0.0571 - val_acc: 0.5669\n",
      "Epoch 22/500\n",
      "3608/3608 [==============================] - 1s 233us/step - loss: 0.0587 - acc: 0.5455 - val_loss: 0.0564 - val_acc: 0.5844\n",
      "Epoch 23/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0589 - acc: 0.5468 - val_loss: 0.0561 - val_acc: 0.5810\n",
      "Epoch 24/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0592 - acc: 0.5399 - val_loss: 0.0561 - val_acc: 0.5869\n",
      "Epoch 25/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0590 - acc: 0.5430 - val_loss: 0.0582 - val_acc: 0.5362\n",
      "Epoch 26/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0597 - acc: 0.5377 - val_loss: 0.0576 - val_acc: 0.5578\n",
      "Epoch 27/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0597 - acc: 0.5358 - val_loss: 0.0565 - val_acc: 0.5761\n",
      "Epoch 28/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0588 - acc: 0.5416 - val_loss: 0.0562 - val_acc: 0.5736\n",
      "Epoch 29/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0588 - acc: 0.5482 - val_loss: 0.0562 - val_acc: 0.5827\n",
      "Epoch 30/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0585 - acc: 0.5479 - val_loss: 0.0562 - val_acc: 0.5727\n",
      "Epoch 31/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0584 - acc: 0.5513 - val_loss: 0.0561 - val_acc: 0.5727\n",
      "Epoch 32/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0588 - acc: 0.5438 - val_loss: 0.0579 - val_acc: 0.5594\n",
      "Epoch 33/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0585 - acc: 0.5499 - val_loss: 0.0569 - val_acc: 0.5744\n",
      "Epoch 34/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0585 - acc: 0.5493 - val_loss: 0.0565 - val_acc: 0.5752\n",
      "Epoch 35/500\n",
      "3608/3608 [==============================] - 1s 254us/step - loss: 0.0586 - acc: 0.5468 - val_loss: 0.0564 - val_acc: 0.5736\n",
      "Epoch 36/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0587 - acc: 0.5419 - val_loss: 0.0565 - val_acc: 0.5786\n",
      "Epoch 37/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0583 - acc: 0.5563 - val_loss: 0.0564 - val_acc: 0.5786\n",
      "Epoch 38/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0587 - acc: 0.5488 - val_loss: 0.0569 - val_acc: 0.5769\n",
      "Epoch 39/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0585 - acc: 0.5510 - val_loss: 0.0561 - val_acc: 0.5844\n",
      "Epoch 40/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0584 - acc: 0.5413 - val_loss: 0.0559 - val_acc: 0.5711\n",
      "Epoch 41/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0583 - acc: 0.5527 - val_loss: 0.0562 - val_acc: 0.5802\n",
      "Epoch 42/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0586 - acc: 0.5449 - val_loss: 0.0585 - val_acc: 0.5511\n",
      "Epoch 43/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0587 - acc: 0.5430 - val_loss: 0.0562 - val_acc: 0.5761\n",
      "Epoch 44/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0589 - acc: 0.5474 - val_loss: 0.0564 - val_acc: 0.5802\n",
      "Epoch 45/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0592 - acc: 0.5396 - val_loss: 0.0578 - val_acc: 0.5594\n",
      "Epoch 46/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0583 - acc: 0.5521 - val_loss: 0.0564 - val_acc: 0.5794\n",
      "Epoch 47/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0584 - acc: 0.5507 - val_loss: 0.0562 - val_acc: 0.5819\n",
      "Epoch 48/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0589 - acc: 0.5468 - val_loss: 0.0566 - val_acc: 0.5802\n",
      "Epoch 49/500\n",
      "3608/3608 [==============================] - 1s 258us/step - loss: 0.0590 - acc: 0.5394 - val_loss: 0.0573 - val_acc: 0.5744\n",
      "Epoch 50/500\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0585 - acc: 0.5540 - val_loss: 0.0581 - val_acc: 0.5553\n",
      "Epoch 51/500\n",
      "3608/3608 [==============================] - 1s 305us/step - loss: 0.0586 - acc: 0.5452 - val_loss: 0.0571 - val_acc: 0.5711\n",
      "Epoch 52/500\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0583 - acc: 0.5518 - val_loss: 0.0568 - val_acc: 0.5719\n",
      "Epoch 53/500\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0586 - acc: 0.5499 - val_loss: 0.0577 - val_acc: 0.5536\n",
      "Epoch 54/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0601 - acc: 0.5308 - val_loss: 0.0563 - val_acc: 0.5777\n",
      "Epoch 55/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0583 - acc: 0.5529 - val_loss: 0.0560 - val_acc: 0.5786\n",
      "Epoch 56/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0582 - acc: 0.5516 - val_loss: 0.0567 - val_acc: 0.5661\n",
      "Epoch 57/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0583 - acc: 0.5504 - val_loss: 0.0580 - val_acc: 0.5653\n",
      "Epoch 58/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0585 - acc: 0.5513 - val_loss: 0.0640 - val_acc: 0.4406\n",
      "Epoch 59/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0598 - acc: 0.5349 - val_loss: 0.0566 - val_acc: 0.5761\n",
      "Epoch 60/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0586 - acc: 0.5524 - val_loss: 0.0561 - val_acc: 0.5802\n",
      "Epoch 61/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0581 - acc: 0.5529 - val_loss: 0.0565 - val_acc: 0.5702\n",
      "Epoch 62/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0582 - acc: 0.5491 - val_loss: 0.0568 - val_acc: 0.5810\n",
      "Epoch 63/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0584 - acc: 0.5449 - val_loss: 0.0574 - val_acc: 0.5553\n",
      "Epoch 64/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0581 - acc: 0.5549 - val_loss: 0.0564 - val_acc: 0.5810\n",
      "Epoch 65/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0583 - acc: 0.5568 - val_loss: 0.0560 - val_acc: 0.5827\n",
      "Epoch 66/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0584 - acc: 0.5529 - val_loss: 0.0559 - val_acc: 0.5761\n",
      "Epoch 67/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0586 - acc: 0.5518 - val_loss: 0.0561 - val_acc: 0.5835\n",
      "Epoch 68/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0586 - acc: 0.5471 - val_loss: 0.0564 - val_acc: 0.5860\n",
      "Epoch 69/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0586 - acc: 0.5441 - val_loss: 0.0565 - val_acc: 0.5802\n",
      "Epoch 70/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0582 - acc: 0.5532 - val_loss: 0.0561 - val_acc: 0.5835\n",
      "Epoch 71/500\n",
      "3608/3608 [==============================] - 1s 237us/step - loss: 0.0585 - acc: 0.5507 - val_loss: 0.0560 - val_acc: 0.5819\n",
      "Epoch 72/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0586 - acc: 0.5507 - val_loss: 0.0567 - val_acc: 0.5744\n",
      "Epoch 73/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0581 - acc: 0.5482 - val_loss: 0.0563 - val_acc: 0.5885\n",
      "Epoch 74/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0582 - acc: 0.5516 - val_loss: 0.0564 - val_acc: 0.5794\n",
      "Epoch 75/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0583 - acc: 0.5521 - val_loss: 0.0559 - val_acc: 0.5769\n",
      "Epoch 76/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0582 - acc: 0.5510 - val_loss: 0.0559 - val_acc: 0.5835\n",
      "Epoch 77/500\n",
      "3608/3608 [==============================] - 1s 235us/step - loss: 0.0582 - acc: 0.5552 - val_loss: 0.0568 - val_acc: 0.5802\n",
      "Epoch 78/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0585 - acc: 0.5452 - val_loss: 0.0562 - val_acc: 0.5835\n",
      "Epoch 79/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0581 - acc: 0.5576 - val_loss: 0.0568 - val_acc: 0.5802\n",
      "Epoch 80/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0581 - acc: 0.5585 - val_loss: 0.0560 - val_acc: 0.5877\n",
      "Epoch 81/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0586 - acc: 0.5499 - val_loss: 0.0588 - val_acc: 0.5520\n",
      "Epoch 82/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0582 - acc: 0.5527 - val_loss: 0.0564 - val_acc: 0.5819\n",
      "Epoch 83/500\n",
      "3608/3608 [==============================] - 1s 235us/step - loss: 0.0582 - acc: 0.5504 - val_loss: 0.0563 - val_acc: 0.5777\n",
      "Epoch 84/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0589 - acc: 0.5524 - val_loss: 0.0564 - val_acc: 0.5835\n",
      "Epoch 85/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0580 - acc: 0.5565 - val_loss: 0.0560 - val_acc: 0.5777\n",
      "Epoch 86/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0581 - acc: 0.5538 - val_loss: 0.0563 - val_acc: 0.5819\n",
      "Epoch 87/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0583 - acc: 0.5485 - val_loss: 0.0567 - val_acc: 0.5827\n",
      "Epoch 88/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0584 - acc: 0.5479 - val_loss: 0.0569 - val_acc: 0.5727\n",
      "Epoch 89/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0584 - acc: 0.5474 - val_loss: 0.0560 - val_acc: 0.5786\n",
      "Epoch 90/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0583 - acc: 0.5546 - val_loss: 0.0566 - val_acc: 0.5727\n",
      "Epoch 91/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0582 - acc: 0.5527 - val_loss: 0.0563 - val_acc: 0.5852\n",
      "Epoch 92/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0585 - acc: 0.5471 - val_loss: 0.0564 - val_acc: 0.5860\n",
      "Epoch 93/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0585 - acc: 0.5471 - val_loss: 0.0565 - val_acc: 0.5794\n",
      "Epoch 94/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0583 - acc: 0.5504 - val_loss: 0.0559 - val_acc: 0.5877\n",
      "Epoch 95/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0580 - acc: 0.5513 - val_loss: 0.0567 - val_acc: 0.5752\n",
      "Epoch 96/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0584 - acc: 0.5524 - val_loss: 0.0563 - val_acc: 0.5819\n",
      "Epoch 97/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0581 - acc: 0.5565 - val_loss: 0.0570 - val_acc: 0.5736\n",
      "Epoch 98/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0581 - acc: 0.5538 - val_loss: 0.0612 - val_acc: 0.5254\n",
      "Epoch 99/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0598 - acc: 0.5349 - val_loss: 0.0560 - val_acc: 0.5810\n",
      "Epoch 100/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0592 - acc: 0.5377 - val_loss: 0.0560 - val_acc: 0.5827\n",
      "Epoch 101/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0583 - acc: 0.5513 - val_loss: 0.0562 - val_acc: 0.5761\n",
      "Epoch 102/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0581 - acc: 0.5574 - val_loss: 0.0561 - val_acc: 0.5835\n",
      "Epoch 103/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0582 - acc: 0.5516 - val_loss: 0.0565 - val_acc: 0.5752\n",
      "Epoch 104/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0582 - acc: 0.5557 - val_loss: 0.0563 - val_acc: 0.5827\n",
      "Epoch 105/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0583 - acc: 0.5502 - val_loss: 0.0573 - val_acc: 0.5528\n",
      "Epoch 106/500\n",
      "3608/3608 [==============================] - 1s 237us/step - loss: 0.0581 - acc: 0.5549 - val_loss: 0.0562 - val_acc: 0.5819\n",
      "Epoch 107/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0589 - acc: 0.5446 - val_loss: 0.0561 - val_acc: 0.5794\n",
      "Epoch 108/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0582 - acc: 0.5507 - val_loss: 0.0562 - val_acc: 0.5869\n",
      "Epoch 109/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0581 - acc: 0.5527 - val_loss: 0.0565 - val_acc: 0.5802\n",
      "Epoch 110/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0587 - acc: 0.5446 - val_loss: 0.0563 - val_acc: 0.5819\n",
      "Epoch 111/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0584 - acc: 0.5527 - val_loss: 0.0563 - val_acc: 0.5827\n",
      "Epoch 112/500\n",
      "3608/3608 [==============================] - 1s 237us/step - loss: 0.0584 - acc: 0.5579 - val_loss: 0.0562 - val_acc: 0.5844\n",
      "Epoch 113/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0580 - acc: 0.5543 - val_loss: 0.0565 - val_acc: 0.5827\n",
      "Epoch 114/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0580 - acc: 0.5588 - val_loss: 0.0563 - val_acc: 0.5802\n",
      "Epoch 115/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0583 - acc: 0.5463 - val_loss: 0.0561 - val_acc: 0.5810\n",
      "Epoch 116/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0582 - acc: 0.5535 - val_loss: 0.0570 - val_acc: 0.5686\n",
      "Epoch 117/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0580 - acc: 0.5538 - val_loss: 0.0563 - val_acc: 0.5744\n",
      "Epoch 118/500\n",
      "3608/3608 [==============================] - 1s 235us/step - loss: 0.0580 - acc: 0.5563 - val_loss: 0.0562 - val_acc: 0.5852\n",
      "Epoch 119/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0581 - acc: 0.5604 - val_loss: 0.0562 - val_acc: 0.5719\n",
      "Epoch 120/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0579 - acc: 0.5568 - val_loss: 0.0559 - val_acc: 0.5819\n",
      "Epoch 121/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0580 - acc: 0.5532 - val_loss: 0.0565 - val_acc: 0.5810\n",
      "Epoch 122/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0581 - acc: 0.5557 - val_loss: 0.0565 - val_acc: 0.5802\n",
      "Epoch 123/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0582 - acc: 0.5485 - val_loss: 0.0559 - val_acc: 0.5744\n",
      "Epoch 124/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0585 - acc: 0.5504 - val_loss: 0.0559 - val_acc: 0.5835\n",
      "Epoch 125/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0582 - acc: 0.5513 - val_loss: 0.0565 - val_acc: 0.5761\n",
      "Epoch 126/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0583 - acc: 0.5518 - val_loss: 0.0561 - val_acc: 0.5844\n",
      "Epoch 127/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0585 - acc: 0.5516 - val_loss: 0.0561 - val_acc: 0.5827\n",
      "Epoch 128/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0581 - acc: 0.5579 - val_loss: 0.0569 - val_acc: 0.5769\n",
      "Epoch 129/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0584 - acc: 0.5557 - val_loss: 0.0578 - val_acc: 0.5644\n",
      "Epoch 130/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0624 - acc: 0.5360 - val_loss: 0.0570 - val_acc: 0.5644\n",
      "Epoch 131/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0603 - acc: 0.5463 - val_loss: 0.0572 - val_acc: 0.5786\n",
      "Epoch 132/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0601 - acc: 0.5482 - val_loss: 0.0588 - val_acc: 0.5428\n",
      "Epoch 133/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0604 - acc: 0.5302 - val_loss: 0.0569 - val_acc: 0.5810\n",
      "Epoch 134/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0596 - acc: 0.5421 - val_loss: 0.0568 - val_acc: 0.5744\n",
      "Epoch 135/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0591 - acc: 0.5432 - val_loss: 0.0570 - val_acc: 0.5669\n",
      "Epoch 136/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0590 - acc: 0.5435 - val_loss: 0.0567 - val_acc: 0.5777\n",
      "Epoch 137/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0594 - acc: 0.5385 - val_loss: 0.0562 - val_acc: 0.5786\n",
      "Epoch 138/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0586 - acc: 0.5438 - val_loss: 0.0566 - val_acc: 0.5744\n",
      "Epoch 139/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0595 - acc: 0.5349 - val_loss: 0.0568 - val_acc: 0.5628\n",
      "Epoch 140/500\n",
      "3608/3608 [==============================] - 1s 237us/step - loss: 0.0585 - acc: 0.5477 - val_loss: 0.0562 - val_acc: 0.5786\n",
      "Epoch 141/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0583 - acc: 0.5488 - val_loss: 0.0563 - val_acc: 0.5794\n",
      "Epoch 142/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0585 - acc: 0.5516 - val_loss: 0.0566 - val_acc: 0.5786\n",
      "Epoch 143/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0584 - acc: 0.5471 - val_loss: 0.0565 - val_acc: 0.5611\n",
      "Epoch 144/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0581 - acc: 0.5527 - val_loss: 0.0595 - val_acc: 0.5378\n",
      "Epoch 145/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0580 - acc: 0.5510 - val_loss: 0.0564 - val_acc: 0.5810\n",
      "Epoch 146/500\n",
      "3608/3608 [==============================] - 1s 237us/step - loss: 0.0581 - acc: 0.5529 - val_loss: 0.0560 - val_acc: 0.5827\n",
      "Epoch 147/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0582 - acc: 0.5510 - val_loss: 0.0561 - val_acc: 0.5761\n",
      "Epoch 148/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0584 - acc: 0.5449 - val_loss: 0.0564 - val_acc: 0.5711\n",
      "Epoch 149/500\n",
      "3608/3608 [==============================] - 1s 236us/step - loss: 0.0586 - acc: 0.5463 - val_loss: 0.0585 - val_acc: 0.5270\n",
      "Epoch 150/500\n",
      "3608/3608 [==============================] - 1s 237us/step - loss: 0.0580 - acc: 0.5513 - val_loss: 0.0561 - val_acc: 0.5869\n",
      "Epoch 151/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0579 - acc: 0.5529 - val_loss: 0.0562 - val_acc: 0.5802\n",
      "Epoch 152/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0583 - acc: 0.5538 - val_loss: 0.0560 - val_acc: 0.5794\n",
      "Epoch 153/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0580 - acc: 0.5524 - val_loss: 0.0571 - val_acc: 0.5644\n",
      "Epoch 154/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0592 - acc: 0.5380 - val_loss: 0.0563 - val_acc: 0.5777\n",
      "Epoch 155/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0581 - acc: 0.5518 - val_loss: 0.0567 - val_acc: 0.5786\n",
      "Epoch 156/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0582 - acc: 0.5507 - val_loss: 0.0575 - val_acc: 0.5603\n",
      "Epoch 157/500\n",
      "3608/3608 [==============================] - 1s 236us/step - loss: 0.0581 - acc: 0.5529 - val_loss: 0.0560 - val_acc: 0.5869\n",
      "Epoch 158/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0579 - acc: 0.5579 - val_loss: 0.0562 - val_acc: 0.5827\n",
      "Epoch 159/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0586 - acc: 0.5513 - val_loss: 0.0569 - val_acc: 0.5810\n",
      "Epoch 160/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0579 - acc: 0.5538 - val_loss: 0.0562 - val_acc: 0.5786\n",
      "Epoch 161/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0580 - acc: 0.5571 - val_loss: 0.0560 - val_acc: 0.5810\n",
      "Epoch 162/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0582 - acc: 0.5549 - val_loss: 0.0565 - val_acc: 0.5810\n",
      "Epoch 163/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0582 - acc: 0.5538 - val_loss: 0.0559 - val_acc: 0.5802\n",
      "Epoch 164/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0580 - acc: 0.5493 - val_loss: 0.0560 - val_acc: 0.5827\n",
      "Epoch 165/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0578 - acc: 0.5543 - val_loss: 0.0577 - val_acc: 0.5769\n",
      "Epoch 166/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0577 - acc: 0.5535 - val_loss: 0.0566 - val_acc: 0.5761\n",
      "Epoch 167/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0577 - acc: 0.5560 - val_loss: 0.0570 - val_acc: 0.5628\n",
      "Epoch 168/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0579 - acc: 0.5507 - val_loss: 0.0568 - val_acc: 0.5702\n",
      "Epoch 169/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0590 - acc: 0.5419 - val_loss: 0.0564 - val_acc: 0.5802\n",
      "Epoch 170/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0582 - acc: 0.5552 - val_loss: 0.0559 - val_acc: 0.5860\n",
      "Epoch 171/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0582 - acc: 0.5524 - val_loss: 0.0561 - val_acc: 0.5777\n",
      "Epoch 172/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0580 - acc: 0.5529 - val_loss: 0.0560 - val_acc: 0.5885\n",
      "Epoch 173/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0580 - acc: 0.5504 - val_loss: 0.0559 - val_acc: 0.5786\n",
      "Epoch 174/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0580 - acc: 0.5554 - val_loss: 0.0587 - val_acc: 0.5528\n",
      "Epoch 175/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0580 - acc: 0.5485 - val_loss: 0.0577 - val_acc: 0.5636\n",
      "Epoch 176/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0579 - acc: 0.5504 - val_loss: 0.0562 - val_acc: 0.5835\n",
      "Epoch 177/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0578 - acc: 0.5543 - val_loss: 0.0562 - val_acc: 0.5860\n",
      "Epoch 178/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0581 - acc: 0.5593 - val_loss: 0.0591 - val_acc: 0.5520\n",
      "Epoch 179/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0582 - acc: 0.5540 - val_loss: 0.0562 - val_acc: 0.5794\n",
      "Epoch 180/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0578 - acc: 0.5557 - val_loss: 0.0568 - val_acc: 0.5752\n",
      "Epoch 181/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0578 - acc: 0.5521 - val_loss: 0.0561 - val_acc: 0.5827\n",
      "Epoch 182/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0580 - acc: 0.5521 - val_loss: 0.0570 - val_acc: 0.5802\n",
      "Epoch 183/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0577 - acc: 0.5546 - val_loss: 0.0569 - val_acc: 0.5686\n",
      "Epoch 184/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0581 - acc: 0.5527 - val_loss: 0.0576 - val_acc: 0.5619\n",
      "Epoch 185/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0578 - acc: 0.5565 - val_loss: 0.0560 - val_acc: 0.5835\n",
      "Epoch 186/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0581 - acc: 0.5557 - val_loss: 0.0572 - val_acc: 0.5852\n",
      "Epoch 187/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0578 - acc: 0.5565 - val_loss: 0.0564 - val_acc: 0.5819\n",
      "Epoch 188/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0578 - acc: 0.5593 - val_loss: 0.0559 - val_acc: 0.5844\n",
      "Epoch 189/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0580 - acc: 0.5554 - val_loss: 0.0559 - val_acc: 0.5844\n",
      "Epoch 190/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0575 - acc: 0.5582 - val_loss: 0.0561 - val_acc: 0.5877\n",
      "Epoch 191/500\n",
      "3608/3608 [==============================] - 1s 234us/step - loss: 0.0580 - acc: 0.5560 - val_loss: 0.0607 - val_acc: 0.5312\n",
      "Epoch 192/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0581 - acc: 0.5546 - val_loss: 0.0569 - val_acc: 0.5752\n",
      "Epoch 193/500\n",
      "3608/3608 [==============================] - 1s 236us/step - loss: 0.0578 - acc: 0.5524 - val_loss: 0.0567 - val_acc: 0.5744\n",
      "Epoch 194/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0582 - acc: 0.5507 - val_loss: 0.0580 - val_acc: 0.5727\n",
      "Epoch 195/500\n",
      "3608/3608 [==============================] - 1s 237us/step - loss: 0.0578 - acc: 0.5579 - val_loss: 0.0560 - val_acc: 0.5777\n",
      "Epoch 196/500\n",
      "3608/3608 [==============================] - 1s 237us/step - loss: 0.0581 - acc: 0.5543 - val_loss: 0.0580 - val_acc: 0.5520\n",
      "Epoch 197/500\n",
      "3608/3608 [==============================] - 1s 236us/step - loss: 0.0583 - acc: 0.5516 - val_loss: 0.0559 - val_acc: 0.5827\n",
      "Epoch 198/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0583 - acc: 0.5496 - val_loss: 0.0563 - val_acc: 0.5852\n",
      "Epoch 199/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0581 - acc: 0.5554 - val_loss: 0.0571 - val_acc: 0.5727\n",
      "Epoch 200/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0586 - acc: 0.5543 - val_loss: 0.0573 - val_acc: 0.5719\n",
      "Epoch 201/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0585 - acc: 0.5504 - val_loss: 0.0565 - val_acc: 0.5835\n",
      "Epoch 202/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0578 - acc: 0.5552 - val_loss: 0.0563 - val_acc: 0.5835\n",
      "Epoch 203/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0581 - acc: 0.5560 - val_loss: 0.0565 - val_acc: 0.5852\n",
      "Epoch 204/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0579 - acc: 0.5554 - val_loss: 0.0563 - val_acc: 0.5827\n",
      "Epoch 205/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0577 - acc: 0.5557 - val_loss: 0.0563 - val_acc: 0.5786\n",
      "Epoch 206/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0581 - acc: 0.5574 - val_loss: 0.0565 - val_acc: 0.5777\n",
      "Epoch 207/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0580 - acc: 0.5568 - val_loss: 0.0557 - val_acc: 0.5869\n",
      "Epoch 208/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0578 - acc: 0.5543 - val_loss: 0.0572 - val_acc: 0.5827\n",
      "Epoch 209/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0580 - acc: 0.5557 - val_loss: 0.0560 - val_acc: 0.5819\n",
      "Epoch 210/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0581 - acc: 0.5491 - val_loss: 0.0560 - val_acc: 0.5786\n",
      "Epoch 211/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0580 - acc: 0.5585 - val_loss: 0.0562 - val_acc: 0.5786\n",
      "Epoch 212/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0580 - acc: 0.5540 - val_loss: 0.0622 - val_acc: 0.5154\n",
      "Epoch 213/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0584 - acc: 0.5496 - val_loss: 0.0562 - val_acc: 0.5702\n",
      "Epoch 214/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0577 - acc: 0.5552 - val_loss: 0.0560 - val_acc: 0.5819\n",
      "Epoch 215/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0576 - acc: 0.5557 - val_loss: 0.0560 - val_acc: 0.5852\n",
      "Epoch 216/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0578 - acc: 0.5513 - val_loss: 0.0563 - val_acc: 0.5802\n",
      "Epoch 217/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0578 - acc: 0.5549 - val_loss: 0.0565 - val_acc: 0.5794\n",
      "Epoch 218/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0577 - acc: 0.5527 - val_loss: 0.0587 - val_acc: 0.5528\n",
      "Epoch 219/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0584 - acc: 0.5499 - val_loss: 0.0570 - val_acc: 0.5669\n",
      "Epoch 220/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0577 - acc: 0.5535 - val_loss: 0.0574 - val_acc: 0.5769\n",
      "Epoch 221/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0575 - acc: 0.5593 - val_loss: 0.0576 - val_acc: 0.5603\n",
      "Epoch 222/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0576 - acc: 0.5560 - val_loss: 0.0562 - val_acc: 0.5669\n",
      "Epoch 223/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0576 - acc: 0.5585 - val_loss: 0.0569 - val_acc: 0.5761\n",
      "Epoch 224/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0578 - acc: 0.5532 - val_loss: 0.0559 - val_acc: 0.5810\n",
      "Epoch 225/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0576 - acc: 0.5593 - val_loss: 0.0590 - val_acc: 0.5486\n",
      "Epoch 226/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0588 - acc: 0.5463 - val_loss: 0.0568 - val_acc: 0.5594\n",
      "Epoch 227/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0577 - acc: 0.5557 - val_loss: 0.0567 - val_acc: 0.5844\n",
      "Epoch 228/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0579 - acc: 0.5543 - val_loss: 0.0565 - val_acc: 0.5727\n",
      "Epoch 229/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0576 - acc: 0.5563 - val_loss: 0.0561 - val_acc: 0.5844\n",
      "Epoch 230/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0576 - acc: 0.5574 - val_loss: 0.0576 - val_acc: 0.5586\n",
      "Epoch 231/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0575 - acc: 0.5568 - val_loss: 0.0561 - val_acc: 0.5827\n",
      "Epoch 232/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0576 - acc: 0.5599 - val_loss: 0.0558 - val_acc: 0.5860\n",
      "Epoch 233/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0579 - acc: 0.5554 - val_loss: 0.0560 - val_acc: 0.5835\n",
      "Epoch 234/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0607 - acc: 0.5111 - val_loss: 0.0565 - val_acc: 0.5702\n",
      "Epoch 235/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0580 - acc: 0.5516 - val_loss: 0.0562 - val_acc: 0.5761\n",
      "Epoch 236/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0583 - acc: 0.5488 - val_loss: 0.0560 - val_acc: 0.5810\n",
      "Epoch 237/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0580 - acc: 0.5516 - val_loss: 0.0564 - val_acc: 0.5619\n",
      "Epoch 238/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0576 - acc: 0.5557 - val_loss: 0.0570 - val_acc: 0.5661\n",
      "Epoch 239/500\n",
      "3608/3608 [==============================] - 1s 255us/step - loss: 0.0581 - acc: 0.5510 - val_loss: 0.0561 - val_acc: 0.5844\n",
      "Epoch 240/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0579 - acc: 0.5521 - val_loss: 0.0574 - val_acc: 0.5752\n",
      "Epoch 241/500\n",
      "3608/3608 [==============================] - 1s 258us/step - loss: 0.0577 - acc: 0.5596 - val_loss: 0.0559 - val_acc: 0.5819\n",
      "Epoch 242/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0578 - acc: 0.5518 - val_loss: 0.0561 - val_acc: 0.5819\n",
      "Epoch 243/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0578 - acc: 0.5571 - val_loss: 0.0562 - val_acc: 0.5860\n",
      "Epoch 244/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0578 - acc: 0.5552 - val_loss: 0.0563 - val_acc: 0.5686\n",
      "Epoch 245/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0579 - acc: 0.5607 - val_loss: 0.0561 - val_acc: 0.5736\n",
      "Epoch 246/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0578 - acc: 0.5560 - val_loss: 0.0562 - val_acc: 0.5794\n",
      "Epoch 247/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0579 - acc: 0.5532 - val_loss: 0.0579 - val_acc: 0.5553\n",
      "Epoch 248/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0579 - acc: 0.5549 - val_loss: 0.0560 - val_acc: 0.5844\n",
      "Epoch 249/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0580 - acc: 0.5516 - val_loss: 0.0560 - val_acc: 0.5860\n",
      "Epoch 250/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0577 - acc: 0.5563 - val_loss: 0.0575 - val_acc: 0.5661\n",
      "Epoch 251/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0579 - acc: 0.5477 - val_loss: 0.0565 - val_acc: 0.5777\n",
      "Epoch 252/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0576 - acc: 0.5593 - val_loss: 0.0564 - val_acc: 0.5736\n",
      "Epoch 253/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0574 - acc: 0.5582 - val_loss: 0.0559 - val_acc: 0.5844\n",
      "Epoch 254/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0576 - acc: 0.5563 - val_loss: 0.0559 - val_acc: 0.5844\n",
      "Epoch 255/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0576 - acc: 0.5579 - val_loss: 0.0562 - val_acc: 0.5736\n",
      "Epoch 256/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0581 - acc: 0.5521 - val_loss: 0.0569 - val_acc: 0.5777\n",
      "Epoch 257/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0579 - acc: 0.5518 - val_loss: 0.0562 - val_acc: 0.5835\n",
      "Epoch 258/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0574 - acc: 0.5543 - val_loss: 0.0571 - val_acc: 0.5769\n",
      "Epoch 259/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0577 - acc: 0.5516 - val_loss: 0.0565 - val_acc: 0.5686\n",
      "Epoch 260/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0579 - acc: 0.5527 - val_loss: 0.0572 - val_acc: 0.5628\n",
      "Epoch 261/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0577 - acc: 0.5513 - val_loss: 0.0560 - val_acc: 0.5686\n",
      "Epoch 262/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0576 - acc: 0.5574 - val_loss: 0.0578 - val_acc: 0.5736\n",
      "Epoch 263/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0578 - acc: 0.5568 - val_loss: 0.0560 - val_acc: 0.5860\n",
      "Epoch 264/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0575 - acc: 0.5529 - val_loss: 0.0593 - val_acc: 0.5387\n",
      "Epoch 265/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0583 - acc: 0.5474 - val_loss: 0.0569 - val_acc: 0.5669\n",
      "Epoch 266/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0583 - acc: 0.5460 - val_loss: 0.0568 - val_acc: 0.5653\n",
      "Epoch 267/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0582 - acc: 0.5510 - val_loss: 0.0567 - val_acc: 0.5752\n",
      "Epoch 268/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0579 - acc: 0.5518 - val_loss: 0.0557 - val_acc: 0.5794\n",
      "Epoch 269/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0576 - acc: 0.5585 - val_loss: 0.0561 - val_acc: 0.5852\n",
      "Epoch 270/500\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0574 - acc: 0.5582 - val_loss: 0.0575 - val_acc: 0.5744\n",
      "Epoch 271/500\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0577 - acc: 0.5568 - val_loss: 0.0560 - val_acc: 0.5736\n",
      "Epoch 272/500\n",
      "3608/3608 [==============================] - 1s 300us/step - loss: 0.0579 - acc: 0.5529 - val_loss: 0.0628 - val_acc: 0.5129\n",
      "Epoch 273/500\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0580 - acc: 0.5510 - val_loss: 0.0563 - val_acc: 0.5752\n",
      "Epoch 274/500\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0579 - acc: 0.5524 - val_loss: 0.0562 - val_acc: 0.5786\n",
      "Epoch 275/500\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0577 - acc: 0.5540 - val_loss: 0.0577 - val_acc: 0.5495\n",
      "Epoch 276/500\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0576 - acc: 0.5571 - val_loss: 0.0573 - val_acc: 0.5686\n",
      "Epoch 277/500\n",
      "3608/3608 [==============================] - 1s 302us/step - loss: 0.0574 - acc: 0.5593 - val_loss: 0.0564 - val_acc: 0.5827\n",
      "Epoch 278/500\n",
      "3608/3608 [==============================] - 1s 291us/step - loss: 0.0592 - acc: 0.5396 - val_loss: 0.0568 - val_acc: 0.5661\n",
      "Epoch 279/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0578 - acc: 0.5510 - val_loss: 0.0558 - val_acc: 0.5835\n",
      "Epoch 280/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0577 - acc: 0.5532 - val_loss: 0.0562 - val_acc: 0.5810\n",
      "Epoch 281/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0575 - acc: 0.5582 - val_loss: 0.0566 - val_acc: 0.5777\n",
      "Epoch 282/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0575 - acc: 0.5546 - val_loss: 0.0559 - val_acc: 0.5761\n",
      "Epoch 283/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0575 - acc: 0.5610 - val_loss: 0.0558 - val_acc: 0.5752\n",
      "Epoch 284/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0578 - acc: 0.5532 - val_loss: 0.0566 - val_acc: 0.5752\n",
      "Epoch 285/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0576 - acc: 0.5579 - val_loss: 0.0566 - val_acc: 0.5777\n",
      "Epoch 286/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0575 - acc: 0.5607 - val_loss: 0.0565 - val_acc: 0.5852\n",
      "Epoch 287/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0580 - acc: 0.5532 - val_loss: 0.0565 - val_acc: 0.5686\n",
      "Epoch 288/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0578 - acc: 0.5518 - val_loss: 0.0577 - val_acc: 0.5370\n",
      "Epoch 289/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0576 - acc: 0.5546 - val_loss: 0.0571 - val_acc: 0.5769\n",
      "Epoch 290/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0579 - acc: 0.5538 - val_loss: 0.0559 - val_acc: 0.5869\n",
      "Epoch 291/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0582 - acc: 0.5554 - val_loss: 0.0568 - val_acc: 0.5702\n",
      "Epoch 292/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0579 - acc: 0.5518 - val_loss: 0.0563 - val_acc: 0.5727\n",
      "Epoch 293/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0579 - acc: 0.5463 - val_loss: 0.0561 - val_acc: 0.5869\n",
      "Epoch 294/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0591 - acc: 0.5457 - val_loss: 0.0576 - val_acc: 0.5736\n",
      "Epoch 295/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0591 - acc: 0.5471 - val_loss: 0.0573 - val_acc: 0.5719\n",
      "Epoch 296/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0578 - acc: 0.5560 - val_loss: 0.0573 - val_acc: 0.5702\n",
      "Epoch 297/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0575 - acc: 0.5582 - val_loss: 0.0562 - val_acc: 0.5827\n",
      "Epoch 298/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0576 - acc: 0.5521 - val_loss: 0.0565 - val_acc: 0.5744\n",
      "Epoch 299/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0580 - acc: 0.5529 - val_loss: 0.0566 - val_acc: 0.5819\n",
      "Epoch 300/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0575 - acc: 0.5563 - val_loss: 0.0563 - val_acc: 0.5802\n",
      "Epoch 301/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0574 - acc: 0.5571 - val_loss: 0.0569 - val_acc: 0.5736\n",
      "Epoch 302/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0577 - acc: 0.5543 - val_loss: 0.0563 - val_acc: 0.5744\n",
      "Epoch 303/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0575 - acc: 0.5576 - val_loss: 0.0560 - val_acc: 0.5777\n",
      "Epoch 304/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0574 - acc: 0.5576 - val_loss: 0.0558 - val_acc: 0.5802\n",
      "Epoch 305/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0574 - acc: 0.5613 - val_loss: 0.0559 - val_acc: 0.5810\n",
      "Epoch 306/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0573 - acc: 0.5535 - val_loss: 0.0559 - val_acc: 0.5844\n",
      "Epoch 307/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0577 - acc: 0.5579 - val_loss: 0.0596 - val_acc: 0.5303\n",
      "Epoch 308/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0578 - acc: 0.5552 - val_loss: 0.0572 - val_acc: 0.5628\n",
      "Epoch 309/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0581 - acc: 0.5546 - val_loss: 0.0559 - val_acc: 0.5810\n",
      "Epoch 310/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0578 - acc: 0.5568 - val_loss: 0.0572 - val_acc: 0.5603\n",
      "Epoch 311/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0578 - acc: 0.5579 - val_loss: 0.0558 - val_acc: 0.5794\n",
      "Epoch 312/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0576 - acc: 0.5549 - val_loss: 0.0570 - val_acc: 0.5810\n",
      "Epoch 313/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0575 - acc: 0.5557 - val_loss: 0.0598 - val_acc: 0.5312\n",
      "Epoch 314/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0576 - acc: 0.5618 - val_loss: 0.0566 - val_acc: 0.5810\n",
      "Epoch 315/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0575 - acc: 0.5557 - val_loss: 0.0560 - val_acc: 0.5819\n",
      "Epoch 316/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0574 - acc: 0.5588 - val_loss: 0.0560 - val_acc: 0.5844\n",
      "Epoch 317/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0574 - acc: 0.5601 - val_loss: 0.0565 - val_acc: 0.5744\n",
      "Epoch 318/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0575 - acc: 0.5521 - val_loss: 0.0565 - val_acc: 0.5769\n",
      "Epoch 319/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0575 - acc: 0.5521 - val_loss: 0.0561 - val_acc: 0.5810\n",
      "Epoch 320/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0578 - acc: 0.5424 - val_loss: 0.0566 - val_acc: 0.5752\n",
      "Epoch 321/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0579 - acc: 0.5585 - val_loss: 0.0566 - val_acc: 0.5827\n",
      "Epoch 322/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0576 - acc: 0.5518 - val_loss: 0.0562 - val_acc: 0.5744\n",
      "Epoch 323/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0575 - acc: 0.5532 - val_loss: 0.0565 - val_acc: 0.5786\n",
      "Epoch 324/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0578 - acc: 0.5527 - val_loss: 0.0558 - val_acc: 0.5835\n",
      "Epoch 325/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0576 - acc: 0.5571 - val_loss: 0.0563 - val_acc: 0.5661\n",
      "Epoch 326/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0576 - acc: 0.5593 - val_loss: 0.0612 - val_acc: 0.5262\n",
      "Epoch 327/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0585 - acc: 0.5449 - val_loss: 0.0577 - val_acc: 0.5694\n",
      "Epoch 328/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0582 - acc: 0.5510 - val_loss: 0.0561 - val_acc: 0.5869\n",
      "Epoch 329/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0576 - acc: 0.5504 - val_loss: 0.0561 - val_acc: 0.5869\n",
      "Epoch 330/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0581 - acc: 0.5557 - val_loss: 0.0565 - val_acc: 0.5727\n",
      "Epoch 331/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0574 - acc: 0.5546 - val_loss: 0.0560 - val_acc: 0.5802\n",
      "Epoch 332/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0574 - acc: 0.5582 - val_loss: 0.0563 - val_acc: 0.5769\n",
      "Epoch 333/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0576 - acc: 0.5560 - val_loss: 0.0563 - val_acc: 0.5827\n",
      "Epoch 334/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0581 - acc: 0.5532 - val_loss: 0.0563 - val_acc: 0.5810\n",
      "Epoch 335/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0576 - acc: 0.5565 - val_loss: 0.0565 - val_acc: 0.5810\n",
      "Epoch 336/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0574 - acc: 0.5571 - val_loss: 0.0558 - val_acc: 0.5844\n",
      "Epoch 337/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0573 - acc: 0.5552 - val_loss: 0.0564 - val_acc: 0.5810\n",
      "Epoch 338/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0575 - acc: 0.5546 - val_loss: 0.0572 - val_acc: 0.5744\n",
      "Epoch 339/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0576 - acc: 0.5557 - val_loss: 0.0564 - val_acc: 0.5802\n",
      "Epoch 340/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0575 - acc: 0.5607 - val_loss: 0.0571 - val_acc: 0.5611\n",
      "Epoch 341/500\n",
      "3608/3608 [==============================] - 1s 254us/step - loss: 0.0577 - acc: 0.5596 - val_loss: 0.0567 - val_acc: 0.5677\n",
      "Epoch 342/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0573 - acc: 0.5593 - val_loss: 0.0568 - val_acc: 0.5677\n",
      "Epoch 343/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0589 - acc: 0.5499 - val_loss: 0.0566 - val_acc: 0.5752\n",
      "Epoch 344/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0591 - acc: 0.5369 - val_loss: 0.0561 - val_acc: 0.5702\n",
      "Epoch 345/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0586 - acc: 0.5496 - val_loss: 0.0564 - val_acc: 0.5769\n",
      "Epoch 346/500\n",
      "3608/3608 [==============================] - 1s 254us/step - loss: 0.0579 - acc: 0.5532 - val_loss: 0.0559 - val_acc: 0.5827\n",
      "Epoch 347/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0575 - acc: 0.5568 - val_loss: 0.0562 - val_acc: 0.5794\n",
      "Epoch 348/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0575 - acc: 0.5576 - val_loss: 0.0577 - val_acc: 0.5653\n",
      "Epoch 349/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0576 - acc: 0.5576 - val_loss: 0.0569 - val_acc: 0.5719\n",
      "Epoch 350/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0580 - acc: 0.5532 - val_loss: 0.0558 - val_acc: 0.5810\n",
      "Epoch 351/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0575 - acc: 0.5554 - val_loss: 0.0563 - val_acc: 0.5711\n",
      "Epoch 352/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0575 - acc: 0.5540 - val_loss: 0.0559 - val_acc: 0.5827\n",
      "Epoch 353/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0574 - acc: 0.5588 - val_loss: 0.0561 - val_acc: 0.5835\n",
      "Epoch 354/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0574 - acc: 0.5613 - val_loss: 0.0561 - val_acc: 0.5802\n",
      "Epoch 355/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0574 - acc: 0.5601 - val_loss: 0.0560 - val_acc: 0.5794\n",
      "Epoch 356/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0577 - acc: 0.5493 - val_loss: 0.0561 - val_acc: 0.5869\n",
      "Epoch 357/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0582 - acc: 0.5491 - val_loss: 0.0564 - val_acc: 0.5661\n",
      "Epoch 358/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0577 - acc: 0.5554 - val_loss: 0.0564 - val_acc: 0.5694\n",
      "Epoch 359/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0580 - acc: 0.5457 - val_loss: 0.0561 - val_acc: 0.5810\n",
      "Epoch 360/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0581 - acc: 0.5479 - val_loss: 0.0561 - val_acc: 0.5702\n",
      "Epoch 361/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0577 - acc: 0.5543 - val_loss: 0.0563 - val_acc: 0.5827\n",
      "Epoch 362/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0576 - acc: 0.5599 - val_loss: 0.0560 - val_acc: 0.5761\n",
      "Epoch 363/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0583 - acc: 0.5532 - val_loss: 0.0574 - val_acc: 0.5702\n",
      "Epoch 364/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0575 - acc: 0.5543 - val_loss: 0.0567 - val_acc: 0.5702\n",
      "Epoch 365/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0575 - acc: 0.5543 - val_loss: 0.0563 - val_acc: 0.5719\n",
      "Epoch 366/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0575 - acc: 0.5571 - val_loss: 0.0559 - val_acc: 0.5827\n",
      "Epoch 367/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0583 - acc: 0.5460 - val_loss: 0.0568 - val_acc: 0.5636\n",
      "Epoch 368/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0584 - acc: 0.5457 - val_loss: 0.0561 - val_acc: 0.5819\n",
      "Epoch 369/500\n",
      "3608/3608 [==============================] - 1s 256us/step - loss: 0.0578 - acc: 0.5546 - val_loss: 0.0561 - val_acc: 0.5794\n",
      "Epoch 370/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0574 - acc: 0.5557 - val_loss: 0.0563 - val_acc: 0.5835\n",
      "Epoch 371/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0574 - acc: 0.5538 - val_loss: 0.0566 - val_acc: 0.5702\n",
      "Epoch 372/500\n",
      "3608/3608 [==============================] - 1s 256us/step - loss: 0.0575 - acc: 0.5601 - val_loss: 0.0592 - val_acc: 0.5345\n",
      "Epoch 373/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0574 - acc: 0.5529 - val_loss: 0.0561 - val_acc: 0.5794\n",
      "Epoch 374/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0578 - acc: 0.5532 - val_loss: 0.0571 - val_acc: 0.5694\n",
      "Epoch 375/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0578 - acc: 0.5527 - val_loss: 0.0565 - val_acc: 0.5794\n",
      "Epoch 376/500\n",
      "3608/3608 [==============================] - 1s 256us/step - loss: 0.0575 - acc: 0.5563 - val_loss: 0.0561 - val_acc: 0.5802\n",
      "Epoch 377/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0575 - acc: 0.5524 - val_loss: 0.0563 - val_acc: 0.5786\n",
      "Epoch 378/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0579 - acc: 0.5552 - val_loss: 0.0587 - val_acc: 0.5553\n",
      "Epoch 379/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0579 - acc: 0.5585 - val_loss: 0.0561 - val_acc: 0.5752\n",
      "Epoch 380/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0576 - acc: 0.5560 - val_loss: 0.0565 - val_acc: 0.5719\n",
      "Epoch 381/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0577 - acc: 0.5565 - val_loss: 0.0562 - val_acc: 0.5761\n",
      "Epoch 382/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0576 - acc: 0.5576 - val_loss: 0.0564 - val_acc: 0.5677\n",
      "Epoch 383/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0577 - acc: 0.5565 - val_loss: 0.0565 - val_acc: 0.5669\n",
      "Epoch 384/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0573 - acc: 0.5554 - val_loss: 0.0564 - val_acc: 0.5761\n",
      "Epoch 385/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0571 - acc: 0.5579 - val_loss: 0.0559 - val_acc: 0.5769\n",
      "Epoch 386/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0574 - acc: 0.5585 - val_loss: 0.0569 - val_acc: 0.5686\n",
      "Epoch 387/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0585 - acc: 0.5538 - val_loss: 0.0571 - val_acc: 0.5702\n",
      "Epoch 388/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0582 - acc: 0.5543 - val_loss: 0.0568 - val_acc: 0.5702\n",
      "Epoch 389/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0585 - acc: 0.5521 - val_loss: 0.0568 - val_acc: 0.5578\n",
      "Epoch 390/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0579 - acc: 0.5535 - val_loss: 0.0564 - val_acc: 0.5794\n",
      "Epoch 391/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0584 - acc: 0.5532 - val_loss: 0.0573 - val_acc: 0.5702\n",
      "Epoch 392/500\n",
      "3608/3608 [==============================] - 1s 304us/step - loss: 0.0598 - acc: 0.5507 - val_loss: 0.0573 - val_acc: 0.5677\n",
      "Epoch 393/500\n",
      "3608/3608 [==============================] - 1s 304us/step - loss: 0.0591 - acc: 0.5482 - val_loss: 0.0565 - val_acc: 0.5736\n",
      "Epoch 394/500\n",
      "3608/3608 [==============================] - 1s 305us/step - loss: 0.0582 - acc: 0.5493 - val_loss: 0.0572 - val_acc: 0.5636\n",
      "Epoch 395/500\n",
      "3608/3608 [==============================] - 1s 308us/step - loss: 0.0583 - acc: 0.5435 - val_loss: 0.0565 - val_acc: 0.5719\n",
      "Epoch 396/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0579 - acc: 0.5529 - val_loss: 0.0581 - val_acc: 0.5669\n",
      "Epoch 397/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0584 - acc: 0.5518 - val_loss: 0.0572 - val_acc: 0.5694\n",
      "Epoch 398/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0586 - acc: 0.5485 - val_loss: 0.0566 - val_acc: 0.5761\n",
      "Epoch 399/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0581 - acc: 0.5532 - val_loss: 0.0566 - val_acc: 0.5719\n",
      "Epoch 400/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0583 - acc: 0.5529 - val_loss: 0.0566 - val_acc: 0.5769\n",
      "Epoch 401/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0583 - acc: 0.5463 - val_loss: 0.0567 - val_acc: 0.5694\n",
      "Epoch 402/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0583 - acc: 0.5452 - val_loss: 0.0575 - val_acc: 0.5677\n",
      "Epoch 403/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0581 - acc: 0.5527 - val_loss: 0.0567 - val_acc: 0.5702\n",
      "Epoch 404/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0582 - acc: 0.5518 - val_loss: 0.0581 - val_acc: 0.5470\n",
      "Epoch 405/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0581 - acc: 0.5535 - val_loss: 0.0568 - val_acc: 0.5744\n",
      "Epoch 406/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0579 - acc: 0.5518 - val_loss: 0.0570 - val_acc: 0.5711\n",
      "Epoch 407/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0578 - acc: 0.5538 - val_loss: 0.0569 - val_acc: 0.5702\n",
      "Epoch 408/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0578 - acc: 0.5513 - val_loss: 0.0560 - val_acc: 0.5860\n",
      "Epoch 409/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0579 - acc: 0.5546 - val_loss: 0.0562 - val_acc: 0.5769\n",
      "Epoch 410/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0579 - acc: 0.5543 - val_loss: 0.0559 - val_acc: 0.5794\n",
      "Epoch 411/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0577 - acc: 0.5546 - val_loss: 0.0581 - val_acc: 0.5594\n",
      "Epoch 412/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0578 - acc: 0.5516 - val_loss: 0.0571 - val_acc: 0.5702\n",
      "Epoch 413/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0577 - acc: 0.5532 - val_loss: 0.0576 - val_acc: 0.5744\n",
      "Epoch 414/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0575 - acc: 0.5543 - val_loss: 0.0565 - val_acc: 0.5769\n",
      "Epoch 415/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0578 - acc: 0.5527 - val_loss: 0.0573 - val_acc: 0.5702\n",
      "Epoch 416/500\n",
      "3608/3608 [==============================] - 1s 240us/step - loss: 0.0577 - acc: 0.5543 - val_loss: 0.0561 - val_acc: 0.5786\n",
      "Epoch 417/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0580 - acc: 0.5532 - val_loss: 0.0592 - val_acc: 0.5353\n",
      "Epoch 418/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0581 - acc: 0.5499 - val_loss: 0.0564 - val_acc: 0.5752\n",
      "Epoch 419/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0578 - acc: 0.5582 - val_loss: 0.0562 - val_acc: 0.5727\n",
      "Epoch 420/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0576 - acc: 0.5543 - val_loss: 0.0563 - val_acc: 0.5761\n",
      "Epoch 421/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0577 - acc: 0.5516 - val_loss: 0.0570 - val_acc: 0.5677\n",
      "Epoch 422/500\n",
      "3608/3608 [==============================] - 1s 238us/step - loss: 0.0576 - acc: 0.5543 - val_loss: 0.0589 - val_acc: 0.5561\n",
      "Epoch 423/500\n",
      "3608/3608 [==============================] - 1s 234us/step - loss: 0.0576 - acc: 0.5535 - val_loss: 0.0563 - val_acc: 0.5810\n",
      "Epoch 424/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0577 - acc: 0.5513 - val_loss: 0.0570 - val_acc: 0.5736\n",
      "Epoch 425/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0579 - acc: 0.5479 - val_loss: 0.0576 - val_acc: 0.5694\n",
      "Epoch 426/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0581 - acc: 0.5599 - val_loss: 0.0564 - val_acc: 0.5794\n",
      "Epoch 427/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0575 - acc: 0.5560 - val_loss: 0.0568 - val_acc: 0.5727\n",
      "Epoch 428/500\n",
      "3608/3608 [==============================] - 1s 244us/step - loss: 0.0579 - acc: 0.5546 - val_loss: 0.0560 - val_acc: 0.5819\n",
      "Epoch 429/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0576 - acc: 0.5552 - val_loss: 0.0561 - val_acc: 0.5744\n",
      "Epoch 430/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0577 - acc: 0.5554 - val_loss: 0.0571 - val_acc: 0.5669\n",
      "Epoch 431/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0579 - acc: 0.5499 - val_loss: 0.0563 - val_acc: 0.5744\n",
      "Epoch 432/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0578 - acc: 0.5574 - val_loss: 0.0572 - val_acc: 0.5711\n",
      "Epoch 433/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0573 - acc: 0.5590 - val_loss: 0.0581 - val_acc: 0.5436\n",
      "Epoch 434/500\n",
      "3608/3608 [==============================] - 1s 257us/step - loss: 0.0577 - acc: 0.5576 - val_loss: 0.0587 - val_acc: 0.5511\n",
      "Epoch 435/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0578 - acc: 0.5557 - val_loss: 0.0569 - val_acc: 0.5794\n",
      "Epoch 436/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0577 - acc: 0.5549 - val_loss: 0.0562 - val_acc: 0.5769\n",
      "Epoch 437/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0574 - acc: 0.5590 - val_loss: 0.0577 - val_acc: 0.5536\n",
      "Epoch 438/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0573 - acc: 0.5574 - val_loss: 0.0566 - val_acc: 0.5769\n",
      "Epoch 439/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0578 - acc: 0.5546 - val_loss: 0.0561 - val_acc: 0.5769\n",
      "Epoch 440/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0575 - acc: 0.5532 - val_loss: 0.0563 - val_acc: 0.5744\n",
      "Epoch 441/500\n",
      "3608/3608 [==============================] - 1s 254us/step - loss: 0.0574 - acc: 0.5521 - val_loss: 0.0560 - val_acc: 0.5885\n",
      "Epoch 442/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0574 - acc: 0.5560 - val_loss: 0.0559 - val_acc: 0.5727\n",
      "Epoch 443/500\n",
      "3608/3608 [==============================] - 1s 258us/step - loss: 0.0579 - acc: 0.5524 - val_loss: 0.0564 - val_acc: 0.5794\n",
      "Epoch 444/500\n",
      "3608/3608 [==============================] - 1s 254us/step - loss: 0.0574 - acc: 0.5574 - val_loss: 0.0559 - val_acc: 0.5761\n",
      "Epoch 445/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0575 - acc: 0.5554 - val_loss: 0.0561 - val_acc: 0.5727\n",
      "Epoch 446/500\n",
      "3608/3608 [==============================] - 1s 256us/step - loss: 0.0575 - acc: 0.5538 - val_loss: 0.0567 - val_acc: 0.5702\n",
      "Epoch 447/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0577 - acc: 0.5516 - val_loss: 0.0574 - val_acc: 0.5719\n",
      "Epoch 448/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0579 - acc: 0.5529 - val_loss: 0.0563 - val_acc: 0.5761\n",
      "Epoch 449/500\n",
      "3608/3608 [==============================] - 1s 254us/step - loss: 0.0575 - acc: 0.5546 - val_loss: 0.0559 - val_acc: 0.5844\n",
      "Epoch 450/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0577 - acc: 0.5607 - val_loss: 0.0563 - val_acc: 0.5827\n",
      "Epoch 451/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0575 - acc: 0.5557 - val_loss: 0.0573 - val_acc: 0.5461\n",
      "Epoch 452/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0574 - acc: 0.5529 - val_loss: 0.0557 - val_acc: 0.5810\n",
      "Epoch 453/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0575 - acc: 0.5596 - val_loss: 0.0561 - val_acc: 0.5744\n",
      "Epoch 454/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0577 - acc: 0.5516 - val_loss: 0.0563 - val_acc: 0.5786\n",
      "Epoch 455/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0575 - acc: 0.5535 - val_loss: 0.0563 - val_acc: 0.5727\n",
      "Epoch 456/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0574 - acc: 0.5610 - val_loss: 0.0560 - val_acc: 0.5877\n",
      "Epoch 457/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0573 - acc: 0.5590 - val_loss: 0.0559 - val_acc: 0.5777\n",
      "Epoch 458/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0574 - acc: 0.5613 - val_loss: 0.0563 - val_acc: 0.5727\n",
      "Epoch 459/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0575 - acc: 0.5538 - val_loss: 0.0561 - val_acc: 0.5794\n",
      "Epoch 460/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0575 - acc: 0.5615 - val_loss: 0.0561 - val_acc: 0.5711\n",
      "Epoch 461/500\n",
      "3608/3608 [==============================] - 1s 242us/step - loss: 0.0574 - acc: 0.5615 - val_loss: 0.0571 - val_acc: 0.5761\n",
      "Epoch 462/500\n",
      "3608/3608 [==============================] - 1s 241us/step - loss: 0.0575 - acc: 0.5560 - val_loss: 0.0571 - val_acc: 0.5810\n",
      "Epoch 463/500\n",
      "3608/3608 [==============================] - 1s 239us/step - loss: 0.0576 - acc: 0.5507 - val_loss: 0.0568 - val_acc: 0.5669\n",
      "Epoch 464/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0574 - acc: 0.5549 - val_loss: 0.0570 - val_acc: 0.5702\n",
      "Epoch 465/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0583 - acc: 0.5468 - val_loss: 0.0570 - val_acc: 0.5686\n",
      "Epoch 466/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0577 - acc: 0.5529 - val_loss: 0.0583 - val_acc: 0.5461\n",
      "Epoch 467/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0578 - acc: 0.5560 - val_loss: 0.0560 - val_acc: 0.5752\n",
      "Epoch 468/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0579 - acc: 0.5543 - val_loss: 0.0559 - val_acc: 0.5786\n",
      "Epoch 469/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0573 - acc: 0.5613 - val_loss: 0.0558 - val_acc: 0.5802\n",
      "Epoch 470/500\n",
      "3608/3608 [==============================] - 1s 243us/step - loss: 0.0574 - acc: 0.5585 - val_loss: 0.0563 - val_acc: 0.5736\n",
      "Epoch 471/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0576 - acc: 0.5557 - val_loss: 0.0566 - val_acc: 0.5636\n",
      "Epoch 472/500\n",
      "3608/3608 [==============================] - 1s 247us/step - loss: 0.0573 - acc: 0.5571 - val_loss: 0.0559 - val_acc: 0.5810\n",
      "Epoch 473/500\n",
      "3608/3608 [==============================] - 1s 246us/step - loss: 0.0576 - acc: 0.5479 - val_loss: 0.0566 - val_acc: 0.5544\n",
      "Epoch 474/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0575 - acc: 0.5563 - val_loss: 0.0558 - val_acc: 0.5802\n",
      "Epoch 475/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0572 - acc: 0.5560 - val_loss: 0.0573 - val_acc: 0.5619\n",
      "Epoch 476/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0574 - acc: 0.5529 - val_loss: 0.0563 - val_acc: 0.5744\n",
      "Epoch 477/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0572 - acc: 0.5554 - val_loss: 0.0558 - val_acc: 0.5827\n",
      "Epoch 478/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0573 - acc: 0.5588 - val_loss: 0.0574 - val_acc: 0.5561\n",
      "Epoch 479/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0574 - acc: 0.5621 - val_loss: 0.0560 - val_acc: 0.5835\n",
      "Epoch 480/500\n",
      "3608/3608 [==============================] - 1s 254us/step - loss: 0.0577 - acc: 0.5568 - val_loss: 0.0559 - val_acc: 0.5719\n",
      "Epoch 481/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0575 - acc: 0.5532 - val_loss: 0.0562 - val_acc: 0.5769\n",
      "Epoch 482/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0575 - acc: 0.5546 - val_loss: 0.0563 - val_acc: 0.5786\n",
      "Epoch 483/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0574 - acc: 0.5552 - val_loss: 0.0561 - val_acc: 0.5744\n",
      "Epoch 484/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0576 - acc: 0.5552 - val_loss: 0.0572 - val_acc: 0.5677\n",
      "Epoch 485/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0582 - acc: 0.5482 - val_loss: 0.0561 - val_acc: 0.5702\n",
      "Epoch 486/500\n",
      "3608/3608 [==============================] - 1s 248us/step - loss: 0.0580 - acc: 0.5427 - val_loss: 0.0566 - val_acc: 0.5669\n",
      "Epoch 487/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0579 - acc: 0.5493 - val_loss: 0.0580 - val_acc: 0.5561\n",
      "Epoch 488/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0586 - acc: 0.5452 - val_loss: 0.0572 - val_acc: 0.5536\n",
      "Epoch 489/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0581 - acc: 0.5463 - val_loss: 0.0565 - val_acc: 0.5644\n",
      "Epoch 490/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0578 - acc: 0.5596 - val_loss: 0.0558 - val_acc: 0.5769\n",
      "Epoch 491/500\n",
      "3608/3608 [==============================] - 1s 245us/step - loss: 0.0577 - acc: 0.5549 - val_loss: 0.0578 - val_acc: 0.5528\n",
      "Epoch 492/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0575 - acc: 0.5529 - val_loss: 0.0583 - val_acc: 0.5370\n",
      "Epoch 493/500\n",
      "3608/3608 [==============================] - 1s 253us/step - loss: 0.0579 - acc: 0.5471 - val_loss: 0.0565 - val_acc: 0.5711\n",
      "Epoch 494/500\n",
      "3608/3608 [==============================] - 1s 249us/step - loss: 0.0576 - acc: 0.5527 - val_loss: 0.0560 - val_acc: 0.5777\n",
      "Epoch 495/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0573 - acc: 0.5560 - val_loss: 0.0577 - val_acc: 0.5486\n",
      "Epoch 496/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0579 - acc: 0.5527 - val_loss: 0.0560 - val_acc: 0.5769\n",
      "Epoch 497/500\n",
      "3608/3608 [==============================] - 1s 252us/step - loss: 0.0574 - acc: 0.5496 - val_loss: 0.0572 - val_acc: 0.5711\n",
      "Epoch 498/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0575 - acc: 0.5549 - val_loss: 0.0560 - val_acc: 0.5744\n",
      "Epoch 499/500\n",
      "3608/3608 [==============================] - 1s 251us/step - loss: 0.0576 - acc: 0.5499 - val_loss: 0.0569 - val_acc: 0.5553\n",
      "Epoch 500/500\n",
      "3608/3608 [==============================] - 1s 250us/step - loss: 0.0574 - acc: 0.5549 - val_loss: 0.0567 - val_acc: 0.5727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77fdfe6f60>"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(9,)))\n",
    "nn_model.add(Dense(units=9, activation='relu'))\n",
    "nn_model.add(Dense(units=12, activation='relu'))\n",
    "nn_model.add(Dense(units=15, activation='relu'))\n",
    "nn_model.add(Dense(units=18, activation='relu'))\n",
    "nn_model.add(Dense(units=12, activation='relu'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "nn_model.fit(x=new_train_x, y=train_y, epochs=500, validation_data=(new_valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EdMGU978qfss"
   },
   "source": [
    "Not a big change. Now I used Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17476
    },
    "colab_type": "code",
    "id": "uYBJDCRmSfD9",
    "outputId": "eb3b2032-d222-4126-f96c-58200263bdb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 9)                 90        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 18)                180       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 36)                684       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 18)                666       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 12)                228       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 1,978\n",
      "Trainable params: 1,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3608 samples, validate on 1203 samples\n",
      "Epoch 1/500\n",
      "3608/3608 [==============================] - 2s 468us/step - loss: 0.0733 - acc: 0.3897 - val_loss: 0.0659 - val_acc: 0.4547\n",
      "Epoch 2/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0689 - acc: 0.4360 - val_loss: 0.0678 - val_acc: 0.4547\n",
      "Epoch 3/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0675 - acc: 0.4393 - val_loss: 0.0645 - val_acc: 0.4672\n",
      "Epoch 4/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0663 - acc: 0.4404 - val_loss: 0.0685 - val_acc: 0.5046\n",
      "Epoch 5/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0657 - acc: 0.4498 - val_loss: 0.0634 - val_acc: 0.5046\n",
      "Epoch 6/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0653 - acc: 0.4581 - val_loss: 0.0638 - val_acc: 0.5270\n",
      "Epoch 7/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0647 - acc: 0.4598 - val_loss: 0.0611 - val_acc: 0.5303\n",
      "Epoch 8/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0646 - acc: 0.4618 - val_loss: 0.0658 - val_acc: 0.5054\n",
      "Epoch 9/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0644 - acc: 0.4701 - val_loss: 0.0671 - val_acc: 0.4555\n",
      "Epoch 10/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0636 - acc: 0.4784 - val_loss: 0.0671 - val_acc: 0.3491\n",
      "Epoch 11/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0635 - acc: 0.4712 - val_loss: 0.0706 - val_acc: 0.2793\n",
      "Epoch 12/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0629 - acc: 0.4834 - val_loss: 0.0660 - val_acc: 0.3932\n",
      "Epoch 13/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0633 - acc: 0.4820 - val_loss: 0.0630 - val_acc: 0.5320\n",
      "Epoch 14/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0632 - acc: 0.4751 - val_loss: 0.0669 - val_acc: 0.4289\n",
      "Epoch 15/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0623 - acc: 0.5017 - val_loss: 0.0703 - val_acc: 0.3466\n",
      "Epoch 16/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0624 - acc: 0.4925 - val_loss: 0.0643 - val_acc: 0.5270\n",
      "Epoch 17/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0627 - acc: 0.4861 - val_loss: 0.0689 - val_acc: 0.3367\n",
      "Epoch 18/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0621 - acc: 0.4994 - val_loss: 0.0674 - val_acc: 0.4431\n",
      "Epoch 19/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0624 - acc: 0.4970 - val_loss: 0.0643 - val_acc: 0.5062\n",
      "Epoch 20/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0621 - acc: 0.4997 - val_loss: 0.0713 - val_acc: 0.2901\n",
      "Epoch 21/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0626 - acc: 0.5044 - val_loss: 0.0630 - val_acc: 0.5387\n",
      "Epoch 22/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0622 - acc: 0.5014 - val_loss: 0.0657 - val_acc: 0.4613\n",
      "Epoch 23/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0619 - acc: 0.5036 - val_loss: 0.0637 - val_acc: 0.5287\n",
      "Epoch 24/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0624 - acc: 0.4967 - val_loss: 0.0663 - val_acc: 0.4298\n",
      "Epoch 25/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0625 - acc: 0.4933 - val_loss: 0.0681 - val_acc: 0.3840\n",
      "Epoch 26/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0622 - acc: 0.4983 - val_loss: 0.0648 - val_acc: 0.4572\n",
      "Epoch 27/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0623 - acc: 0.4961 - val_loss: 0.0652 - val_acc: 0.5154\n",
      "Epoch 28/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0618 - acc: 0.5019 - val_loss: 0.0655 - val_acc: 0.4647\n",
      "Epoch 29/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0612 - acc: 0.5161 - val_loss: 0.0638 - val_acc: 0.5245\n",
      "Epoch 30/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0614 - acc: 0.5119 - val_loss: 0.0661 - val_acc: 0.4256\n",
      "Epoch 31/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0613 - acc: 0.5155 - val_loss: 0.0639 - val_acc: 0.5145\n",
      "Epoch 32/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0621 - acc: 0.5025 - val_loss: 0.0625 - val_acc: 0.5212\n",
      "Epoch 33/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0612 - acc: 0.5078 - val_loss: 0.0683 - val_acc: 0.3525\n",
      "Epoch 34/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0613 - acc: 0.5078 - val_loss: 0.0636 - val_acc: 0.4879\n",
      "Epoch 35/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0615 - acc: 0.5064 - val_loss: 0.0627 - val_acc: 0.5362\n",
      "Epoch 36/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0617 - acc: 0.5008 - val_loss: 0.0670 - val_acc: 0.3807\n",
      "Epoch 37/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0615 - acc: 0.5089 - val_loss: 0.0689 - val_acc: 0.3383\n",
      "Epoch 38/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0619 - acc: 0.4994 - val_loss: 0.0653 - val_acc: 0.4347\n",
      "Epoch 39/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0619 - acc: 0.4972 - val_loss: 0.0601 - val_acc: 0.5220\n",
      "Epoch 40/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0612 - acc: 0.5141 - val_loss: 0.0629 - val_acc: 0.5154\n",
      "Epoch 41/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0616 - acc: 0.5042 - val_loss: 0.0686 - val_acc: 0.3475\n",
      "Epoch 42/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0614 - acc: 0.5114 - val_loss: 0.0633 - val_acc: 0.5054\n",
      "Epoch 43/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0615 - acc: 0.5111 - val_loss: 0.0669 - val_acc: 0.3791\n",
      "Epoch 44/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0610 - acc: 0.5177 - val_loss: 0.0675 - val_acc: 0.3741\n",
      "Epoch 45/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0617 - acc: 0.5122 - val_loss: 0.0658 - val_acc: 0.4680\n",
      "Epoch 46/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0609 - acc: 0.5139 - val_loss: 0.0672 - val_acc: 0.3741\n",
      "Epoch 47/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0611 - acc: 0.5155 - val_loss: 0.0696 - val_acc: 0.3383\n",
      "Epoch 48/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0613 - acc: 0.5108 - val_loss: 0.0624 - val_acc: 0.5154\n",
      "Epoch 49/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0607 - acc: 0.5236 - val_loss: 0.0607 - val_acc: 0.5245\n",
      "Epoch 50/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0612 - acc: 0.5091 - val_loss: 0.0706 - val_acc: 0.3117\n",
      "Epoch 51/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0609 - acc: 0.5230 - val_loss: 0.0657 - val_acc: 0.4605\n",
      "Epoch 52/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0624 - acc: 0.4961 - val_loss: 0.0635 - val_acc: 0.4805\n",
      "Epoch 53/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0614 - acc: 0.5094 - val_loss: 0.0691 - val_acc: 0.3416\n",
      "Epoch 54/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0612 - acc: 0.5130 - val_loss: 0.0612 - val_acc: 0.5245\n",
      "Epoch 55/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0613 - acc: 0.5058 - val_loss: 0.0620 - val_acc: 0.5145\n",
      "Epoch 56/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0618 - acc: 0.5075 - val_loss: 0.0688 - val_acc: 0.3541\n",
      "Epoch 57/500\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0613 - acc: 0.5111 - val_loss: 0.0666 - val_acc: 0.4206\n",
      "Epoch 58/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0607 - acc: 0.5125 - val_loss: 0.0688 - val_acc: 0.3483\n",
      "Epoch 59/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0611 - acc: 0.5150 - val_loss: 0.0697 - val_acc: 0.3466\n",
      "Epoch 60/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0610 - acc: 0.5116 - val_loss: 0.0648 - val_acc: 0.5071\n",
      "Epoch 61/500\n",
      "3608/3608 [==============================] - 1s 283us/step - loss: 0.0608 - acc: 0.5183 - val_loss: 0.0662 - val_acc: 0.4156\n",
      "Epoch 62/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0616 - acc: 0.5091 - val_loss: 0.0692 - val_acc: 0.3466\n",
      "Epoch 63/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0616 - acc: 0.5072 - val_loss: 0.0664 - val_acc: 0.4198\n",
      "Epoch 64/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0607 - acc: 0.5188 - val_loss: 0.0667 - val_acc: 0.4231\n",
      "Epoch 65/500\n",
      "3608/3608 [==============================] - 1s 282us/step - loss: 0.0610 - acc: 0.5213 - val_loss: 0.0650 - val_acc: 0.4580\n",
      "Epoch 66/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0605 - acc: 0.5172 - val_loss: 0.0685 - val_acc: 0.3749\n",
      "Epoch 67/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0611 - acc: 0.5186 - val_loss: 0.0655 - val_acc: 0.4888\n",
      "Epoch 68/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0619 - acc: 0.5022 - val_loss: 0.0618 - val_acc: 0.5328\n",
      "Epoch 69/500\n",
      "3608/3608 [==============================] - 1s 286us/step - loss: 0.0616 - acc: 0.5080 - val_loss: 0.0639 - val_acc: 0.5104\n",
      "Epoch 70/500\n",
      "3608/3608 [==============================] - 1s 284us/step - loss: 0.0613 - acc: 0.5064 - val_loss: 0.0692 - val_acc: 0.3541\n",
      "Epoch 71/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0612 - acc: 0.5133 - val_loss: 0.0642 - val_acc: 0.4988\n",
      "Epoch 72/500\n",
      "3608/3608 [==============================] - 1s 285us/step - loss: 0.0610 - acc: 0.5144 - val_loss: 0.0606 - val_acc: 0.5353\n",
      "Epoch 73/500\n",
      "3608/3608 [==============================] - 1s 281us/step - loss: 0.0609 - acc: 0.5119 - val_loss: 0.0680 - val_acc: 0.3666\n",
      "Epoch 74/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0609 - acc: 0.5139 - val_loss: 0.0617 - val_acc: 0.5071\n",
      "Epoch 75/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0610 - acc: 0.5169 - val_loss: 0.0660 - val_acc: 0.4580\n",
      "Epoch 76/500\n",
      "3608/3608 [==============================] - 1s 276us/step - loss: 0.0612 - acc: 0.5097 - val_loss: 0.0684 - val_acc: 0.3466\n",
      "Epoch 77/500\n",
      "3608/3608 [==============================] - 1s 278us/step - loss: 0.0614 - acc: 0.5108 - val_loss: 0.0655 - val_acc: 0.4613\n",
      "Epoch 78/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0610 - acc: 0.5089 - val_loss: 0.0686 - val_acc: 0.3275\n",
      "Epoch 79/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0611 - acc: 0.5161 - val_loss: 0.0696 - val_acc: 0.3483\n",
      "Epoch 80/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0610 - acc: 0.5130 - val_loss: 0.0633 - val_acc: 0.5112\n",
      "Epoch 81/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0617 - acc: 0.5030 - val_loss: 0.0671 - val_acc: 0.4140\n",
      "Epoch 82/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0611 - acc: 0.5119 - val_loss: 0.0622 - val_acc: 0.5029\n",
      "Epoch 83/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0609 - acc: 0.5141 - val_loss: 0.0616 - val_acc: 0.5229\n",
      "Epoch 84/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0610 - acc: 0.5111 - val_loss: 0.0634 - val_acc: 0.5079\n",
      "Epoch 85/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0612 - acc: 0.5105 - val_loss: 0.0650 - val_acc: 0.4530\n",
      "Epoch 86/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0606 - acc: 0.5186 - val_loss: 0.0620 - val_acc: 0.5212\n",
      "Epoch 87/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0614 - acc: 0.4997 - val_loss: 0.0598 - val_acc: 0.5378\n",
      "Epoch 88/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0612 - acc: 0.5086 - val_loss: 0.0625 - val_acc: 0.5287\n",
      "Epoch 89/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0612 - acc: 0.5158 - val_loss: 0.0597 - val_acc: 0.5461\n",
      "Epoch 90/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0610 - acc: 0.5177 - val_loss: 0.0640 - val_acc: 0.4938\n",
      "Epoch 91/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0614 - acc: 0.5094 - val_loss: 0.0673 - val_acc: 0.3574\n",
      "Epoch 92/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0617 - acc: 0.4983 - val_loss: 0.0619 - val_acc: 0.5212\n",
      "Epoch 93/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0610 - acc: 0.5150 - val_loss: 0.0621 - val_acc: 0.5229\n",
      "Epoch 94/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0606 - acc: 0.5155 - val_loss: 0.0683 - val_acc: 0.3649\n",
      "Epoch 95/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0610 - acc: 0.5197 - val_loss: 0.0614 - val_acc: 0.5179\n",
      "Epoch 96/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0612 - acc: 0.5133 - val_loss: 0.0609 - val_acc: 0.5195\n",
      "Epoch 97/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0611 - acc: 0.5136 - val_loss: 0.0632 - val_acc: 0.4813\n",
      "Epoch 98/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0614 - acc: 0.5164 - val_loss: 0.0655 - val_acc: 0.4331\n",
      "Epoch 99/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0610 - acc: 0.5133 - val_loss: 0.0648 - val_acc: 0.4555\n",
      "Epoch 100/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0609 - acc: 0.5172 - val_loss: 0.0609 - val_acc: 0.4938\n",
      "Epoch 101/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0609 - acc: 0.5194 - val_loss: 0.0678 - val_acc: 0.3749\n",
      "Epoch 102/500\n",
      "3608/3608 [==============================] - 1s 330us/step - loss: 0.0609 - acc: 0.5139 - val_loss: 0.0680 - val_acc: 0.3782\n",
      "Epoch 103/500\n",
      "3608/3608 [==============================] - 1s 326us/step - loss: 0.0609 - acc: 0.5116 - val_loss: 0.0686 - val_acc: 0.3566\n",
      "Epoch 104/500\n",
      "3608/3608 [==============================] - 1s 325us/step - loss: 0.0611 - acc: 0.5116 - val_loss: 0.0701 - val_acc: 0.3134\n",
      "Epoch 105/500\n",
      "3608/3608 [==============================] - 1s 330us/step - loss: 0.0616 - acc: 0.5164 - val_loss: 0.0692 - val_acc: 0.3458\n",
      "Epoch 106/500\n",
      "3608/3608 [==============================] - 1s 327us/step - loss: 0.0613 - acc: 0.5111 - val_loss: 0.0687 - val_acc: 0.3500\n",
      "Epoch 107/500\n",
      "3608/3608 [==============================] - 1s 329us/step - loss: 0.0609 - acc: 0.5158 - val_loss: 0.0698 - val_acc: 0.3599\n",
      "Epoch 108/500\n",
      "3608/3608 [==============================] - 1s 327us/step - loss: 0.0610 - acc: 0.5202 - val_loss: 0.0652 - val_acc: 0.4456\n",
      "Epoch 109/500\n",
      "3608/3608 [==============================] - 1s 326us/step - loss: 0.0609 - acc: 0.5211 - val_loss: 0.0607 - val_acc: 0.5204\n",
      "Epoch 110/500\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0611 - acc: 0.5050 - val_loss: 0.0575 - val_acc: 0.5561\n",
      "Epoch 111/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0618 - acc: 0.5058 - val_loss: 0.0697 - val_acc: 0.3466\n",
      "Epoch 112/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0608 - acc: 0.5141 - val_loss: 0.0694 - val_acc: 0.3616\n",
      "Epoch 113/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0620 - acc: 0.4997 - val_loss: 0.0630 - val_acc: 0.5054\n",
      "Epoch 114/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0613 - acc: 0.5125 - val_loss: 0.0614 - val_acc: 0.5187\n",
      "Epoch 115/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0619 - acc: 0.5116 - val_loss: 0.0593 - val_acc: 0.5387\n",
      "Epoch 116/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0616 - acc: 0.5069 - val_loss: 0.0606 - val_acc: 0.5270\n",
      "Epoch 117/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0612 - acc: 0.5144 - val_loss: 0.0690 - val_acc: 0.3658\n",
      "Epoch 118/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0609 - acc: 0.5180 - val_loss: 0.0660 - val_acc: 0.4564\n",
      "Epoch 119/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0609 - acc: 0.5161 - val_loss: 0.0712 - val_acc: 0.2751\n",
      "Epoch 120/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0614 - acc: 0.5111 - val_loss: 0.0721 - val_acc: 0.3375\n",
      "Epoch 121/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0614 - acc: 0.5100 - val_loss: 0.0631 - val_acc: 0.5029\n",
      "Epoch 122/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0612 - acc: 0.5144 - val_loss: 0.0646 - val_acc: 0.4647\n",
      "Epoch 123/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0614 - acc: 0.5058 - val_loss: 0.0684 - val_acc: 0.3558\n",
      "Epoch 124/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0614 - acc: 0.5072 - val_loss: 0.0693 - val_acc: 0.3466\n",
      "Epoch 125/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0610 - acc: 0.5116 - val_loss: 0.0698 - val_acc: 0.3466\n",
      "Epoch 126/500\n",
      "3608/3608 [==============================] - 1s 257us/step - loss: 0.0609 - acc: 0.5158 - val_loss: 0.0661 - val_acc: 0.4372\n",
      "Epoch 127/500\n",
      "3608/3608 [==============================] - 1s 257us/step - loss: 0.0612 - acc: 0.5114 - val_loss: 0.0619 - val_acc: 0.5270\n",
      "Epoch 128/500\n",
      "3608/3608 [==============================] - 1s 256us/step - loss: 0.0609 - acc: 0.5125 - val_loss: 0.0639 - val_acc: 0.5154\n",
      "Epoch 129/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0608 - acc: 0.5169 - val_loss: 0.0687 - val_acc: 0.3558\n",
      "Epoch 130/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0611 - acc: 0.5141 - val_loss: 0.0641 - val_acc: 0.4938\n",
      "Epoch 131/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0610 - acc: 0.5155 - val_loss: 0.0674 - val_acc: 0.3840\n",
      "Epoch 132/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0610 - acc: 0.5139 - val_loss: 0.0650 - val_acc: 0.4979\n",
      "Epoch 133/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0611 - acc: 0.5133 - val_loss: 0.0702 - val_acc: 0.3475\n",
      "Epoch 134/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0608 - acc: 0.5188 - val_loss: 0.0606 - val_acc: 0.5345\n",
      "Epoch 135/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0610 - acc: 0.5136 - val_loss: 0.0681 - val_acc: 0.3333\n",
      "Epoch 136/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0608 - acc: 0.5180 - val_loss: 0.0638 - val_acc: 0.5054\n",
      "Epoch 137/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0606 - acc: 0.5175 - val_loss: 0.0592 - val_acc: 0.5320\n",
      "Epoch 138/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0612 - acc: 0.5125 - val_loss: 0.0701 - val_acc: 0.2851\n",
      "Epoch 139/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0611 - acc: 0.5094 - val_loss: 0.0643 - val_acc: 0.4963\n",
      "Epoch 140/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0614 - acc: 0.5055 - val_loss: 0.0666 - val_acc: 0.3749\n",
      "Epoch 141/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0608 - acc: 0.5152 - val_loss: 0.0658 - val_acc: 0.4497\n",
      "Epoch 142/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0611 - acc: 0.5161 - val_loss: 0.0645 - val_acc: 0.4821\n",
      "Epoch 143/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0613 - acc: 0.5083 - val_loss: 0.0654 - val_acc: 0.4738\n",
      "Epoch 144/500\n",
      "3608/3608 [==============================] - 1s 258us/step - loss: 0.0614 - acc: 0.5080 - val_loss: 0.0708 - val_acc: 0.3450\n",
      "Epoch 145/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0613 - acc: 0.5116 - val_loss: 0.0605 - val_acc: 0.5387\n",
      "Epoch 146/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0605 - acc: 0.5247 - val_loss: 0.0608 - val_acc: 0.5137\n",
      "Epoch 147/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0609 - acc: 0.5197 - val_loss: 0.0644 - val_acc: 0.4347\n",
      "Epoch 148/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0612 - acc: 0.5091 - val_loss: 0.0654 - val_acc: 0.4730\n",
      "Epoch 149/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0609 - acc: 0.5197 - val_loss: 0.0650 - val_acc: 0.4805\n",
      "Epoch 150/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0611 - acc: 0.5097 - val_loss: 0.0694 - val_acc: 0.3084\n",
      "Epoch 151/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0619 - acc: 0.5086 - val_loss: 0.0688 - val_acc: 0.3400\n",
      "Epoch 152/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0620 - acc: 0.5086 - val_loss: 0.0658 - val_acc: 0.4381\n",
      "Epoch 153/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0609 - acc: 0.5147 - val_loss: 0.0674 - val_acc: 0.3741\n",
      "Epoch 154/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0613 - acc: 0.5147 - val_loss: 0.0593 - val_acc: 0.5345\n",
      "Epoch 155/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0613 - acc: 0.4997 - val_loss: 0.0606 - val_acc: 0.5220\n",
      "Epoch 156/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0616 - acc: 0.5111 - val_loss: 0.0626 - val_acc: 0.5062\n",
      "Epoch 157/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0623 - acc: 0.4964 - val_loss: 0.0637 - val_acc: 0.5079\n",
      "Epoch 158/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0611 - acc: 0.5097 - val_loss: 0.0652 - val_acc: 0.4497\n",
      "Epoch 159/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0608 - acc: 0.5114 - val_loss: 0.0697 - val_acc: 0.3458\n",
      "Epoch 160/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0609 - acc: 0.5150 - val_loss: 0.0628 - val_acc: 0.5062\n",
      "Epoch 161/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0610 - acc: 0.5139 - val_loss: 0.0652 - val_acc: 0.4339\n",
      "Epoch 162/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0608 - acc: 0.5180 - val_loss: 0.0667 - val_acc: 0.3915\n",
      "Epoch 163/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0607 - acc: 0.5252 - val_loss: 0.0702 - val_acc: 0.2718\n",
      "Epoch 164/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0610 - acc: 0.5164 - val_loss: 0.0674 - val_acc: 0.4032\n",
      "Epoch 165/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0619 - acc: 0.5033 - val_loss: 0.0681 - val_acc: 0.4115\n",
      "Epoch 166/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0614 - acc: 0.5058 - val_loss: 0.0694 - val_acc: 0.3508\n",
      "Epoch 167/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0606 - acc: 0.5197 - val_loss: 0.0679 - val_acc: 0.3899\n",
      "Epoch 168/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0610 - acc: 0.5094 - val_loss: 0.0664 - val_acc: 0.4173\n",
      "Epoch 169/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0605 - acc: 0.5200 - val_loss: 0.0589 - val_acc: 0.5420\n",
      "Epoch 170/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0618 - acc: 0.5083 - val_loss: 0.0666 - val_acc: 0.4173\n",
      "Epoch 171/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0613 - acc: 0.5152 - val_loss: 0.0594 - val_acc: 0.5387\n",
      "Epoch 172/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0606 - acc: 0.5202 - val_loss: 0.0689 - val_acc: 0.3425\n",
      "Epoch 173/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0615 - acc: 0.5111 - val_loss: 0.0656 - val_acc: 0.5362\n",
      "Epoch 174/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0612 - acc: 0.5042 - val_loss: 0.0683 - val_acc: 0.3583\n",
      "Epoch 175/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0605 - acc: 0.5288 - val_loss: 0.0665 - val_acc: 0.3890\n",
      "Epoch 176/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0615 - acc: 0.5125 - val_loss: 0.0680 - val_acc: 0.3566\n",
      "Epoch 177/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0612 - acc: 0.5103 - val_loss: 0.0683 - val_acc: 0.3616\n",
      "Epoch 178/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0609 - acc: 0.5147 - val_loss: 0.0592 - val_acc: 0.5378\n",
      "Epoch 179/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0614 - acc: 0.5067 - val_loss: 0.0653 - val_acc: 0.4339\n",
      "Epoch 180/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0611 - acc: 0.5111 - val_loss: 0.0668 - val_acc: 0.3940\n",
      "Epoch 181/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0606 - acc: 0.5186 - val_loss: 0.0642 - val_acc: 0.4796\n",
      "Epoch 182/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0607 - acc: 0.5191 - val_loss: 0.0674 - val_acc: 0.3857\n",
      "Epoch 183/500\n",
      "3608/3608 [==============================] - 1s 277us/step - loss: 0.0610 - acc: 0.5094 - val_loss: 0.0577 - val_acc: 0.5544\n",
      "Epoch 184/500\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0611 - acc: 0.5180 - val_loss: 0.0649 - val_acc: 0.4663\n",
      "Epoch 185/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0611 - acc: 0.5194 - val_loss: 0.0657 - val_acc: 0.4223\n",
      "Epoch 186/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0608 - acc: 0.5136 - val_loss: 0.0681 - val_acc: 0.3466\n",
      "Epoch 187/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0607 - acc: 0.5227 - val_loss: 0.0647 - val_acc: 0.4539\n",
      "Epoch 188/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0610 - acc: 0.5147 - val_loss: 0.0652 - val_acc: 0.4555\n",
      "Epoch 189/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0608 - acc: 0.5213 - val_loss: 0.0690 - val_acc: 0.3757\n",
      "Epoch 190/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0610 - acc: 0.5150 - val_loss: 0.0647 - val_acc: 0.4480\n",
      "Epoch 191/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0607 - acc: 0.5166 - val_loss: 0.0682 - val_acc: 0.3608\n",
      "Epoch 192/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0606 - acc: 0.5216 - val_loss: 0.0637 - val_acc: 0.4971\n",
      "Epoch 193/500\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0606 - acc: 0.5244 - val_loss: 0.0629 - val_acc: 0.5254\n",
      "Epoch 194/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0609 - acc: 0.5202 - val_loss: 0.0649 - val_acc: 0.4746\n",
      "Epoch 195/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0606 - acc: 0.5177 - val_loss: 0.0689 - val_acc: 0.3558\n",
      "Epoch 196/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0609 - acc: 0.5200 - val_loss: 0.0634 - val_acc: 0.4954\n",
      "Epoch 197/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0606 - acc: 0.5247 - val_loss: 0.0701 - val_acc: 0.3450\n",
      "Epoch 198/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0613 - acc: 0.5030 - val_loss: 0.0615 - val_acc: 0.5145\n",
      "Epoch 199/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0610 - acc: 0.5180 - val_loss: 0.0690 - val_acc: 0.3541\n",
      "Epoch 200/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0611 - acc: 0.5141 - val_loss: 0.0675 - val_acc: 0.3583\n",
      "Epoch 201/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0609 - acc: 0.5155 - val_loss: 0.0611 - val_acc: 0.5287\n",
      "Epoch 202/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0609 - acc: 0.5161 - val_loss: 0.0719 - val_acc: 0.3367\n",
      "Epoch 203/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0608 - acc: 0.5208 - val_loss: 0.0623 - val_acc: 0.5137\n",
      "Epoch 204/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0609 - acc: 0.5158 - val_loss: 0.0647 - val_acc: 0.4564\n",
      "Epoch 205/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0605 - acc: 0.5211 - val_loss: 0.0627 - val_acc: 0.5104\n",
      "Epoch 206/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0604 - acc: 0.5194 - val_loss: 0.0686 - val_acc: 0.3441\n",
      "Epoch 207/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0610 - acc: 0.5202 - val_loss: 0.0643 - val_acc: 0.4522\n",
      "Epoch 208/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0607 - acc: 0.5222 - val_loss: 0.0678 - val_acc: 0.3682\n",
      "Epoch 209/500\n",
      "3608/3608 [==============================] - 1s 280us/step - loss: 0.0609 - acc: 0.5225 - val_loss: 0.0607 - val_acc: 0.5187\n",
      "Epoch 210/500\n",
      "3608/3608 [==============================] - 1s 329us/step - loss: 0.0608 - acc: 0.5241 - val_loss: 0.0712 - val_acc: 0.2860\n",
      "Epoch 211/500\n",
      "3608/3608 [==============================] - 1s 328us/step - loss: 0.0606 - acc: 0.5244 - val_loss: 0.0704 - val_acc: 0.3666\n",
      "Epoch 212/500\n",
      "3608/3608 [==============================] - 1s 331us/step - loss: 0.0610 - acc: 0.5150 - val_loss: 0.0664 - val_acc: 0.4522\n",
      "Epoch 213/500\n",
      "3608/3608 [==============================] - 1s 331us/step - loss: 0.0608 - acc: 0.5155 - val_loss: 0.0635 - val_acc: 0.5229\n",
      "Epoch 214/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0613 - acc: 0.5133 - val_loss: 0.0635 - val_acc: 0.5087\n",
      "Epoch 215/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0607 - acc: 0.5230 - val_loss: 0.0627 - val_acc: 0.5312\n",
      "Epoch 216/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0609 - acc: 0.5200 - val_loss: 0.0630 - val_acc: 0.5154\n",
      "Epoch 217/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0611 - acc: 0.5166 - val_loss: 0.0656 - val_acc: 0.4131\n",
      "Epoch 218/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0608 - acc: 0.5241 - val_loss: 0.0666 - val_acc: 0.3915\n",
      "Epoch 219/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0609 - acc: 0.5222 - val_loss: 0.0670 - val_acc: 0.3641\n",
      "Epoch 220/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0612 - acc: 0.5180 - val_loss: 0.0640 - val_acc: 0.5046\n",
      "Epoch 221/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0609 - acc: 0.5227 - val_loss: 0.0680 - val_acc: 0.3574\n",
      "Epoch 222/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0610 - acc: 0.5025 - val_loss: 0.0627 - val_acc: 0.5195\n",
      "Epoch 223/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0610 - acc: 0.5197 - val_loss: 0.0652 - val_acc: 0.4672\n",
      "Epoch 224/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0612 - acc: 0.5072 - val_loss: 0.0700 - val_acc: 0.3425\n",
      "Epoch 225/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0615 - acc: 0.5047 - val_loss: 0.0686 - val_acc: 0.3283\n",
      "Epoch 226/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0616 - acc: 0.5033 - val_loss: 0.0689 - val_acc: 0.3308\n",
      "Epoch 227/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0610 - acc: 0.5191 - val_loss: 0.0644 - val_acc: 0.4938\n",
      "Epoch 228/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0612 - acc: 0.5122 - val_loss: 0.0603 - val_acc: 0.5320\n",
      "Epoch 229/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0615 - acc: 0.5164 - val_loss: 0.0660 - val_acc: 0.4364\n",
      "Epoch 230/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0608 - acc: 0.5158 - val_loss: 0.0654 - val_acc: 0.4605\n",
      "Epoch 231/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0605 - acc: 0.5249 - val_loss: 0.0649 - val_acc: 0.4547\n",
      "Epoch 232/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0612 - acc: 0.5083 - val_loss: 0.0713 - val_acc: 0.2801\n",
      "Epoch 233/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0608 - acc: 0.5211 - val_loss: 0.0686 - val_acc: 0.3317\n",
      "Epoch 234/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0611 - acc: 0.5175 - val_loss: 0.0687 - val_acc: 0.3508\n",
      "Epoch 235/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0608 - acc: 0.5164 - val_loss: 0.0682 - val_acc: 0.3774\n",
      "Epoch 236/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0606 - acc: 0.5238 - val_loss: 0.0679 - val_acc: 0.3433\n",
      "Epoch 237/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0607 - acc: 0.5188 - val_loss: 0.0708 - val_acc: 0.2843\n",
      "Epoch 238/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0625 - acc: 0.4911 - val_loss: 0.0639 - val_acc: 0.4904\n",
      "Epoch 239/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0607 - acc: 0.5202 - val_loss: 0.0694 - val_acc: 0.3475\n",
      "Epoch 240/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0637 - acc: 0.4728 - val_loss: 0.0652 - val_acc: 0.4996\n",
      "Epoch 241/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0638 - acc: 0.4748 - val_loss: 0.0610 - val_acc: 0.4855\n",
      "Epoch 242/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0640 - acc: 0.4753 - val_loss: 0.0604 - val_acc: 0.4755\n",
      "Epoch 243/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0633 - acc: 0.4850 - val_loss: 0.0638 - val_acc: 0.5220\n",
      "Epoch 244/500\n",
      "3608/3608 [==============================] - 1s 275us/step - loss: 0.0619 - acc: 0.5139 - val_loss: 0.0664 - val_acc: 0.3732\n",
      "Epoch 245/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0613 - acc: 0.5097 - val_loss: 0.0663 - val_acc: 0.4057\n",
      "Epoch 246/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0612 - acc: 0.5094 - val_loss: 0.0638 - val_acc: 0.4863\n",
      "Epoch 247/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0614 - acc: 0.5053 - val_loss: 0.0690 - val_acc: 0.3358\n",
      "Epoch 248/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0616 - acc: 0.5152 - val_loss: 0.0702 - val_acc: 0.3317\n",
      "Epoch 249/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0617 - acc: 0.5133 - val_loss: 0.0661 - val_acc: 0.4015\n",
      "Epoch 250/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0613 - acc: 0.5211 - val_loss: 0.0661 - val_acc: 0.4389\n",
      "Epoch 251/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0615 - acc: 0.5183 - val_loss: 0.0716 - val_acc: 0.3175\n",
      "Epoch 252/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0615 - acc: 0.5169 - val_loss: 0.0620 - val_acc: 0.5212\n",
      "Epoch 253/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0611 - acc: 0.5202 - val_loss: 0.0673 - val_acc: 0.3549\n",
      "Epoch 254/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0612 - acc: 0.5161 - val_loss: 0.0628 - val_acc: 0.5237\n",
      "Epoch 255/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0609 - acc: 0.5191 - val_loss: 0.0664 - val_acc: 0.4381\n",
      "Epoch 256/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0611 - acc: 0.5191 - val_loss: 0.0694 - val_acc: 0.3234\n",
      "Epoch 257/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0610 - acc: 0.5219 - val_loss: 0.0641 - val_acc: 0.4705\n",
      "Epoch 258/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0613 - acc: 0.5197 - val_loss: 0.0638 - val_acc: 0.4846\n",
      "Epoch 259/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0610 - acc: 0.5139 - val_loss: 0.0627 - val_acc: 0.5212\n",
      "Epoch 260/500\n",
      "3608/3608 [==============================] - 1s 257us/step - loss: 0.0615 - acc: 0.5155 - val_loss: 0.0644 - val_acc: 0.5195\n",
      "Epoch 261/500\n",
      "3608/3608 [==============================] - 1s 257us/step - loss: 0.0610 - acc: 0.5236 - val_loss: 0.0640 - val_acc: 0.4888\n",
      "Epoch 262/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0616 - acc: 0.5064 - val_loss: 0.0615 - val_acc: 0.5303\n",
      "Epoch 263/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0613 - acc: 0.5166 - val_loss: 0.0654 - val_acc: 0.4713\n",
      "Epoch 264/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0612 - acc: 0.5105 - val_loss: 0.0677 - val_acc: 0.2943\n",
      "Epoch 265/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0620 - acc: 0.4981 - val_loss: 0.0632 - val_acc: 0.4855\n",
      "Epoch 266/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0617 - acc: 0.5114 - val_loss: 0.0662 - val_acc: 0.4148\n",
      "Epoch 267/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0611 - acc: 0.5238 - val_loss: 0.0637 - val_acc: 0.4963\n",
      "Epoch 268/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0615 - acc: 0.5125 - val_loss: 0.0587 - val_acc: 0.5520\n",
      "Epoch 269/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0619 - acc: 0.5069 - val_loss: 0.0689 - val_acc: 0.3400\n",
      "Epoch 270/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0612 - acc: 0.5158 - val_loss: 0.0668 - val_acc: 0.3766\n",
      "Epoch 271/500\n",
      "3608/3608 [==============================] - 1s 258us/step - loss: 0.0616 - acc: 0.5175 - val_loss: 0.0636 - val_acc: 0.5104\n",
      "Epoch 272/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0614 - acc: 0.5164 - val_loss: 0.0701 - val_acc: 0.2843\n",
      "Epoch 273/500\n",
      "3608/3608 [==============================] - 1s 255us/step - loss: 0.0613 - acc: 0.5205 - val_loss: 0.0714 - val_acc: 0.3159\n",
      "Epoch 274/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0611 - acc: 0.5225 - val_loss: 0.0655 - val_acc: 0.4505\n",
      "Epoch 275/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0613 - acc: 0.5158 - val_loss: 0.0669 - val_acc: 0.3583\n",
      "Epoch 276/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0615 - acc: 0.5158 - val_loss: 0.0686 - val_acc: 0.3541\n",
      "Epoch 277/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0613 - acc: 0.5136 - val_loss: 0.0667 - val_acc: 0.4306\n",
      "Epoch 278/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0613 - acc: 0.5172 - val_loss: 0.0612 - val_acc: 0.5345\n",
      "Epoch 279/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0615 - acc: 0.5161 - val_loss: 0.0634 - val_acc: 0.5012\n",
      "Epoch 280/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0610 - acc: 0.5302 - val_loss: 0.0626 - val_acc: 0.5278\n",
      "Epoch 281/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0622 - acc: 0.5003 - val_loss: 0.0623 - val_acc: 0.5087\n",
      "Epoch 282/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0616 - acc: 0.5089 - val_loss: 0.0612 - val_acc: 0.5187\n",
      "Epoch 283/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0620 - acc: 0.5030 - val_loss: 0.0704 - val_acc: 0.2835\n",
      "Epoch 284/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0612 - acc: 0.5213 - val_loss: 0.0616 - val_acc: 0.5162\n",
      "Epoch 285/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0610 - acc: 0.5230 - val_loss: 0.0620 - val_acc: 0.5229\n",
      "Epoch 286/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0613 - acc: 0.5164 - val_loss: 0.0682 - val_acc: 0.3641\n",
      "Epoch 287/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0612 - acc: 0.5188 - val_loss: 0.0676 - val_acc: 0.3558\n",
      "Epoch 288/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0610 - acc: 0.5216 - val_loss: 0.0622 - val_acc: 0.5154\n",
      "Epoch 289/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0611 - acc: 0.5177 - val_loss: 0.0710 - val_acc: 0.3325\n",
      "Epoch 290/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0612 - acc: 0.5161 - val_loss: 0.0621 - val_acc: 0.5129\n",
      "Epoch 291/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0610 - acc: 0.5244 - val_loss: 0.0617 - val_acc: 0.5187\n",
      "Epoch 292/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0611 - acc: 0.5100 - val_loss: 0.0662 - val_acc: 0.4239\n",
      "Epoch 293/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0616 - acc: 0.5172 - val_loss: 0.0675 - val_acc: 0.3234\n",
      "Epoch 294/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0619 - acc: 0.4961 - val_loss: 0.0622 - val_acc: 0.5112\n",
      "Epoch 295/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0612 - acc: 0.5067 - val_loss: 0.0644 - val_acc: 0.4971\n",
      "Epoch 296/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0610 - acc: 0.5194 - val_loss: 0.0714 - val_acc: 0.3209\n",
      "Epoch 297/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0616 - acc: 0.5155 - val_loss: 0.0612 - val_acc: 0.5337\n",
      "Epoch 298/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0612 - acc: 0.5205 - val_loss: 0.0687 - val_acc: 0.3500\n",
      "Epoch 299/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0614 - acc: 0.5139 - val_loss: 0.0680 - val_acc: 0.3458\n",
      "Epoch 300/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0620 - acc: 0.5047 - val_loss: 0.0704 - val_acc: 0.3042\n",
      "Epoch 301/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0621 - acc: 0.5050 - val_loss: 0.0601 - val_acc: 0.5362\n",
      "Epoch 302/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0618 - acc: 0.5127 - val_loss: 0.0684 - val_acc: 0.3599\n",
      "Epoch 303/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0627 - acc: 0.5003 - val_loss: 0.0639 - val_acc: 0.4938\n",
      "Epoch 304/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0620 - acc: 0.5139 - val_loss: 0.0625 - val_acc: 0.5229\n",
      "Epoch 305/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0616 - acc: 0.5122 - val_loss: 0.0626 - val_acc: 0.5071\n",
      "Epoch 306/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0611 - acc: 0.5241 - val_loss: 0.0716 - val_acc: 0.2702\n",
      "Epoch 307/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0616 - acc: 0.5136 - val_loss: 0.0695 - val_acc: 0.2893\n",
      "Epoch 308/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0617 - acc: 0.5147 - val_loss: 0.0690 - val_acc: 0.3308\n",
      "Epoch 309/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0616 - acc: 0.5100 - val_loss: 0.0708 - val_acc: 0.3383\n",
      "Epoch 310/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0615 - acc: 0.5105 - val_loss: 0.0638 - val_acc: 0.4755\n",
      "Epoch 311/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0617 - acc: 0.5111 - val_loss: 0.0686 - val_acc: 0.3608\n",
      "Epoch 312/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0616 - acc: 0.5166 - val_loss: 0.0603 - val_acc: 0.5362\n",
      "Epoch 313/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0614 - acc: 0.5136 - val_loss: 0.0686 - val_acc: 0.3283\n",
      "Epoch 314/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0613 - acc: 0.5166 - val_loss: 0.0699 - val_acc: 0.3051\n",
      "Epoch 315/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0620 - acc: 0.5050 - val_loss: 0.0642 - val_acc: 0.4921\n",
      "Epoch 316/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0619 - acc: 0.5019 - val_loss: 0.0636 - val_acc: 0.5212\n",
      "Epoch 317/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0623 - acc: 0.4992 - val_loss: 0.0612 - val_acc: 0.5087\n",
      "Epoch 318/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0620 - acc: 0.5053 - val_loss: 0.0665 - val_acc: 0.3990\n",
      "Epoch 319/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0624 - acc: 0.5011 - val_loss: 0.0605 - val_acc: 0.5278\n",
      "Epoch 320/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0615 - acc: 0.5072 - val_loss: 0.0679 - val_acc: 0.3791\n",
      "Epoch 321/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0625 - acc: 0.4839 - val_loss: 0.0658 - val_acc: 0.4547\n",
      "Epoch 322/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0634 - acc: 0.4795 - val_loss: 0.0666 - val_acc: 0.3333\n",
      "Epoch 323/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0634 - acc: 0.4709 - val_loss: 0.0630 - val_acc: 0.5046\n",
      "Epoch 324/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0622 - acc: 0.5086 - val_loss: 0.0623 - val_acc: 0.4888\n",
      "Epoch 325/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0620 - acc: 0.4964 - val_loss: 0.0613 - val_acc: 0.5145\n",
      "Epoch 326/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0616 - acc: 0.5061 - val_loss: 0.0681 - val_acc: 0.3450\n",
      "Epoch 327/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0626 - acc: 0.4945 - val_loss: 0.0610 - val_acc: 0.5303\n",
      "Epoch 328/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0618 - acc: 0.5075 - val_loss: 0.0631 - val_acc: 0.5254\n",
      "Epoch 329/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0616 - acc: 0.5105 - val_loss: 0.0640 - val_acc: 0.5154\n",
      "Epoch 330/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0622 - acc: 0.5017 - val_loss: 0.0627 - val_acc: 0.4688\n",
      "Epoch 331/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0617 - acc: 0.4994 - val_loss: 0.0608 - val_acc: 0.5154\n",
      "Epoch 332/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0617 - acc: 0.5006 - val_loss: 0.0619 - val_acc: 0.5037\n",
      "Epoch 333/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0614 - acc: 0.5069 - val_loss: 0.0601 - val_acc: 0.5195\n",
      "Epoch 334/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0614 - acc: 0.4997 - val_loss: 0.0611 - val_acc: 0.5154\n",
      "Epoch 335/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0620 - acc: 0.4986 - val_loss: 0.0614 - val_acc: 0.5262\n",
      "Epoch 336/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0622 - acc: 0.4956 - val_loss: 0.0634 - val_acc: 0.5112\n",
      "Epoch 337/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0616 - acc: 0.4983 - val_loss: 0.0618 - val_acc: 0.5187\n",
      "Epoch 338/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0617 - acc: 0.4978 - val_loss: 0.0611 - val_acc: 0.5145\n",
      "Epoch 339/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0631 - acc: 0.4767 - val_loss: 0.0629 - val_acc: 0.4697\n",
      "Epoch 340/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0619 - acc: 0.4967 - val_loss: 0.0628 - val_acc: 0.4846\n",
      "Epoch 341/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0617 - acc: 0.4994 - val_loss: 0.0644 - val_acc: 0.4580\n",
      "Epoch 342/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0616 - acc: 0.5011 - val_loss: 0.0634 - val_acc: 0.4838\n",
      "Epoch 343/500\n",
      "3608/3608 [==============================] - 1s 258us/step - loss: 0.0620 - acc: 0.4983 - val_loss: 0.0608 - val_acc: 0.5154\n",
      "Epoch 344/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0614 - acc: 0.5072 - val_loss: 0.0623 - val_acc: 0.4963\n",
      "Epoch 345/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0614 - acc: 0.5047 - val_loss: 0.0624 - val_acc: 0.5170\n",
      "Epoch 346/500\n",
      "3608/3608 [==============================] - 1s 258us/step - loss: 0.0612 - acc: 0.5100 - val_loss: 0.0615 - val_acc: 0.5062\n",
      "Epoch 347/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0612 - acc: 0.5061 - val_loss: 0.0618 - val_acc: 0.5195\n",
      "Epoch 348/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0609 - acc: 0.5127 - val_loss: 0.0616 - val_acc: 0.5179\n",
      "Epoch 349/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0616 - acc: 0.4997 - val_loss: 0.0606 - val_acc: 0.5262\n",
      "Epoch 350/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0615 - acc: 0.5000 - val_loss: 0.0620 - val_acc: 0.5137\n",
      "Epoch 351/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0616 - acc: 0.4994 - val_loss: 0.0621 - val_acc: 0.5204\n",
      "Epoch 352/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0612 - acc: 0.5083 - val_loss: 0.0638 - val_acc: 0.5046\n",
      "Epoch 353/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0613 - acc: 0.5100 - val_loss: 0.0651 - val_acc: 0.4738\n",
      "Epoch 354/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0618 - acc: 0.5025 - val_loss: 0.0631 - val_acc: 0.4855\n",
      "Epoch 355/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0615 - acc: 0.5064 - val_loss: 0.0662 - val_acc: 0.4073\n",
      "Epoch 356/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0615 - acc: 0.5022 - val_loss: 0.0630 - val_acc: 0.5021\n",
      "Epoch 357/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0618 - acc: 0.4983 - val_loss: 0.0644 - val_acc: 0.4821\n",
      "Epoch 358/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0614 - acc: 0.5017 - val_loss: 0.0605 - val_acc: 0.5295\n",
      "Epoch 359/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0615 - acc: 0.5055 - val_loss: 0.0613 - val_acc: 0.5237\n",
      "Epoch 360/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0613 - acc: 0.4994 - val_loss: 0.0614 - val_acc: 0.5121\n",
      "Epoch 361/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0617 - acc: 0.5028 - val_loss: 0.0597 - val_acc: 0.5229\n",
      "Epoch 362/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0622 - acc: 0.4970 - val_loss: 0.0616 - val_acc: 0.5071\n",
      "Epoch 363/500\n",
      "3608/3608 [==============================] - 1s 279us/step - loss: 0.0617 - acc: 0.5039 - val_loss: 0.0675 - val_acc: 0.3882\n",
      "Epoch 364/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0615 - acc: 0.5097 - val_loss: 0.0642 - val_acc: 0.4913\n",
      "Epoch 365/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0618 - acc: 0.5017 - val_loss: 0.0621 - val_acc: 0.5170\n",
      "Epoch 366/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0613 - acc: 0.5080 - val_loss: 0.0625 - val_acc: 0.5071\n",
      "Epoch 367/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0616 - acc: 0.4989 - val_loss: 0.0605 - val_acc: 0.5179\n",
      "Epoch 368/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0615 - acc: 0.5136 - val_loss: 0.0606 - val_acc: 0.5079\n",
      "Epoch 369/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0617 - acc: 0.4917 - val_loss: 0.0631 - val_acc: 0.4904\n",
      "Epoch 370/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0618 - acc: 0.5017 - val_loss: 0.0615 - val_acc: 0.5096\n",
      "Epoch 371/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0615 - acc: 0.5033 - val_loss: 0.0614 - val_acc: 0.5212\n",
      "Epoch 372/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0614 - acc: 0.5094 - val_loss: 0.0616 - val_acc: 0.5204\n",
      "Epoch 373/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0611 - acc: 0.5064 - val_loss: 0.0631 - val_acc: 0.5079\n",
      "Epoch 374/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0614 - acc: 0.5025 - val_loss: 0.0684 - val_acc: 0.3466\n",
      "Epoch 375/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0624 - acc: 0.4878 - val_loss: 0.0604 - val_acc: 0.5212\n",
      "Epoch 376/500\n",
      "3608/3608 [==============================] - 1s 273us/step - loss: 0.0617 - acc: 0.4981 - val_loss: 0.0659 - val_acc: 0.4106\n",
      "Epoch 377/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0614 - acc: 0.5086 - val_loss: 0.0677 - val_acc: 0.3450\n",
      "Epoch 378/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0613 - acc: 0.5091 - val_loss: 0.0623 - val_acc: 0.5129\n",
      "Epoch 379/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0613 - acc: 0.5086 - val_loss: 0.0606 - val_acc: 0.5270\n",
      "Epoch 380/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0615 - acc: 0.5072 - val_loss: 0.0591 - val_acc: 0.5254\n",
      "Epoch 381/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0617 - acc: 0.5058 - val_loss: 0.0598 - val_acc: 0.5320\n",
      "Epoch 382/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0617 - acc: 0.4978 - val_loss: 0.0664 - val_acc: 0.4090\n",
      "Epoch 383/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0613 - acc: 0.5022 - val_loss: 0.0638 - val_acc: 0.4846\n",
      "Epoch 384/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0613 - acc: 0.5119 - val_loss: 0.0614 - val_acc: 0.5129\n",
      "Epoch 385/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0613 - acc: 0.5067 - val_loss: 0.0629 - val_acc: 0.4963\n",
      "Epoch 386/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0616 - acc: 0.5008 - val_loss: 0.0611 - val_acc: 0.5104\n",
      "Epoch 387/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0625 - acc: 0.4886 - val_loss: 0.0652 - val_acc: 0.4131\n",
      "Epoch 388/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0620 - acc: 0.4956 - val_loss: 0.0595 - val_acc: 0.5212\n",
      "Epoch 389/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0617 - acc: 0.4950 - val_loss: 0.0609 - val_acc: 0.5204\n",
      "Epoch 390/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0614 - acc: 0.5061 - val_loss: 0.0601 - val_acc: 0.5237\n",
      "Epoch 391/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0618 - acc: 0.4981 - val_loss: 0.0606 - val_acc: 0.5287\n",
      "Epoch 392/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0613 - acc: 0.5028 - val_loss: 0.0596 - val_acc: 0.5328\n",
      "Epoch 393/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0618 - acc: 0.4964 - val_loss: 0.0612 - val_acc: 0.5245\n",
      "Epoch 394/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0618 - acc: 0.5064 - val_loss: 0.0614 - val_acc: 0.5270\n",
      "Epoch 395/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0617 - acc: 0.5028 - val_loss: 0.0621 - val_acc: 0.5179\n",
      "Epoch 396/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0613 - acc: 0.5033 - val_loss: 0.0611 - val_acc: 0.5229\n",
      "Epoch 397/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0612 - acc: 0.5061 - val_loss: 0.0610 - val_acc: 0.5254\n",
      "Epoch 398/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0614 - acc: 0.5058 - val_loss: 0.0609 - val_acc: 0.5245\n",
      "Epoch 399/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0611 - acc: 0.5050 - val_loss: 0.0607 - val_acc: 0.5270\n",
      "Epoch 400/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0614 - acc: 0.5097 - val_loss: 0.0614 - val_acc: 0.5204\n",
      "Epoch 401/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0618 - acc: 0.4953 - val_loss: 0.0600 - val_acc: 0.5287\n",
      "Epoch 402/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0614 - acc: 0.5072 - val_loss: 0.0621 - val_acc: 0.5212\n",
      "Epoch 403/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0613 - acc: 0.5089 - val_loss: 0.0610 - val_acc: 0.5170\n",
      "Epoch 404/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0615 - acc: 0.5000 - val_loss: 0.0601 - val_acc: 0.5179\n",
      "Epoch 405/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0627 - acc: 0.4831 - val_loss: 0.0617 - val_acc: 0.5295\n",
      "Epoch 406/500\n",
      "3608/3608 [==============================] - 1s 256us/step - loss: 0.0625 - acc: 0.4917 - val_loss: 0.0628 - val_acc: 0.5137\n",
      "Epoch 407/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0614 - acc: 0.5044 - val_loss: 0.0612 - val_acc: 0.5237\n",
      "Epoch 408/500\n",
      "3608/3608 [==============================] - 1s 258us/step - loss: 0.0611 - acc: 0.5100 - val_loss: 0.0606 - val_acc: 0.5254\n",
      "Epoch 409/500\n",
      "3608/3608 [==============================] - 1s 258us/step - loss: 0.0614 - acc: 0.5067 - val_loss: 0.0619 - val_acc: 0.5112\n",
      "Epoch 410/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0638 - acc: 0.4842 - val_loss: 0.0641 - val_acc: 0.4730\n",
      "Epoch 411/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0635 - acc: 0.4903 - val_loss: 0.0628 - val_acc: 0.4979\n",
      "Epoch 412/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0635 - acc: 0.4839 - val_loss: 0.0632 - val_acc: 0.4963\n",
      "Epoch 413/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0635 - acc: 0.4867 - val_loss: 0.0618 - val_acc: 0.5087\n",
      "Epoch 414/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0636 - acc: 0.4939 - val_loss: 0.0650 - val_acc: 0.4539\n",
      "Epoch 415/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0634 - acc: 0.4889 - val_loss: 0.0691 - val_acc: 0.3209\n",
      "Epoch 416/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0631 - acc: 0.4895 - val_loss: 0.0674 - val_acc: 0.4007\n",
      "Epoch 417/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0636 - acc: 0.4823 - val_loss: 0.0609 - val_acc: 0.5162\n",
      "Epoch 418/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0633 - acc: 0.4895 - val_loss: 0.0675 - val_acc: 0.3350\n",
      "Epoch 419/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0642 - acc: 0.4814 - val_loss: 0.0629 - val_acc: 0.4904\n",
      "Epoch 420/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0637 - acc: 0.4867 - val_loss: 0.0659 - val_acc: 0.4032\n",
      "Epoch 421/500\n",
      "3608/3608 [==============================] - 1s 255us/step - loss: 0.0637 - acc: 0.4795 - val_loss: 0.0673 - val_acc: 0.3990\n",
      "Epoch 422/500\n",
      "3608/3608 [==============================] - 1s 301us/step - loss: 0.0636 - acc: 0.4886 - val_loss: 0.0611 - val_acc: 0.5062\n",
      "Epoch 423/500\n",
      "3608/3608 [==============================] - 1s 326us/step - loss: 0.0635 - acc: 0.4864 - val_loss: 0.0691 - val_acc: 0.3475\n",
      "Epoch 424/500\n",
      "3608/3608 [==============================] - 1s 325us/step - loss: 0.0636 - acc: 0.4856 - val_loss: 0.0620 - val_acc: 0.4996\n",
      "Epoch 425/500\n",
      "3608/3608 [==============================] - 1s 326us/step - loss: 0.0646 - acc: 0.4770 - val_loss: 0.0629 - val_acc: 0.5104\n",
      "Epoch 426/500\n",
      "3608/3608 [==============================] - 1s 327us/step - loss: 0.0636 - acc: 0.4836 - val_loss: 0.0617 - val_acc: 0.4996\n",
      "Epoch 427/500\n",
      "3608/3608 [==============================] - 1s 328us/step - loss: 0.0632 - acc: 0.4911 - val_loss: 0.0664 - val_acc: 0.4248\n",
      "Epoch 428/500\n",
      "3608/3608 [==============================] - 1s 325us/step - loss: 0.0637 - acc: 0.4859 - val_loss: 0.0620 - val_acc: 0.4988\n",
      "Epoch 429/500\n",
      "3608/3608 [==============================] - 1s 327us/step - loss: 0.0636 - acc: 0.4856 - val_loss: 0.0602 - val_acc: 0.5112\n",
      "Epoch 430/500\n",
      "3608/3608 [==============================] - 1s 332us/step - loss: 0.0631 - acc: 0.4922 - val_loss: 0.0670 - val_acc: 0.4381\n",
      "Epoch 431/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0631 - acc: 0.4933 - val_loss: 0.0663 - val_acc: 0.4439\n",
      "Epoch 432/500\n",
      "3608/3608 [==============================] - 1s 256us/step - loss: 0.0630 - acc: 0.4903 - val_loss: 0.0616 - val_acc: 0.5012\n",
      "Epoch 433/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0630 - acc: 0.4911 - val_loss: 0.0639 - val_acc: 0.5004\n",
      "Epoch 434/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0632 - acc: 0.4897 - val_loss: 0.0653 - val_acc: 0.4688\n",
      "Epoch 435/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0636 - acc: 0.4889 - val_loss: 0.0619 - val_acc: 0.5004\n",
      "Epoch 436/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0637 - acc: 0.4795 - val_loss: 0.0673 - val_acc: 0.3948\n",
      "Epoch 437/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0641 - acc: 0.4812 - val_loss: 0.0611 - val_acc: 0.5104\n",
      "Epoch 438/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0634 - acc: 0.4828 - val_loss: 0.0646 - val_acc: 0.4730\n",
      "Epoch 439/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0634 - acc: 0.4895 - val_loss: 0.0608 - val_acc: 0.5137\n",
      "Epoch 440/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0632 - acc: 0.4897 - val_loss: 0.0621 - val_acc: 0.5037\n",
      "Epoch 441/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0635 - acc: 0.4881 - val_loss: 0.0607 - val_acc: 0.5096\n",
      "Epoch 442/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0638 - acc: 0.4820 - val_loss: 0.0618 - val_acc: 0.5004\n",
      "Epoch 443/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0646 - acc: 0.4720 - val_loss: 0.0639 - val_acc: 0.4580\n",
      "Epoch 444/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0645 - acc: 0.4756 - val_loss: 0.0609 - val_acc: 0.5004\n",
      "Epoch 445/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0650 - acc: 0.4681 - val_loss: 0.0620 - val_acc: 0.5062\n",
      "Epoch 446/500\n",
      "3608/3608 [==============================] - 1s 259us/step - loss: 0.0640 - acc: 0.4792 - val_loss: 0.0623 - val_acc: 0.4622\n",
      "Epoch 447/500\n",
      "3608/3608 [==============================] - 1s 260us/step - loss: 0.0642 - acc: 0.4751 - val_loss: 0.0647 - val_acc: 0.4223\n",
      "Epoch 448/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0633 - acc: 0.4875 - val_loss: 0.0623 - val_acc: 0.4738\n",
      "Epoch 449/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0634 - acc: 0.4823 - val_loss: 0.0652 - val_acc: 0.4406\n",
      "Epoch 450/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0630 - acc: 0.4878 - val_loss: 0.0616 - val_acc: 0.5037\n",
      "Epoch 451/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0635 - acc: 0.4903 - val_loss: 0.0632 - val_acc: 0.4722\n",
      "Epoch 452/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0633 - acc: 0.4850 - val_loss: 0.0605 - val_acc: 0.5079\n",
      "Epoch 453/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0627 - acc: 0.4928 - val_loss: 0.0616 - val_acc: 0.4946\n",
      "Epoch 454/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0634 - acc: 0.4842 - val_loss: 0.0616 - val_acc: 0.5012\n",
      "Epoch 455/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0629 - acc: 0.4892 - val_loss: 0.0619 - val_acc: 0.4979\n",
      "Epoch 456/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0629 - acc: 0.4945 - val_loss: 0.0619 - val_acc: 0.4971\n",
      "Epoch 457/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0629 - acc: 0.4886 - val_loss: 0.0615 - val_acc: 0.5087\n",
      "Epoch 458/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0637 - acc: 0.4875 - val_loss: 0.0616 - val_acc: 0.4988\n",
      "Epoch 459/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0636 - acc: 0.4906 - val_loss: 0.0608 - val_acc: 0.4971\n",
      "Epoch 460/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0631 - acc: 0.4925 - val_loss: 0.0612 - val_acc: 0.5071\n",
      "Epoch 461/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0635 - acc: 0.4889 - val_loss: 0.0631 - val_acc: 0.4738\n",
      "Epoch 462/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0637 - acc: 0.4881 - val_loss: 0.0603 - val_acc: 0.5079\n",
      "Epoch 463/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0638 - acc: 0.4864 - val_loss: 0.0638 - val_acc: 0.4913\n",
      "Epoch 464/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0634 - acc: 0.4895 - val_loss: 0.0669 - val_acc: 0.4148\n",
      "Epoch 465/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0664 - acc: 0.4548 - val_loss: 0.0627 - val_acc: 0.4913\n",
      "Epoch 466/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0647 - acc: 0.4709 - val_loss: 0.0662 - val_acc: 0.4464\n",
      "Epoch 467/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0637 - acc: 0.4909 - val_loss: 0.0670 - val_acc: 0.3732\n",
      "Epoch 468/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0634 - acc: 0.4812 - val_loss: 0.0632 - val_acc: 0.4788\n",
      "Epoch 469/500\n",
      "3608/3608 [==============================] - 1s 262us/step - loss: 0.0633 - acc: 0.4784 - val_loss: 0.0648 - val_acc: 0.4256\n",
      "Epoch 470/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0636 - acc: 0.4828 - val_loss: 0.0620 - val_acc: 0.5029\n",
      "Epoch 471/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0638 - acc: 0.4798 - val_loss: 0.0669 - val_acc: 0.4131\n",
      "Epoch 472/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0636 - acc: 0.4920 - val_loss: 0.0632 - val_acc: 0.4979\n",
      "Epoch 473/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0629 - acc: 0.4939 - val_loss: 0.0683 - val_acc: 0.3383\n",
      "Epoch 474/500\n",
      "3608/3608 [==============================] - 1s 258us/step - loss: 0.0636 - acc: 0.4884 - val_loss: 0.0681 - val_acc: 0.3633\n",
      "Epoch 475/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0638 - acc: 0.4834 - val_loss: 0.0658 - val_acc: 0.4156\n",
      "Epoch 476/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0633 - acc: 0.4895 - val_loss: 0.0642 - val_acc: 0.4771\n",
      "Epoch 477/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0635 - acc: 0.4798 - val_loss: 0.0651 - val_acc: 0.4705\n",
      "Epoch 478/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0637 - acc: 0.4823 - val_loss: 0.0623 - val_acc: 0.4888\n",
      "Epoch 479/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0634 - acc: 0.4878 - val_loss: 0.0671 - val_acc: 0.4190\n",
      "Epoch 480/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0637 - acc: 0.4809 - val_loss: 0.0656 - val_acc: 0.4780\n",
      "Epoch 481/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0640 - acc: 0.4903 - val_loss: 0.0647 - val_acc: 0.4638\n",
      "Epoch 482/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0638 - acc: 0.4817 - val_loss: 0.0654 - val_acc: 0.4480\n",
      "Epoch 483/500\n",
      "3608/3608 [==============================] - 1s 271us/step - loss: 0.0639 - acc: 0.4853 - val_loss: 0.0653 - val_acc: 0.4572\n",
      "Epoch 484/500\n",
      "3608/3608 [==============================] - 1s 272us/step - loss: 0.0634 - acc: 0.4836 - val_loss: 0.0660 - val_acc: 0.4480\n",
      "Epoch 485/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0633 - acc: 0.4881 - val_loss: 0.0609 - val_acc: 0.5071\n",
      "Epoch 486/500\n",
      "3608/3608 [==============================] - 1s 274us/step - loss: 0.0634 - acc: 0.4817 - val_loss: 0.0687 - val_acc: 0.3317\n",
      "Epoch 487/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0634 - acc: 0.4800 - val_loss: 0.0697 - val_acc: 0.3009\n",
      "Epoch 488/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0632 - acc: 0.4920 - val_loss: 0.0636 - val_acc: 0.4888\n",
      "Epoch 489/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0640 - acc: 0.4778 - val_loss: 0.0691 - val_acc: 0.3308\n",
      "Epoch 490/500\n",
      "3608/3608 [==============================] - 1s 266us/step - loss: 0.0637 - acc: 0.4864 - val_loss: 0.0676 - val_acc: 0.4023\n",
      "Epoch 491/500\n",
      "3608/3608 [==============================] - 1s 265us/step - loss: 0.0634 - acc: 0.4834 - val_loss: 0.0675 - val_acc: 0.4023\n",
      "Epoch 492/500\n",
      "3608/3608 [==============================] - 1s 268us/step - loss: 0.0633 - acc: 0.4823 - val_loss: 0.0650 - val_acc: 0.4672\n",
      "Epoch 493/500\n",
      "3608/3608 [==============================] - 1s 270us/step - loss: 0.0640 - acc: 0.4773 - val_loss: 0.0639 - val_acc: 0.4347\n",
      "Epoch 494/500\n",
      "3608/3608 [==============================] - 1s 269us/step - loss: 0.0635 - acc: 0.4856 - val_loss: 0.0664 - val_acc: 0.4115\n",
      "Epoch 495/500\n",
      "3608/3608 [==============================] - 1s 264us/step - loss: 0.0635 - acc: 0.4917 - val_loss: 0.0614 - val_acc: 0.4996\n",
      "Epoch 496/500\n",
      "3608/3608 [==============================] - 1s 267us/step - loss: 0.0636 - acc: 0.4875 - val_loss: 0.0619 - val_acc: 0.5012\n",
      "Epoch 497/500\n",
      "3608/3608 [==============================] - 1s 263us/step - loss: 0.0637 - acc: 0.4817 - val_loss: 0.0620 - val_acc: 0.5004\n",
      "Epoch 498/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0634 - acc: 0.4859 - val_loss: 0.0670 - val_acc: 0.4015\n",
      "Epoch 499/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0637 - acc: 0.4850 - val_loss: 0.0656 - val_acc: 0.4431\n",
      "Epoch 500/500\n",
      "3608/3608 [==============================] - 1s 261us/step - loss: 0.0634 - acc: 0.4884 - val_loss: 0.0684 - val_acc: 0.3458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77fd950588>"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(9,)))\n",
    "nn_model.add(Dense(units=9, activation='relu'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(units=18, activation='relu'))\n",
    "nn_model.add(Dense(units=36, activation='relu'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(units=18, activation='relu'))\n",
    "nn_model.add(Dense(units=12, activation='relu'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "nn_model.fit(x=new_train_x, y=train_y, epochs=500, validation_data=(new_valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBZFs1j2XQt6"
   },
   "source": [
    "### 6) cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyfaelcQMuiw"
   },
   "outputs": [],
   "source": [
    "result_cv = pd.DataFrame(columns=['detail', 'accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6olYRVWoeOEO"
   },
   "outputs": [],
   "source": [
    "dev_x = pd.concat((train_x, valid_x), axis=0)\n",
    "dev_y = pd.concat((train_y, valid_y), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ri7xIF2MqxNu"
   },
   "source": [
    "10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNXw9t0_eXcm"
   },
   "outputs": [],
   "source": [
    "x_fold = np.array_split(dev_x, 10)\n",
    "y_fold = np.array_split(dev_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WjJEKm7EIVkH"
   },
   "outputs": [],
   "source": [
    "dev_y = dev_y.reset_index()\n",
    "dev_y = dev_y.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34476
    },
    "colab_type": "code",
    "id": "rzHBt7BTXSpT",
    "outputId": "8f8a63e6-2c1f-404a-dff5-0a10f9c17bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_47 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 12)                204       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 2,386\n",
      "Trainable params: 2,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4329 samples, validate on 482 samples\n",
      "Epoch 1/100\n",
      "4329/4329 [==============================] - 2s 365us/step - loss: 0.0761 - acc: 0.3812 - val_loss: 0.0666 - val_acc: 0.4855\n",
      "Epoch 2/100\n",
      "4329/4329 [==============================] - 1s 211us/step - loss: 0.0675 - acc: 0.4479 - val_loss: 0.0639 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0654 - acc: 0.4544 - val_loss: 0.0618 - val_acc: 0.4855\n",
      "Epoch 4/100\n",
      "4329/4329 [==============================] - 1s 211us/step - loss: 0.0638 - acc: 0.4736 - val_loss: 0.0617 - val_acc: 0.5041\n",
      "Epoch 5/100\n",
      "4329/4329 [==============================] - 1s 215us/step - loss: 0.0627 - acc: 0.4918 - val_loss: 0.0608 - val_acc: 0.5041\n",
      "Epoch 6/100\n",
      "4329/4329 [==============================] - 1s 214us/step - loss: 0.0621 - acc: 0.4911 - val_loss: 0.0596 - val_acc: 0.4917\n",
      "Epoch 7/100\n",
      "4329/4329 [==============================] - 1s 206us/step - loss: 0.0610 - acc: 0.5117 - val_loss: 0.0593 - val_acc: 0.5311\n",
      "Epoch 8/100\n",
      "4329/4329 [==============================] - 1s 208us/step - loss: 0.0591 - acc: 0.5357 - val_loss: 0.0591 - val_acc: 0.5187\n",
      "Epoch 9/100\n",
      "4329/4329 [==============================] - 1s 208us/step - loss: 0.0591 - acc: 0.5341 - val_loss: 0.0582 - val_acc: 0.5207\n",
      "Epoch 10/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0588 - acc: 0.5410 - val_loss: 0.0579 - val_acc: 0.5290\n",
      "Epoch 11/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0582 - acc: 0.5440 - val_loss: 0.0576 - val_acc: 0.5332\n",
      "Epoch 12/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0586 - acc: 0.5352 - val_loss: 0.0587 - val_acc: 0.5145\n",
      "Epoch 13/100\n",
      "4329/4329 [==============================] - 1s 211us/step - loss: 0.0580 - acc: 0.5456 - val_loss: 0.0581 - val_acc: 0.5228\n",
      "Epoch 14/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0579 - acc: 0.5468 - val_loss: 0.0601 - val_acc: 0.5083\n",
      "Epoch 15/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0580 - acc: 0.5433 - val_loss: 0.0593 - val_acc: 0.5207\n",
      "Epoch 16/100\n",
      "4329/4329 [==============================] - 1s 212us/step - loss: 0.0582 - acc: 0.5484 - val_loss: 0.0576 - val_acc: 0.5353\n",
      "Epoch 17/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0571 - acc: 0.5500 - val_loss: 0.0576 - val_acc: 0.5270\n",
      "Epoch 18/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0569 - acc: 0.5581 - val_loss: 0.0581 - val_acc: 0.5249\n",
      "Epoch 19/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0569 - acc: 0.5579 - val_loss: 0.0577 - val_acc: 0.5166\n",
      "Epoch 20/100\n",
      "4329/4329 [==============================] - 1s 211us/step - loss: 0.0569 - acc: 0.5553 - val_loss: 0.0587 - val_acc: 0.5332\n",
      "Epoch 21/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0569 - acc: 0.5579 - val_loss: 0.0575 - val_acc: 0.5498\n",
      "Epoch 22/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0567 - acc: 0.5562 - val_loss: 0.0575 - val_acc: 0.5187\n",
      "Epoch 23/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0568 - acc: 0.5616 - val_loss: 0.0579 - val_acc: 0.5249\n",
      "Epoch 24/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0567 - acc: 0.5574 - val_loss: 0.0580 - val_acc: 0.5353\n",
      "Epoch 25/100\n",
      "4329/4329 [==============================] - 1s 238us/step - loss: 0.0569 - acc: 0.5618 - val_loss: 0.0577 - val_acc: 0.5270\n",
      "Epoch 26/100\n",
      "4329/4329 [==============================] - 1s 270us/step - loss: 0.0564 - acc: 0.5627 - val_loss: 0.0581 - val_acc: 0.5290\n",
      "Epoch 27/100\n",
      "4329/4329 [==============================] - 1s 264us/step - loss: 0.0565 - acc: 0.5636 - val_loss: 0.0584 - val_acc: 0.5456\n",
      "Epoch 28/100\n",
      "4329/4329 [==============================] - 1s 265us/step - loss: 0.0565 - acc: 0.5590 - val_loss: 0.0590 - val_acc: 0.5373\n",
      "Epoch 29/100\n",
      "4329/4329 [==============================] - 1s 264us/step - loss: 0.0566 - acc: 0.5646 - val_loss: 0.0575 - val_acc: 0.5311\n",
      "Epoch 30/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0567 - acc: 0.5639 - val_loss: 0.0575 - val_acc: 0.5498\n",
      "Epoch 31/100\n",
      "4329/4329 [==============================] - 1s 206us/step - loss: 0.0560 - acc: 0.5694 - val_loss: 0.0580 - val_acc: 0.5436\n",
      "Epoch 32/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0562 - acc: 0.5676 - val_loss: 0.0576 - val_acc: 0.5539\n",
      "Epoch 33/100\n",
      "4329/4329 [==============================] - 1s 205us/step - loss: 0.0563 - acc: 0.5636 - val_loss: 0.0575 - val_acc: 0.5436\n",
      "Epoch 34/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0561 - acc: 0.5720 - val_loss: 0.0574 - val_acc: 0.5373\n",
      "Epoch 35/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0564 - acc: 0.5623 - val_loss: 0.0575 - val_acc: 0.5560\n",
      "Epoch 36/100\n",
      "4329/4329 [==============================] - 1s 204us/step - loss: 0.0565 - acc: 0.5713 - val_loss: 0.0574 - val_acc: 0.5539\n",
      "Epoch 37/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0563 - acc: 0.5708 - val_loss: 0.0573 - val_acc: 0.5477\n",
      "Epoch 38/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0564 - acc: 0.5641 - val_loss: 0.0572 - val_acc: 0.5498\n",
      "Epoch 39/100\n",
      "4329/4329 [==============================] - 1s 204us/step - loss: 0.0563 - acc: 0.5660 - val_loss: 0.0597 - val_acc: 0.5187\n",
      "Epoch 40/100\n",
      "4329/4329 [==============================] - 1s 199us/step - loss: 0.0562 - acc: 0.5676 - val_loss: 0.0572 - val_acc: 0.5498\n",
      "Epoch 41/100\n",
      "4329/4329 [==============================] - 1s 202us/step - loss: 0.0559 - acc: 0.5733 - val_loss: 0.0578 - val_acc: 0.5477\n",
      "Epoch 42/100\n",
      "4329/4329 [==============================] - 1s 206us/step - loss: 0.0561 - acc: 0.5724 - val_loss: 0.0573 - val_acc: 0.5436\n",
      "Epoch 43/100\n",
      "4329/4329 [==============================] - 1s 205us/step - loss: 0.0559 - acc: 0.5754 - val_loss: 0.0574 - val_acc: 0.5581\n",
      "Epoch 44/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0561 - acc: 0.5699 - val_loss: 0.0569 - val_acc: 0.5664\n",
      "Epoch 45/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0558 - acc: 0.5757 - val_loss: 0.0577 - val_acc: 0.5436\n",
      "Epoch 46/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0560 - acc: 0.5745 - val_loss: 0.0578 - val_acc: 0.5311\n",
      "Epoch 47/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0560 - acc: 0.5752 - val_loss: 0.0571 - val_acc: 0.5436\n",
      "Epoch 48/100\n",
      "4329/4329 [==============================] - 1s 208us/step - loss: 0.0563 - acc: 0.5706 - val_loss: 0.0572 - val_acc: 0.5602\n",
      "Epoch 49/100\n",
      "4329/4329 [==============================] - 1s 216us/step - loss: 0.0558 - acc: 0.5754 - val_loss: 0.0576 - val_acc: 0.5664\n",
      "Epoch 50/100\n",
      "4329/4329 [==============================] - 1s 208us/step - loss: 0.0556 - acc: 0.5833 - val_loss: 0.0575 - val_acc: 0.5394\n",
      "Epoch 51/100\n",
      "4329/4329 [==============================] - 1s 211us/step - loss: 0.0556 - acc: 0.5775 - val_loss: 0.0579 - val_acc: 0.5477\n",
      "Epoch 52/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0561 - acc: 0.5747 - val_loss: 0.0570 - val_acc: 0.5539\n",
      "Epoch 53/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0559 - acc: 0.5745 - val_loss: 0.0591 - val_acc: 0.5477\n",
      "Epoch 54/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0565 - acc: 0.5731 - val_loss: 0.0572 - val_acc: 0.5498\n",
      "Epoch 55/100\n",
      "4329/4329 [==============================] - 1s 213us/step - loss: 0.0560 - acc: 0.5750 - val_loss: 0.0572 - val_acc: 0.5560\n",
      "Epoch 56/100\n",
      "4329/4329 [==============================] - 1s 211us/step - loss: 0.0557 - acc: 0.5773 - val_loss: 0.0577 - val_acc: 0.5622\n",
      "Epoch 57/100\n",
      "4329/4329 [==============================] - 1s 211us/step - loss: 0.0557 - acc: 0.5759 - val_loss: 0.0575 - val_acc: 0.5581\n",
      "Epoch 58/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0557 - acc: 0.5743 - val_loss: 0.0580 - val_acc: 0.5373\n",
      "Epoch 59/100\n",
      "4329/4329 [==============================] - 1s 211us/step - loss: 0.0555 - acc: 0.5817 - val_loss: 0.0579 - val_acc: 0.5560\n",
      "Epoch 60/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0556 - acc: 0.5770 - val_loss: 0.0570 - val_acc: 0.5477\n",
      "Epoch 61/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0558 - acc: 0.5740 - val_loss: 0.0581 - val_acc: 0.5373\n",
      "Epoch 62/100\n",
      "4329/4329 [==============================] - 1s 212us/step - loss: 0.0555 - acc: 0.5789 - val_loss: 0.0586 - val_acc: 0.5270\n",
      "Epoch 63/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0557 - acc: 0.5800 - val_loss: 0.0575 - val_acc: 0.5498\n",
      "Epoch 64/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0559 - acc: 0.5754 - val_loss: 0.0577 - val_acc: 0.5539\n",
      "Epoch 65/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0555 - acc: 0.5849 - val_loss: 0.0572 - val_acc: 0.5622\n",
      "Epoch 66/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0557 - acc: 0.5754 - val_loss: 0.0573 - val_acc: 0.5394\n",
      "Epoch 67/100\n",
      "4329/4329 [==============================] - 1s 208us/step - loss: 0.0552 - acc: 0.5867 - val_loss: 0.0589 - val_acc: 0.5207\n",
      "Epoch 68/100\n",
      "4329/4329 [==============================] - 1s 205us/step - loss: 0.0560 - acc: 0.5724 - val_loss: 0.0624 - val_acc: 0.4751\n",
      "Epoch 69/100\n",
      "4329/4329 [==============================] - 1s 214us/step - loss: 0.0562 - acc: 0.5745 - val_loss: 0.0584 - val_acc: 0.5373\n",
      "Epoch 70/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0552 - acc: 0.5842 - val_loss: 0.0572 - val_acc: 0.5456\n",
      "Epoch 71/100\n",
      "4329/4329 [==============================] - 1s 211us/step - loss: 0.0554 - acc: 0.5840 - val_loss: 0.0572 - val_acc: 0.5560\n",
      "Epoch 72/100\n",
      "4329/4329 [==============================] - 1s 208us/step - loss: 0.0551 - acc: 0.5863 - val_loss: 0.0575 - val_acc: 0.5560\n",
      "Epoch 73/100\n",
      "4329/4329 [==============================] - 1s 206us/step - loss: 0.0551 - acc: 0.5844 - val_loss: 0.0580 - val_acc: 0.5519\n",
      "Epoch 74/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0552 - acc: 0.5807 - val_loss: 0.0582 - val_acc: 0.5415\n",
      "Epoch 75/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0555 - acc: 0.5828 - val_loss: 0.0570 - val_acc: 0.5560\n",
      "Epoch 76/100\n",
      "4329/4329 [==============================] - 1s 208us/step - loss: 0.0552 - acc: 0.5847 - val_loss: 0.0573 - val_acc: 0.5581\n",
      "Epoch 77/100\n",
      "4329/4329 [==============================] - 1s 206us/step - loss: 0.0552 - acc: 0.5830 - val_loss: 0.0577 - val_acc: 0.5498\n",
      "Epoch 78/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0555 - acc: 0.5814 - val_loss: 0.0585 - val_acc: 0.5436\n",
      "Epoch 79/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0553 - acc: 0.5833 - val_loss: 0.0576 - val_acc: 0.5519\n",
      "Epoch 80/100\n",
      "4329/4329 [==============================] - 1s 206us/step - loss: 0.0555 - acc: 0.5800 - val_loss: 0.0578 - val_acc: 0.5456\n",
      "Epoch 81/100\n",
      "4329/4329 [==============================] - 1s 204us/step - loss: 0.0551 - acc: 0.5860 - val_loss: 0.0580 - val_acc: 0.5498\n",
      "Epoch 82/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0552 - acc: 0.5851 - val_loss: 0.0582 - val_acc: 0.5602\n",
      "Epoch 83/100\n",
      "4329/4329 [==============================] - 1s 208us/step - loss: 0.0551 - acc: 0.5865 - val_loss: 0.0579 - val_acc: 0.5436\n",
      "Epoch 84/100\n",
      "4329/4329 [==============================] - 1s 210us/step - loss: 0.0550 - acc: 0.5828 - val_loss: 0.0581 - val_acc: 0.5539\n",
      "Epoch 85/100\n",
      "4329/4329 [==============================] - 1s 208us/step - loss: 0.0552 - acc: 0.5824 - val_loss: 0.0576 - val_acc: 0.5581\n",
      "Epoch 86/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0551 - acc: 0.5828 - val_loss: 0.0571 - val_acc: 0.5581\n",
      "Epoch 87/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0550 - acc: 0.5842 - val_loss: 0.0582 - val_acc: 0.5373\n",
      "Epoch 88/100\n",
      "4329/4329 [==============================] - 1s 206us/step - loss: 0.0550 - acc: 0.5891 - val_loss: 0.0575 - val_acc: 0.5581\n",
      "Epoch 89/100\n",
      "4329/4329 [==============================] - 1s 201us/step - loss: 0.0548 - acc: 0.5914 - val_loss: 0.0578 - val_acc: 0.5581\n",
      "Epoch 90/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0548 - acc: 0.5879 - val_loss: 0.0577 - val_acc: 0.5498\n",
      "Epoch 91/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0550 - acc: 0.5918 - val_loss: 0.0583 - val_acc: 0.5436\n",
      "Epoch 92/100\n",
      "4329/4329 [==============================] - 1s 205us/step - loss: 0.0548 - acc: 0.5888 - val_loss: 0.0573 - val_acc: 0.5685\n",
      "Epoch 93/100\n",
      "4329/4329 [==============================] - 1s 208us/step - loss: 0.0550 - acc: 0.5867 - val_loss: 0.0580 - val_acc: 0.5664\n",
      "Epoch 94/100\n",
      "4329/4329 [==============================] - 1s 206us/step - loss: 0.0547 - acc: 0.5948 - val_loss: 0.0579 - val_acc: 0.5622\n",
      "Epoch 95/100\n",
      "4329/4329 [==============================] - 1s 212us/step - loss: 0.0548 - acc: 0.5891 - val_loss: 0.0589 - val_acc: 0.5498\n",
      "Epoch 96/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0550 - acc: 0.5854 - val_loss: 0.0574 - val_acc: 0.5332\n",
      "Epoch 97/100\n",
      "4329/4329 [==============================] - 1s 207us/step - loss: 0.0556 - acc: 0.5824 - val_loss: 0.0591 - val_acc: 0.5498\n",
      "Epoch 98/100\n",
      "4329/4329 [==============================] - 1s 208us/step - loss: 0.0554 - acc: 0.5814 - val_loss: 0.0575 - val_acc: 0.5622\n",
      "Epoch 99/100\n",
      "4329/4329 [==============================] - 1s 211us/step - loss: 0.0550 - acc: 0.5891 - val_loss: 0.0577 - val_acc: 0.5498\n",
      "Epoch 100/100\n",
      "4329/4329 [==============================] - 1s 209us/step - loss: 0.0551 - acc: 0.5870 - val_loss: 0.0570 - val_acc: 0.5581\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0551 - acc: 0.5861 - val_loss: 0.0542 - val_acc: 0.5904\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0552 - acc: 0.5845 - val_loss: 0.0546 - val_acc: 0.5946\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0551 - acc: 0.5891 - val_loss: 0.0556 - val_acc: 0.5717\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0549 - acc: 0.5875 - val_loss: 0.0540 - val_acc: 0.6029\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0549 - acc: 0.5885 - val_loss: 0.0545 - val_acc: 0.5988\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0552 - acc: 0.5848 - val_loss: 0.0553 - val_acc: 0.5884\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0547 - acc: 0.5878 - val_loss: 0.0548 - val_acc: 0.5904\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0547 - acc: 0.5938 - val_loss: 0.0547 - val_acc: 0.5863\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0547 - acc: 0.5866 - val_loss: 0.0548 - val_acc: 0.5925\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0546 - acc: 0.5898 - val_loss: 0.0557 - val_acc: 0.5821\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0548 - acc: 0.5866 - val_loss: 0.0563 - val_acc: 0.5696\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0547 - acc: 0.5868 - val_loss: 0.0552 - val_acc: 0.5821\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0550 - acc: 0.5818 - val_loss: 0.0555 - val_acc: 0.5821\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 203us/step - loss: 0.0548 - acc: 0.5905 - val_loss: 0.0558 - val_acc: 0.5884\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0548 - acc: 0.5919 - val_loss: 0.0550 - val_acc: 0.5863\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0546 - acc: 0.5889 - val_loss: 0.0564 - val_acc: 0.5717\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0547 - acc: 0.5910 - val_loss: 0.0554 - val_acc: 0.5842\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0551 - acc: 0.5845 - val_loss: 0.0556 - val_acc: 0.5759\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0545 - acc: 0.5928 - val_loss: 0.0575 - val_acc: 0.5738\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0548 - acc: 0.5875 - val_loss: 0.0559 - val_acc: 0.5696\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0546 - acc: 0.5889 - val_loss: 0.0559 - val_acc: 0.5780\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0545 - acc: 0.5931 - val_loss: 0.0559 - val_acc: 0.5759\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0546 - acc: 0.5924 - val_loss: 0.0564 - val_acc: 0.5738\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0545 - acc: 0.5917 - val_loss: 0.0575 - val_acc: 0.5468\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0546 - acc: 0.5896 - val_loss: 0.0559 - val_acc: 0.5738\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0545 - acc: 0.5915 - val_loss: 0.0559 - val_acc: 0.5780\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0544 - acc: 0.5954 - val_loss: 0.0561 - val_acc: 0.5800\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0543 - acc: 0.5958 - val_loss: 0.0571 - val_acc: 0.5655\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0545 - acc: 0.5938 - val_loss: 0.0581 - val_acc: 0.5676\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0545 - acc: 0.5915 - val_loss: 0.0565 - val_acc: 0.5676\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0545 - acc: 0.5887 - val_loss: 0.0567 - val_acc: 0.5676\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0547 - acc: 0.5887 - val_loss: 0.0566 - val_acc: 0.5759\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0548 - acc: 0.5889 - val_loss: 0.0566 - val_acc: 0.5613\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0544 - acc: 0.5903 - val_loss: 0.0564 - val_acc: 0.5780\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0543 - acc: 0.5961 - val_loss: 0.0562 - val_acc: 0.5821\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0542 - acc: 0.6000 - val_loss: 0.0561 - val_acc: 0.5696\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0544 - acc: 0.5926 - val_loss: 0.0566 - val_acc: 0.5738\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0542 - acc: 0.5958 - val_loss: 0.0562 - val_acc: 0.5821\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0543 - acc: 0.5919 - val_loss: 0.0559 - val_acc: 0.5759\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0541 - acc: 0.5970 - val_loss: 0.0568 - val_acc: 0.5696\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0541 - acc: 0.5954 - val_loss: 0.0562 - val_acc: 0.5821\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0551 - acc: 0.5891 - val_loss: 0.0567 - val_acc: 0.5717\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0543 - acc: 0.5917 - val_loss: 0.0567 - val_acc: 0.5655\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0542 - acc: 0.5940 - val_loss: 0.0560 - val_acc: 0.5780\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0542 - acc: 0.5954 - val_loss: 0.0569 - val_acc: 0.5759\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0544 - acc: 0.5919 - val_loss: 0.0571 - val_acc: 0.5655\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0542 - acc: 0.5949 - val_loss: 0.0560 - val_acc: 0.5759\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0540 - acc: 0.5984 - val_loss: 0.0578 - val_acc: 0.5613\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0541 - acc: 0.5961 - val_loss: 0.0580 - val_acc: 0.5593\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0539 - acc: 0.5986 - val_loss: 0.0567 - val_acc: 0.5780\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0544 - acc: 0.5940 - val_loss: 0.0602 - val_acc: 0.5405\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0548 - acc: 0.5921 - val_loss: 0.0566 - val_acc: 0.5800\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0539 - acc: 0.5988 - val_loss: 0.0565 - val_acc: 0.5780\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0540 - acc: 0.5949 - val_loss: 0.0572 - val_acc: 0.5655\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0542 - acc: 0.5979 - val_loss: 0.0570 - val_acc: 0.5696\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0541 - acc: 0.5952 - val_loss: 0.0571 - val_acc: 0.5759\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0539 - acc: 0.5949 - val_loss: 0.0566 - val_acc: 0.5780\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0540 - acc: 0.5963 - val_loss: 0.0576 - val_acc: 0.5696\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0538 - acc: 0.6000 - val_loss: 0.0569 - val_acc: 0.5738\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0540 - acc: 0.5972 - val_loss: 0.0576 - val_acc: 0.5717\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0541 - acc: 0.5942 - val_loss: 0.0566 - val_acc: 0.5759\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0539 - acc: 0.5952 - val_loss: 0.0562 - val_acc: 0.5842\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0540 - acc: 0.5977 - val_loss: 0.0569 - val_acc: 0.5800\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0537 - acc: 0.6014 - val_loss: 0.0572 - val_acc: 0.5593\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0536 - acc: 0.6032 - val_loss: 0.0568 - val_acc: 0.5800\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0536 - acc: 0.6039 - val_loss: 0.0573 - val_acc: 0.5696\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0541 - acc: 0.5958 - val_loss: 0.0572 - val_acc: 0.5780\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0538 - acc: 0.6009 - val_loss: 0.0566 - val_acc: 0.5780\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0535 - acc: 0.6048 - val_loss: 0.0574 - val_acc: 0.5655\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0536 - acc: 0.5970 - val_loss: 0.0572 - val_acc: 0.5530\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0534 - acc: 0.6028 - val_loss: 0.0575 - val_acc: 0.5572\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0543 - acc: 0.5938 - val_loss: 0.0580 - val_acc: 0.5696\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0541 - acc: 0.5970 - val_loss: 0.0571 - val_acc: 0.5655\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0535 - acc: 0.6016 - val_loss: 0.0576 - val_acc: 0.5551\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0537 - acc: 0.5949 - val_loss: 0.0586 - val_acc: 0.5593\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0535 - acc: 0.6035 - val_loss: 0.0572 - val_acc: 0.5717\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0536 - acc: 0.5993 - val_loss: 0.0578 - val_acc: 0.5676\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0538 - acc: 0.6025 - val_loss: 0.0591 - val_acc: 0.5593\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0535 - acc: 0.6007 - val_loss: 0.0566 - val_acc: 0.5717\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0534 - acc: 0.6025 - val_loss: 0.0574 - val_acc: 0.5696\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0536 - acc: 0.5988 - val_loss: 0.0578 - val_acc: 0.5593\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0533 - acc: 0.6023 - val_loss: 0.0576 - val_acc: 0.5780\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0534 - acc: 0.6044 - val_loss: 0.0581 - val_acc: 0.5613\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0533 - acc: 0.5991 - val_loss: 0.0592 - val_acc: 0.5655\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0536 - acc: 0.6018 - val_loss: 0.0580 - val_acc: 0.5613\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0535 - acc: 0.6044 - val_loss: 0.0580 - val_acc: 0.5676\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0537 - acc: 0.5975 - val_loss: 0.0580 - val_acc: 0.5655\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0533 - acc: 0.6037 - val_loss: 0.0583 - val_acc: 0.5676\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0534 - acc: 0.6030 - val_loss: 0.0584 - val_acc: 0.5593\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0536 - acc: 0.6012 - val_loss: 0.0581 - val_acc: 0.5696\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0535 - acc: 0.6009 - val_loss: 0.0577 - val_acc: 0.5696\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0537 - acc: 0.5972 - val_loss: 0.0582 - val_acc: 0.5655\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0532 - acc: 0.6083 - val_loss: 0.0583 - val_acc: 0.5738\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0534 - acc: 0.6025 - val_loss: 0.0573 - val_acc: 0.5717\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0532 - acc: 0.6072 - val_loss: 0.0578 - val_acc: 0.5655\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0533 - acc: 0.6039 - val_loss: 0.0582 - val_acc: 0.5593\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0533 - acc: 0.6044 - val_loss: 0.0579 - val_acc: 0.5759\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0533 - acc: 0.6060 - val_loss: 0.0589 - val_acc: 0.5655\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0529 - acc: 0.6083 - val_loss: 0.0583 - val_acc: 0.5572\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0529 - acc: 0.6104 - val_loss: 0.0588 - val_acc: 0.5593\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0538 - acc: 0.6016 - val_loss: 0.0524 - val_acc: 0.6112\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0543 - acc: 0.5986 - val_loss: 0.0529 - val_acc: 0.6008\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0545 - acc: 0.5935 - val_loss: 0.0530 - val_acc: 0.5988\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0537 - acc: 0.6016 - val_loss: 0.0539 - val_acc: 0.5904\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0536 - acc: 0.6021 - val_loss: 0.0530 - val_acc: 0.6071\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0535 - acc: 0.6044 - val_loss: 0.0540 - val_acc: 0.5904\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0531 - acc: 0.6072 - val_loss: 0.0531 - val_acc: 0.6008\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0535 - acc: 0.6014 - val_loss: 0.0534 - val_acc: 0.5967\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0533 - acc: 0.6039 - val_loss: 0.0543 - val_acc: 0.5946\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0536 - acc: 0.6042 - val_loss: 0.0525 - val_acc: 0.6008\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0536 - acc: 0.6058 - val_loss: 0.0545 - val_acc: 0.5884\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0535 - acc: 0.6046 - val_loss: 0.0538 - val_acc: 0.5925\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0533 - acc: 0.6067 - val_loss: 0.0544 - val_acc: 0.5800\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0533 - acc: 0.6074 - val_loss: 0.0553 - val_acc: 0.5821\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0533 - acc: 0.6053 - val_loss: 0.0540 - val_acc: 0.5904\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0533 - acc: 0.6048 - val_loss: 0.0538 - val_acc: 0.6008\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 205us/step - loss: 0.0531 - acc: 0.6085 - val_loss: 0.0539 - val_acc: 0.6008\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0533 - acc: 0.6030 - val_loss: 0.0549 - val_acc: 0.5904\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0530 - acc: 0.6104 - val_loss: 0.0546 - val_acc: 0.5842\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0531 - acc: 0.6088 - val_loss: 0.0550 - val_acc: 0.5863\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0530 - acc: 0.6092 - val_loss: 0.0543 - val_acc: 0.5821\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0529 - acc: 0.6062 - val_loss: 0.0545 - val_acc: 0.5863\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0527 - acc: 0.6150 - val_loss: 0.0546 - val_acc: 0.5800\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0527 - acc: 0.6125 - val_loss: 0.0559 - val_acc: 0.5738\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0533 - acc: 0.6081 - val_loss: 0.0545 - val_acc: 0.5842\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0531 - acc: 0.6111 - val_loss: 0.0545 - val_acc: 0.5863\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0530 - acc: 0.6044 - val_loss: 0.0567 - val_acc: 0.5821\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0531 - acc: 0.6065 - val_loss: 0.0549 - val_acc: 0.5863\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0525 - acc: 0.6164 - val_loss: 0.0551 - val_acc: 0.5821\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0531 - acc: 0.6067 - val_loss: 0.0554 - val_acc: 0.5821\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0529 - acc: 0.6122 - val_loss: 0.0544 - val_acc: 0.5884\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0531 - acc: 0.6081 - val_loss: 0.0538 - val_acc: 0.5842\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0525 - acc: 0.6109 - val_loss: 0.0558 - val_acc: 0.5800\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0525 - acc: 0.6134 - val_loss: 0.0551 - val_acc: 0.5780\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0527 - acc: 0.6118 - val_loss: 0.0546 - val_acc: 0.5800\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0526 - acc: 0.6157 - val_loss: 0.0554 - val_acc: 0.5842\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0526 - acc: 0.6139 - val_loss: 0.0555 - val_acc: 0.5738\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0527 - acc: 0.6109 - val_loss: 0.0542 - val_acc: 0.5842\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0527 - acc: 0.6109 - val_loss: 0.0556 - val_acc: 0.5925\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0522 - acc: 0.6155 - val_loss: 0.0561 - val_acc: 0.5780\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0527 - acc: 0.6111 - val_loss: 0.0571 - val_acc: 0.5738\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0525 - acc: 0.6166 - val_loss: 0.0554 - val_acc: 0.5842\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0524 - acc: 0.6164 - val_loss: 0.0550 - val_acc: 0.5717\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0524 - acc: 0.6092 - val_loss: 0.0569 - val_acc: 0.5759\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0525 - acc: 0.6120 - val_loss: 0.0557 - val_acc: 0.5738\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0526 - acc: 0.6127 - val_loss: 0.0557 - val_acc: 0.5800\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0527 - acc: 0.6125 - val_loss: 0.0550 - val_acc: 0.5800\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0527 - acc: 0.6111 - val_loss: 0.0561 - val_acc: 0.5780\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0522 - acc: 0.6208 - val_loss: 0.0554 - val_acc: 0.5863\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0524 - acc: 0.6164 - val_loss: 0.0556 - val_acc: 0.5759\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0521 - acc: 0.6196 - val_loss: 0.0568 - val_acc: 0.5634\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0524 - acc: 0.6127 - val_loss: 0.0557 - val_acc: 0.5800\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0523 - acc: 0.6182 - val_loss: 0.0554 - val_acc: 0.5821\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 263us/step - loss: 0.0523 - acc: 0.6136 - val_loss: 0.0571 - val_acc: 0.5780\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 262us/step - loss: 0.0525 - acc: 0.6134 - val_loss: 0.0552 - val_acc: 0.5925\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 264us/step - loss: 0.0519 - acc: 0.6192 - val_loss: 0.0557 - val_acc: 0.5759\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 264us/step - loss: 0.0520 - acc: 0.6155 - val_loss: 0.0565 - val_acc: 0.5759\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 268us/step - loss: 0.0521 - acc: 0.6173 - val_loss: 0.0556 - val_acc: 0.5800\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 265us/step - loss: 0.0519 - acc: 0.6182 - val_loss: 0.0566 - val_acc: 0.5759\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 262us/step - loss: 0.0522 - acc: 0.6164 - val_loss: 0.0554 - val_acc: 0.5863\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 262us/step - loss: 0.0520 - acc: 0.6180 - val_loss: 0.0573 - val_acc: 0.5800\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0519 - acc: 0.6215 - val_loss: 0.0565 - val_acc: 0.5759\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0521 - acc: 0.6201 - val_loss: 0.0553 - val_acc: 0.5842\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0519 - acc: 0.6199 - val_loss: 0.0553 - val_acc: 0.5821\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0518 - acc: 0.6134 - val_loss: 0.0560 - val_acc: 0.5738\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0520 - acc: 0.6180 - val_loss: 0.0573 - val_acc: 0.5842\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0518 - acc: 0.6189 - val_loss: 0.0559 - val_acc: 0.5759\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0521 - acc: 0.6173 - val_loss: 0.0553 - val_acc: 0.5821\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 205us/step - loss: 0.0522 - acc: 0.6180 - val_loss: 0.0553 - val_acc: 0.5800\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 205us/step - loss: 0.0519 - acc: 0.6210 - val_loss: 0.0566 - val_acc: 0.5842\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0518 - acc: 0.6206 - val_loss: 0.0558 - val_acc: 0.5780\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0521 - acc: 0.6141 - val_loss: 0.0564 - val_acc: 0.5821\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 204us/step - loss: 0.0522 - acc: 0.6176 - val_loss: 0.0561 - val_acc: 0.5780\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0519 - acc: 0.6169 - val_loss: 0.0568 - val_acc: 0.5759\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 205us/step - loss: 0.0521 - acc: 0.6173 - val_loss: 0.0565 - val_acc: 0.5842\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0518 - acc: 0.6208 - val_loss: 0.0563 - val_acc: 0.5759\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0516 - acc: 0.6162 - val_loss: 0.0561 - val_acc: 0.5821\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 203us/step - loss: 0.0515 - acc: 0.6259 - val_loss: 0.0562 - val_acc: 0.5800\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0518 - acc: 0.6212 - val_loss: 0.0566 - val_acc: 0.5821\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0519 - acc: 0.6148 - val_loss: 0.0557 - val_acc: 0.5884\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 204us/step - loss: 0.0518 - acc: 0.6187 - val_loss: 0.0566 - val_acc: 0.5738\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0516 - acc: 0.6261 - val_loss: 0.0574 - val_acc: 0.5780\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0522 - acc: 0.6145 - val_loss: 0.0570 - val_acc: 0.5696\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0523 - acc: 0.6178 - val_loss: 0.0559 - val_acc: 0.5717\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0518 - acc: 0.6215 - val_loss: 0.0563 - val_acc: 0.5738\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0519 - acc: 0.6187 - val_loss: 0.0567 - val_acc: 0.5655\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0517 - acc: 0.6199 - val_loss: 0.0570 - val_acc: 0.5780\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0516 - acc: 0.6236 - val_loss: 0.0563 - val_acc: 0.5863\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0518 - acc: 0.6229 - val_loss: 0.0567 - val_acc: 0.5821\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0517 - acc: 0.6196 - val_loss: 0.0562 - val_acc: 0.5821\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0516 - acc: 0.6180 - val_loss: 0.0560 - val_acc: 0.5780\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0516 - acc: 0.6203 - val_loss: 0.0556 - val_acc: 0.5780\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0515 - acc: 0.6270 - val_loss: 0.0562 - val_acc: 0.5676\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0515 - acc: 0.6219 - val_loss: 0.0555 - val_acc: 0.5821\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0514 - acc: 0.6238 - val_loss: 0.0580 - val_acc: 0.5655\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0515 - acc: 0.6208 - val_loss: 0.0563 - val_acc: 0.5696\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0514 - acc: 0.6249 - val_loss: 0.0560 - val_acc: 0.5800\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0514 - acc: 0.6252 - val_loss: 0.0565 - val_acc: 0.5696\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0513 - acc: 0.6233 - val_loss: 0.0553 - val_acc: 0.5800\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0510 - acc: 0.6291 - val_loss: 0.0562 - val_acc: 0.5717\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0520 - acc: 0.6169 - val_loss: 0.0516 - val_acc: 0.6154\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0522 - acc: 0.6141 - val_loss: 0.0516 - val_acc: 0.6195\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0520 - acc: 0.6189 - val_loss: 0.0530 - val_acc: 0.6175\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 204us/step - loss: 0.0518 - acc: 0.6206 - val_loss: 0.0521 - val_acc: 0.6154\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0524 - acc: 0.6176 - val_loss: 0.0523 - val_acc: 0.6050\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0518 - acc: 0.6206 - val_loss: 0.0525 - val_acc: 0.6154\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0514 - acc: 0.6236 - val_loss: 0.0534 - val_acc: 0.6071\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0515 - acc: 0.6219 - val_loss: 0.0528 - val_acc: 0.6175\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0518 - acc: 0.6201 - val_loss: 0.0537 - val_acc: 0.5988\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0517 - acc: 0.6210 - val_loss: 0.0528 - val_acc: 0.6154\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0515 - acc: 0.6242 - val_loss: 0.0540 - val_acc: 0.6091\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0520 - acc: 0.6171 - val_loss: 0.0539 - val_acc: 0.6050\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0514 - acc: 0.6201 - val_loss: 0.0548 - val_acc: 0.5884\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0515 - acc: 0.6242 - val_loss: 0.0550 - val_acc: 0.6008\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0515 - acc: 0.6249 - val_loss: 0.0535 - val_acc: 0.6008\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0520 - acc: 0.6201 - val_loss: 0.0554 - val_acc: 0.5800\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0513 - acc: 0.6231 - val_loss: 0.0539 - val_acc: 0.5988\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0513 - acc: 0.6240 - val_loss: 0.0541 - val_acc: 0.5904\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0512 - acc: 0.6229 - val_loss: 0.0537 - val_acc: 0.6154\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0511 - acc: 0.6249 - val_loss: 0.0545 - val_acc: 0.5904\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0512 - acc: 0.6233 - val_loss: 0.0539 - val_acc: 0.5884\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0511 - acc: 0.6240 - val_loss: 0.0538 - val_acc: 0.5967\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0507 - acc: 0.6293 - val_loss: 0.0574 - val_acc: 0.5530\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0516 - acc: 0.6185 - val_loss: 0.0544 - val_acc: 0.5988\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0512 - acc: 0.6242 - val_loss: 0.0547 - val_acc: 0.5800\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0513 - acc: 0.6219 - val_loss: 0.0547 - val_acc: 0.5821\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0515 - acc: 0.6203 - val_loss: 0.0548 - val_acc: 0.5884\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0511 - acc: 0.6266 - val_loss: 0.0548 - val_acc: 0.5967\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0512 - acc: 0.6263 - val_loss: 0.0542 - val_acc: 0.5988\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0509 - acc: 0.6303 - val_loss: 0.0550 - val_acc: 0.5884\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0513 - acc: 0.6245 - val_loss: 0.0542 - val_acc: 0.5988\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0511 - acc: 0.6259 - val_loss: 0.0545 - val_acc: 0.5925\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0509 - acc: 0.6282 - val_loss: 0.0548 - val_acc: 0.5946\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0511 - acc: 0.6238 - val_loss: 0.0544 - val_acc: 0.6008\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0509 - acc: 0.6296 - val_loss: 0.0549 - val_acc: 0.6008\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0510 - acc: 0.6275 - val_loss: 0.0546 - val_acc: 0.5904\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0510 - acc: 0.6289 - val_loss: 0.0558 - val_acc: 0.5884\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0510 - acc: 0.6252 - val_loss: 0.0557 - val_acc: 0.5863\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0512 - acc: 0.6254 - val_loss: 0.0553 - val_acc: 0.5780\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0511 - acc: 0.6275 - val_loss: 0.0546 - val_acc: 0.5967\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0510 - acc: 0.6277 - val_loss: 0.0553 - val_acc: 0.5842\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0509 - acc: 0.6279 - val_loss: 0.0557 - val_acc: 0.5800\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0510 - acc: 0.6284 - val_loss: 0.0549 - val_acc: 0.5863\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0508 - acc: 0.6286 - val_loss: 0.0551 - val_acc: 0.5884\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0506 - acc: 0.6305 - val_loss: 0.0551 - val_acc: 0.5967\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0505 - acc: 0.6298 - val_loss: 0.0552 - val_acc: 0.5821\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0508 - acc: 0.6279 - val_loss: 0.0548 - val_acc: 0.5759\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0507 - acc: 0.6263 - val_loss: 0.0545 - val_acc: 0.6008\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0512 - acc: 0.6242 - val_loss: 0.0540 - val_acc: 0.5925\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0509 - acc: 0.6252 - val_loss: 0.0546 - val_acc: 0.5967\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0506 - acc: 0.6309 - val_loss: 0.0560 - val_acc: 0.5842\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0505 - acc: 0.6314 - val_loss: 0.0550 - val_acc: 0.5863\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0505 - acc: 0.6323 - val_loss: 0.0556 - val_acc: 0.5863\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0507 - acc: 0.6282 - val_loss: 0.0555 - val_acc: 0.5821\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0505 - acc: 0.6328 - val_loss: 0.0546 - val_acc: 0.5904\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 245us/step - loss: 0.0508 - acc: 0.6284 - val_loss: 0.0562 - val_acc: 0.5800\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 267us/step - loss: 0.0506 - acc: 0.6293 - val_loss: 0.0549 - val_acc: 0.5842\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 264us/step - loss: 0.0505 - acc: 0.6298 - val_loss: 0.0552 - val_acc: 0.5904\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 269us/step - loss: 0.0508 - acc: 0.6305 - val_loss: 0.0561 - val_acc: 0.5759\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 263us/step - loss: 0.0508 - acc: 0.6284 - val_loss: 0.0563 - val_acc: 0.5884\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0510 - acc: 0.6277 - val_loss: 0.0549 - val_acc: 0.5884\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0509 - acc: 0.6300 - val_loss: 0.0561 - val_acc: 0.5738\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0509 - acc: 0.6252 - val_loss: 0.0550 - val_acc: 0.5904\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0505 - acc: 0.6328 - val_loss: 0.0549 - val_acc: 0.5821\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0504 - acc: 0.6344 - val_loss: 0.0558 - val_acc: 0.5738\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0504 - acc: 0.6321 - val_loss: 0.0566 - val_acc: 0.5738\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0507 - acc: 0.6289 - val_loss: 0.0554 - val_acc: 0.5842\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0506 - acc: 0.6335 - val_loss: 0.0556 - val_acc: 0.5800\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0507 - acc: 0.6291 - val_loss: 0.0563 - val_acc: 0.5676\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0506 - acc: 0.6293 - val_loss: 0.0546 - val_acc: 0.5904\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0502 - acc: 0.6323 - val_loss: 0.0551 - val_acc: 0.5780\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0499 - acc: 0.6353 - val_loss: 0.0558 - val_acc: 0.5863\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0505 - acc: 0.6326 - val_loss: 0.0558 - val_acc: 0.5904\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0504 - acc: 0.6372 - val_loss: 0.0568 - val_acc: 0.5676\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0510 - acc: 0.6328 - val_loss: 0.0564 - val_acc: 0.5717\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0509 - acc: 0.6282 - val_loss: 0.0562 - val_acc: 0.5759\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0505 - acc: 0.6298 - val_loss: 0.0555 - val_acc: 0.5738\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0504 - acc: 0.6330 - val_loss: 0.0551 - val_acc: 0.5821\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0500 - acc: 0.6351 - val_loss: 0.0552 - val_acc: 0.5863\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0503 - acc: 0.6363 - val_loss: 0.0564 - val_acc: 0.5738\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 223us/step - loss: 0.0505 - acc: 0.6309 - val_loss: 0.0564 - val_acc: 0.5863\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0504 - acc: 0.6323 - val_loss: 0.0552 - val_acc: 0.5925\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0502 - acc: 0.6330 - val_loss: 0.0558 - val_acc: 0.5863\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0500 - acc: 0.6358 - val_loss: 0.0554 - val_acc: 0.5821\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0504 - acc: 0.6335 - val_loss: 0.0564 - val_acc: 0.5780\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0499 - acc: 0.6358 - val_loss: 0.0565 - val_acc: 0.5800\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0500 - acc: 0.6339 - val_loss: 0.0564 - val_acc: 0.5759\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0502 - acc: 0.6344 - val_loss: 0.0556 - val_acc: 0.5780\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0502 - acc: 0.6365 - val_loss: 0.0562 - val_acc: 0.5696\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0502 - acc: 0.6360 - val_loss: 0.0561 - val_acc: 0.5696\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0500 - acc: 0.6344 - val_loss: 0.0552 - val_acc: 0.5884\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0500 - acc: 0.6370 - val_loss: 0.0570 - val_acc: 0.5634\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0500 - acc: 0.6346 - val_loss: 0.0571 - val_acc: 0.5655\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0503 - acc: 0.6346 - val_loss: 0.0554 - val_acc: 0.5780\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0502 - acc: 0.6374 - val_loss: 0.0560 - val_acc: 0.5821\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0503 - acc: 0.6337 - val_loss: 0.0550 - val_acc: 0.5821\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0502 - acc: 0.6400 - val_loss: 0.0554 - val_acc: 0.5925\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0498 - acc: 0.6388 - val_loss: 0.0558 - val_acc: 0.5759\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0498 - acc: 0.6395 - val_loss: 0.0554 - val_acc: 0.5863\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0502 - acc: 0.6356 - val_loss: 0.0558 - val_acc: 0.5842\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0507 - acc: 0.6298 - val_loss: 0.0508 - val_acc: 0.6320\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0510 - acc: 0.6270 - val_loss: 0.0511 - val_acc: 0.6299\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0509 - acc: 0.6314 - val_loss: 0.0507 - val_acc: 0.6362\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0504 - acc: 0.6298 - val_loss: 0.0529 - val_acc: 0.6195\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0510 - acc: 0.6277 - val_loss: 0.0518 - val_acc: 0.6320\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0503 - acc: 0.6296 - val_loss: 0.0518 - val_acc: 0.6299\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0509 - acc: 0.6286 - val_loss: 0.0513 - val_acc: 0.6424\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0506 - acc: 0.6312 - val_loss: 0.0512 - val_acc: 0.6362\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0503 - acc: 0.6326 - val_loss: 0.0516 - val_acc: 0.6279\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0502 - acc: 0.6307 - val_loss: 0.0525 - val_acc: 0.6175\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0503 - acc: 0.6309 - val_loss: 0.0525 - val_acc: 0.6175\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0502 - acc: 0.6335 - val_loss: 0.0528 - val_acc: 0.6154\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0502 - acc: 0.6351 - val_loss: 0.0526 - val_acc: 0.6071\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0502 - acc: 0.6370 - val_loss: 0.0527 - val_acc: 0.6216\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0498 - acc: 0.6356 - val_loss: 0.0525 - val_acc: 0.6237\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0499 - acc: 0.6404 - val_loss: 0.0539 - val_acc: 0.6154\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0501 - acc: 0.6349 - val_loss: 0.0544 - val_acc: 0.6029\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0503 - acc: 0.6314 - val_loss: 0.0528 - val_acc: 0.6133\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0506 - acc: 0.6293 - val_loss: 0.0531 - val_acc: 0.6133\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0499 - acc: 0.6365 - val_loss: 0.0531 - val_acc: 0.6091\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0497 - acc: 0.6404 - val_loss: 0.0527 - val_acc: 0.6175\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0503 - acc: 0.6339 - val_loss: 0.0529 - val_acc: 0.6133\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0497 - acc: 0.6390 - val_loss: 0.0529 - val_acc: 0.6216\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0502 - acc: 0.6330 - val_loss: 0.0538 - val_acc: 0.6133\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0510 - acc: 0.6268 - val_loss: 0.0539 - val_acc: 0.6071\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0502 - acc: 0.6328 - val_loss: 0.0530 - val_acc: 0.6071\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0500 - acc: 0.6344 - val_loss: 0.0539 - val_acc: 0.6071\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0499 - acc: 0.6395 - val_loss: 0.0532 - val_acc: 0.6237\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0499 - acc: 0.6372 - val_loss: 0.0534 - val_acc: 0.6008\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0502 - acc: 0.6353 - val_loss: 0.0540 - val_acc: 0.6008\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0499 - acc: 0.6381 - val_loss: 0.0551 - val_acc: 0.5863\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0504 - acc: 0.6379 - val_loss: 0.0532 - val_acc: 0.6091\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0506 - acc: 0.6328 - val_loss: 0.0545 - val_acc: 0.6029\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0498 - acc: 0.6390 - val_loss: 0.0542 - val_acc: 0.6071\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0498 - acc: 0.6374 - val_loss: 0.0539 - val_acc: 0.6050\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0496 - acc: 0.6448 - val_loss: 0.0539 - val_acc: 0.6133\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0498 - acc: 0.6400 - val_loss: 0.0543 - val_acc: 0.6029\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0495 - acc: 0.6404 - val_loss: 0.0553 - val_acc: 0.5946\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0499 - acc: 0.6351 - val_loss: 0.0536 - val_acc: 0.6091\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0501 - acc: 0.6349 - val_loss: 0.0542 - val_acc: 0.6029\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0496 - acc: 0.6397 - val_loss: 0.0555 - val_acc: 0.6008\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0502 - acc: 0.6358 - val_loss: 0.0545 - val_acc: 0.6071\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0500 - acc: 0.6351 - val_loss: 0.0544 - val_acc: 0.6008\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0495 - acc: 0.6413 - val_loss: 0.0541 - val_acc: 0.6071\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0495 - acc: 0.6432 - val_loss: 0.0548 - val_acc: 0.6091\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0496 - acc: 0.6413 - val_loss: 0.0533 - val_acc: 0.6050\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0502 - acc: 0.6309 - val_loss: 0.0535 - val_acc: 0.6112\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0496 - acc: 0.6418 - val_loss: 0.0543 - val_acc: 0.6091\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0496 - acc: 0.6416 - val_loss: 0.0544 - val_acc: 0.5988\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0497 - acc: 0.6393 - val_loss: 0.0553 - val_acc: 0.5800\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0499 - acc: 0.6397 - val_loss: 0.0542 - val_acc: 0.6008\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0497 - acc: 0.6397 - val_loss: 0.0546 - val_acc: 0.6050\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0498 - acc: 0.6413 - val_loss: 0.0548 - val_acc: 0.6029\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0494 - acc: 0.6416 - val_loss: 0.0545 - val_acc: 0.6050\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0494 - acc: 0.6434 - val_loss: 0.0549 - val_acc: 0.6133\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0495 - acc: 0.6409 - val_loss: 0.0539 - val_acc: 0.6071\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0496 - acc: 0.6363 - val_loss: 0.0554 - val_acc: 0.5988\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0495 - acc: 0.6430 - val_loss: 0.0555 - val_acc: 0.5863\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0493 - acc: 0.6434 - val_loss: 0.0544 - val_acc: 0.6050\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0496 - acc: 0.6383 - val_loss: 0.0555 - val_acc: 0.5800\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0496 - acc: 0.6390 - val_loss: 0.0553 - val_acc: 0.6091\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0497 - acc: 0.6386 - val_loss: 0.0551 - val_acc: 0.6050\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0492 - acc: 0.6413 - val_loss: 0.0548 - val_acc: 0.5884\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0493 - acc: 0.6388 - val_loss: 0.0540 - val_acc: 0.6008\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0492 - acc: 0.6432 - val_loss: 0.0559 - val_acc: 0.5884\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0493 - acc: 0.6436 - val_loss: 0.0556 - val_acc: 0.5988\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0493 - acc: 0.6430 - val_loss: 0.0542 - val_acc: 0.6050\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0497 - acc: 0.6404 - val_loss: 0.0546 - val_acc: 0.6050\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0495 - acc: 0.6418 - val_loss: 0.0554 - val_acc: 0.6008\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0496 - acc: 0.6370 - val_loss: 0.0545 - val_acc: 0.5988\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0492 - acc: 0.6436 - val_loss: 0.0560 - val_acc: 0.5800\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0496 - acc: 0.6406 - val_loss: 0.0561 - val_acc: 0.5904\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0495 - acc: 0.6353 - val_loss: 0.0558 - val_acc: 0.5946\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0492 - acc: 0.6460 - val_loss: 0.0550 - val_acc: 0.6008\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0492 - acc: 0.6434 - val_loss: 0.0558 - val_acc: 0.5946\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0495 - acc: 0.6404 - val_loss: 0.0562 - val_acc: 0.5925\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0495 - acc: 0.6400 - val_loss: 0.0564 - val_acc: 0.5759\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0496 - acc: 0.6395 - val_loss: 0.0566 - val_acc: 0.5780\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0498 - acc: 0.6425 - val_loss: 0.0548 - val_acc: 0.6050\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0492 - acc: 0.6430 - val_loss: 0.0553 - val_acc: 0.5863\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0493 - acc: 0.6425 - val_loss: 0.0559 - val_acc: 0.5925\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0494 - acc: 0.6439 - val_loss: 0.0551 - val_acc: 0.5863\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0494 - acc: 0.6376 - val_loss: 0.0558 - val_acc: 0.5967\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0494 - acc: 0.6418 - val_loss: 0.0566 - val_acc: 0.5780\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0491 - acc: 0.6432 - val_loss: 0.0566 - val_acc: 0.5759\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0493 - acc: 0.6423 - val_loss: 0.0552 - val_acc: 0.5884\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0495 - acc: 0.6400 - val_loss: 0.0575 - val_acc: 0.5800\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0494 - acc: 0.6402 - val_loss: 0.0574 - val_acc: 0.5780\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0492 - acc: 0.6462 - val_loss: 0.0568 - val_acc: 0.5884\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0499 - acc: 0.6416 - val_loss: 0.0566 - val_acc: 0.5821\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0491 - acc: 0.6460 - val_loss: 0.0560 - val_acc: 0.5904\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 222us/step - loss: 0.0497 - acc: 0.6393 - val_loss: 0.0563 - val_acc: 0.5821\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 222us/step - loss: 0.0492 - acc: 0.6425 - val_loss: 0.0558 - val_acc: 0.5842\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 222us/step - loss: 0.0490 - acc: 0.6457 - val_loss: 0.0558 - val_acc: 0.5904\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0489 - acc: 0.6460 - val_loss: 0.0557 - val_acc: 0.5946\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0492 - acc: 0.6413 - val_loss: 0.0554 - val_acc: 0.5904\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0489 - acc: 0.6462 - val_loss: 0.0551 - val_acc: 0.5925\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 223us/step - loss: 0.0491 - acc: 0.6464 - val_loss: 0.0556 - val_acc: 0.5904\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0488 - acc: 0.6453 - val_loss: 0.0565 - val_acc: 0.5676\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0487 - acc: 0.6457 - val_loss: 0.0565 - val_acc: 0.5696\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0491 - acc: 0.6471 - val_loss: 0.0529 - val_acc: 0.6008\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0496 - acc: 0.6390 - val_loss: 0.0526 - val_acc: 0.6050\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0490 - acc: 0.6464 - val_loss: 0.0533 - val_acc: 0.6008\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0490 - acc: 0.6480 - val_loss: 0.0543 - val_acc: 0.6008\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0496 - acc: 0.6413 - val_loss: 0.0546 - val_acc: 0.5988\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 222us/step - loss: 0.0491 - acc: 0.6455 - val_loss: 0.0532 - val_acc: 0.6008\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0491 - acc: 0.6434 - val_loss: 0.0536 - val_acc: 0.5967\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0488 - acc: 0.6480 - val_loss: 0.0550 - val_acc: 0.5800\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0494 - acc: 0.6460 - val_loss: 0.0547 - val_acc: 0.5904\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0492 - acc: 0.6439 - val_loss: 0.0544 - val_acc: 0.5946\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0486 - acc: 0.6540 - val_loss: 0.0541 - val_acc: 0.5884\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0492 - acc: 0.6430 - val_loss: 0.0543 - val_acc: 0.5842\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0489 - acc: 0.6499 - val_loss: 0.0555 - val_acc: 0.5800\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0487 - acc: 0.6506 - val_loss: 0.0549 - val_acc: 0.5925\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0488 - acc: 0.6501 - val_loss: 0.0546 - val_acc: 0.5884\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0489 - acc: 0.6480 - val_loss: 0.0557 - val_acc: 0.5884\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0488 - acc: 0.6499 - val_loss: 0.0556 - val_acc: 0.5925\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0491 - acc: 0.6490 - val_loss: 0.0553 - val_acc: 0.5780\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0487 - acc: 0.6476 - val_loss: 0.0560 - val_acc: 0.5863\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0486 - acc: 0.6536 - val_loss: 0.0565 - val_acc: 0.5780\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0484 - acc: 0.6566 - val_loss: 0.0561 - val_acc: 0.5800\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0488 - acc: 0.6467 - val_loss: 0.0565 - val_acc: 0.5800\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0489 - acc: 0.6471 - val_loss: 0.0555 - val_acc: 0.5800\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0489 - acc: 0.6464 - val_loss: 0.0565 - val_acc: 0.5717\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0487 - acc: 0.6510 - val_loss: 0.0569 - val_acc: 0.5759\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0491 - acc: 0.6425 - val_loss: 0.0551 - val_acc: 0.5821\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0490 - acc: 0.6467 - val_loss: 0.0566 - val_acc: 0.5800\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0491 - acc: 0.6490 - val_loss: 0.0561 - val_acc: 0.5780\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0486 - acc: 0.6499 - val_loss: 0.0556 - val_acc: 0.5863\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0486 - acc: 0.6508 - val_loss: 0.0571 - val_acc: 0.5842\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0489 - acc: 0.6487 - val_loss: 0.0583 - val_acc: 0.5696\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0485 - acc: 0.6529 - val_loss: 0.0567 - val_acc: 0.5884\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0484 - acc: 0.6515 - val_loss: 0.0574 - val_acc: 0.5759\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0486 - acc: 0.6448 - val_loss: 0.0569 - val_acc: 0.5800\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0485 - acc: 0.6503 - val_loss: 0.0567 - val_acc: 0.5800\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0485 - acc: 0.6513 - val_loss: 0.0562 - val_acc: 0.5717\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0488 - acc: 0.6480 - val_loss: 0.0561 - val_acc: 0.5821\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0487 - acc: 0.6494 - val_loss: 0.0573 - val_acc: 0.5676\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0485 - acc: 0.6520 - val_loss: 0.0561 - val_acc: 0.5863\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0487 - acc: 0.6499 - val_loss: 0.0559 - val_acc: 0.5800\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0487 - acc: 0.6515 - val_loss: 0.0575 - val_acc: 0.5676\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0486 - acc: 0.6527 - val_loss: 0.0563 - val_acc: 0.5821\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0484 - acc: 0.6508 - val_loss: 0.0567 - val_acc: 0.5717\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0482 - acc: 0.6531 - val_loss: 0.0572 - val_acc: 0.5655\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0481 - acc: 0.6524 - val_loss: 0.0564 - val_acc: 0.5717\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0481 - acc: 0.6545 - val_loss: 0.0573 - val_acc: 0.5759\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0482 - acc: 0.6515 - val_loss: 0.0578 - val_acc: 0.5717\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0487 - acc: 0.6506 - val_loss: 0.0568 - val_acc: 0.5717\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0486 - acc: 0.6492 - val_loss: 0.0575 - val_acc: 0.5696\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0484 - acc: 0.6522 - val_loss: 0.0570 - val_acc: 0.5759\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0484 - acc: 0.6536 - val_loss: 0.0578 - val_acc: 0.5655\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0484 - acc: 0.6538 - val_loss: 0.0567 - val_acc: 0.5696\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0484 - acc: 0.6513 - val_loss: 0.0576 - val_acc: 0.5717\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0485 - acc: 0.6497 - val_loss: 0.0573 - val_acc: 0.5593\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0487 - acc: 0.6513 - val_loss: 0.0566 - val_acc: 0.5780\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0481 - acc: 0.6522 - val_loss: 0.0569 - val_acc: 0.5717\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0482 - acc: 0.6524 - val_loss: 0.0568 - val_acc: 0.5759\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0485 - acc: 0.6540 - val_loss: 0.0565 - val_acc: 0.5696\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0484 - acc: 0.6524 - val_loss: 0.0578 - val_acc: 0.5780\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0485 - acc: 0.6492 - val_loss: 0.0576 - val_acc: 0.5800\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0482 - acc: 0.6547 - val_loss: 0.0578 - val_acc: 0.5593\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0480 - acc: 0.6566 - val_loss: 0.0576 - val_acc: 0.5634\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0481 - acc: 0.6561 - val_loss: 0.0590 - val_acc: 0.5613\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0484 - acc: 0.6506 - val_loss: 0.0589 - val_acc: 0.5655\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0483 - acc: 0.6529 - val_loss: 0.0582 - val_acc: 0.5613\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0485 - acc: 0.6483 - val_loss: 0.0601 - val_acc: 0.5489\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0481 - acc: 0.6594 - val_loss: 0.0580 - val_acc: 0.5738\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0485 - acc: 0.6547 - val_loss: 0.0579 - val_acc: 0.5676\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0482 - acc: 0.6529 - val_loss: 0.0583 - val_acc: 0.5613\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0481 - acc: 0.6543 - val_loss: 0.0592 - val_acc: 0.5572\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0484 - acc: 0.6517 - val_loss: 0.0583 - val_acc: 0.5655\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0482 - acc: 0.6533 - val_loss: 0.0591 - val_acc: 0.5613\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0483 - acc: 0.6538 - val_loss: 0.0586 - val_acc: 0.5676\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0486 - acc: 0.6536 - val_loss: 0.0601 - val_acc: 0.5551\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0481 - acc: 0.6536 - val_loss: 0.0598 - val_acc: 0.5530\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0484 - acc: 0.6508 - val_loss: 0.0579 - val_acc: 0.5717\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0480 - acc: 0.6543 - val_loss: 0.0580 - val_acc: 0.5717\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0481 - acc: 0.6531 - val_loss: 0.0591 - val_acc: 0.5593\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0479 - acc: 0.6591 - val_loss: 0.0590 - val_acc: 0.5551\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0479 - acc: 0.6598 - val_loss: 0.0596 - val_acc: 0.5593\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0477 - acc: 0.6603 - val_loss: 0.0601 - val_acc: 0.5509\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0480 - acc: 0.6587 - val_loss: 0.0591 - val_acc: 0.5696\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0484 - acc: 0.6506 - val_loss: 0.0583 - val_acc: 0.5572\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0479 - acc: 0.6554 - val_loss: 0.0586 - val_acc: 0.5509\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0479 - acc: 0.6570 - val_loss: 0.0587 - val_acc: 0.5572\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 264us/step - loss: 0.0477 - acc: 0.6561 - val_loss: 0.0578 - val_acc: 0.5613\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 266us/step - loss: 0.0479 - acc: 0.6591 - val_loss: 0.0585 - val_acc: 0.5509\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 260us/step - loss: 0.0479 - acc: 0.6575 - val_loss: 0.0600 - val_acc: 0.5489\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 263us/step - loss: 0.0479 - acc: 0.6591 - val_loss: 0.0584 - val_acc: 0.5613\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 264us/step - loss: 0.0479 - acc: 0.6531 - val_loss: 0.0585 - val_acc: 0.5676\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 265us/step - loss: 0.0480 - acc: 0.6561 - val_loss: 0.0602 - val_acc: 0.5593\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 262us/step - loss: 0.0478 - acc: 0.6582 - val_loss: 0.0597 - val_acc: 0.5634\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 263us/step - loss: 0.0482 - acc: 0.6547 - val_loss: 0.0588 - val_acc: 0.5572\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 264us/step - loss: 0.0476 - acc: 0.6603 - val_loss: 0.0598 - val_acc: 0.5509\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0476 - acc: 0.6607 - val_loss: 0.0583 - val_acc: 0.5717\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 204us/step - loss: 0.0477 - acc: 0.6570 - val_loss: 0.0607 - val_acc: 0.5551\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 202us/step - loss: 0.0479 - acc: 0.6520 - val_loss: 0.0589 - val_acc: 0.5655\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 203us/step - loss: 0.0480 - acc: 0.6575 - val_loss: 0.0594 - val_acc: 0.5593\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0478 - acc: 0.6543 - val_loss: 0.0597 - val_acc: 0.5613\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 202us/step - loss: 0.0475 - acc: 0.6605 - val_loss: 0.0590 - val_acc: 0.5676\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 204us/step - loss: 0.0495 - acc: 0.6423 - val_loss: 0.0457 - val_acc: 0.6798\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0498 - acc: 0.6383 - val_loss: 0.0462 - val_acc: 0.6798\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0496 - acc: 0.6427 - val_loss: 0.0474 - val_acc: 0.6653\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0495 - acc: 0.6430 - val_loss: 0.0449 - val_acc: 0.6757\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 201us/step - loss: 0.0493 - acc: 0.6436 - val_loss: 0.0453 - val_acc: 0.6881\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0489 - acc: 0.6517 - val_loss: 0.0464 - val_acc: 0.6778\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0490 - acc: 0.6439 - val_loss: 0.0462 - val_acc: 0.6674\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0489 - acc: 0.6478 - val_loss: 0.0465 - val_acc: 0.6611\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0490 - acc: 0.6453 - val_loss: 0.0470 - val_acc: 0.6632\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0494 - acc: 0.6411 - val_loss: 0.0469 - val_acc: 0.6778\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0493 - acc: 0.6418 - val_loss: 0.0478 - val_acc: 0.6528\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 203us/step - loss: 0.0493 - acc: 0.6430 - val_loss: 0.0465 - val_acc: 0.6674\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0489 - acc: 0.6478 - val_loss: 0.0477 - val_acc: 0.6653\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0495 - acc: 0.6411 - val_loss: 0.0463 - val_acc: 0.6778\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0489 - acc: 0.6492 - val_loss: 0.0473 - val_acc: 0.6715\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0493 - acc: 0.6432 - val_loss: 0.0459 - val_acc: 0.6819\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0490 - acc: 0.6476 - val_loss: 0.0470 - val_acc: 0.6736\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0488 - acc: 0.6473 - val_loss: 0.0479 - val_acc: 0.6486\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0486 - acc: 0.6492 - val_loss: 0.0481 - val_acc: 0.6632\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0487 - acc: 0.6473 - val_loss: 0.0481 - val_acc: 0.6590\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0488 - acc: 0.6480 - val_loss: 0.0472 - val_acc: 0.6674\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0486 - acc: 0.6499 - val_loss: 0.0483 - val_acc: 0.6549\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0486 - acc: 0.6492 - val_loss: 0.0483 - val_acc: 0.6590\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0485 - acc: 0.6497 - val_loss: 0.0465 - val_acc: 0.6757\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0486 - acc: 0.6503 - val_loss: 0.0493 - val_acc: 0.6486\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0488 - acc: 0.6483 - val_loss: 0.0491 - val_acc: 0.6778\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0488 - acc: 0.6508 - val_loss: 0.0480 - val_acc: 0.6611\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0487 - acc: 0.6501 - val_loss: 0.0480 - val_acc: 0.6590\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0487 - acc: 0.6515 - val_loss: 0.0480 - val_acc: 0.6632\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0491 - acc: 0.6462 - val_loss: 0.0476 - val_acc: 0.6466\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0484 - acc: 0.6531 - val_loss: 0.0476 - val_acc: 0.6653\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0486 - acc: 0.6487 - val_loss: 0.0491 - val_acc: 0.6528\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0491 - acc: 0.6464 - val_loss: 0.0490 - val_acc: 0.6507\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0487 - acc: 0.6457 - val_loss: 0.0480 - val_acc: 0.6653\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0484 - acc: 0.6529 - val_loss: 0.0478 - val_acc: 0.6674\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0484 - acc: 0.6531 - val_loss: 0.0476 - val_acc: 0.6611\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0485 - acc: 0.6513 - val_loss: 0.0474 - val_acc: 0.6736\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0484 - acc: 0.6506 - val_loss: 0.0477 - val_acc: 0.6653\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0491 - acc: 0.6467 - val_loss: 0.0475 - val_acc: 0.6736\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0486 - acc: 0.6503 - val_loss: 0.0486 - val_acc: 0.6424\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0485 - acc: 0.6490 - val_loss: 0.0478 - val_acc: 0.6632\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0482 - acc: 0.6527 - val_loss: 0.0492 - val_acc: 0.6507\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0486 - acc: 0.6492 - val_loss: 0.0486 - val_acc: 0.6632\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0485 - acc: 0.6501 - val_loss: 0.0482 - val_acc: 0.6590\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0483 - acc: 0.6529 - val_loss: 0.0489 - val_acc: 0.6486\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0483 - acc: 0.6522 - val_loss: 0.0477 - val_acc: 0.6715\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0486 - acc: 0.6494 - val_loss: 0.0493 - val_acc: 0.6590\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0484 - acc: 0.6506 - val_loss: 0.0496 - val_acc: 0.6486\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0484 - acc: 0.6506 - val_loss: 0.0491 - val_acc: 0.6486\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0483 - acc: 0.6497 - val_loss: 0.0493 - val_acc: 0.6341\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0483 - acc: 0.6499 - val_loss: 0.0488 - val_acc: 0.6507\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 222us/step - loss: 0.0489 - acc: 0.6464 - val_loss: 0.0497 - val_acc: 0.6445\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0491 - acc: 0.6492 - val_loss: 0.0484 - val_acc: 0.6403\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0483 - acc: 0.6515 - val_loss: 0.0515 - val_acc: 0.6341\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0489 - acc: 0.6462 - val_loss: 0.0488 - val_acc: 0.6445\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0482 - acc: 0.6510 - val_loss: 0.0489 - val_acc: 0.6590\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0481 - acc: 0.6531 - val_loss: 0.0495 - val_acc: 0.6424\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0483 - acc: 0.6513 - val_loss: 0.0491 - val_acc: 0.6445\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0479 - acc: 0.6543 - val_loss: 0.0487 - val_acc: 0.6486\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0487 - acc: 0.6483 - val_loss: 0.0497 - val_acc: 0.6466\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0490 - acc: 0.6503 - val_loss: 0.0484 - val_acc: 0.6674\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0483 - acc: 0.6506 - val_loss: 0.0498 - val_acc: 0.6424\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0486 - acc: 0.6483 - val_loss: 0.0500 - val_acc: 0.6466\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0481 - acc: 0.6533 - val_loss: 0.0498 - val_acc: 0.6299\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0480 - acc: 0.6492 - val_loss: 0.0497 - val_acc: 0.6486\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0481 - acc: 0.6536 - val_loss: 0.0491 - val_acc: 0.6383\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0482 - acc: 0.6550 - val_loss: 0.0491 - val_acc: 0.6403\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0483 - acc: 0.6550 - val_loss: 0.0507 - val_acc: 0.6362\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0483 - acc: 0.6522 - val_loss: 0.0489 - val_acc: 0.6590\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0483 - acc: 0.6499 - val_loss: 0.0498 - val_acc: 0.6486\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0483 - acc: 0.6499 - val_loss: 0.0499 - val_acc: 0.6445\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0483 - acc: 0.6510 - val_loss: 0.0496 - val_acc: 0.6549\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0483 - acc: 0.6536 - val_loss: 0.0498 - val_acc: 0.6507\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0483 - acc: 0.6515 - val_loss: 0.0496 - val_acc: 0.6486\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0482 - acc: 0.6517 - val_loss: 0.0491 - val_acc: 0.6570\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0482 - acc: 0.6522 - val_loss: 0.0498 - val_acc: 0.6424\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0483 - acc: 0.6538 - val_loss: 0.0504 - val_acc: 0.6486\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0481 - acc: 0.6554 - val_loss: 0.0482 - val_acc: 0.6653\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0479 - acc: 0.6575 - val_loss: 0.0494 - val_acc: 0.6570\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0481 - acc: 0.6554 - val_loss: 0.0510 - val_acc: 0.6528\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0480 - acc: 0.6547 - val_loss: 0.0510 - val_acc: 0.6403\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0479 - acc: 0.6545 - val_loss: 0.0509 - val_acc: 0.6320\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0480 - acc: 0.6529 - val_loss: 0.0502 - val_acc: 0.6362\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 266us/step - loss: 0.0482 - acc: 0.6510 - val_loss: 0.0503 - val_acc: 0.6445\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 266us/step - loss: 0.0483 - acc: 0.6510 - val_loss: 0.0496 - val_acc: 0.6507\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 265us/step - loss: 0.0481 - acc: 0.6550 - val_loss: 0.0506 - val_acc: 0.6424\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 263us/step - loss: 0.0481 - acc: 0.6545 - val_loss: 0.0519 - val_acc: 0.6195\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0485 - acc: 0.6487 - val_loss: 0.0504 - val_acc: 0.6424\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0485 - acc: 0.6492 - val_loss: 0.0520 - val_acc: 0.6299\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0485 - acc: 0.6497 - val_loss: 0.0496 - val_acc: 0.6445\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 205us/step - loss: 0.0480 - acc: 0.6529 - val_loss: 0.0500 - val_acc: 0.6528\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0489 - acc: 0.6490 - val_loss: 0.0498 - val_acc: 0.6362\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0481 - acc: 0.6501 - val_loss: 0.0505 - val_acc: 0.6341\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0476 - acc: 0.6575 - val_loss: 0.0505 - val_acc: 0.6383\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0483 - acc: 0.6517 - val_loss: 0.0510 - val_acc: 0.6320\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0481 - acc: 0.6552 - val_loss: 0.0518 - val_acc: 0.6175\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0482 - acc: 0.6497 - val_loss: 0.0508 - val_acc: 0.6341\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0479 - acc: 0.6568 - val_loss: 0.0499 - val_acc: 0.6424\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0480 - acc: 0.6561 - val_loss: 0.0506 - val_acc: 0.6424\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0481 - acc: 0.6533 - val_loss: 0.0506 - val_acc: 0.6299\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0905 - acc: 0.3102 - val_loss: 0.0628 - val_acc: 0.5385\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0843 - acc: 0.3180 - val_loss: 0.0675 - val_acc: 0.5634\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0805 - acc: 0.3263 - val_loss: 0.0691 - val_acc: 0.5489\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0801 - acc: 0.3266 - val_loss: 0.0683 - val_acc: 0.5260\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0798 - acc: 0.3379 - val_loss: 0.0700 - val_acc: 0.5447\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0797 - acc: 0.3370 - val_loss: 0.0698 - val_acc: 0.5405\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0795 - acc: 0.3363 - val_loss: 0.0691 - val_acc: 0.5343\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0793 - acc: 0.3386 - val_loss: 0.0686 - val_acc: 0.5343\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0793 - acc: 0.3441 - val_loss: 0.0691 - val_acc: 0.4948\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0792 - acc: 0.3406 - val_loss: 0.0693 - val_acc: 0.5135\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0791 - acc: 0.3453 - val_loss: 0.0694 - val_acc: 0.5281\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0791 - acc: 0.3434 - val_loss: 0.0710 - val_acc: 0.5489\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0790 - acc: 0.3448 - val_loss: 0.0694 - val_acc: 0.5696\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 203us/step - loss: 0.0793 - acc: 0.3450 - val_loss: 0.0689 - val_acc: 0.5114\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0790 - acc: 0.3416 - val_loss: 0.0685 - val_acc: 0.5177\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0790 - acc: 0.3464 - val_loss: 0.0697 - val_acc: 0.5655\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0789 - acc: 0.3499 - val_loss: 0.0692 - val_acc: 0.5572\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0786 - acc: 0.3506 - val_loss: 0.0690 - val_acc: 0.5509\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0787 - acc: 0.3540 - val_loss: 0.0696 - val_acc: 0.5094\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0786 - acc: 0.3529 - val_loss: 0.0698 - val_acc: 0.5156\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0785 - acc: 0.3464 - val_loss: 0.0705 - val_acc: 0.5364\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 202us/step - loss: 0.0785 - acc: 0.3540 - val_loss: 0.0722 - val_acc: 0.4990\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0785 - acc: 0.3559 - val_loss: 0.0703 - val_acc: 0.5114\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 205us/step - loss: 0.0782 - acc: 0.3561 - val_loss: 0.0690 - val_acc: 0.5218\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0783 - acc: 0.3554 - val_loss: 0.0694 - val_acc: 0.5343\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0783 - acc: 0.3487 - val_loss: 0.0710 - val_acc: 0.5260\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0781 - acc: 0.3547 - val_loss: 0.0700 - val_acc: 0.5322\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0783 - acc: 0.3557 - val_loss: 0.0694 - val_acc: 0.5218\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0781 - acc: 0.3575 - val_loss: 0.0700 - val_acc: 0.5198\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0783 - acc: 0.3492 - val_loss: 0.0684 - val_acc: 0.5447\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0783 - acc: 0.3522 - val_loss: 0.0701 - val_acc: 0.4990\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0781 - acc: 0.3552 - val_loss: 0.0697 - val_acc: 0.5239\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0781 - acc: 0.3533 - val_loss: 0.0706 - val_acc: 0.5094\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0781 - acc: 0.3510 - val_loss: 0.0714 - val_acc: 0.5052\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0779 - acc: 0.3594 - val_loss: 0.0705 - val_acc: 0.5322\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0778 - acc: 0.3614 - val_loss: 0.0701 - val_acc: 0.5114\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0780 - acc: 0.3570 - val_loss: 0.0704 - val_acc: 0.5073\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0781 - acc: 0.3545 - val_loss: 0.0697 - val_acc: 0.5239\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0778 - acc: 0.3552 - val_loss: 0.0699 - val_acc: 0.5177\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0776 - acc: 0.3619 - val_loss: 0.0704 - val_acc: 0.5281\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0778 - acc: 0.3540 - val_loss: 0.0708 - val_acc: 0.5260\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0779 - acc: 0.3527 - val_loss: 0.0719 - val_acc: 0.5177\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0777 - acc: 0.3624 - val_loss: 0.0712 - val_acc: 0.5239\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0776 - acc: 0.3584 - val_loss: 0.0709 - val_acc: 0.4886\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0779 - acc: 0.3589 - val_loss: 0.0707 - val_acc: 0.4886\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0776 - acc: 0.3630 - val_loss: 0.0697 - val_acc: 0.5198\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0777 - acc: 0.3570 - val_loss: 0.0707 - val_acc: 0.5239\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0773 - acc: 0.3674 - val_loss: 0.0720 - val_acc: 0.5031\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0777 - acc: 0.3621 - val_loss: 0.0716 - val_acc: 0.4678\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0776 - acc: 0.3536 - val_loss: 0.0713 - val_acc: 0.4948\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0776 - acc: 0.3600 - val_loss: 0.0720 - val_acc: 0.5198\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0775 - acc: 0.3633 - val_loss: 0.0710 - val_acc: 0.4948\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0776 - acc: 0.3619 - val_loss: 0.0706 - val_acc: 0.5218\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0775 - acc: 0.3573 - val_loss: 0.0707 - val_acc: 0.5218\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0774 - acc: 0.3633 - val_loss: 0.0713 - val_acc: 0.5052\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0774 - acc: 0.3624 - val_loss: 0.0700 - val_acc: 0.5094\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0772 - acc: 0.3598 - val_loss: 0.0723 - val_acc: 0.4595\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0772 - acc: 0.3596 - val_loss: 0.0723 - val_acc: 0.5135\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0771 - acc: 0.3600 - val_loss: 0.0713 - val_acc: 0.5156\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0772 - acc: 0.3630 - val_loss: 0.0707 - val_acc: 0.5073\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0772 - acc: 0.3681 - val_loss: 0.0710 - val_acc: 0.4927\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0771 - acc: 0.3635 - val_loss: 0.0716 - val_acc: 0.5052\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0767 - acc: 0.3684 - val_loss: 0.0716 - val_acc: 0.5177\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0771 - acc: 0.3686 - val_loss: 0.0728 - val_acc: 0.4574\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0774 - acc: 0.3517 - val_loss: 0.0723 - val_acc: 0.4802\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0772 - acc: 0.3589 - val_loss: 0.0713 - val_acc: 0.5135\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0769 - acc: 0.3640 - val_loss: 0.0707 - val_acc: 0.5052\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0769 - acc: 0.3635 - val_loss: 0.0696 - val_acc: 0.5031\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0767 - acc: 0.3658 - val_loss: 0.0697 - val_acc: 0.5114\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0764 - acc: 0.3725 - val_loss: 0.0724 - val_acc: 0.5218\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0768 - acc: 0.3667 - val_loss: 0.0719 - val_acc: 0.5135\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0767 - acc: 0.3707 - val_loss: 0.0720 - val_acc: 0.4844\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0770 - acc: 0.3658 - val_loss: 0.0723 - val_acc: 0.4823\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 222us/step - loss: 0.0767 - acc: 0.3677 - val_loss: 0.0729 - val_acc: 0.4511\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0769 - acc: 0.3644 - val_loss: 0.0689 - val_acc: 0.5239\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0764 - acc: 0.3693 - val_loss: 0.0708 - val_acc: 0.4990\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0766 - acc: 0.3651 - val_loss: 0.0716 - val_acc: 0.4844\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0764 - acc: 0.3711 - val_loss: 0.0700 - val_acc: 0.5052\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0766 - acc: 0.3674 - val_loss: 0.0713 - val_acc: 0.4761\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0763 - acc: 0.3665 - val_loss: 0.0710 - val_acc: 0.5114\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0761 - acc: 0.3723 - val_loss: 0.0700 - val_acc: 0.5218\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0762 - acc: 0.3700 - val_loss: 0.0697 - val_acc: 0.5198\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0763 - acc: 0.3737 - val_loss: 0.0710 - val_acc: 0.5364\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0765 - acc: 0.3727 - val_loss: 0.0712 - val_acc: 0.5301\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0767 - acc: 0.3718 - val_loss: 0.0697 - val_acc: 0.5052\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0764 - acc: 0.3691 - val_loss: 0.0725 - val_acc: 0.4823\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0762 - acc: 0.3785 - val_loss: 0.0719 - val_acc: 0.4948\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0760 - acc: 0.3774 - val_loss: 0.0717 - val_acc: 0.4865\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0760 - acc: 0.3718 - val_loss: 0.0712 - val_acc: 0.4990\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0761 - acc: 0.3767 - val_loss: 0.0719 - val_acc: 0.4719\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0759 - acc: 0.3716 - val_loss: 0.0727 - val_acc: 0.4948\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0765 - acc: 0.3716 - val_loss: 0.0718 - val_acc: 0.4927\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0760 - acc: 0.3760 - val_loss: 0.0722 - val_acc: 0.5135\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0762 - acc: 0.3697 - val_loss: 0.0726 - val_acc: 0.4782\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0760 - acc: 0.3760 - val_loss: 0.0728 - val_acc: 0.4886\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0757 - acc: 0.3785 - val_loss: 0.0711 - val_acc: 0.5073\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0757 - acc: 0.3758 - val_loss: 0.0742 - val_acc: 0.4470\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0758 - acc: 0.3748 - val_loss: 0.0703 - val_acc: 0.5156\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0759 - acc: 0.3721 - val_loss: 0.0720 - val_acc: 0.4782\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0762 - acc: 0.3718 - val_loss: 0.0714 - val_acc: 0.4740\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0827 - acc: 0.3007 - val_loss: 0.0692 - val_acc: 0.5156\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0815 - acc: 0.3023 - val_loss: 0.0694 - val_acc: 0.5114\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0809 - acc: 0.3162 - val_loss: 0.0702 - val_acc: 0.5156\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0805 - acc: 0.3199 - val_loss: 0.0699 - val_acc: 0.5364\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0806 - acc: 0.3139 - val_loss: 0.0693 - val_acc: 0.5322\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0804 - acc: 0.3201 - val_loss: 0.0695 - val_acc: 0.5010\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0803 - acc: 0.3180 - val_loss: 0.0716 - val_acc: 0.4802\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0805 - acc: 0.3215 - val_loss: 0.0723 - val_acc: 0.5135\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0802 - acc: 0.3199 - val_loss: 0.0714 - val_acc: 0.5260\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0798 - acc: 0.3293 - val_loss: 0.0701 - val_acc: 0.5052\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0800 - acc: 0.3182 - val_loss: 0.0701 - val_acc: 0.5156\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0797 - acc: 0.3203 - val_loss: 0.0693 - val_acc: 0.5468\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0798 - acc: 0.3289 - val_loss: 0.0708 - val_acc: 0.5509\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0795 - acc: 0.3300 - val_loss: 0.0724 - val_acc: 0.5530\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0796 - acc: 0.3309 - val_loss: 0.0717 - val_acc: 0.5156\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0798 - acc: 0.3212 - val_loss: 0.0722 - val_acc: 0.5177\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0795 - acc: 0.3312 - val_loss: 0.0696 - val_acc: 0.4990\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0796 - acc: 0.3321 - val_loss: 0.0686 - val_acc: 0.4906\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0795 - acc: 0.3333 - val_loss: 0.0690 - val_acc: 0.5530\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0793 - acc: 0.3349 - val_loss: 0.0688 - val_acc: 0.5177\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0792 - acc: 0.3323 - val_loss: 0.0711 - val_acc: 0.5364\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0793 - acc: 0.3298 - val_loss: 0.0693 - val_acc: 0.5364\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0792 - acc: 0.3316 - val_loss: 0.0708 - val_acc: 0.5281\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0789 - acc: 0.3316 - val_loss: 0.0699 - val_acc: 0.5073\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0790 - acc: 0.3367 - val_loss: 0.0684 - val_acc: 0.5239\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0788 - acc: 0.3374 - val_loss: 0.0718 - val_acc: 0.5177\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0795 - acc: 0.3309 - val_loss: 0.0704 - val_acc: 0.4802\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0790 - acc: 0.3344 - val_loss: 0.0715 - val_acc: 0.5135\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0789 - acc: 0.3386 - val_loss: 0.0718 - val_acc: 0.5052\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0787 - acc: 0.3381 - val_loss: 0.0704 - val_acc: 0.5405\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0788 - acc: 0.3330 - val_loss: 0.0698 - val_acc: 0.5281\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0784 - acc: 0.3497 - val_loss: 0.0715 - val_acc: 0.4948\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0786 - acc: 0.3388 - val_loss: 0.0739 - val_acc: 0.4158\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0788 - acc: 0.3339 - val_loss: 0.0733 - val_acc: 0.4823\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0789 - acc: 0.3374 - val_loss: 0.0721 - val_acc: 0.5052\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0787 - acc: 0.3409 - val_loss: 0.0726 - val_acc: 0.4595\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0786 - acc: 0.3386 - val_loss: 0.0725 - val_acc: 0.4990\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0784 - acc: 0.3462 - val_loss: 0.0714 - val_acc: 0.5052\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0782 - acc: 0.3455 - val_loss: 0.0728 - val_acc: 0.5031\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0783 - acc: 0.3460 - val_loss: 0.0717 - val_acc: 0.4906\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0784 - acc: 0.3420 - val_loss: 0.0694 - val_acc: 0.5114\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0783 - acc: 0.3467 - val_loss: 0.0726 - val_acc: 0.4802\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0784 - acc: 0.3430 - val_loss: 0.0725 - val_acc: 0.4740\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0780 - acc: 0.3436 - val_loss: 0.0725 - val_acc: 0.5114\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0782 - acc: 0.3453 - val_loss: 0.0719 - val_acc: 0.4990\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0786 - acc: 0.3379 - val_loss: 0.0711 - val_acc: 0.4740\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0781 - acc: 0.3453 - val_loss: 0.0715 - val_acc: 0.4927\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0783 - acc: 0.3423 - val_loss: 0.0712 - val_acc: 0.4678\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0782 - acc: 0.3425 - val_loss: 0.0725 - val_acc: 0.4574\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0781 - acc: 0.3473 - val_loss: 0.0706 - val_acc: 0.4906\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0783 - acc: 0.3386 - val_loss: 0.0714 - val_acc: 0.4844\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0781 - acc: 0.3494 - val_loss: 0.0734 - val_acc: 0.4906\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0781 - acc: 0.3436 - val_loss: 0.0722 - val_acc: 0.4990\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0781 - acc: 0.3446 - val_loss: 0.0714 - val_acc: 0.4990\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0779 - acc: 0.3473 - val_loss: 0.0735 - val_acc: 0.4553\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0779 - acc: 0.3527 - val_loss: 0.0698 - val_acc: 0.4906\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0782 - acc: 0.3436 - val_loss: 0.0714 - val_acc: 0.4553\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0777 - acc: 0.3448 - val_loss: 0.0733 - val_acc: 0.4844\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0777 - acc: 0.3483 - val_loss: 0.0731 - val_acc: 0.4449\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0777 - acc: 0.3506 - val_loss: 0.0722 - val_acc: 0.4906\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0781 - acc: 0.3446 - val_loss: 0.0717 - val_acc: 0.4906\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0779 - acc: 0.3450 - val_loss: 0.0710 - val_acc: 0.5156\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0777 - acc: 0.3423 - val_loss: 0.0745 - val_acc: 0.4470\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0778 - acc: 0.3446 - val_loss: 0.0727 - val_acc: 0.5114\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0777 - acc: 0.3476 - val_loss: 0.0723 - val_acc: 0.5094\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0773 - acc: 0.3515 - val_loss: 0.0718 - val_acc: 0.5010\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0774 - acc: 0.3531 - val_loss: 0.0705 - val_acc: 0.4657\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0772 - acc: 0.3517 - val_loss: 0.0733 - val_acc: 0.4699\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0774 - acc: 0.3483 - val_loss: 0.0715 - val_acc: 0.4615\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0775 - acc: 0.3513 - val_loss: 0.0722 - val_acc: 0.5010\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0774 - acc: 0.3499 - val_loss: 0.0713 - val_acc: 0.5010\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0771 - acc: 0.3561 - val_loss: 0.0734 - val_acc: 0.4844\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0775 - acc: 0.3453 - val_loss: 0.0739 - val_acc: 0.5135\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0773 - acc: 0.3561 - val_loss: 0.0731 - val_acc: 0.4615\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0774 - acc: 0.3559 - val_loss: 0.0725 - val_acc: 0.4969\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 222us/step - loss: 0.0770 - acc: 0.3596 - val_loss: 0.0724 - val_acc: 0.4844\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0777 - acc: 0.3510 - val_loss: 0.0719 - val_acc: 0.4678\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0774 - acc: 0.3515 - val_loss: 0.0734 - val_acc: 0.4802\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0777 - acc: 0.3494 - val_loss: 0.0715 - val_acc: 0.4927\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0776 - acc: 0.3529 - val_loss: 0.0721 - val_acc: 0.4865\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0772 - acc: 0.3517 - val_loss: 0.0735 - val_acc: 0.4886\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0772 - acc: 0.3547 - val_loss: 0.0734 - val_acc: 0.4886\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0772 - acc: 0.3501 - val_loss: 0.0736 - val_acc: 0.4532\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0774 - acc: 0.3536 - val_loss: 0.0718 - val_acc: 0.4969\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0770 - acc: 0.3594 - val_loss: 0.0754 - val_acc: 0.4491\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0769 - acc: 0.3559 - val_loss: 0.0744 - val_acc: 0.4969\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0774 - acc: 0.3524 - val_loss: 0.0746 - val_acc: 0.4636\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0769 - acc: 0.3545 - val_loss: 0.0723 - val_acc: 0.5094\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0768 - acc: 0.3591 - val_loss: 0.0728 - val_acc: 0.4699\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0771 - acc: 0.3612 - val_loss: 0.0742 - val_acc: 0.4719\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0773 - acc: 0.3575 - val_loss: 0.0737 - val_acc: 0.5010\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0774 - acc: 0.3503 - val_loss: 0.0719 - val_acc: 0.5094\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 205us/step - loss: 0.0770 - acc: 0.3540 - val_loss: 0.0723 - val_acc: 0.4615\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0769 - acc: 0.3566 - val_loss: 0.0728 - val_acc: 0.4782\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0769 - acc: 0.3580 - val_loss: 0.0734 - val_acc: 0.4678\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0766 - acc: 0.3651 - val_loss: 0.0735 - val_acc: 0.4782\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0769 - acc: 0.3529 - val_loss: 0.0740 - val_acc: 0.5010\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0767 - acc: 0.3605 - val_loss: 0.0738 - val_acc: 0.4678\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0766 - acc: 0.3573 - val_loss: 0.0714 - val_acc: 0.5198\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0767 - acc: 0.3642 - val_loss: 0.0726 - val_acc: 0.4927\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0789 - acc: 0.3436 - val_loss: 0.0669 - val_acc: 0.5301\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0787 - acc: 0.3501 - val_loss: 0.0681 - val_acc: 0.5135\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0790 - acc: 0.3460 - val_loss: 0.0691 - val_acc: 0.5301\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0785 - acc: 0.3457 - val_loss: 0.0665 - val_acc: 0.4948\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0784 - acc: 0.3460 - val_loss: 0.0687 - val_acc: 0.5052\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0783 - acc: 0.3520 - val_loss: 0.0683 - val_acc: 0.5031\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0781 - acc: 0.3508 - val_loss: 0.0690 - val_acc: 0.4990\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 214us/step - loss: 0.0783 - acc: 0.3462 - val_loss: 0.0682 - val_acc: 0.5301\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0779 - acc: 0.3522 - val_loss: 0.0693 - val_acc: 0.4532\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0780 - acc: 0.3515 - val_loss: 0.0679 - val_acc: 0.5177\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0780 - acc: 0.3492 - val_loss: 0.0685 - val_acc: 0.5010\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0781 - acc: 0.3531 - val_loss: 0.0686 - val_acc: 0.4969\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0780 - acc: 0.3490 - val_loss: 0.0692 - val_acc: 0.4886\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0779 - acc: 0.3545 - val_loss: 0.0674 - val_acc: 0.4927\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0780 - acc: 0.3480 - val_loss: 0.0698 - val_acc: 0.4678\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0782 - acc: 0.3494 - val_loss: 0.0689 - val_acc: 0.4990\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0780 - acc: 0.3554 - val_loss: 0.0679 - val_acc: 0.4906\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0780 - acc: 0.3501 - val_loss: 0.0695 - val_acc: 0.4574\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0778 - acc: 0.3471 - val_loss: 0.0695 - val_acc: 0.4615\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 223us/step - loss: 0.0779 - acc: 0.3522 - val_loss: 0.0686 - val_acc: 0.4844\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 261us/step - loss: 0.0775 - acc: 0.3603 - val_loss: 0.0693 - val_acc: 0.4927\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 259us/step - loss: 0.0777 - acc: 0.3554 - val_loss: 0.0728 - val_acc: 0.4407\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 263us/step - loss: 0.0776 - acc: 0.3557 - val_loss: 0.0699 - val_acc: 0.4802\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 267us/step - loss: 0.0775 - acc: 0.3575 - val_loss: 0.0706 - val_acc: 0.4511\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 263us/step - loss: 0.0773 - acc: 0.3624 - val_loss: 0.0706 - val_acc: 0.4886\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 259us/step - loss: 0.0774 - acc: 0.3600 - val_loss: 0.0714 - val_acc: 0.4927\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 262us/step - loss: 0.0773 - acc: 0.3596 - val_loss: 0.0727 - val_acc: 0.4033\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 264us/step - loss: 0.0777 - acc: 0.3577 - val_loss: 0.0685 - val_acc: 0.4948\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 248us/step - loss: 0.0775 - acc: 0.3607 - val_loss: 0.0735 - val_acc: 0.4324\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0771 - acc: 0.3640 - val_loss: 0.0706 - val_acc: 0.4844\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0773 - acc: 0.3570 - val_loss: 0.0698 - val_acc: 0.4636\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0771 - acc: 0.3654 - val_loss: 0.0702 - val_acc: 0.4802\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0773 - acc: 0.3552 - val_loss: 0.0707 - val_acc: 0.4782\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0770 - acc: 0.3580 - val_loss: 0.0720 - val_acc: 0.4844\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0771 - acc: 0.3672 - val_loss: 0.0739 - val_acc: 0.4595\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0773 - acc: 0.3577 - val_loss: 0.0701 - val_acc: 0.4969\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0776 - acc: 0.3607 - val_loss: 0.0708 - val_acc: 0.4553\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0769 - acc: 0.3651 - val_loss: 0.0714 - val_acc: 0.4906\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0769 - acc: 0.3614 - val_loss: 0.0691 - val_acc: 0.4844\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0768 - acc: 0.3665 - val_loss: 0.0707 - val_acc: 0.4802\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0770 - acc: 0.3644 - val_loss: 0.0701 - val_acc: 0.5135\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 208us/step - loss: 0.0767 - acc: 0.3651 - val_loss: 0.0706 - val_acc: 0.4699\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0769 - acc: 0.3679 - val_loss: 0.0721 - val_acc: 0.4595\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0769 - acc: 0.3621 - val_loss: 0.0720 - val_acc: 0.4241\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0768 - acc: 0.3626 - val_loss: 0.0719 - val_acc: 0.4595\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0770 - acc: 0.3621 - val_loss: 0.0721 - val_acc: 0.4865\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0769 - acc: 0.3587 - val_loss: 0.0733 - val_acc: 0.4262\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0768 - acc: 0.3665 - val_loss: 0.0729 - val_acc: 0.4678\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0770 - acc: 0.3561 - val_loss: 0.0712 - val_acc: 0.4407\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0769 - acc: 0.3651 - val_loss: 0.0725 - val_acc: 0.4948\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0769 - acc: 0.3654 - val_loss: 0.0712 - val_acc: 0.4782\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0768 - acc: 0.3686 - val_loss: 0.0738 - val_acc: 0.4470\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0767 - acc: 0.3684 - val_loss: 0.0722 - val_acc: 0.4657\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0764 - acc: 0.3695 - val_loss: 0.0710 - val_acc: 0.4802\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0764 - acc: 0.3649 - val_loss: 0.0711 - val_acc: 0.4636\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0764 - acc: 0.3702 - val_loss: 0.0712 - val_acc: 0.4532\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0765 - acc: 0.3665 - val_loss: 0.0728 - val_acc: 0.4324\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0763 - acc: 0.3695 - val_loss: 0.0729 - val_acc: 0.4449\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 206us/step - loss: 0.0766 - acc: 0.3640 - val_loss: 0.0737 - val_acc: 0.4387\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0765 - acc: 0.3665 - val_loss: 0.0725 - val_acc: 0.4553\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 210us/step - loss: 0.0766 - acc: 0.3677 - val_loss: 0.0735 - val_acc: 0.4532\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0762 - acc: 0.3681 - val_loss: 0.0731 - val_acc: 0.4262\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 205us/step - loss: 0.0765 - acc: 0.3635 - val_loss: 0.0738 - val_acc: 0.4283\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 207us/step - loss: 0.0770 - acc: 0.3656 - val_loss: 0.0716 - val_acc: 0.4595\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0765 - acc: 0.3704 - val_loss: 0.0730 - val_acc: 0.4387\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 211us/step - loss: 0.0764 - acc: 0.3709 - val_loss: 0.0720 - val_acc: 0.4345\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0768 - acc: 0.3640 - val_loss: 0.0732 - val_acc: 0.4366\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0765 - acc: 0.3746 - val_loss: 0.0742 - val_acc: 0.4428\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0765 - acc: 0.3695 - val_loss: 0.0739 - val_acc: 0.4449\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0763 - acc: 0.3691 - val_loss: 0.0747 - val_acc: 0.4033\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0765 - acc: 0.3730 - val_loss: 0.0740 - val_acc: 0.4387\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0762 - acc: 0.3727 - val_loss: 0.0720 - val_acc: 0.4491\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 219us/step - loss: 0.0762 - acc: 0.3734 - val_loss: 0.0727 - val_acc: 0.4511\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0772 - acc: 0.3598 - val_loss: 0.0729 - val_acc: 0.4387\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0765 - acc: 0.3741 - val_loss: 0.0735 - val_acc: 0.4553\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0763 - acc: 0.3681 - val_loss: 0.0722 - val_acc: 0.4491\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0765 - acc: 0.3677 - val_loss: 0.0722 - val_acc: 0.4927\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0763 - acc: 0.3711 - val_loss: 0.0735 - val_acc: 0.4595\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 216us/step - loss: 0.0767 - acc: 0.3654 - val_loss: 0.0723 - val_acc: 0.4470\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0760 - acc: 0.3711 - val_loss: 0.0730 - val_acc: 0.4553\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0760 - acc: 0.3734 - val_loss: 0.0741 - val_acc: 0.4428\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 215us/step - loss: 0.0765 - acc: 0.3709 - val_loss: 0.0724 - val_acc: 0.4553\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 213us/step - loss: 0.0764 - acc: 0.3741 - val_loss: 0.0719 - val_acc: 0.4511\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 212us/step - loss: 0.0766 - acc: 0.3734 - val_loss: 0.0737 - val_acc: 0.4449\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 209us/step - loss: 0.0762 - acc: 0.3751 - val_loss: 0.0716 - val_acc: 0.4595\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0759 - acc: 0.3771 - val_loss: 0.0744 - val_acc: 0.4366\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0759 - acc: 0.3776 - val_loss: 0.0734 - val_acc: 0.4387\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0764 - acc: 0.3718 - val_loss: 0.0721 - val_acc: 0.4740\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0772 - acc: 0.3635 - val_loss: 0.0726 - val_acc: 0.4428\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0760 - acc: 0.3767 - val_loss: 0.0740 - val_acc: 0.4366\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0755 - acc: 0.3794 - val_loss: 0.0718 - val_acc: 0.4574\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 218us/step - loss: 0.0757 - acc: 0.3707 - val_loss: 0.0743 - val_acc: 0.4283\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0756 - acc: 0.3762 - val_loss: 0.0722 - val_acc: 0.4615\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 223us/step - loss: 0.0759 - acc: 0.3661 - val_loss: 0.0734 - val_acc: 0.4283\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0757 - acc: 0.3764 - val_loss: 0.0723 - val_acc: 0.4511\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0760 - acc: 0.3700 - val_loss: 0.0742 - val_acc: 0.4283\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 220us/step - loss: 0.0758 - acc: 0.3778 - val_loss: 0.0721 - val_acc: 0.4657\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 217us/step - loss: 0.0758 - acc: 0.3748 - val_loss: 0.0738 - val_acc: 0.4532\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0755 - acc: 0.3806 - val_loss: 0.0736 - val_acc: 0.4532\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 221us/step - loss: 0.0760 - acc: 0.3721 - val_loss: 0.0751 - val_acc: 0.4241\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=20, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=16, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=12, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 10):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=100, validation_data=(dev_valid_x, dev_valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "5xhyzsAVfjEZ",
    "outputId": "ee64fa65-fb70-4993-87ca-b36d06ed055a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_y = nn_model.predict(valid_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result_cv = result_cv.append({'detail':'default', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KY5hmHMZq71o"
   },
   "source": [
    "Dropout & cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34666
    },
    "colab_type": "code",
    "id": "362qLx6vOutf",
    "outputId": "c9a16ca0-6933-4f3a-c267-793976551700"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 12)                204       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 2,386\n",
      "Trainable params: 2,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4329 samples, validate on 482 samples\n",
      "Epoch 1/100\n",
      "4329/4329 [==============================] - 2s 447us/step - loss: 0.0779 - acc: 0.3576 - val_loss: 0.0675 - val_acc: 0.4834\n",
      "Epoch 2/100\n",
      "4329/4329 [==============================] - 1s 253us/step - loss: 0.0699 - acc: 0.4359 - val_loss: 0.0654 - val_acc: 0.4813\n",
      "Epoch 3/100\n",
      "4329/4329 [==============================] - 1s 250us/step - loss: 0.0677 - acc: 0.4440 - val_loss: 0.0636 - val_acc: 0.4917\n",
      "Epoch 4/100\n",
      "4329/4329 [==============================] - 1s 247us/step - loss: 0.0666 - acc: 0.4481 - val_loss: 0.0639 - val_acc: 0.4959\n",
      "Epoch 5/100\n",
      "4329/4329 [==============================] - 1s 249us/step - loss: 0.0664 - acc: 0.4509 - val_loss: 0.0629 - val_acc: 0.4979\n",
      "Epoch 6/100\n",
      "4329/4329 [==============================] - 1s 251us/step - loss: 0.0657 - acc: 0.4502 - val_loss: 0.0626 - val_acc: 0.4938\n",
      "Epoch 7/100\n",
      "4329/4329 [==============================] - 1s 247us/step - loss: 0.0652 - acc: 0.4558 - val_loss: 0.0618 - val_acc: 0.4959\n",
      "Epoch 8/100\n",
      "4329/4329 [==============================] - 1s 290us/step - loss: 0.0652 - acc: 0.4537 - val_loss: 0.0616 - val_acc: 0.5021\n",
      "Epoch 9/100\n",
      "4329/4329 [==============================] - 1s 299us/step - loss: 0.0649 - acc: 0.4645 - val_loss: 0.0615 - val_acc: 0.4710\n",
      "Epoch 10/100\n",
      "4329/4329 [==============================] - 1s 301us/step - loss: 0.0646 - acc: 0.4560 - val_loss: 0.0612 - val_acc: 0.4730\n",
      "Epoch 11/100\n",
      "4329/4329 [==============================] - 1s 302us/step - loss: 0.0648 - acc: 0.4581 - val_loss: 0.0616 - val_acc: 0.4710\n",
      "Epoch 12/100\n",
      "4329/4329 [==============================] - 1s 259us/step - loss: 0.0644 - acc: 0.4615 - val_loss: 0.0615 - val_acc: 0.4813\n",
      "Epoch 13/100\n",
      "4329/4329 [==============================] - 1s 247us/step - loss: 0.0643 - acc: 0.4680 - val_loss: 0.0615 - val_acc: 0.4668\n",
      "Epoch 14/100\n",
      "4329/4329 [==============================] - 1s 245us/step - loss: 0.0643 - acc: 0.4615 - val_loss: 0.0611 - val_acc: 0.4834\n",
      "Epoch 15/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0638 - acc: 0.4680 - val_loss: 0.0614 - val_acc: 0.4793\n",
      "Epoch 16/100\n",
      "4329/4329 [==============================] - 1s 243us/step - loss: 0.0639 - acc: 0.4629 - val_loss: 0.0608 - val_acc: 0.4772\n",
      "Epoch 17/100\n",
      "4329/4329 [==============================] - 1s 244us/step - loss: 0.0639 - acc: 0.4655 - val_loss: 0.0608 - val_acc: 0.4730\n",
      "Epoch 18/100\n",
      "4329/4329 [==============================] - 1s 244us/step - loss: 0.0638 - acc: 0.4627 - val_loss: 0.0610 - val_acc: 0.4855\n",
      "Epoch 19/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0635 - acc: 0.4680 - val_loss: 0.0614 - val_acc: 0.4793\n",
      "Epoch 20/100\n",
      "4329/4329 [==============================] - 1s 246us/step - loss: 0.0632 - acc: 0.4749 - val_loss: 0.0609 - val_acc: 0.4896\n",
      "Epoch 21/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0631 - acc: 0.4731 - val_loss: 0.0600 - val_acc: 0.4834\n",
      "Epoch 22/100\n",
      "4329/4329 [==============================] - 1s 245us/step - loss: 0.0632 - acc: 0.4731 - val_loss: 0.0598 - val_acc: 0.4751\n",
      "Epoch 23/100\n",
      "4329/4329 [==============================] - 1s 243us/step - loss: 0.0634 - acc: 0.4791 - val_loss: 0.0597 - val_acc: 0.4772\n",
      "Epoch 24/100\n",
      "4329/4329 [==============================] - 1s 245us/step - loss: 0.0630 - acc: 0.4768 - val_loss: 0.0607 - val_acc: 0.5000\n",
      "Epoch 25/100\n",
      "4329/4329 [==============================] - 1s 243us/step - loss: 0.0625 - acc: 0.4826 - val_loss: 0.0597 - val_acc: 0.4979\n",
      "Epoch 26/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0629 - acc: 0.4860 - val_loss: 0.0598 - val_acc: 0.4917\n",
      "Epoch 27/100\n",
      "4329/4329 [==============================] - 1s 245us/step - loss: 0.0626 - acc: 0.4812 - val_loss: 0.0595 - val_acc: 0.4876\n",
      "Epoch 28/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0626 - acc: 0.4851 - val_loss: 0.0593 - val_acc: 0.4834\n",
      "Epoch 29/100\n",
      "4329/4329 [==============================] - 1s 247us/step - loss: 0.0621 - acc: 0.4971 - val_loss: 0.0593 - val_acc: 0.5041\n",
      "Epoch 30/100\n",
      "4329/4329 [==============================] - 1s 247us/step - loss: 0.0624 - acc: 0.4897 - val_loss: 0.0599 - val_acc: 0.4979\n",
      "Epoch 31/100\n",
      "4329/4329 [==============================] - 1s 248us/step - loss: 0.0630 - acc: 0.4849 - val_loss: 0.0629 - val_acc: 0.4793\n",
      "Epoch 32/100\n",
      "4329/4329 [==============================] - 1s 249us/step - loss: 0.0624 - acc: 0.4934 - val_loss: 0.0596 - val_acc: 0.4876\n",
      "Epoch 33/100\n",
      "4329/4329 [==============================] - 1s 254us/step - loss: 0.0620 - acc: 0.4955 - val_loss: 0.0596 - val_acc: 0.5041\n",
      "Epoch 34/100\n",
      "4329/4329 [==============================] - 1s 240us/step - loss: 0.0622 - acc: 0.4964 - val_loss: 0.0610 - val_acc: 0.4834\n",
      "Epoch 35/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0622 - acc: 0.4876 - val_loss: 0.0590 - val_acc: 0.5021\n",
      "Epoch 36/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0622 - acc: 0.4930 - val_loss: 0.0596 - val_acc: 0.5021\n",
      "Epoch 37/100\n",
      "4329/4329 [==============================] - 1s 238us/step - loss: 0.0620 - acc: 0.5020 - val_loss: 0.0591 - val_acc: 0.5062\n",
      "Epoch 38/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0617 - acc: 0.5003 - val_loss: 0.0591 - val_acc: 0.5041\n",
      "Epoch 39/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0618 - acc: 0.4985 - val_loss: 0.0589 - val_acc: 0.4938\n",
      "Epoch 40/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0615 - acc: 0.5024 - val_loss: 0.0587 - val_acc: 0.5021\n",
      "Epoch 41/100\n",
      "4329/4329 [==============================] - 1s 240us/step - loss: 0.0619 - acc: 0.4992 - val_loss: 0.0593 - val_acc: 0.5041\n",
      "Epoch 42/100\n",
      "4329/4329 [==============================] - 1s 239us/step - loss: 0.0615 - acc: 0.4990 - val_loss: 0.0590 - val_acc: 0.5062\n",
      "Epoch 43/100\n",
      "4329/4329 [==============================] - 1s 243us/step - loss: 0.0620 - acc: 0.4976 - val_loss: 0.0594 - val_acc: 0.5062\n",
      "Epoch 44/100\n",
      "4329/4329 [==============================] - 1s 239us/step - loss: 0.0613 - acc: 0.5015 - val_loss: 0.0595 - val_acc: 0.5083\n",
      "Epoch 45/100\n",
      "4329/4329 [==============================] - 1s 238us/step - loss: 0.0614 - acc: 0.5020 - val_loss: 0.0596 - val_acc: 0.5000\n",
      "Epoch 46/100\n",
      "4329/4329 [==============================] - 1s 238us/step - loss: 0.0616 - acc: 0.5027 - val_loss: 0.0591 - val_acc: 0.5062\n",
      "Epoch 47/100\n",
      "4329/4329 [==============================] - 1s 244us/step - loss: 0.0614 - acc: 0.5054 - val_loss: 0.0595 - val_acc: 0.5000\n",
      "Epoch 48/100\n",
      "4329/4329 [==============================] - 1s 245us/step - loss: 0.0612 - acc: 0.5082 - val_loss: 0.0586 - val_acc: 0.5062\n",
      "Epoch 49/100\n",
      "4329/4329 [==============================] - 1s 246us/step - loss: 0.0618 - acc: 0.5015 - val_loss: 0.0594 - val_acc: 0.5041\n",
      "Epoch 50/100\n",
      "4329/4329 [==============================] - 1s 248us/step - loss: 0.0610 - acc: 0.5022 - val_loss: 0.0586 - val_acc: 0.5041\n",
      "Epoch 51/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0614 - acc: 0.5033 - val_loss: 0.0592 - val_acc: 0.5041\n",
      "Epoch 52/100\n",
      "4329/4329 [==============================] - 1s 245us/step - loss: 0.0613 - acc: 0.5047 - val_loss: 0.0597 - val_acc: 0.5021\n",
      "Epoch 53/100\n",
      "4329/4329 [==============================] - 1s 248us/step - loss: 0.0610 - acc: 0.5091 - val_loss: 0.0588 - val_acc: 0.5104\n",
      "Epoch 54/100\n",
      "4329/4329 [==============================] - 1s 240us/step - loss: 0.0614 - acc: 0.5017 - val_loss: 0.0587 - val_acc: 0.5021\n",
      "Epoch 55/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0614 - acc: 0.5040 - val_loss: 0.0591 - val_acc: 0.5062\n",
      "Epoch 56/100\n",
      "4329/4329 [==============================] - 1s 239us/step - loss: 0.0606 - acc: 0.5142 - val_loss: 0.0593 - val_acc: 0.5041\n",
      "Epoch 57/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0613 - acc: 0.5057 - val_loss: 0.0586 - val_acc: 0.5062\n",
      "Epoch 58/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0604 - acc: 0.5124 - val_loss: 0.0592 - val_acc: 0.5041\n",
      "Epoch 59/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0611 - acc: 0.5096 - val_loss: 0.0584 - val_acc: 0.5083\n",
      "Epoch 60/100\n",
      "4329/4329 [==============================] - 1s 240us/step - loss: 0.0607 - acc: 0.5103 - val_loss: 0.0587 - val_acc: 0.5041\n",
      "Epoch 61/100\n",
      "4329/4329 [==============================] - 1s 237us/step - loss: 0.0604 - acc: 0.5167 - val_loss: 0.0590 - val_acc: 0.5021\n",
      "Epoch 62/100\n",
      "4329/4329 [==============================] - 1s 239us/step - loss: 0.0609 - acc: 0.5131 - val_loss: 0.0593 - val_acc: 0.5000\n",
      "Epoch 63/100\n",
      "4329/4329 [==============================] - 1s 240us/step - loss: 0.0610 - acc: 0.5070 - val_loss: 0.0589 - val_acc: 0.5041\n",
      "Epoch 64/100\n",
      "4329/4329 [==============================] - 1s 238us/step - loss: 0.0605 - acc: 0.5105 - val_loss: 0.0586 - val_acc: 0.5062\n",
      "Epoch 65/100\n",
      "4329/4329 [==============================] - 1s 239us/step - loss: 0.0603 - acc: 0.5133 - val_loss: 0.0581 - val_acc: 0.5041\n",
      "Epoch 66/100\n",
      "4329/4329 [==============================] - 1s 236us/step - loss: 0.0605 - acc: 0.5100 - val_loss: 0.0583 - val_acc: 0.5041\n",
      "Epoch 67/100\n",
      "4329/4329 [==============================] - 1s 236us/step - loss: 0.0607 - acc: 0.5084 - val_loss: 0.0585 - val_acc: 0.5083\n",
      "Epoch 68/100\n",
      "4329/4329 [==============================] - 1s 235us/step - loss: 0.0605 - acc: 0.5137 - val_loss: 0.0586 - val_acc: 0.5249\n",
      "Epoch 69/100\n",
      "4329/4329 [==============================] - 1s 235us/step - loss: 0.0605 - acc: 0.5200 - val_loss: 0.0583 - val_acc: 0.5270\n",
      "Epoch 70/100\n",
      "4329/4329 [==============================] - 1s 240us/step - loss: 0.0600 - acc: 0.5260 - val_loss: 0.0586 - val_acc: 0.5249\n",
      "Epoch 71/100\n",
      "4329/4329 [==============================] - 1s 237us/step - loss: 0.0605 - acc: 0.5274 - val_loss: 0.0586 - val_acc: 0.5249\n",
      "Epoch 72/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0603 - acc: 0.5241 - val_loss: 0.0588 - val_acc: 0.5207\n",
      "Epoch 73/100\n",
      "4329/4329 [==============================] - 1s 236us/step - loss: 0.0607 - acc: 0.5198 - val_loss: 0.0583 - val_acc: 0.5311\n",
      "Epoch 74/100\n",
      "4329/4329 [==============================] - 1s 237us/step - loss: 0.0607 - acc: 0.5241 - val_loss: 0.0588 - val_acc: 0.5145\n",
      "Epoch 75/100\n",
      "4329/4329 [==============================] - 1s 237us/step - loss: 0.0603 - acc: 0.5262 - val_loss: 0.0584 - val_acc: 0.5249\n",
      "Epoch 76/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0604 - acc: 0.5239 - val_loss: 0.0585 - val_acc: 0.5311\n",
      "Epoch 77/100\n",
      "4329/4329 [==============================] - 1s 243us/step - loss: 0.0600 - acc: 0.5241 - val_loss: 0.0585 - val_acc: 0.5249\n",
      "Epoch 78/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0602 - acc: 0.5267 - val_loss: 0.0590 - val_acc: 0.5187\n",
      "Epoch 79/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0602 - acc: 0.5274 - val_loss: 0.0588 - val_acc: 0.5249\n",
      "Epoch 80/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0603 - acc: 0.5234 - val_loss: 0.0591 - val_acc: 0.5290\n",
      "Epoch 81/100\n",
      "4329/4329 [==============================] - 1s 237us/step - loss: 0.0608 - acc: 0.5163 - val_loss: 0.0587 - val_acc: 0.5270\n",
      "Epoch 82/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0602 - acc: 0.5248 - val_loss: 0.0588 - val_acc: 0.5249\n",
      "Epoch 83/100\n",
      "4329/4329 [==============================] - 1s 243us/step - loss: 0.0601 - acc: 0.5246 - val_loss: 0.0587 - val_acc: 0.5166\n",
      "Epoch 84/100\n",
      "4329/4329 [==============================] - 1s 244us/step - loss: 0.0601 - acc: 0.5251 - val_loss: 0.0589 - val_acc: 0.5373\n",
      "Epoch 85/100\n",
      "4329/4329 [==============================] - 1s 240us/step - loss: 0.0599 - acc: 0.5315 - val_loss: 0.0580 - val_acc: 0.5311\n",
      "Epoch 86/100\n",
      "4329/4329 [==============================] - 1s 240us/step - loss: 0.0602 - acc: 0.5234 - val_loss: 0.0583 - val_acc: 0.5249\n",
      "Epoch 87/100\n",
      "4329/4329 [==============================] - 1s 239us/step - loss: 0.0601 - acc: 0.5255 - val_loss: 0.0585 - val_acc: 0.5249\n",
      "Epoch 88/100\n",
      "4329/4329 [==============================] - 1s 239us/step - loss: 0.0603 - acc: 0.5274 - val_loss: 0.0582 - val_acc: 0.5290\n",
      "Epoch 89/100\n",
      "4329/4329 [==============================] - 1s 248us/step - loss: 0.0603 - acc: 0.5248 - val_loss: 0.0586 - val_acc: 0.5228\n",
      "Epoch 90/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0602 - acc: 0.5285 - val_loss: 0.0584 - val_acc: 0.5249\n",
      "Epoch 91/100\n",
      "4329/4329 [==============================] - 1s 241us/step - loss: 0.0602 - acc: 0.5246 - val_loss: 0.0594 - val_acc: 0.5083\n",
      "Epoch 92/100\n",
      "4329/4329 [==============================] - 1s 239us/step - loss: 0.0600 - acc: 0.5248 - val_loss: 0.0585 - val_acc: 0.5207\n",
      "Epoch 93/100\n",
      "4329/4329 [==============================] - 1s 238us/step - loss: 0.0599 - acc: 0.5209 - val_loss: 0.0584 - val_acc: 0.5207\n",
      "Epoch 94/100\n",
      "4329/4329 [==============================] - 1s 237us/step - loss: 0.0598 - acc: 0.5267 - val_loss: 0.0583 - val_acc: 0.5166\n",
      "Epoch 95/100\n",
      "4329/4329 [==============================] - 1s 243us/step - loss: 0.0600 - acc: 0.5290 - val_loss: 0.0594 - val_acc: 0.5145\n",
      "Epoch 96/100\n",
      "4329/4329 [==============================] - 1s 239us/step - loss: 0.0601 - acc: 0.5288 - val_loss: 0.0585 - val_acc: 0.5249\n",
      "Epoch 97/100\n",
      "4329/4329 [==============================] - 1s 237us/step - loss: 0.0596 - acc: 0.5274 - val_loss: 0.0582 - val_acc: 0.5270\n",
      "Epoch 98/100\n",
      "4329/4329 [==============================] - 1s 238us/step - loss: 0.0600 - acc: 0.5241 - val_loss: 0.0581 - val_acc: 0.5187\n",
      "Epoch 99/100\n",
      "4329/4329 [==============================] - 1s 244us/step - loss: 0.0601 - acc: 0.5288 - val_loss: 0.0583 - val_acc: 0.5228\n",
      "Epoch 100/100\n",
      "4329/4329 [==============================] - 1s 242us/step - loss: 0.0602 - acc: 0.5244 - val_loss: 0.0584 - val_acc: 0.5228\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 246us/step - loss: 0.0598 - acc: 0.5259 - val_loss: 0.0576 - val_acc: 0.5385\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0599 - acc: 0.5284 - val_loss: 0.0578 - val_acc: 0.5343\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0600 - acc: 0.5233 - val_loss: 0.0582 - val_acc: 0.5405\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0602 - acc: 0.5212 - val_loss: 0.0586 - val_acc: 0.5156\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0601 - acc: 0.5286 - val_loss: 0.0578 - val_acc: 0.5322\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0598 - acc: 0.5321 - val_loss: 0.0578 - val_acc: 0.5447\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0600 - acc: 0.5298 - val_loss: 0.0578 - val_acc: 0.5281\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0596 - acc: 0.5284 - val_loss: 0.0578 - val_acc: 0.5281\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0595 - acc: 0.5252 - val_loss: 0.0585 - val_acc: 0.5426\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0597 - acc: 0.5321 - val_loss: 0.0582 - val_acc: 0.5385\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0595 - acc: 0.5309 - val_loss: 0.0580 - val_acc: 0.5426\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0598 - acc: 0.5293 - val_loss: 0.0577 - val_acc: 0.5385\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0597 - acc: 0.5256 - val_loss: 0.0585 - val_acc: 0.5281\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0597 - acc: 0.5305 - val_loss: 0.0577 - val_acc: 0.5426\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0600 - acc: 0.5314 - val_loss: 0.0578 - val_acc: 0.5489\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0594 - acc: 0.5319 - val_loss: 0.0578 - val_acc: 0.5551\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0598 - acc: 0.5259 - val_loss: 0.0578 - val_acc: 0.5426\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0598 - acc: 0.5328 - val_loss: 0.0576 - val_acc: 0.5468\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0601 - acc: 0.5275 - val_loss: 0.0576 - val_acc: 0.5447\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0595 - acc: 0.5339 - val_loss: 0.0578 - val_acc: 0.5405\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0596 - acc: 0.5300 - val_loss: 0.0580 - val_acc: 0.5489\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0596 - acc: 0.5342 - val_loss: 0.0583 - val_acc: 0.5281\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0595 - acc: 0.5353 - val_loss: 0.0578 - val_acc: 0.5530\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0596 - acc: 0.5333 - val_loss: 0.0576 - val_acc: 0.5551\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0595 - acc: 0.5307 - val_loss: 0.0581 - val_acc: 0.5447\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0598 - acc: 0.5296 - val_loss: 0.0580 - val_acc: 0.5343\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0592 - acc: 0.5333 - val_loss: 0.0579 - val_acc: 0.5364\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0599 - acc: 0.5321 - val_loss: 0.0577 - val_acc: 0.5364\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0594 - acc: 0.5356 - val_loss: 0.0578 - val_acc: 0.5322\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0595 - acc: 0.5376 - val_loss: 0.0581 - val_acc: 0.5405\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0590 - acc: 0.5367 - val_loss: 0.0583 - val_acc: 0.5405\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0596 - acc: 0.5342 - val_loss: 0.0577 - val_acc: 0.5426\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0594 - acc: 0.5339 - val_loss: 0.0578 - val_acc: 0.5426\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0598 - acc: 0.5289 - val_loss: 0.0583 - val_acc: 0.5385\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 246us/step - loss: 0.0596 - acc: 0.5335 - val_loss: 0.0582 - val_acc: 0.5405\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0593 - acc: 0.5339 - val_loss: 0.0579 - val_acc: 0.5468\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0591 - acc: 0.5328 - val_loss: 0.0578 - val_acc: 0.5426\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0593 - acc: 0.5367 - val_loss: 0.0578 - val_acc: 0.5447\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 245us/step - loss: 0.0592 - acc: 0.5328 - val_loss: 0.0583 - val_acc: 0.5364\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0595 - acc: 0.5284 - val_loss: 0.0579 - val_acc: 0.5301\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0596 - acc: 0.5330 - val_loss: 0.0579 - val_acc: 0.5447\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0596 - acc: 0.5319 - val_loss: 0.0581 - val_acc: 0.5405\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0592 - acc: 0.5353 - val_loss: 0.0583 - val_acc: 0.5385\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0592 - acc: 0.5339 - val_loss: 0.0586 - val_acc: 0.5364\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0588 - acc: 0.5349 - val_loss: 0.0592 - val_acc: 0.5322\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0595 - acc: 0.5307 - val_loss: 0.0588 - val_acc: 0.5239\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0592 - acc: 0.5360 - val_loss: 0.0584 - val_acc: 0.5364\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 245us/step - loss: 0.0595 - acc: 0.5279 - val_loss: 0.0582 - val_acc: 0.5426\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0593 - acc: 0.5386 - val_loss: 0.0581 - val_acc: 0.5385\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0593 - acc: 0.5374 - val_loss: 0.0579 - val_acc: 0.5343\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0591 - acc: 0.5314 - val_loss: 0.0581 - val_acc: 0.5301\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0594 - acc: 0.5372 - val_loss: 0.0582 - val_acc: 0.5364\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0590 - acc: 0.5335 - val_loss: 0.0578 - val_acc: 0.5385\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 245us/step - loss: 0.0589 - acc: 0.5420 - val_loss: 0.0581 - val_acc: 0.5447\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0592 - acc: 0.5360 - val_loss: 0.0581 - val_acc: 0.5301\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0593 - acc: 0.5388 - val_loss: 0.0584 - val_acc: 0.5301\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0593 - acc: 0.5326 - val_loss: 0.0586 - val_acc: 0.5468\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0596 - acc: 0.5303 - val_loss: 0.0583 - val_acc: 0.5301\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 249us/step - loss: 0.0586 - acc: 0.5443 - val_loss: 0.0589 - val_acc: 0.5281\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0591 - acc: 0.5383 - val_loss: 0.0581 - val_acc: 0.5426\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0589 - acc: 0.5346 - val_loss: 0.0582 - val_acc: 0.5385\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0590 - acc: 0.5397 - val_loss: 0.0583 - val_acc: 0.5364\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0591 - acc: 0.5349 - val_loss: 0.0584 - val_acc: 0.5343\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0594 - acc: 0.5328 - val_loss: 0.0586 - val_acc: 0.5260\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0590 - acc: 0.5321 - val_loss: 0.0583 - val_acc: 0.5447\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0590 - acc: 0.5344 - val_loss: 0.0583 - val_acc: 0.5405\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0588 - acc: 0.5434 - val_loss: 0.0581 - val_acc: 0.5447\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0591 - acc: 0.5390 - val_loss: 0.0581 - val_acc: 0.5447\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0589 - acc: 0.5413 - val_loss: 0.0582 - val_acc: 0.5343\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0584 - acc: 0.5409 - val_loss: 0.0584 - val_acc: 0.5343\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0587 - acc: 0.5460 - val_loss: 0.0588 - val_acc: 0.5239\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0589 - acc: 0.5381 - val_loss: 0.0584 - val_acc: 0.5385\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0588 - acc: 0.5393 - val_loss: 0.0579 - val_acc: 0.5447\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0585 - acc: 0.5430 - val_loss: 0.0585 - val_acc: 0.5343\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0589 - acc: 0.5376 - val_loss: 0.0580 - val_acc: 0.5426\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0588 - acc: 0.5434 - val_loss: 0.0582 - val_acc: 0.5364\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0589 - acc: 0.5400 - val_loss: 0.0582 - val_acc: 0.5343\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0585 - acc: 0.5430 - val_loss: 0.0581 - val_acc: 0.5405\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0588 - acc: 0.5400 - val_loss: 0.0583 - val_acc: 0.5343\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0588 - acc: 0.5386 - val_loss: 0.0580 - val_acc: 0.5322\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0589 - acc: 0.5416 - val_loss: 0.0580 - val_acc: 0.5426\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0592 - acc: 0.5370 - val_loss: 0.0580 - val_acc: 0.5281\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0590 - acc: 0.5376 - val_loss: 0.0581 - val_acc: 0.5405\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0587 - acc: 0.5434 - val_loss: 0.0580 - val_acc: 0.5426\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0587 - acc: 0.5404 - val_loss: 0.0582 - val_acc: 0.5385\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0582 - acc: 0.5411 - val_loss: 0.0584 - val_acc: 0.5426\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0588 - acc: 0.5406 - val_loss: 0.0584 - val_acc: 0.5385\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0591 - acc: 0.5406 - val_loss: 0.0582 - val_acc: 0.5405\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 246us/step - loss: 0.0591 - acc: 0.5356 - val_loss: 0.0579 - val_acc: 0.5426\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0588 - acc: 0.5423 - val_loss: 0.0579 - val_acc: 0.5426\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0587 - acc: 0.5400 - val_loss: 0.0581 - val_acc: 0.5426\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0585 - acc: 0.5402 - val_loss: 0.0578 - val_acc: 0.5364\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0589 - acc: 0.5409 - val_loss: 0.0581 - val_acc: 0.5447\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0588 - acc: 0.5439 - val_loss: 0.0581 - val_acc: 0.5405\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0586 - acc: 0.5409 - val_loss: 0.0582 - val_acc: 0.5385\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0588 - acc: 0.5367 - val_loss: 0.0579 - val_acc: 0.5385\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0592 - acc: 0.5358 - val_loss: 0.0580 - val_acc: 0.5405\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0585 - acc: 0.5393 - val_loss: 0.0581 - val_acc: 0.5301\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0586 - acc: 0.5409 - val_loss: 0.0583 - val_acc: 0.5426\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0590 - acc: 0.5365 - val_loss: 0.0585 - val_acc: 0.5260\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0591 - acc: 0.5353 - val_loss: 0.0562 - val_acc: 0.5717\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0589 - acc: 0.5351 - val_loss: 0.0556 - val_acc: 0.5759\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0588 - acc: 0.5370 - val_loss: 0.0563 - val_acc: 0.5593\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0588 - acc: 0.5404 - val_loss: 0.0560 - val_acc: 0.5759\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0589 - acc: 0.5339 - val_loss: 0.0567 - val_acc: 0.5780\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0591 - acc: 0.5367 - val_loss: 0.0562 - val_acc: 0.5696\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0587 - acc: 0.5367 - val_loss: 0.0591 - val_acc: 0.5530\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0590 - acc: 0.5397 - val_loss: 0.0559 - val_acc: 0.5696\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0590 - acc: 0.5383 - val_loss: 0.0560 - val_acc: 0.5780\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0585 - acc: 0.5400 - val_loss: 0.0557 - val_acc: 0.5634\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0589 - acc: 0.5367 - val_loss: 0.0569 - val_acc: 0.5738\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0591 - acc: 0.5395 - val_loss: 0.0568 - val_acc: 0.5696\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0585 - acc: 0.5395 - val_loss: 0.0566 - val_acc: 0.5676\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0590 - acc: 0.5342 - val_loss: 0.0557 - val_acc: 0.5572\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0593 - acc: 0.5337 - val_loss: 0.0557 - val_acc: 0.5800\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0585 - acc: 0.5383 - val_loss: 0.0556 - val_acc: 0.5717\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0588 - acc: 0.5367 - val_loss: 0.0563 - val_acc: 0.5676\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0587 - acc: 0.5365 - val_loss: 0.0558 - val_acc: 0.5572\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0587 - acc: 0.5346 - val_loss: 0.0560 - val_acc: 0.5717\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0591 - acc: 0.5358 - val_loss: 0.0567 - val_acc: 0.5676\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0583 - acc: 0.5423 - val_loss: 0.0562 - val_acc: 0.5676\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 276us/step - loss: 0.0584 - acc: 0.5393 - val_loss: 0.0570 - val_acc: 0.5655\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 292us/step - loss: 0.0585 - acc: 0.5376 - val_loss: 0.0573 - val_acc: 0.5717\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 295us/step - loss: 0.0588 - acc: 0.5400 - val_loss: 0.0589 - val_acc: 0.5364\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 297us/step - loss: 0.0586 - acc: 0.5423 - val_loss: 0.0565 - val_acc: 0.5717\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 293us/step - loss: 0.0590 - acc: 0.5370 - val_loss: 0.0558 - val_acc: 0.5676\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 295us/step - loss: 0.0588 - acc: 0.5420 - val_loss: 0.0565 - val_acc: 0.5884\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 293us/step - loss: 0.0587 - acc: 0.5370 - val_loss: 0.0565 - val_acc: 0.5634\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 301us/step - loss: 0.0588 - acc: 0.5342 - val_loss: 0.0561 - val_acc: 0.5655\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 254us/step - loss: 0.0583 - acc: 0.5469 - val_loss: 0.0563 - val_acc: 0.5593\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0586 - acc: 0.5333 - val_loss: 0.0566 - val_acc: 0.5613\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0584 - acc: 0.5411 - val_loss: 0.0560 - val_acc: 0.5593\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0589 - acc: 0.5413 - val_loss: 0.0571 - val_acc: 0.5676\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0583 - acc: 0.5397 - val_loss: 0.0570 - val_acc: 0.5676\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0588 - acc: 0.5464 - val_loss: 0.0563 - val_acc: 0.5655\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0588 - acc: 0.5434 - val_loss: 0.0564 - val_acc: 0.5676\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0585 - acc: 0.5439 - val_loss: 0.0569 - val_acc: 0.5613\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0591 - acc: 0.5370 - val_loss: 0.0567 - val_acc: 0.5676\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0590 - acc: 0.5358 - val_loss: 0.0567 - val_acc: 0.5717\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0582 - acc: 0.5402 - val_loss: 0.0570 - val_acc: 0.5821\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0587 - acc: 0.5443 - val_loss: 0.0572 - val_acc: 0.5634\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 246us/step - loss: 0.0586 - acc: 0.5418 - val_loss: 0.0562 - val_acc: 0.5780\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0584 - acc: 0.5471 - val_loss: 0.0568 - val_acc: 0.5800\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0586 - acc: 0.5446 - val_loss: 0.0562 - val_acc: 0.5634\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 248us/step - loss: 0.0588 - acc: 0.5383 - val_loss: 0.0566 - val_acc: 0.5676\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 247us/step - loss: 0.0586 - acc: 0.5441 - val_loss: 0.0573 - val_acc: 0.5613\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 248us/step - loss: 0.0580 - acc: 0.5436 - val_loss: 0.0565 - val_acc: 0.5696\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 249us/step - loss: 0.0587 - acc: 0.5448 - val_loss: 0.0565 - val_acc: 0.5842\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 252us/step - loss: 0.0583 - acc: 0.5478 - val_loss: 0.0560 - val_acc: 0.5759\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 249us/step - loss: 0.0583 - acc: 0.5386 - val_loss: 0.0560 - val_acc: 0.5676\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 248us/step - loss: 0.0583 - acc: 0.5430 - val_loss: 0.0563 - val_acc: 0.5676\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 247us/step - loss: 0.0580 - acc: 0.5455 - val_loss: 0.0555 - val_acc: 0.5676\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 247us/step - loss: 0.0587 - acc: 0.5413 - val_loss: 0.0558 - val_acc: 0.5780\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 246us/step - loss: 0.0587 - acc: 0.5376 - val_loss: 0.0571 - val_acc: 0.5676\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0585 - acc: 0.5370 - val_loss: 0.0564 - val_acc: 0.5738\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0586 - acc: 0.5383 - val_loss: 0.0563 - val_acc: 0.5800\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0584 - acc: 0.5436 - val_loss: 0.0558 - val_acc: 0.5696\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0584 - acc: 0.5471 - val_loss: 0.0575 - val_acc: 0.5613\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0586 - acc: 0.5439 - val_loss: 0.0575 - val_acc: 0.5593\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0584 - acc: 0.5450 - val_loss: 0.0566 - val_acc: 0.5676\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 245us/step - loss: 0.0586 - acc: 0.5446 - val_loss: 0.0564 - val_acc: 0.5821\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0585 - acc: 0.5430 - val_loss: 0.0558 - val_acc: 0.5738\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0583 - acc: 0.5386 - val_loss: 0.0568 - val_acc: 0.5634\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0587 - acc: 0.5404 - val_loss: 0.0556 - val_acc: 0.5696\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0582 - acc: 0.5432 - val_loss: 0.0568 - val_acc: 0.5551\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0583 - acc: 0.5360 - val_loss: 0.0579 - val_acc: 0.5676\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0584 - acc: 0.5420 - val_loss: 0.0566 - val_acc: 0.5780\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0584 - acc: 0.5425 - val_loss: 0.0568 - val_acc: 0.5634\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0584 - acc: 0.5476 - val_loss: 0.0560 - val_acc: 0.5800\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 245us/step - loss: 0.0584 - acc: 0.5506 - val_loss: 0.0561 - val_acc: 0.5842\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0589 - acc: 0.5356 - val_loss: 0.0562 - val_acc: 0.5676\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0584 - acc: 0.5425 - val_loss: 0.0562 - val_acc: 0.5759\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0583 - acc: 0.5432 - val_loss: 0.0559 - val_acc: 0.5863\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0584 - acc: 0.5425 - val_loss: 0.0566 - val_acc: 0.5696\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0583 - acc: 0.5464 - val_loss: 0.0600 - val_acc: 0.5385\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0584 - acc: 0.5453 - val_loss: 0.0557 - val_acc: 0.5634\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0581 - acc: 0.5455 - val_loss: 0.0567 - val_acc: 0.5696\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0581 - acc: 0.5464 - val_loss: 0.0576 - val_acc: 0.5717\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0590 - acc: 0.5413 - val_loss: 0.0568 - val_acc: 0.5759\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0582 - acc: 0.5448 - val_loss: 0.0558 - val_acc: 0.5738\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0581 - acc: 0.5457 - val_loss: 0.0571 - val_acc: 0.5780\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0586 - acc: 0.5370 - val_loss: 0.0567 - val_acc: 0.5821\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0581 - acc: 0.5476 - val_loss: 0.0559 - val_acc: 0.5717\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0581 - acc: 0.5499 - val_loss: 0.0569 - val_acc: 0.5593\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0583 - acc: 0.5411 - val_loss: 0.0567 - val_acc: 0.5717\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0587 - acc: 0.5439 - val_loss: 0.0562 - val_acc: 0.5634\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0583 - acc: 0.5434 - val_loss: 0.0565 - val_acc: 0.5593\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0581 - acc: 0.5448 - val_loss: 0.0569 - val_acc: 0.5613\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0584 - acc: 0.5508 - val_loss: 0.0566 - val_acc: 0.5634\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0585 - acc: 0.5427 - val_loss: 0.0568 - val_acc: 0.5759\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0587 - acc: 0.5455 - val_loss: 0.0562 - val_acc: 0.5676\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0585 - acc: 0.5404 - val_loss: 0.0558 - val_acc: 0.5696\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0584 - acc: 0.5400 - val_loss: 0.0568 - val_acc: 0.5717\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 245us/step - loss: 0.0585 - acc: 0.5469 - val_loss: 0.0563 - val_acc: 0.5738\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0582 - acc: 0.5386 - val_loss: 0.0555 - val_acc: 0.5821\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0584 - acc: 0.5473 - val_loss: 0.0565 - val_acc: 0.5676\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0587 - acc: 0.5448 - val_loss: 0.0566 - val_acc: 0.5634\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 271us/step - loss: 0.0582 - acc: 0.5478 - val_loss: 0.0566 - val_acc: 0.5634\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 292us/step - loss: 0.0582 - acc: 0.5520 - val_loss: 0.0568 - val_acc: 0.5717\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 295us/step - loss: 0.0581 - acc: 0.5471 - val_loss: 0.0561 - val_acc: 0.5696\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 291us/step - loss: 0.0582 - acc: 0.5473 - val_loss: 0.0565 - val_acc: 0.5426\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 271us/step - loss: 0.0583 - acc: 0.5527 - val_loss: 0.0565 - val_acc: 0.5426\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0581 - acc: 0.5478 - val_loss: 0.0565 - val_acc: 0.5509\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0579 - acc: 0.5559 - val_loss: 0.0566 - val_acc: 0.5468\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0581 - acc: 0.5527 - val_loss: 0.0568 - val_acc: 0.5551\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0583 - acc: 0.5494 - val_loss: 0.0570 - val_acc: 0.5489\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0583 - acc: 0.5497 - val_loss: 0.0572 - val_acc: 0.5281\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0586 - acc: 0.5476 - val_loss: 0.0572 - val_acc: 0.5551\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0579 - acc: 0.5570 - val_loss: 0.0571 - val_acc: 0.5509\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0583 - acc: 0.5520 - val_loss: 0.0573 - val_acc: 0.5239\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0578 - acc: 0.5515 - val_loss: 0.0571 - val_acc: 0.5489\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0583 - acc: 0.5520 - val_loss: 0.0569 - val_acc: 0.5405\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0584 - acc: 0.5487 - val_loss: 0.0576 - val_acc: 0.5385\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0582 - acc: 0.5476 - val_loss: 0.0573 - val_acc: 0.5468\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0580 - acc: 0.5587 - val_loss: 0.0573 - val_acc: 0.5447\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0582 - acc: 0.5499 - val_loss: 0.0574 - val_acc: 0.5468\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0581 - acc: 0.5499 - val_loss: 0.0565 - val_acc: 0.5572\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0580 - acc: 0.5550 - val_loss: 0.0581 - val_acc: 0.5343\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0579 - acc: 0.5536 - val_loss: 0.0570 - val_acc: 0.5593\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0582 - acc: 0.5471 - val_loss: 0.0572 - val_acc: 0.5385\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0581 - acc: 0.5545 - val_loss: 0.0573 - val_acc: 0.5343\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0581 - acc: 0.5467 - val_loss: 0.0575 - val_acc: 0.5551\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0587 - acc: 0.5494 - val_loss: 0.0572 - val_acc: 0.5426\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 226us/step - loss: 0.0582 - acc: 0.5506 - val_loss: 0.0577 - val_acc: 0.5322\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0584 - acc: 0.5483 - val_loss: 0.0573 - val_acc: 0.5364\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 227us/step - loss: 0.0587 - acc: 0.5441 - val_loss: 0.0573 - val_acc: 0.5468\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0582 - acc: 0.5473 - val_loss: 0.0574 - val_acc: 0.5509\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0579 - acc: 0.5582 - val_loss: 0.0571 - val_acc: 0.5509\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0580 - acc: 0.5478 - val_loss: 0.0576 - val_acc: 0.5447\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 227us/step - loss: 0.0581 - acc: 0.5508 - val_loss: 0.0567 - val_acc: 0.5551\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0582 - acc: 0.5554 - val_loss: 0.0574 - val_acc: 0.5364\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0581 - acc: 0.5499 - val_loss: 0.0571 - val_acc: 0.5572\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0576 - acc: 0.5598 - val_loss: 0.0571 - val_acc: 0.5530\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0583 - acc: 0.5439 - val_loss: 0.0573 - val_acc: 0.5405\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0583 - acc: 0.5536 - val_loss: 0.0576 - val_acc: 0.5489\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0581 - acc: 0.5520 - val_loss: 0.0573 - val_acc: 0.5489\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0582 - acc: 0.5545 - val_loss: 0.0580 - val_acc: 0.5447\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0582 - acc: 0.5543 - val_loss: 0.0569 - val_acc: 0.5551\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0580 - acc: 0.5538 - val_loss: 0.0574 - val_acc: 0.5322\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0581 - acc: 0.5531 - val_loss: 0.0571 - val_acc: 0.5489\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0582 - acc: 0.5480 - val_loss: 0.0577 - val_acc: 0.5385\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0582 - acc: 0.5464 - val_loss: 0.0575 - val_acc: 0.5509\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0583 - acc: 0.5520 - val_loss: 0.0573 - val_acc: 0.5634\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0581 - acc: 0.5568 - val_loss: 0.0577 - val_acc: 0.5572\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0577 - acc: 0.5642 - val_loss: 0.0573 - val_acc: 0.5530\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0579 - acc: 0.5570 - val_loss: 0.0574 - val_acc: 0.5426\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0580 - acc: 0.5557 - val_loss: 0.0574 - val_acc: 0.5509\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0584 - acc: 0.5483 - val_loss: 0.0571 - val_acc: 0.5426\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0580 - acc: 0.5513 - val_loss: 0.0573 - val_acc: 0.5530\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0586 - acc: 0.5497 - val_loss: 0.0574 - val_acc: 0.5530\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0580 - acc: 0.5570 - val_loss: 0.0572 - val_acc: 0.5572\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0579 - acc: 0.5529 - val_loss: 0.0570 - val_acc: 0.5530\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0578 - acc: 0.5513 - val_loss: 0.0572 - val_acc: 0.5489\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0577 - acc: 0.5561 - val_loss: 0.0577 - val_acc: 0.5468\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0578 - acc: 0.5564 - val_loss: 0.0572 - val_acc: 0.5509\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0580 - acc: 0.5566 - val_loss: 0.0578 - val_acc: 0.5530\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0584 - acc: 0.5492 - val_loss: 0.0571 - val_acc: 0.5593\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0577 - acc: 0.5508 - val_loss: 0.0572 - val_acc: 0.5509\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0581 - acc: 0.5510 - val_loss: 0.0575 - val_acc: 0.5509\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0580 - acc: 0.5545 - val_loss: 0.0572 - val_acc: 0.5551\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0578 - acc: 0.5545 - val_loss: 0.0576 - val_acc: 0.5572\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0577 - acc: 0.5612 - val_loss: 0.0573 - val_acc: 0.5613\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0580 - acc: 0.5513 - val_loss: 0.0574 - val_acc: 0.5509\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0584 - acc: 0.5446 - val_loss: 0.0574 - val_acc: 0.5468\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0577 - acc: 0.5517 - val_loss: 0.0579 - val_acc: 0.5447\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0581 - acc: 0.5557 - val_loss: 0.0572 - val_acc: 0.5509\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0578 - acc: 0.5568 - val_loss: 0.0575 - val_acc: 0.5447\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0585 - acc: 0.5527 - val_loss: 0.0573 - val_acc: 0.5613\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0577 - acc: 0.5584 - val_loss: 0.0574 - val_acc: 0.5468\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0580 - acc: 0.5568 - val_loss: 0.0573 - val_acc: 0.5551\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0580 - acc: 0.5582 - val_loss: 0.0578 - val_acc: 0.5509\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0579 - acc: 0.5508 - val_loss: 0.0573 - val_acc: 0.5593\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0582 - acc: 0.5448 - val_loss: 0.0582 - val_acc: 0.5281\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0576 - acc: 0.5559 - val_loss: 0.0575 - val_acc: 0.5468\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0584 - acc: 0.5497 - val_loss: 0.0582 - val_acc: 0.5426\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0579 - acc: 0.5594 - val_loss: 0.0577 - val_acc: 0.5447\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 245us/step - loss: 0.0582 - acc: 0.5524 - val_loss: 0.0576 - val_acc: 0.5426\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0575 - acc: 0.5598 - val_loss: 0.0575 - val_acc: 0.5572\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0580 - acc: 0.5527 - val_loss: 0.0577 - val_acc: 0.5468\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0581 - acc: 0.5483 - val_loss: 0.0575 - val_acc: 0.5447\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0574 - acc: 0.5575 - val_loss: 0.0578 - val_acc: 0.5468\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0581 - acc: 0.5513 - val_loss: 0.0575 - val_acc: 0.5426\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0578 - acc: 0.5513 - val_loss: 0.0578 - val_acc: 0.5489\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0578 - acc: 0.5596 - val_loss: 0.0577 - val_acc: 0.5447\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0577 - acc: 0.5582 - val_loss: 0.0578 - val_acc: 0.5468\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0574 - acc: 0.5600 - val_loss: 0.0575 - val_acc: 0.5530\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0578 - acc: 0.5570 - val_loss: 0.0578 - val_acc: 0.5489\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0578 - acc: 0.5540 - val_loss: 0.0573 - val_acc: 0.5551\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0578 - acc: 0.5559 - val_loss: 0.0574 - val_acc: 0.5509\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0578 - acc: 0.5577 - val_loss: 0.0577 - val_acc: 0.5593\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0579 - acc: 0.5547 - val_loss: 0.0576 - val_acc: 0.5530\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0574 - acc: 0.5550 - val_loss: 0.0573 - val_acc: 0.5405\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0578 - acc: 0.5520 - val_loss: 0.0574 - val_acc: 0.5468\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0576 - acc: 0.5554 - val_loss: 0.0576 - val_acc: 0.5634\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0579 - acc: 0.5582 - val_loss: 0.0574 - val_acc: 0.5551\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0580 - acc: 0.5561 - val_loss: 0.0580 - val_acc: 0.5593\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0581 - acc: 0.5564 - val_loss: 0.0575 - val_acc: 0.5530\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0580 - acc: 0.5554 - val_loss: 0.0572 - val_acc: 0.5634\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0579 - acc: 0.5564 - val_loss: 0.0579 - val_acc: 0.5489\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0577 - acc: 0.5573 - val_loss: 0.0572 - val_acc: 0.5613\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0580 - acc: 0.5515 - val_loss: 0.0572 - val_acc: 0.5676\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0578 - acc: 0.5533 - val_loss: 0.0573 - val_acc: 0.5509\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0579 - acc: 0.5497 - val_loss: 0.0577 - val_acc: 0.5509\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0574 - acc: 0.5552 - val_loss: 0.0579 - val_acc: 0.5593\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0578 - acc: 0.5508 - val_loss: 0.0576 - val_acc: 0.5655\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0573 - acc: 0.5619 - val_loss: 0.0576 - val_acc: 0.5655\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0579 - acc: 0.5527 - val_loss: 0.0577 - val_acc: 0.5655\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0577 - acc: 0.5575 - val_loss: 0.0580 - val_acc: 0.5551\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0580 - acc: 0.5545 - val_loss: 0.0582 - val_acc: 0.5572\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0578 - acc: 0.5529 - val_loss: 0.0580 - val_acc: 0.5613\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0578 - acc: 0.5561 - val_loss: 0.0581 - val_acc: 0.5634\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0582 - acc: 0.5425 - val_loss: 0.0579 - val_acc: 0.5572\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0579 - acc: 0.5533 - val_loss: 0.0578 - val_acc: 0.5634\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0581 - acc: 0.5427 - val_loss: 0.0579 - val_acc: 0.5468\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 227us/step - loss: 0.0577 - acc: 0.5538 - val_loss: 0.0580 - val_acc: 0.5655\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0575 - acc: 0.5552 - val_loss: 0.0580 - val_acc: 0.5572\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0577 - acc: 0.5598 - val_loss: 0.0579 - val_acc: 0.5572\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0575 - acc: 0.5568 - val_loss: 0.0582 - val_acc: 0.5634\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0573 - acc: 0.5610 - val_loss: 0.0585 - val_acc: 0.5468\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0579 - acc: 0.5536 - val_loss: 0.0582 - val_acc: 0.5530\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0574 - acc: 0.5582 - val_loss: 0.0580 - val_acc: 0.5509\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0577 - acc: 0.5536 - val_loss: 0.0581 - val_acc: 0.5593\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0580 - acc: 0.5566 - val_loss: 0.0590 - val_acc: 0.5509\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0575 - acc: 0.5591 - val_loss: 0.0589 - val_acc: 0.5385\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0576 - acc: 0.5589 - val_loss: 0.0584 - val_acc: 0.5572\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0572 - acc: 0.5651 - val_loss: 0.0585 - val_acc: 0.5551\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0577 - acc: 0.5570 - val_loss: 0.0581 - val_acc: 0.5593\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0578 - acc: 0.5533 - val_loss: 0.0580 - val_acc: 0.5593\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0578 - acc: 0.5566 - val_loss: 0.0583 - val_acc: 0.5613\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0576 - acc: 0.5584 - val_loss: 0.0582 - val_acc: 0.5572\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0575 - acc: 0.5547 - val_loss: 0.0578 - val_acc: 0.5551\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0575 - acc: 0.5564 - val_loss: 0.0583 - val_acc: 0.5551\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0575 - acc: 0.5614 - val_loss: 0.0580 - val_acc: 0.5509\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0575 - acc: 0.5621 - val_loss: 0.0586 - val_acc: 0.5551\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0576 - acc: 0.5610 - val_loss: 0.0585 - val_acc: 0.5530\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0577 - acc: 0.5554 - val_loss: 0.0579 - val_acc: 0.5551\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0577 - acc: 0.5545 - val_loss: 0.0582 - val_acc: 0.5551\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0575 - acc: 0.5584 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0576 - acc: 0.5619 - val_loss: 0.0585 - val_acc: 0.5426\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0575 - acc: 0.5559 - val_loss: 0.0583 - val_acc: 0.5634\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0579 - acc: 0.5584 - val_loss: 0.0586 - val_acc: 0.5551\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0572 - acc: 0.5624 - val_loss: 0.0587 - val_acc: 0.5593\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0570 - acc: 0.5580 - val_loss: 0.0584 - val_acc: 0.5593\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0577 - acc: 0.5533 - val_loss: 0.0587 - val_acc: 0.5468\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0578 - acc: 0.5520 - val_loss: 0.0589 - val_acc: 0.5509\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0577 - acc: 0.5570 - val_loss: 0.0585 - val_acc: 0.5613\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0578 - acc: 0.5552 - val_loss: 0.0592 - val_acc: 0.5489\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0574 - acc: 0.5603 - val_loss: 0.0587 - val_acc: 0.5593\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0577 - acc: 0.5628 - val_loss: 0.0583 - val_acc: 0.5530\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0572 - acc: 0.5656 - val_loss: 0.0582 - val_acc: 0.5551\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0576 - acc: 0.5612 - val_loss: 0.0584 - val_acc: 0.5572\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0577 - acc: 0.5610 - val_loss: 0.0584 - val_acc: 0.5634\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0573 - acc: 0.5573 - val_loss: 0.0582 - val_acc: 0.5489\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0580 - acc: 0.5580 - val_loss: 0.0583 - val_acc: 0.5489\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0578 - acc: 0.5547 - val_loss: 0.0581 - val_acc: 0.5489\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0579 - acc: 0.5554 - val_loss: 0.0585 - val_acc: 0.5551\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0573 - acc: 0.5598 - val_loss: 0.0587 - val_acc: 0.5551\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0573 - acc: 0.5598 - val_loss: 0.0586 - val_acc: 0.5489\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0576 - acc: 0.5573 - val_loss: 0.0580 - val_acc: 0.5489\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0579 - acc: 0.5554 - val_loss: 0.0583 - val_acc: 0.5530\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0576 - acc: 0.5520 - val_loss: 0.0593 - val_acc: 0.5468\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0574 - acc: 0.5596 - val_loss: 0.0592 - val_acc: 0.5447\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0578 - acc: 0.5524 - val_loss: 0.0581 - val_acc: 0.5676\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0573 - acc: 0.5582 - val_loss: 0.0581 - val_acc: 0.5509\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0578 - acc: 0.5547 - val_loss: 0.0579 - val_acc: 0.5634\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0576 - acc: 0.5577 - val_loss: 0.0582 - val_acc: 0.5530\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0575 - acc: 0.5658 - val_loss: 0.0581 - val_acc: 0.5572\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0575 - acc: 0.5568 - val_loss: 0.0582 - val_acc: 0.5593\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0575 - acc: 0.5552 - val_loss: 0.0584 - val_acc: 0.5509\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0576 - acc: 0.5543 - val_loss: 0.0587 - val_acc: 0.5301\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0574 - acc: 0.5654 - val_loss: 0.0584 - val_acc: 0.5530\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0575 - acc: 0.5520 - val_loss: 0.0587 - val_acc: 0.5509\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0577 - acc: 0.5594 - val_loss: 0.0589 - val_acc: 0.5468\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0574 - acc: 0.5654 - val_loss: 0.0585 - val_acc: 0.5593\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0573 - acc: 0.5700 - val_loss: 0.0582 - val_acc: 0.5551\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0576 - acc: 0.5587 - val_loss: 0.0581 - val_acc: 0.5509\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0575 - acc: 0.5630 - val_loss: 0.0589 - val_acc: 0.5426\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0576 - acc: 0.5589 - val_loss: 0.0581 - val_acc: 0.5468\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0572 - acc: 0.5633 - val_loss: 0.0583 - val_acc: 0.5551\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0578 - acc: 0.5559 - val_loss: 0.0583 - val_acc: 0.5696\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0573 - acc: 0.5557 - val_loss: 0.0589 - val_acc: 0.5468\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0575 - acc: 0.5582 - val_loss: 0.0583 - val_acc: 0.5426\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 247us/step - loss: 0.0573 - acc: 0.5600 - val_loss: 0.0589 - val_acc: 0.5530\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0575 - acc: 0.5538 - val_loss: 0.0588 - val_acc: 0.5385\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0574 - acc: 0.5540 - val_loss: 0.0584 - val_acc: 0.5530\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0576 - acc: 0.5607 - val_loss: 0.0589 - val_acc: 0.5655\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0574 - acc: 0.5570 - val_loss: 0.0580 - val_acc: 0.5468\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0574 - acc: 0.5584 - val_loss: 0.0582 - val_acc: 0.5613\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0575 - acc: 0.5610 - val_loss: 0.0582 - val_acc: 0.5447\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0573 - acc: 0.5607 - val_loss: 0.0583 - val_acc: 0.5509\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0572 - acc: 0.5566 - val_loss: 0.0584 - val_acc: 0.5551\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0580 - acc: 0.5545 - val_loss: 0.0578 - val_acc: 0.5489\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0578 - acc: 0.5640 - val_loss: 0.0584 - val_acc: 0.5530\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0573 - acc: 0.5596 - val_loss: 0.0583 - val_acc: 0.5509\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0570 - acc: 0.5607 - val_loss: 0.0594 - val_acc: 0.5489\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0575 - acc: 0.5545 - val_loss: 0.0583 - val_acc: 0.5530\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0575 - acc: 0.5554 - val_loss: 0.0590 - val_acc: 0.5530\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0574 - acc: 0.5637 - val_loss: 0.0592 - val_acc: 0.5385\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0574 - acc: 0.5605 - val_loss: 0.0588 - val_acc: 0.5551\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0575 - acc: 0.5642 - val_loss: 0.0589 - val_acc: 0.5593\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0570 - acc: 0.5642 - val_loss: 0.0586 - val_acc: 0.5530\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0574 - acc: 0.5573 - val_loss: 0.0586 - val_acc: 0.5468\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0575 - acc: 0.5584 - val_loss: 0.0589 - val_acc: 0.5468\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0574 - acc: 0.5635 - val_loss: 0.0589 - val_acc: 0.5447\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0574 - acc: 0.5621 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0571 - acc: 0.5610 - val_loss: 0.0592 - val_acc: 0.5364\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0569 - acc: 0.5566 - val_loss: 0.0592 - val_acc: 0.5447\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0576 - acc: 0.5603 - val_loss: 0.0595 - val_acc: 0.5489\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0574 - acc: 0.5545 - val_loss: 0.0592 - val_acc: 0.5447\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0572 - acc: 0.5619 - val_loss: 0.0589 - val_acc: 0.5447\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0574 - acc: 0.5577 - val_loss: 0.0590 - val_acc: 0.5468\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0573 - acc: 0.5584 - val_loss: 0.0586 - val_acc: 0.5426\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0570 - acc: 0.5637 - val_loss: 0.0588 - val_acc: 0.5426\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0571 - acc: 0.5584 - val_loss: 0.0597 - val_acc: 0.5447\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0573 - acc: 0.5580 - val_loss: 0.0590 - val_acc: 0.5343\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0572 - acc: 0.5610 - val_loss: 0.0597 - val_acc: 0.5385\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0572 - acc: 0.5612 - val_loss: 0.0594 - val_acc: 0.5426\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0572 - acc: 0.5654 - val_loss: 0.0598 - val_acc: 0.5322\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0572 - acc: 0.5644 - val_loss: 0.0600 - val_acc: 0.5385\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0572 - acc: 0.5630 - val_loss: 0.0593 - val_acc: 0.5385\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0575 - acc: 0.5670 - val_loss: 0.0595 - val_acc: 0.5364\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 283us/step - loss: 0.0570 - acc: 0.5644 - val_loss: 0.0599 - val_acc: 0.5322\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 290us/step - loss: 0.0574 - acc: 0.5603 - val_loss: 0.0598 - val_acc: 0.5426\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 293us/step - loss: 0.0571 - acc: 0.5626 - val_loss: 0.0598 - val_acc: 0.5447\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 289us/step - loss: 0.0572 - acc: 0.5640 - val_loss: 0.0597 - val_acc: 0.5322\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 292us/step - loss: 0.0570 - acc: 0.5630 - val_loss: 0.0601 - val_acc: 0.5385\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 291us/step - loss: 0.0571 - acc: 0.5591 - val_loss: 0.0603 - val_acc: 0.5405\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 294us/step - loss: 0.0574 - acc: 0.5582 - val_loss: 0.0599 - val_acc: 0.5426\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 292us/step - loss: 0.0570 - acc: 0.5543 - val_loss: 0.0601 - val_acc: 0.5364\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0571 - acc: 0.5624 - val_loss: 0.0601 - val_acc: 0.5405\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 226us/step - loss: 0.0574 - acc: 0.5610 - val_loss: 0.0601 - val_acc: 0.5281\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0572 - acc: 0.5658 - val_loss: 0.0607 - val_acc: 0.5343\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0574 - acc: 0.5591 - val_loss: 0.0603 - val_acc: 0.5322\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0575 - acc: 0.5640 - val_loss: 0.0599 - val_acc: 0.5301\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 224us/step - loss: 0.0568 - acc: 0.5700 - val_loss: 0.0600 - val_acc: 0.5343\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 227us/step - loss: 0.0570 - acc: 0.5612 - val_loss: 0.0595 - val_acc: 0.5489\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0574 - acc: 0.5607 - val_loss: 0.0598 - val_acc: 0.5364\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 225us/step - loss: 0.0574 - acc: 0.5587 - val_loss: 0.0599 - val_acc: 0.5218\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0570 - acc: 0.5626 - val_loss: 0.0605 - val_acc: 0.5281\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0574 - acc: 0.5624 - val_loss: 0.0600 - val_acc: 0.5385\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0572 - acc: 0.5619 - val_loss: 0.0607 - val_acc: 0.5281\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0571 - acc: 0.5612 - val_loss: 0.0603 - val_acc: 0.5426\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0573 - acc: 0.5621 - val_loss: 0.0603 - val_acc: 0.5301\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0575 - acc: 0.5545 - val_loss: 0.0605 - val_acc: 0.5281\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0569 - acc: 0.5630 - val_loss: 0.0599 - val_acc: 0.5385\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0569 - acc: 0.5651 - val_loss: 0.0600 - val_acc: 0.5301\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0571 - acc: 0.5642 - val_loss: 0.0603 - val_acc: 0.5364\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0574 - acc: 0.5566 - val_loss: 0.0603 - val_acc: 0.5156\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0571 - acc: 0.5635 - val_loss: 0.0606 - val_acc: 0.5343\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0572 - acc: 0.5568 - val_loss: 0.0599 - val_acc: 0.5322\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0577 - acc: 0.5667 - val_loss: 0.0602 - val_acc: 0.5239\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0573 - acc: 0.5617 - val_loss: 0.0609 - val_acc: 0.5114\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0574 - acc: 0.5594 - val_loss: 0.0613 - val_acc: 0.5198\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0570 - acc: 0.5651 - val_loss: 0.0602 - val_acc: 0.5239\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0572 - acc: 0.5617 - val_loss: 0.0603 - val_acc: 0.5239\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0571 - acc: 0.5621 - val_loss: 0.0603 - val_acc: 0.5343\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0572 - acc: 0.5628 - val_loss: 0.0602 - val_acc: 0.5218\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0572 - acc: 0.5649 - val_loss: 0.0602 - val_acc: 0.5260\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0570 - acc: 0.5635 - val_loss: 0.0607 - val_acc: 0.5239\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0568 - acc: 0.5686 - val_loss: 0.0605 - val_acc: 0.5198\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0571 - acc: 0.5621 - val_loss: 0.0601 - val_acc: 0.5301\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0575 - acc: 0.5596 - val_loss: 0.0608 - val_acc: 0.5156\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0569 - acc: 0.5640 - val_loss: 0.0610 - val_acc: 0.5281\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0571 - acc: 0.5663 - val_loss: 0.0601 - val_acc: 0.5281\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0574 - acc: 0.5564 - val_loss: 0.0604 - val_acc: 0.5281\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0568 - acc: 0.5672 - val_loss: 0.0605 - val_acc: 0.5281\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0569 - acc: 0.5621 - val_loss: 0.0600 - val_acc: 0.5343\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 227us/step - loss: 0.0574 - acc: 0.5626 - val_loss: 0.0607 - val_acc: 0.5364\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0571 - acc: 0.5587 - val_loss: 0.0600 - val_acc: 0.5322\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0571 - acc: 0.5610 - val_loss: 0.0607 - val_acc: 0.5239\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0571 - acc: 0.5621 - val_loss: 0.0602 - val_acc: 0.5218\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0576 - acc: 0.5550 - val_loss: 0.0605 - val_acc: 0.5260\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0567 - acc: 0.5640 - val_loss: 0.0611 - val_acc: 0.4969\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0571 - acc: 0.5628 - val_loss: 0.0606 - val_acc: 0.5281\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0572 - acc: 0.5605 - val_loss: 0.0607 - val_acc: 0.5343\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0574 - acc: 0.5568 - val_loss: 0.0605 - val_acc: 0.5301\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 226us/step - loss: 0.0571 - acc: 0.5621 - val_loss: 0.0603 - val_acc: 0.5322\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0571 - acc: 0.5614 - val_loss: 0.0607 - val_acc: 0.5218\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0574 - acc: 0.5624 - val_loss: 0.0605 - val_acc: 0.5260\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0569 - acc: 0.5612 - val_loss: 0.0607 - val_acc: 0.5260\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0569 - acc: 0.5584 - val_loss: 0.0609 - val_acc: 0.5281\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0573 - acc: 0.5568 - val_loss: 0.0609 - val_acc: 0.5177\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0572 - acc: 0.5612 - val_loss: 0.0607 - val_acc: 0.5239\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0573 - acc: 0.5591 - val_loss: 0.0599 - val_acc: 0.5260\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0567 - acc: 0.5647 - val_loss: 0.0601 - val_acc: 0.5322\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0566 - acc: 0.5658 - val_loss: 0.0605 - val_acc: 0.5281\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0571 - acc: 0.5621 - val_loss: 0.0601 - val_acc: 0.5260\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0570 - acc: 0.5614 - val_loss: 0.0604 - val_acc: 0.5343\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0573 - acc: 0.5584 - val_loss: 0.0609 - val_acc: 0.5198\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0569 - acc: 0.5559 - val_loss: 0.0603 - val_acc: 0.5281\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0572 - acc: 0.5603 - val_loss: 0.0598 - val_acc: 0.5281\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0570 - acc: 0.5624 - val_loss: 0.0605 - val_acc: 0.5218\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0566 - acc: 0.5677 - val_loss: 0.0614 - val_acc: 0.5198\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0572 - acc: 0.5614 - val_loss: 0.0611 - val_acc: 0.5239\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0570 - acc: 0.5612 - val_loss: 0.0610 - val_acc: 0.5239\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 273us/step - loss: 0.0571 - acc: 0.5624 - val_loss: 0.0605 - val_acc: 0.5260\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 293us/step - loss: 0.0572 - acc: 0.5630 - val_loss: 0.0598 - val_acc: 0.5322\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 295us/step - loss: 0.0568 - acc: 0.5684 - val_loss: 0.0607 - val_acc: 0.5343\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 297us/step - loss: 0.0564 - acc: 0.5688 - val_loss: 0.0611 - val_acc: 0.5177\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 262us/step - loss: 0.0572 - acc: 0.5614 - val_loss: 0.0610 - val_acc: 0.5135\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0576 - acc: 0.5582 - val_loss: 0.0536 - val_acc: 0.6029\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0576 - acc: 0.5564 - val_loss: 0.0539 - val_acc: 0.5988\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0578 - acc: 0.5587 - val_loss: 0.0537 - val_acc: 0.5904\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0573 - acc: 0.5654 - val_loss: 0.0541 - val_acc: 0.5967\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 246us/step - loss: 0.0581 - acc: 0.5497 - val_loss: 0.0542 - val_acc: 0.6050\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0576 - acc: 0.5554 - val_loss: 0.0538 - val_acc: 0.5988\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0578 - acc: 0.5510 - val_loss: 0.0540 - val_acc: 0.5904\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0576 - acc: 0.5550 - val_loss: 0.0539 - val_acc: 0.5925\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0578 - acc: 0.5573 - val_loss: 0.0543 - val_acc: 0.5946\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0578 - acc: 0.5527 - val_loss: 0.0544 - val_acc: 0.5884\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0575 - acc: 0.5605 - val_loss: 0.0537 - val_acc: 0.5925\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0579 - acc: 0.5566 - val_loss: 0.0541 - val_acc: 0.5967\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0574 - acc: 0.5610 - val_loss: 0.0542 - val_acc: 0.5925\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0574 - acc: 0.5587 - val_loss: 0.0536 - val_acc: 0.5925\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0574 - acc: 0.5547 - val_loss: 0.0540 - val_acc: 0.5946\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0575 - acc: 0.5547 - val_loss: 0.0545 - val_acc: 0.5925\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0576 - acc: 0.5561 - val_loss: 0.0542 - val_acc: 0.5967\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0576 - acc: 0.5598 - val_loss: 0.0544 - val_acc: 0.5967\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0581 - acc: 0.5536 - val_loss: 0.0541 - val_acc: 0.5946\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0578 - acc: 0.5580 - val_loss: 0.0544 - val_acc: 0.5967\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0578 - acc: 0.5564 - val_loss: 0.0541 - val_acc: 0.5946\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0578 - acc: 0.5527 - val_loss: 0.0543 - val_acc: 0.5925\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0573 - acc: 0.5584 - val_loss: 0.0550 - val_acc: 0.5904\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0574 - acc: 0.5580 - val_loss: 0.0542 - val_acc: 0.5904\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0574 - acc: 0.5559 - val_loss: 0.0543 - val_acc: 0.5925\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0574 - acc: 0.5582 - val_loss: 0.0545 - val_acc: 0.5904\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0576 - acc: 0.5570 - val_loss: 0.0542 - val_acc: 0.5759\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0574 - acc: 0.5568 - val_loss: 0.0541 - val_acc: 0.5904\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0577 - acc: 0.5536 - val_loss: 0.0544 - val_acc: 0.5884\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0575 - acc: 0.5617 - val_loss: 0.0548 - val_acc: 0.5821\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0577 - acc: 0.5543 - val_loss: 0.0549 - val_acc: 0.5717\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0570 - acc: 0.5589 - val_loss: 0.0548 - val_acc: 0.5925\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0574 - acc: 0.5573 - val_loss: 0.0547 - val_acc: 0.5946\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0576 - acc: 0.5552 - val_loss: 0.0547 - val_acc: 0.5842\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0573 - acc: 0.5596 - val_loss: 0.0553 - val_acc: 0.5925\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0576 - acc: 0.5503 - val_loss: 0.0545 - val_acc: 0.5904\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0574 - acc: 0.5587 - val_loss: 0.0545 - val_acc: 0.5904\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0577 - acc: 0.5527 - val_loss: 0.0545 - val_acc: 0.5863\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0573 - acc: 0.5577 - val_loss: 0.0544 - val_acc: 0.5863\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0574 - acc: 0.5566 - val_loss: 0.0550 - val_acc: 0.5821\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0577 - acc: 0.5543 - val_loss: 0.0544 - val_acc: 0.5842\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0571 - acc: 0.5591 - val_loss: 0.0546 - val_acc: 0.5821\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0576 - acc: 0.5580 - val_loss: 0.0549 - val_acc: 0.5759\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0576 - acc: 0.5533 - val_loss: 0.0545 - val_acc: 0.5780\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0575 - acc: 0.5594 - val_loss: 0.0545 - val_acc: 0.5863\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0572 - acc: 0.5577 - val_loss: 0.0552 - val_acc: 0.5800\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0570 - acc: 0.5651 - val_loss: 0.0546 - val_acc: 0.5863\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0575 - acc: 0.5543 - val_loss: 0.0545 - val_acc: 0.5821\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0572 - acc: 0.5540 - val_loss: 0.0548 - val_acc: 0.5780\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0574 - acc: 0.5545 - val_loss: 0.0543 - val_acc: 0.5800\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0573 - acc: 0.5667 - val_loss: 0.0547 - val_acc: 0.5800\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0572 - acc: 0.5596 - val_loss: 0.0546 - val_acc: 0.5884\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0573 - acc: 0.5577 - val_loss: 0.0547 - val_acc: 0.5821\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0575 - acc: 0.5605 - val_loss: 0.0545 - val_acc: 0.5842\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0573 - acc: 0.5547 - val_loss: 0.0549 - val_acc: 0.5800\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0575 - acc: 0.5547 - val_loss: 0.0548 - val_acc: 0.5904\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0573 - acc: 0.5596 - val_loss: 0.0547 - val_acc: 0.5821\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0575 - acc: 0.5587 - val_loss: 0.0551 - val_acc: 0.5800\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0574 - acc: 0.5524 - val_loss: 0.0547 - val_acc: 0.5884\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0570 - acc: 0.5598 - val_loss: 0.0548 - val_acc: 0.5884\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0575 - acc: 0.5591 - val_loss: 0.0548 - val_acc: 0.5821\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0572 - acc: 0.5580 - val_loss: 0.0546 - val_acc: 0.5738\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0573 - acc: 0.5564 - val_loss: 0.0548 - val_acc: 0.5780\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0578 - acc: 0.5515 - val_loss: 0.0550 - val_acc: 0.5863\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0574 - acc: 0.5536 - val_loss: 0.0549 - val_acc: 0.5800\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0573 - acc: 0.5647 - val_loss: 0.0548 - val_acc: 0.5821\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0574 - acc: 0.5587 - val_loss: 0.0550 - val_acc: 0.5780\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0573 - acc: 0.5575 - val_loss: 0.0551 - val_acc: 0.5842\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0574 - acc: 0.5624 - val_loss: 0.0549 - val_acc: 0.5821\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0575 - acc: 0.5598 - val_loss: 0.0551 - val_acc: 0.5821\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0574 - acc: 0.5557 - val_loss: 0.0552 - val_acc: 0.5884\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0574 - acc: 0.5554 - val_loss: 0.0550 - val_acc: 0.5738\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0573 - acc: 0.5559 - val_loss: 0.0554 - val_acc: 0.5821\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0574 - acc: 0.5624 - val_loss: 0.0550 - val_acc: 0.5759\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0569 - acc: 0.5617 - val_loss: 0.0547 - val_acc: 0.5821\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0570 - acc: 0.5647 - val_loss: 0.0547 - val_acc: 0.5821\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0572 - acc: 0.5554 - val_loss: 0.0546 - val_acc: 0.5821\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0572 - acc: 0.5630 - val_loss: 0.0551 - val_acc: 0.5759\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0574 - acc: 0.5552 - val_loss: 0.0553 - val_acc: 0.5780\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0575 - acc: 0.5596 - val_loss: 0.0548 - val_acc: 0.5800\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0573 - acc: 0.5531 - val_loss: 0.0546 - val_acc: 0.5759\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0572 - acc: 0.5557 - val_loss: 0.0554 - val_acc: 0.5717\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0576 - acc: 0.5557 - val_loss: 0.0548 - val_acc: 0.5738\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0571 - acc: 0.5566 - val_loss: 0.0550 - val_acc: 0.5738\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0572 - acc: 0.5577 - val_loss: 0.0550 - val_acc: 0.5759\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0571 - acc: 0.5603 - val_loss: 0.0552 - val_acc: 0.5717\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0575 - acc: 0.5568 - val_loss: 0.0547 - val_acc: 0.5821\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0568 - acc: 0.5665 - val_loss: 0.0548 - val_acc: 0.5676\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0575 - acc: 0.5570 - val_loss: 0.0554 - val_acc: 0.5738\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0571 - acc: 0.5658 - val_loss: 0.0549 - val_acc: 0.5780\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0571 - acc: 0.5531 - val_loss: 0.0550 - val_acc: 0.5821\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0572 - acc: 0.5570 - val_loss: 0.0550 - val_acc: 0.5821\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0570 - acc: 0.5598 - val_loss: 0.0544 - val_acc: 0.5842\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0572 - acc: 0.5536 - val_loss: 0.0554 - val_acc: 0.5676\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0571 - acc: 0.5596 - val_loss: 0.0548 - val_acc: 0.5717\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0571 - acc: 0.5566 - val_loss: 0.0554 - val_acc: 0.5738\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0570 - acc: 0.5589 - val_loss: 0.0545 - val_acc: 0.5780\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0569 - acc: 0.5661 - val_loss: 0.0545 - val_acc: 0.5759\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0572 - acc: 0.5503 - val_loss: 0.0548 - val_acc: 0.5800\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0569 - acc: 0.5654 - val_loss: 0.0548 - val_acc: 0.5821\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0893 - acc: 0.2945 - val_loss: 0.0627 - val_acc: 0.5509\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0872 - acc: 0.3048 - val_loss: 0.0646 - val_acc: 0.4532\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0857 - acc: 0.3132 - val_loss: 0.0682 - val_acc: 0.4470\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0820 - acc: 0.3062 - val_loss: 0.0728 - val_acc: 0.4366\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0813 - acc: 0.3088 - val_loss: 0.0720 - val_acc: 0.4366\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0810 - acc: 0.3166 - val_loss: 0.0729 - val_acc: 0.4428\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0808 - acc: 0.3219 - val_loss: 0.0720 - val_acc: 0.4366\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0809 - acc: 0.3180 - val_loss: 0.0724 - val_acc: 0.4366\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0810 - acc: 0.3171 - val_loss: 0.0721 - val_acc: 0.4387\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0808 - acc: 0.3180 - val_loss: 0.0722 - val_acc: 0.4304\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0807 - acc: 0.3162 - val_loss: 0.0726 - val_acc: 0.4304\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0807 - acc: 0.3182 - val_loss: 0.0734 - val_acc: 0.4345\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0808 - acc: 0.3176 - val_loss: 0.0723 - val_acc: 0.4324\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0806 - acc: 0.3192 - val_loss: 0.0720 - val_acc: 0.4324\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0807 - acc: 0.3180 - val_loss: 0.0723 - val_acc: 0.4366\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0806 - acc: 0.3217 - val_loss: 0.0721 - val_acc: 0.4387\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0804 - acc: 0.3217 - val_loss: 0.0715 - val_acc: 0.4387\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0806 - acc: 0.3208 - val_loss: 0.0723 - val_acc: 0.4366\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 245us/step - loss: 0.0806 - acc: 0.3189 - val_loss: 0.0721 - val_acc: 0.4387\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0806 - acc: 0.3173 - val_loss: 0.0729 - val_acc: 0.4324\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 247us/step - loss: 0.0805 - acc: 0.3192 - val_loss: 0.0722 - val_acc: 0.4304\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0807 - acc: 0.3152 - val_loss: 0.0723 - val_acc: 0.4345\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 245us/step - loss: 0.0806 - acc: 0.3169 - val_loss: 0.0722 - val_acc: 0.4407\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0805 - acc: 0.3173 - val_loss: 0.0721 - val_acc: 0.4345\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 246us/step - loss: 0.0805 - acc: 0.3215 - val_loss: 0.0729 - val_acc: 0.4304\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0804 - acc: 0.3194 - val_loss: 0.0717 - val_acc: 0.4449\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0805 - acc: 0.3189 - val_loss: 0.0728 - val_acc: 0.4345\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0805 - acc: 0.3212 - val_loss: 0.0727 - val_acc: 0.4304\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0806 - acc: 0.3157 - val_loss: 0.0728 - val_acc: 0.4304\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0803 - acc: 0.3150 - val_loss: 0.0733 - val_acc: 0.4304\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0805 - acc: 0.3152 - val_loss: 0.0723 - val_acc: 0.4449\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0802 - acc: 0.3176 - val_loss: 0.0722 - val_acc: 0.4304\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 243us/step - loss: 0.0805 - acc: 0.3185 - val_loss: 0.0721 - val_acc: 0.4304\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0805 - acc: 0.3166 - val_loss: 0.0720 - val_acc: 0.4449\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0803 - acc: 0.3194 - val_loss: 0.0731 - val_acc: 0.4304\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0803 - acc: 0.3185 - val_loss: 0.0726 - val_acc: 0.4304\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 242us/step - loss: 0.0803 - acc: 0.3229 - val_loss: 0.0719 - val_acc: 0.4366\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0805 - acc: 0.3208 - val_loss: 0.0728 - val_acc: 0.4491\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0804 - acc: 0.3215 - val_loss: 0.0735 - val_acc: 0.4553\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0804 - acc: 0.3169 - val_loss: 0.0763 - val_acc: 0.4428\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0802 - acc: 0.3196 - val_loss: 0.0746 - val_acc: 0.4428\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0801 - acc: 0.3224 - val_loss: 0.0740 - val_acc: 0.4449\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0800 - acc: 0.3254 - val_loss: 0.0739 - val_acc: 0.4553\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0799 - acc: 0.3270 - val_loss: 0.0735 - val_acc: 0.4574\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0800 - acc: 0.3231 - val_loss: 0.0736 - val_acc: 0.4449\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0800 - acc: 0.3270 - val_loss: 0.0736 - val_acc: 0.4491\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0800 - acc: 0.3194 - val_loss: 0.0742 - val_acc: 0.4428\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0800 - acc: 0.3159 - val_loss: 0.0738 - val_acc: 0.4366\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0799 - acc: 0.3252 - val_loss: 0.0739 - val_acc: 0.4428\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0799 - acc: 0.3189 - val_loss: 0.0738 - val_acc: 0.4387\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0799 - acc: 0.3203 - val_loss: 0.0742 - val_acc: 0.4366\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0799 - acc: 0.3199 - val_loss: 0.0736 - val_acc: 0.4366\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0799 - acc: 0.3219 - val_loss: 0.0740 - val_acc: 0.4345\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0799 - acc: 0.3212 - val_loss: 0.0721 - val_acc: 0.4449\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0799 - acc: 0.3229 - val_loss: 0.0739 - val_acc: 0.4428\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0798 - acc: 0.3252 - val_loss: 0.0736 - val_acc: 0.4428\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0799 - acc: 0.3229 - val_loss: 0.0727 - val_acc: 0.4428\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0800 - acc: 0.3199 - val_loss: 0.0747 - val_acc: 0.4407\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0798 - acc: 0.3212 - val_loss: 0.0740 - val_acc: 0.4387\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0798 - acc: 0.3173 - val_loss: 0.0732 - val_acc: 0.4428\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0797 - acc: 0.3208 - val_loss: 0.0734 - val_acc: 0.4387\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0796 - acc: 0.3254 - val_loss: 0.0733 - val_acc: 0.4428\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0798 - acc: 0.3226 - val_loss: 0.0728 - val_acc: 0.4470\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0799 - acc: 0.3236 - val_loss: 0.0738 - val_acc: 0.4428\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0798 - acc: 0.3247 - val_loss: 0.0728 - val_acc: 0.4387\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0799 - acc: 0.3240 - val_loss: 0.0739 - val_acc: 0.4407\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0797 - acc: 0.3289 - val_loss: 0.0734 - val_acc: 0.4407\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0798 - acc: 0.3252 - val_loss: 0.0732 - val_acc: 0.4449\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0798 - acc: 0.3242 - val_loss: 0.0732 - val_acc: 0.4470\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0798 - acc: 0.3273 - val_loss: 0.0731 - val_acc: 0.4470\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0798 - acc: 0.3231 - val_loss: 0.0735 - val_acc: 0.4387\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0798 - acc: 0.3229 - val_loss: 0.0733 - val_acc: 0.4345\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0798 - acc: 0.3242 - val_loss: 0.0739 - val_acc: 0.4428\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0797 - acc: 0.3261 - val_loss: 0.0730 - val_acc: 0.4449\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0799 - acc: 0.3256 - val_loss: 0.0731 - val_acc: 0.4449\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0797 - acc: 0.3279 - val_loss: 0.0733 - val_acc: 0.4387\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0800 - acc: 0.3273 - val_loss: 0.0740 - val_acc: 0.4345\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0796 - acc: 0.3240 - val_loss: 0.0738 - val_acc: 0.4387\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0797 - acc: 0.3291 - val_loss: 0.0732 - val_acc: 0.4407\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0799 - acc: 0.3224 - val_loss: 0.0729 - val_acc: 0.4407\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0797 - acc: 0.3247 - val_loss: 0.0717 - val_acc: 0.4449\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0799 - acc: 0.3312 - val_loss: 0.0730 - val_acc: 0.4324\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0796 - acc: 0.3312 - val_loss: 0.0724 - val_acc: 0.4345\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0797 - acc: 0.3249 - val_loss: 0.0735 - val_acc: 0.4387\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0797 - acc: 0.3303 - val_loss: 0.0743 - val_acc: 0.4387\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0796 - acc: 0.3240 - val_loss: 0.0733 - val_acc: 0.4387\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0797 - acc: 0.3293 - val_loss: 0.0748 - val_acc: 0.4407\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0797 - acc: 0.3279 - val_loss: 0.0730 - val_acc: 0.4324\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0797 - acc: 0.3249 - val_loss: 0.0729 - val_acc: 0.4428\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0798 - acc: 0.3296 - val_loss: 0.0732 - val_acc: 0.4407\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 227us/step - loss: 0.0797 - acc: 0.3275 - val_loss: 0.0736 - val_acc: 0.4345\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 227us/step - loss: 0.0797 - acc: 0.3305 - val_loss: 0.0733 - val_acc: 0.4324\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0796 - acc: 0.3249 - val_loss: 0.0728 - val_acc: 0.4615\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0797 - acc: 0.3238 - val_loss: 0.0722 - val_acc: 0.4595\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0798 - acc: 0.3291 - val_loss: 0.0744 - val_acc: 0.4366\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0797 - acc: 0.3282 - val_loss: 0.0747 - val_acc: 0.4470\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0797 - acc: 0.3254 - val_loss: 0.0735 - val_acc: 0.4449\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0796 - acc: 0.3291 - val_loss: 0.0733 - val_acc: 0.4387\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0796 - acc: 0.3233 - val_loss: 0.0725 - val_acc: 0.4387\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0797 - acc: 0.3296 - val_loss: 0.0731 - val_acc: 0.4407\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0807 - acc: 0.3030 - val_loss: 0.0733 - val_acc: 0.4823\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0807 - acc: 0.3023 - val_loss: 0.0735 - val_acc: 0.4761\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0805 - acc: 0.3129 - val_loss: 0.0734 - val_acc: 0.4574\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0804 - acc: 0.3155 - val_loss: 0.0741 - val_acc: 0.4782\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0804 - acc: 0.3095 - val_loss: 0.0745 - val_acc: 0.4740\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0804 - acc: 0.3132 - val_loss: 0.0735 - val_acc: 0.4574\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0804 - acc: 0.3113 - val_loss: 0.0743 - val_acc: 0.4574\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0806 - acc: 0.3134 - val_loss: 0.0736 - val_acc: 0.4574\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0804 - acc: 0.3139 - val_loss: 0.0735 - val_acc: 0.4574\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0805 - acc: 0.3129 - val_loss: 0.0737 - val_acc: 0.4574\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0804 - acc: 0.3106 - val_loss: 0.0739 - val_acc: 0.4574\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0803 - acc: 0.3099 - val_loss: 0.0736 - val_acc: 0.4574\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0805 - acc: 0.3145 - val_loss: 0.0736 - val_acc: 0.4574\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0805 - acc: 0.3141 - val_loss: 0.0739 - val_acc: 0.4574\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0804 - acc: 0.3134 - val_loss: 0.0738 - val_acc: 0.4574\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0803 - acc: 0.3143 - val_loss: 0.0733 - val_acc: 0.4574\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0804 - acc: 0.3143 - val_loss: 0.0736 - val_acc: 0.4574\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0806 - acc: 0.3111 - val_loss: 0.0739 - val_acc: 0.4574\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0804 - acc: 0.3152 - val_loss: 0.0741 - val_acc: 0.4574\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0803 - acc: 0.3141 - val_loss: 0.0743 - val_acc: 0.4574\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0804 - acc: 0.3143 - val_loss: 0.0736 - val_acc: 0.4574\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0803 - acc: 0.3134 - val_loss: 0.0750 - val_acc: 0.4574\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0803 - acc: 0.3139 - val_loss: 0.0745 - val_acc: 0.4574\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 266us/step - loss: 0.0803 - acc: 0.3136 - val_loss: 0.0734 - val_acc: 0.4574\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 290us/step - loss: 0.0804 - acc: 0.3162 - val_loss: 0.0731 - val_acc: 0.4574\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 291us/step - loss: 0.0803 - acc: 0.3143 - val_loss: 0.0737 - val_acc: 0.4574\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 293us/step - loss: 0.0802 - acc: 0.3129 - val_loss: 0.0735 - val_acc: 0.4574\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 290us/step - loss: 0.0803 - acc: 0.3118 - val_loss: 0.0737 - val_acc: 0.4574\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 295us/step - loss: 0.0804 - acc: 0.3145 - val_loss: 0.0736 - val_acc: 0.4740\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 290us/step - loss: 0.0803 - acc: 0.3127 - val_loss: 0.0728 - val_acc: 0.4574\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 292us/step - loss: 0.0803 - acc: 0.3132 - val_loss: 0.0744 - val_acc: 0.4532\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 262us/step - loss: 0.0804 - acc: 0.3074 - val_loss: 0.0731 - val_acc: 0.4511\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0802 - acc: 0.3148 - val_loss: 0.0732 - val_acc: 0.4574\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0804 - acc: 0.3090 - val_loss: 0.0748 - val_acc: 0.4574\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0802 - acc: 0.3127 - val_loss: 0.0736 - val_acc: 0.4574\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 244us/step - loss: 0.0803 - acc: 0.3166 - val_loss: 0.0745 - val_acc: 0.4574\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0802 - acc: 0.3122 - val_loss: 0.0750 - val_acc: 0.4574\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0803 - acc: 0.3132 - val_loss: 0.0746 - val_acc: 0.4574\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0803 - acc: 0.3106 - val_loss: 0.0740 - val_acc: 0.4574\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0803 - acc: 0.3143 - val_loss: 0.0738 - val_acc: 0.4574\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0803 - acc: 0.3139 - val_loss: 0.0743 - val_acc: 0.4574\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0803 - acc: 0.3148 - val_loss: 0.0745 - val_acc: 0.4574\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0803 - acc: 0.3129 - val_loss: 0.0743 - val_acc: 0.4574\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0803 - acc: 0.3162 - val_loss: 0.0737 - val_acc: 0.4574\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0802 - acc: 0.3127 - val_loss: 0.0735 - val_acc: 0.4574\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0802 - acc: 0.3166 - val_loss: 0.0752 - val_acc: 0.4574\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0803 - acc: 0.3132 - val_loss: 0.0746 - val_acc: 0.4574\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0802 - acc: 0.3104 - val_loss: 0.0750 - val_acc: 0.4574\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0802 - acc: 0.3155 - val_loss: 0.0731 - val_acc: 0.4574\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 226us/step - loss: 0.0803 - acc: 0.3115 - val_loss: 0.0731 - val_acc: 0.4574\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0802 - acc: 0.3145 - val_loss: 0.0740 - val_acc: 0.4553\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0803 - acc: 0.3122 - val_loss: 0.0736 - val_acc: 0.4574\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0803 - acc: 0.3102 - val_loss: 0.0735 - val_acc: 0.4574\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0802 - acc: 0.3127 - val_loss: 0.0733 - val_acc: 0.4636\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0803 - acc: 0.3102 - val_loss: 0.0731 - val_acc: 0.4574\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0802 - acc: 0.3134 - val_loss: 0.0741 - val_acc: 0.4553\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0803 - acc: 0.3132 - val_loss: 0.0737 - val_acc: 0.4574\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0802 - acc: 0.3162 - val_loss: 0.0736 - val_acc: 0.4532\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0802 - acc: 0.3162 - val_loss: 0.0736 - val_acc: 0.4574\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0801 - acc: 0.3111 - val_loss: 0.0738 - val_acc: 0.4553\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0802 - acc: 0.3141 - val_loss: 0.0730 - val_acc: 0.4574\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0802 - acc: 0.3132 - val_loss: 0.0739 - val_acc: 0.4553\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0801 - acc: 0.3164 - val_loss: 0.0738 - val_acc: 0.4532\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0802 - acc: 0.3136 - val_loss: 0.0739 - val_acc: 0.4532\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0803 - acc: 0.3104 - val_loss: 0.0742 - val_acc: 0.4532\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0803 - acc: 0.3159 - val_loss: 0.0742 - val_acc: 0.4511\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0802 - acc: 0.3145 - val_loss: 0.0739 - val_acc: 0.4511\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0801 - acc: 0.3171 - val_loss: 0.0732 - val_acc: 0.4511\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0802 - acc: 0.3171 - val_loss: 0.0735 - val_acc: 0.4511\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0801 - acc: 0.3139 - val_loss: 0.0734 - val_acc: 0.4511\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0802 - acc: 0.3169 - val_loss: 0.0724 - val_acc: 0.4636\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0800 - acc: 0.3118 - val_loss: 0.0739 - val_acc: 0.4657\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0800 - acc: 0.3162 - val_loss: 0.0720 - val_acc: 0.4532\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0800 - acc: 0.3171 - val_loss: 0.0734 - val_acc: 0.4532\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0801 - acc: 0.3132 - val_loss: 0.0743 - val_acc: 0.4678\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0802 - acc: 0.3185 - val_loss: 0.0745 - val_acc: 0.4491\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0802 - acc: 0.3092 - val_loss: 0.0738 - val_acc: 0.4678\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0801 - acc: 0.3134 - val_loss: 0.0737 - val_acc: 0.4511\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0802 - acc: 0.3141 - val_loss: 0.0739 - val_acc: 0.4657\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0801 - acc: 0.3157 - val_loss: 0.0736 - val_acc: 0.4615\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0801 - acc: 0.3134 - val_loss: 0.0735 - val_acc: 0.4511\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0801 - acc: 0.3092 - val_loss: 0.0739 - val_acc: 0.4532\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0801 - acc: 0.3173 - val_loss: 0.0736 - val_acc: 0.4532\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0801 - acc: 0.3162 - val_loss: 0.0728 - val_acc: 0.4657\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 241us/step - loss: 0.0801 - acc: 0.3166 - val_loss: 0.0736 - val_acc: 0.4532\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0802 - acc: 0.3148 - val_loss: 0.0741 - val_acc: 0.4532\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0802 - acc: 0.3159 - val_loss: 0.0737 - val_acc: 0.4532\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0800 - acc: 0.3162 - val_loss: 0.0740 - val_acc: 0.4532\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0802 - acc: 0.3113 - val_loss: 0.0751 - val_acc: 0.4511\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0800 - acc: 0.3148 - val_loss: 0.0733 - val_acc: 0.4615\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0803 - acc: 0.3074 - val_loss: 0.0739 - val_acc: 0.4595\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 276us/step - loss: 0.0800 - acc: 0.3182 - val_loss: 0.0735 - val_acc: 0.4678\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 294us/step - loss: 0.0800 - acc: 0.3129 - val_loss: 0.0734 - val_acc: 0.4491\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 293us/step - loss: 0.0801 - acc: 0.3171 - val_loss: 0.0734 - val_acc: 0.4511\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 293us/step - loss: 0.0803 - acc: 0.3139 - val_loss: 0.0740 - val_acc: 0.4491\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 262us/step - loss: 0.0802 - acc: 0.3125 - val_loss: 0.0737 - val_acc: 0.4428\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0800 - acc: 0.3166 - val_loss: 0.0736 - val_acc: 0.4657\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0802 - acc: 0.3143 - val_loss: 0.0732 - val_acc: 0.4532\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0801 - acc: 0.3081 - val_loss: 0.0737 - val_acc: 0.4553\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0800 - acc: 0.3132 - val_loss: 0.0736 - val_acc: 0.4532\n",
      "Train on 4330 samples, validate on 481 samples\n",
      "Epoch 1/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0803 - acc: 0.3122 - val_loss: 0.0724 - val_acc: 0.4428\n",
      "Epoch 2/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0805 - acc: 0.3141 - val_loss: 0.0728 - val_acc: 0.4636\n",
      "Epoch 3/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0803 - acc: 0.3118 - val_loss: 0.0742 - val_acc: 0.4615\n",
      "Epoch 4/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0802 - acc: 0.3139 - val_loss: 0.0742 - val_acc: 0.4449\n",
      "Epoch 5/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0803 - acc: 0.3111 - val_loss: 0.0735 - val_acc: 0.4428\n",
      "Epoch 6/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0803 - acc: 0.3166 - val_loss: 0.0736 - val_acc: 0.4428\n",
      "Epoch 7/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0803 - acc: 0.3141 - val_loss: 0.0743 - val_acc: 0.4428\n",
      "Epoch 8/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0802 - acc: 0.3125 - val_loss: 0.0721 - val_acc: 0.4574\n",
      "Epoch 9/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0804 - acc: 0.3120 - val_loss: 0.0742 - val_acc: 0.4615\n",
      "Epoch 10/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0803 - acc: 0.3157 - val_loss: 0.0744 - val_acc: 0.4574\n",
      "Epoch 11/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0802 - acc: 0.3134 - val_loss: 0.0733 - val_acc: 0.4428\n",
      "Epoch 12/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0803 - acc: 0.3169 - val_loss: 0.0729 - val_acc: 0.4553\n",
      "Epoch 13/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0802 - acc: 0.3118 - val_loss: 0.0742 - val_acc: 0.4532\n",
      "Epoch 14/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0801 - acc: 0.3185 - val_loss: 0.0734 - val_acc: 0.4553\n",
      "Epoch 15/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0803 - acc: 0.3189 - val_loss: 0.0735 - val_acc: 0.4532\n",
      "Epoch 16/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0802 - acc: 0.3129 - val_loss: 0.0736 - val_acc: 0.4553\n",
      "Epoch 17/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0803 - acc: 0.3164 - val_loss: 0.0746 - val_acc: 0.4428\n",
      "Epoch 18/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0801 - acc: 0.3166 - val_loss: 0.0738 - val_acc: 0.4428\n",
      "Epoch 19/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0802 - acc: 0.3159 - val_loss: 0.0731 - val_acc: 0.4574\n",
      "Epoch 20/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0802 - acc: 0.3102 - val_loss: 0.0743 - val_acc: 0.4428\n",
      "Epoch 21/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0801 - acc: 0.3125 - val_loss: 0.0748 - val_acc: 0.4407\n",
      "Epoch 22/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0802 - acc: 0.3148 - val_loss: 0.0732 - val_acc: 0.4449\n",
      "Epoch 23/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0803 - acc: 0.3159 - val_loss: 0.0740 - val_acc: 0.4657\n",
      "Epoch 24/100\n",
      "4330/4330 [==============================] - 1s 229us/step - loss: 0.0802 - acc: 0.3185 - val_loss: 0.0741 - val_acc: 0.4553\n",
      "Epoch 25/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0803 - acc: 0.3141 - val_loss: 0.0749 - val_acc: 0.4449\n",
      "Epoch 26/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0801 - acc: 0.3171 - val_loss: 0.0742 - val_acc: 0.4470\n",
      "Epoch 27/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0802 - acc: 0.3111 - val_loss: 0.0743 - val_acc: 0.4595\n",
      "Epoch 28/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0802 - acc: 0.3134 - val_loss: 0.0746 - val_acc: 0.4428\n",
      "Epoch 29/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0802 - acc: 0.3129 - val_loss: 0.0739 - val_acc: 0.4615\n",
      "Epoch 30/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0803 - acc: 0.3155 - val_loss: 0.0741 - val_acc: 0.4615\n",
      "Epoch 31/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0803 - acc: 0.3171 - val_loss: 0.0746 - val_acc: 0.4574\n",
      "Epoch 32/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0801 - acc: 0.3166 - val_loss: 0.0739 - val_acc: 0.4615\n",
      "Epoch 33/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0802 - acc: 0.3148 - val_loss: 0.0743 - val_acc: 0.4574\n",
      "Epoch 34/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0802 - acc: 0.3148 - val_loss: 0.0744 - val_acc: 0.4366\n",
      "Epoch 35/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0803 - acc: 0.3152 - val_loss: 0.0744 - val_acc: 0.4553\n",
      "Epoch 36/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0801 - acc: 0.3118 - val_loss: 0.0740 - val_acc: 0.4615\n",
      "Epoch 37/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0802 - acc: 0.3122 - val_loss: 0.0741 - val_acc: 0.4428\n",
      "Epoch 38/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0801 - acc: 0.3152 - val_loss: 0.0741 - val_acc: 0.4595\n",
      "Epoch 39/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0802 - acc: 0.3208 - val_loss: 0.0742 - val_acc: 0.4553\n",
      "Epoch 40/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0802 - acc: 0.3164 - val_loss: 0.0739 - val_acc: 0.4678\n",
      "Epoch 41/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0802 - acc: 0.3171 - val_loss: 0.0744 - val_acc: 0.4574\n",
      "Epoch 42/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0803 - acc: 0.3136 - val_loss: 0.0745 - val_acc: 0.4366\n",
      "Epoch 43/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0801 - acc: 0.3155 - val_loss: 0.0737 - val_acc: 0.4657\n",
      "Epoch 44/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0800 - acc: 0.3215 - val_loss: 0.0736 - val_acc: 0.4553\n",
      "Epoch 45/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0802 - acc: 0.3173 - val_loss: 0.0746 - val_acc: 0.4574\n",
      "Epoch 46/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0801 - acc: 0.3226 - val_loss: 0.0739 - val_acc: 0.4636\n",
      "Epoch 47/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0801 - acc: 0.3201 - val_loss: 0.0736 - val_acc: 0.4574\n",
      "Epoch 48/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0802 - acc: 0.3164 - val_loss: 0.0740 - val_acc: 0.4678\n",
      "Epoch 49/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0801 - acc: 0.3176 - val_loss: 0.0739 - val_acc: 0.4407\n",
      "Epoch 50/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0800 - acc: 0.3169 - val_loss: 0.0735 - val_acc: 0.4553\n",
      "Epoch 51/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0802 - acc: 0.3169 - val_loss: 0.0736 - val_acc: 0.4636\n",
      "Epoch 52/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0800 - acc: 0.3222 - val_loss: 0.0747 - val_acc: 0.4574\n",
      "Epoch 53/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0802 - acc: 0.3194 - val_loss: 0.0741 - val_acc: 0.4657\n",
      "Epoch 54/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0801 - acc: 0.3171 - val_loss: 0.0737 - val_acc: 0.4595\n",
      "Epoch 55/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0801 - acc: 0.3173 - val_loss: 0.0735 - val_acc: 0.4532\n",
      "Epoch 56/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0800 - acc: 0.3201 - val_loss: 0.0738 - val_acc: 0.4615\n",
      "Epoch 57/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0801 - acc: 0.3187 - val_loss: 0.0735 - val_acc: 0.4553\n",
      "Epoch 58/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0801 - acc: 0.3187 - val_loss: 0.0737 - val_acc: 0.4574\n",
      "Epoch 59/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0801 - acc: 0.3180 - val_loss: 0.0741 - val_acc: 0.4615\n",
      "Epoch 60/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0800 - acc: 0.3252 - val_loss: 0.0735 - val_acc: 0.4615\n",
      "Epoch 61/100\n",
      "4330/4330 [==============================] - 1s 240us/step - loss: 0.0800 - acc: 0.3236 - val_loss: 0.0734 - val_acc: 0.4636\n",
      "Epoch 62/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0801 - acc: 0.3192 - val_loss: 0.0739 - val_acc: 0.4491\n",
      "Epoch 63/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0801 - acc: 0.3206 - val_loss: 0.0738 - val_acc: 0.4511\n",
      "Epoch 64/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0800 - acc: 0.3224 - val_loss: 0.0737 - val_acc: 0.4511\n",
      "Epoch 65/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0800 - acc: 0.3268 - val_loss: 0.0734 - val_acc: 0.4511\n",
      "Epoch 66/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0800 - acc: 0.3231 - val_loss: 0.0735 - val_acc: 0.4595\n",
      "Epoch 67/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0801 - acc: 0.3208 - val_loss: 0.0742 - val_acc: 0.4657\n",
      "Epoch 68/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0800 - acc: 0.3189 - val_loss: 0.0738 - val_acc: 0.4553\n",
      "Epoch 69/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0799 - acc: 0.3247 - val_loss: 0.0739 - val_acc: 0.4615\n",
      "Epoch 70/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0800 - acc: 0.3224 - val_loss: 0.0735 - val_acc: 0.4574\n",
      "Epoch 71/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0800 - acc: 0.3199 - val_loss: 0.0740 - val_acc: 0.4407\n",
      "Epoch 72/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0801 - acc: 0.3162 - val_loss: 0.0743 - val_acc: 0.4595\n",
      "Epoch 73/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0801 - acc: 0.3169 - val_loss: 0.0742 - val_acc: 0.4387\n",
      "Epoch 74/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0802 - acc: 0.3203 - val_loss: 0.0753 - val_acc: 0.4428\n",
      "Epoch 75/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0801 - acc: 0.3125 - val_loss: 0.0743 - val_acc: 0.4491\n",
      "Epoch 76/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0800 - acc: 0.3229 - val_loss: 0.0744 - val_acc: 0.4387\n",
      "Epoch 77/100\n",
      "4330/4330 [==============================] - 1s 235us/step - loss: 0.0802 - acc: 0.3185 - val_loss: 0.0742 - val_acc: 0.4324\n",
      "Epoch 78/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0801 - acc: 0.3145 - val_loss: 0.0739 - val_acc: 0.4407\n",
      "Epoch 79/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0801 - acc: 0.3210 - val_loss: 0.0740 - val_acc: 0.4449\n",
      "Epoch 80/100\n",
      "4330/4330 [==============================] - 1s 239us/step - loss: 0.0800 - acc: 0.3169 - val_loss: 0.0737 - val_acc: 0.4428\n",
      "Epoch 81/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0799 - acc: 0.3252 - val_loss: 0.0743 - val_acc: 0.4449\n",
      "Epoch 82/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0800 - acc: 0.3247 - val_loss: 0.0749 - val_acc: 0.4345\n",
      "Epoch 83/100\n",
      "4330/4330 [==============================] - 1s 237us/step - loss: 0.0799 - acc: 0.3259 - val_loss: 0.0740 - val_acc: 0.4407\n",
      "Epoch 84/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0800 - acc: 0.3196 - val_loss: 0.0740 - val_acc: 0.4387\n",
      "Epoch 85/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0799 - acc: 0.3254 - val_loss: 0.0746 - val_acc: 0.4491\n",
      "Epoch 86/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0800 - acc: 0.3305 - val_loss: 0.0739 - val_acc: 0.4615\n",
      "Epoch 87/100\n",
      "4330/4330 [==============================] - 1s 234us/step - loss: 0.0799 - acc: 0.3222 - val_loss: 0.0738 - val_acc: 0.4553\n",
      "Epoch 88/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0799 - acc: 0.3215 - val_loss: 0.0739 - val_acc: 0.4553\n",
      "Epoch 89/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0800 - acc: 0.3203 - val_loss: 0.0745 - val_acc: 0.4553\n",
      "Epoch 90/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0801 - acc: 0.3231 - val_loss: 0.0738 - val_acc: 0.4491\n",
      "Epoch 91/100\n",
      "4330/4330 [==============================] - 1s 231us/step - loss: 0.0800 - acc: 0.3210 - val_loss: 0.0745 - val_acc: 0.4532\n",
      "Epoch 92/100\n",
      "4330/4330 [==============================] - 1s 232us/step - loss: 0.0800 - acc: 0.3242 - val_loss: 0.0740 - val_acc: 0.4491\n",
      "Epoch 93/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0799 - acc: 0.3229 - val_loss: 0.0748 - val_acc: 0.4511\n",
      "Epoch 94/100\n",
      "4330/4330 [==============================] - 1s 238us/step - loss: 0.0799 - acc: 0.3268 - val_loss: 0.0739 - val_acc: 0.4511\n",
      "Epoch 95/100\n",
      "4330/4330 [==============================] - 1s 228us/step - loss: 0.0799 - acc: 0.3222 - val_loss: 0.0737 - val_acc: 0.4553\n",
      "Epoch 96/100\n",
      "4330/4330 [==============================] - 1s 230us/step - loss: 0.0799 - acc: 0.3240 - val_loss: 0.0747 - val_acc: 0.4553\n",
      "Epoch 97/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0799 - acc: 0.3275 - val_loss: 0.0734 - val_acc: 0.4532\n",
      "Epoch 98/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0798 - acc: 0.3259 - val_loss: 0.0730 - val_acc: 0.4511\n",
      "Epoch 99/100\n",
      "4330/4330 [==============================] - 1s 233us/step - loss: 0.0802 - acc: 0.3169 - val_loss: 0.0745 - val_acc: 0.4511\n",
      "Epoch 100/100\n",
      "4330/4330 [==============================] - 1s 236us/step - loss: 0.0800 - acc: 0.3236 - val_loss: 0.0742 - val_acc: 0.4511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(units=20, activation='sigmoid'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(units=16, activation='sigmoid'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(units=12, activation='sigmoid'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 10):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=100, validation_data=(dev_valid_x, dev_valid_y))\n",
    "  \n",
    "pred_y = nn_model.predict(valid_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result_cv = result_cv.append({'detail':'dropout 0.2', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "GX2rUSs5SyoZ",
    "outputId": "b4d16960-053a-4bba-adda-9bf30f4ee0cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detail</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>0.431421</td>\n",
       "      <td>0.212384</td>\n",
       "      <td>0.173564</td>\n",
       "      <td>0.152795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dropout 0.2</td>\n",
       "      <td>0.453865</td>\n",
       "      <td>0.092566</td>\n",
       "      <td>0.168310</td>\n",
       "      <td>0.118978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        detail  accuracy  precision    recall        f1\n",
       "0      default  0.431421   0.212384  0.173564  0.152795\n",
       "1  dropout 0.2  0.453865   0.092566  0.168310  0.118978"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tpcSR60FU8vZ"
   },
   "source": [
    "Dropout doesn't make big difference. Try again with 3 fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LmpiC51Si9Y"
   },
   "outputs": [],
   "source": [
    "x_fold = np.array_split(dev_x, 3)\n",
    "y_fold = np.array_split(dev_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 20757
    },
    "colab_type": "code",
    "id": "MfRWzgXYSklt",
    "outputId": "604e89be-ee50-41fc-e8cb-8ccfbcee7774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 12)                204       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 2,386\n",
      "Trainable params: 2,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 2s 480us/step - loss: 0.0813 - acc: 0.2869 - val_loss: 0.0759 - val_acc: 0.4539\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0701 - acc: 0.4384 - val_loss: 0.0684 - val_acc: 0.4476\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 225us/step - loss: 0.0675 - acc: 0.4434 - val_loss: 0.0654 - val_acc: 0.4670\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0649 - acc: 0.4640 - val_loss: 0.0679 - val_acc: 0.4358\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0640 - acc: 0.4727 - val_loss: 0.0638 - val_acc: 0.4888\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0630 - acc: 0.4889 - val_loss: 0.0635 - val_acc: 0.4838\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 238us/step - loss: 0.0623 - acc: 0.4936 - val_loss: 0.0622 - val_acc: 0.4906\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0612 - acc: 0.5089 - val_loss: 0.0597 - val_acc: 0.5243\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0602 - acc: 0.5320 - val_loss: 0.0591 - val_acc: 0.5324\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0596 - acc: 0.5332 - val_loss: 0.0586 - val_acc: 0.5418\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0596 - acc: 0.5404 - val_loss: 0.0647 - val_acc: 0.4906\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0611 - acc: 0.5083 - val_loss: 0.0605 - val_acc: 0.5249\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0589 - acc: 0.5398 - val_loss: 0.0583 - val_acc: 0.5337\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0584 - acc: 0.5447 - val_loss: 0.0580 - val_acc: 0.5393\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0587 - acc: 0.5416 - val_loss: 0.0611 - val_acc: 0.5075\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0583 - acc: 0.5488 - val_loss: 0.0586 - val_acc: 0.5362\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0587 - acc: 0.5463 - val_loss: 0.0577 - val_acc: 0.5499\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0584 - acc: 0.5491 - val_loss: 0.0579 - val_acc: 0.5424\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0582 - acc: 0.5525 - val_loss: 0.0582 - val_acc: 0.5418\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0586 - acc: 0.5504 - val_loss: 0.0578 - val_acc: 0.5474\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0591 - acc: 0.5379 - val_loss: 0.0580 - val_acc: 0.5436\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0577 - acc: 0.5507 - val_loss: 0.0572 - val_acc: 0.5511\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0578 - acc: 0.5538 - val_loss: 0.0576 - val_acc: 0.5486\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0585 - acc: 0.5469 - val_loss: 0.0577 - val_acc: 0.5530\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0577 - acc: 0.5563 - val_loss: 0.0578 - val_acc: 0.5486\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 238us/step - loss: 0.0576 - acc: 0.5606 - val_loss: 0.0571 - val_acc: 0.5511\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0575 - acc: 0.5538 - val_loss: 0.0568 - val_acc: 0.5480\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0574 - acc: 0.5544 - val_loss: 0.0568 - val_acc: 0.5461\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0573 - acc: 0.5557 - val_loss: 0.0567 - val_acc: 0.5493\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0571 - acc: 0.5582 - val_loss: 0.0568 - val_acc: 0.5493\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0573 - acc: 0.5519 - val_loss: 0.0564 - val_acc: 0.5480\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0573 - acc: 0.5638 - val_loss: 0.0570 - val_acc: 0.5449\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0574 - acc: 0.5547 - val_loss: 0.0569 - val_acc: 0.5499\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0569 - acc: 0.5638 - val_loss: 0.0573 - val_acc: 0.5655\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0568 - acc: 0.5641 - val_loss: 0.0573 - val_acc: 0.5449\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0565 - acc: 0.5684 - val_loss: 0.0562 - val_acc: 0.5555\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0566 - acc: 0.5622 - val_loss: 0.0567 - val_acc: 0.5555\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0569 - acc: 0.5613 - val_loss: 0.0566 - val_acc: 0.5592\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0566 - acc: 0.5669 - val_loss: 0.0566 - val_acc: 0.5655\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0564 - acc: 0.5697 - val_loss: 0.0566 - val_acc: 0.5461\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0567 - acc: 0.5656 - val_loss: 0.0565 - val_acc: 0.5499\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0565 - acc: 0.5703 - val_loss: 0.0581 - val_acc: 0.5524\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0572 - acc: 0.5550 - val_loss: 0.0565 - val_acc: 0.5517\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0562 - acc: 0.5719 - val_loss: 0.0569 - val_acc: 0.5648\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0566 - acc: 0.5697 - val_loss: 0.0575 - val_acc: 0.5517\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0568 - acc: 0.5728 - val_loss: 0.0586 - val_acc: 0.5330\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0565 - acc: 0.5659 - val_loss: 0.0569 - val_acc: 0.5648\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0564 - acc: 0.5694 - val_loss: 0.0569 - val_acc: 0.5655\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0567 - acc: 0.5641 - val_loss: 0.0590 - val_acc: 0.5443\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0566 - acc: 0.5691 - val_loss: 0.0568 - val_acc: 0.5586\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0562 - acc: 0.5725 - val_loss: 0.0579 - val_acc: 0.5499\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0564 - acc: 0.5709 - val_loss: 0.0575 - val_acc: 0.5455\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 223us/step - loss: 0.0566 - acc: 0.5716 - val_loss: 0.0564 - val_acc: 0.5580\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0561 - acc: 0.5737 - val_loss: 0.0565 - val_acc: 0.5667\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0559 - acc: 0.5750 - val_loss: 0.0572 - val_acc: 0.5586\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0566 - acc: 0.5613 - val_loss: 0.0565 - val_acc: 0.5605\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0561 - acc: 0.5772 - val_loss: 0.0568 - val_acc: 0.5586\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0559 - acc: 0.5769 - val_loss: 0.0569 - val_acc: 0.5586\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0559 - acc: 0.5772 - val_loss: 0.0569 - val_acc: 0.5599\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0559 - acc: 0.5766 - val_loss: 0.0570 - val_acc: 0.5542\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0558 - acc: 0.5787 - val_loss: 0.0567 - val_acc: 0.5530\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0558 - acc: 0.5750 - val_loss: 0.0566 - val_acc: 0.5549\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0561 - acc: 0.5716 - val_loss: 0.0565 - val_acc: 0.5561\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0566 - acc: 0.5653 - val_loss: 0.0572 - val_acc: 0.5468\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0557 - acc: 0.5803 - val_loss: 0.0575 - val_acc: 0.5592\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0565 - acc: 0.5744 - val_loss: 0.0581 - val_acc: 0.5355\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0562 - acc: 0.5719 - val_loss: 0.0572 - val_acc: 0.5461\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0557 - acc: 0.5819 - val_loss: 0.0573 - val_acc: 0.5549\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0559 - acc: 0.5769 - val_loss: 0.0574 - val_acc: 0.5480\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0558 - acc: 0.5737 - val_loss: 0.0574 - val_acc: 0.5517\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0561 - acc: 0.5759 - val_loss: 0.0572 - val_acc: 0.5549\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0557 - acc: 0.5812 - val_loss: 0.0569 - val_acc: 0.5542\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0556 - acc: 0.5797 - val_loss: 0.0569 - val_acc: 0.5630\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0557 - acc: 0.5803 - val_loss: 0.0574 - val_acc: 0.5499\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0554 - acc: 0.5847 - val_loss: 0.0568 - val_acc: 0.5567\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0557 - acc: 0.5741 - val_loss: 0.0568 - val_acc: 0.5580\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0553 - acc: 0.5825 - val_loss: 0.0579 - val_acc: 0.5380\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0559 - acc: 0.5781 - val_loss: 0.0575 - val_acc: 0.5480\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0557 - acc: 0.5781 - val_loss: 0.0569 - val_acc: 0.5549\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0559 - acc: 0.5762 - val_loss: 0.0578 - val_acc: 0.5443\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0553 - acc: 0.5837 - val_loss: 0.0579 - val_acc: 0.5461\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0555 - acc: 0.5819 - val_loss: 0.0569 - val_acc: 0.5474\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0553 - acc: 0.5812 - val_loss: 0.0570 - val_acc: 0.5468\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 240us/step - loss: 0.0552 - acc: 0.5828 - val_loss: 0.0574 - val_acc: 0.5524\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0551 - acc: 0.5859 - val_loss: 0.0572 - val_acc: 0.5580\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0551 - acc: 0.5843 - val_loss: 0.0571 - val_acc: 0.5468\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0551 - acc: 0.5794 - val_loss: 0.0577 - val_acc: 0.5530\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0552 - acc: 0.5843 - val_loss: 0.0577 - val_acc: 0.5480\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0550 - acc: 0.5881 - val_loss: 0.0576 - val_acc: 0.5511\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0552 - acc: 0.5850 - val_loss: 0.0575 - val_acc: 0.5517\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0550 - acc: 0.5822 - val_loss: 0.0576 - val_acc: 0.5586\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0550 - acc: 0.5840 - val_loss: 0.0575 - val_acc: 0.5505\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0551 - acc: 0.5840 - val_loss: 0.0580 - val_acc: 0.5461\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0550 - acc: 0.5790 - val_loss: 0.0574 - val_acc: 0.5549\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0550 - acc: 0.5853 - val_loss: 0.0580 - val_acc: 0.5493\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0552 - acc: 0.5812 - val_loss: 0.0572 - val_acc: 0.5586\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0549 - acc: 0.5878 - val_loss: 0.0574 - val_acc: 0.5517\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0550 - acc: 0.5843 - val_loss: 0.0572 - val_acc: 0.5430\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0546 - acc: 0.5893 - val_loss: 0.0580 - val_acc: 0.5405\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0551 - acc: 0.5822 - val_loss: 0.0584 - val_acc: 0.5418\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0550 - acc: 0.5812 - val_loss: 0.0577 - val_acc: 0.5511\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0550 - acc: 0.5825 - val_loss: 0.0587 - val_acc: 0.5405\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0552 - acc: 0.5815 - val_loss: 0.0574 - val_acc: 0.5561\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0548 - acc: 0.5862 - val_loss: 0.0577 - val_acc: 0.5499\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0552 - acc: 0.5756 - val_loss: 0.0577 - val_acc: 0.5567\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 238us/step - loss: 0.0546 - acc: 0.5868 - val_loss: 0.0575 - val_acc: 0.5405\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0546 - acc: 0.5859 - val_loss: 0.0581 - val_acc: 0.5567\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0548 - acc: 0.5850 - val_loss: 0.0578 - val_acc: 0.5418\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0546 - acc: 0.5865 - val_loss: 0.0577 - val_acc: 0.5542\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0546 - acc: 0.5906 - val_loss: 0.0588 - val_acc: 0.5461\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0544 - acc: 0.5868 - val_loss: 0.0580 - val_acc: 0.5474\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0547 - acc: 0.5853 - val_loss: 0.0591 - val_acc: 0.5424\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0547 - acc: 0.5881 - val_loss: 0.0584 - val_acc: 0.5411\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0544 - acc: 0.5900 - val_loss: 0.0582 - val_acc: 0.5536\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0546 - acc: 0.5878 - val_loss: 0.0579 - val_acc: 0.5517\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0546 - acc: 0.5925 - val_loss: 0.0577 - val_acc: 0.5493\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0544 - acc: 0.5893 - val_loss: 0.0580 - val_acc: 0.5455\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0542 - acc: 0.5965 - val_loss: 0.0580 - val_acc: 0.5474\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0543 - acc: 0.5896 - val_loss: 0.0595 - val_acc: 0.5393\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0547 - acc: 0.5900 - val_loss: 0.0581 - val_acc: 0.5561\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0551 - acc: 0.5862 - val_loss: 0.0583 - val_acc: 0.5349\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0552 - acc: 0.5884 - val_loss: 0.0584 - val_acc: 0.5411\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0542 - acc: 0.5921 - val_loss: 0.0581 - val_acc: 0.5455\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0541 - acc: 0.5943 - val_loss: 0.0575 - val_acc: 0.5630\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0543 - acc: 0.5915 - val_loss: 0.0604 - val_acc: 0.5206\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 239us/step - loss: 0.0542 - acc: 0.5903 - val_loss: 0.0587 - val_acc: 0.5436\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0541 - acc: 0.5959 - val_loss: 0.0581 - val_acc: 0.5480\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0539 - acc: 0.5953 - val_loss: 0.0580 - val_acc: 0.5511\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0548 - acc: 0.5868 - val_loss: 0.0581 - val_acc: 0.5424\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0542 - acc: 0.5893 - val_loss: 0.0583 - val_acc: 0.5599\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0538 - acc: 0.5915 - val_loss: 0.0579 - val_acc: 0.5567\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0540 - acc: 0.5978 - val_loss: 0.0582 - val_acc: 0.5499\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0543 - acc: 0.5925 - val_loss: 0.0608 - val_acc: 0.5137\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0542 - acc: 0.5931 - val_loss: 0.0590 - val_acc: 0.5393\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0541 - acc: 0.5968 - val_loss: 0.0581 - val_acc: 0.5524\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0544 - acc: 0.5865 - val_loss: 0.0578 - val_acc: 0.5555\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0543 - acc: 0.5918 - val_loss: 0.0585 - val_acc: 0.5430\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0547 - acc: 0.5828 - val_loss: 0.0581 - val_acc: 0.5486\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0539 - acc: 0.5962 - val_loss: 0.0580 - val_acc: 0.5499\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 225us/step - loss: 0.0538 - acc: 0.5993 - val_loss: 0.0586 - val_acc: 0.5561\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0538 - acc: 0.5946 - val_loss: 0.0590 - val_acc: 0.5430\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0537 - acc: 0.6002 - val_loss: 0.0586 - val_acc: 0.5555\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0538 - acc: 0.5968 - val_loss: 0.0590 - val_acc: 0.5455\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0542 - acc: 0.5959 - val_loss: 0.0594 - val_acc: 0.5318\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0538 - acc: 0.5953 - val_loss: 0.0590 - val_acc: 0.5424\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 225us/step - loss: 0.0538 - acc: 0.5965 - val_loss: 0.0591 - val_acc: 0.5443\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0537 - acc: 0.6034 - val_loss: 0.0587 - val_acc: 0.5430\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0535 - acc: 0.6040 - val_loss: 0.0585 - val_acc: 0.5549\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0536 - acc: 0.6024 - val_loss: 0.0593 - val_acc: 0.5387\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0539 - acc: 0.5968 - val_loss: 0.0589 - val_acc: 0.5486\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0536 - acc: 0.5965 - val_loss: 0.0590 - val_acc: 0.5468\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0534 - acc: 0.6006 - val_loss: 0.0587 - val_acc: 0.5449\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0538 - acc: 0.5946 - val_loss: 0.0599 - val_acc: 0.5362\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 222us/step - loss: 0.0539 - acc: 0.5949 - val_loss: 0.0595 - val_acc: 0.5362\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0535 - acc: 0.5965 - val_loss: 0.0588 - val_acc: 0.5499\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0535 - acc: 0.5981 - val_loss: 0.0587 - val_acc: 0.5499\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 224us/step - loss: 0.0538 - acc: 0.6002 - val_loss: 0.0593 - val_acc: 0.5536\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 225us/step - loss: 0.0536 - acc: 0.6024 - val_loss: 0.0589 - val_acc: 0.5493\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0532 - acc: 0.5996 - val_loss: 0.0590 - val_acc: 0.5430\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0532 - acc: 0.6046 - val_loss: 0.0589 - val_acc: 0.5468\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 222us/step - loss: 0.0534 - acc: 0.6009 - val_loss: 0.0594 - val_acc: 0.5424\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0533 - acc: 0.6087 - val_loss: 0.0590 - val_acc: 0.5430\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0530 - acc: 0.6062 - val_loss: 0.0599 - val_acc: 0.5355\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0533 - acc: 0.6049 - val_loss: 0.0588 - val_acc: 0.5511\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0534 - acc: 0.6040 - val_loss: 0.0590 - val_acc: 0.5524\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0536 - acc: 0.6002 - val_loss: 0.0585 - val_acc: 0.5524\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0540 - acc: 0.5962 - val_loss: 0.0589 - val_acc: 0.5486\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0533 - acc: 0.6040 - val_loss: 0.0595 - val_acc: 0.5387\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0538 - acc: 0.5971 - val_loss: 0.0594 - val_acc: 0.5399\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 258us/step - loss: 0.0534 - acc: 0.6074 - val_loss: 0.0589 - val_acc: 0.5474\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 289us/step - loss: 0.0532 - acc: 0.6080 - val_loss: 0.0601 - val_acc: 0.5399\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 286us/step - loss: 0.0533 - acc: 0.5999 - val_loss: 0.0591 - val_acc: 0.5468\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 281us/step - loss: 0.0532 - acc: 0.6034 - val_loss: 0.0588 - val_acc: 0.5517\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 285us/step - loss: 0.0531 - acc: 0.6102 - val_loss: 0.0589 - val_acc: 0.5430\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 287us/step - loss: 0.0531 - acc: 0.6049 - val_loss: 0.0602 - val_acc: 0.5411\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 284us/step - loss: 0.0530 - acc: 0.6118 - val_loss: 0.0592 - val_acc: 0.5430\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 284us/step - loss: 0.0530 - acc: 0.6062 - val_loss: 0.0612 - val_acc: 0.5181\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 286us/step - loss: 0.0530 - acc: 0.6112 - val_loss: 0.0590 - val_acc: 0.5574\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 284us/step - loss: 0.0527 - acc: 0.6090 - val_loss: 0.0599 - val_acc: 0.5368\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 284us/step - loss: 0.0535 - acc: 0.6012 - val_loss: 0.0599 - val_acc: 0.5418\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 262us/step - loss: 0.0530 - acc: 0.6059 - val_loss: 0.0598 - val_acc: 0.5424\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0533 - acc: 0.6031 - val_loss: 0.0592 - val_acc: 0.5411\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0529 - acc: 0.6071 - val_loss: 0.0589 - val_acc: 0.5474\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0529 - acc: 0.6149 - val_loss: 0.0594 - val_acc: 0.5411\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0530 - acc: 0.6052 - val_loss: 0.0589 - val_acc: 0.5461\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0532 - acc: 0.6049 - val_loss: 0.0592 - val_acc: 0.5499\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0529 - acc: 0.6080 - val_loss: 0.0593 - val_acc: 0.5486\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 225us/step - loss: 0.0530 - acc: 0.6084 - val_loss: 0.0596 - val_acc: 0.5405\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0527 - acc: 0.6102 - val_loss: 0.0591 - val_acc: 0.5468\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 224us/step - loss: 0.0526 - acc: 0.6121 - val_loss: 0.0594 - val_acc: 0.5468\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0528 - acc: 0.6115 - val_loss: 0.0596 - val_acc: 0.5449\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 221us/step - loss: 0.0536 - acc: 0.6012 - val_loss: 0.0593 - val_acc: 0.5493\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 223us/step - loss: 0.0530 - acc: 0.6059 - val_loss: 0.0588 - val_acc: 0.5592\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 223us/step - loss: 0.0529 - acc: 0.6059 - val_loss: 0.0593 - val_acc: 0.5480\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 218us/step - loss: 0.0526 - acc: 0.6133 - val_loss: 0.0593 - val_acc: 0.5480\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 221us/step - loss: 0.0529 - acc: 0.6090 - val_loss: 0.0597 - val_acc: 0.5480\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0527 - acc: 0.6093 - val_loss: 0.0593 - val_acc: 0.5517\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 217us/step - loss: 0.0530 - acc: 0.6090 - val_loss: 0.0597 - val_acc: 0.5486\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 221us/step - loss: 0.0531 - acc: 0.6071 - val_loss: 0.0593 - val_acc: 0.5474\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 222us/step - loss: 0.0526 - acc: 0.6105 - val_loss: 0.0590 - val_acc: 0.5461\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 1s 222us/step - loss: 0.0558 - acc: 0.5819 - val_loss: 0.0541 - val_acc: 0.5935\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 220us/step - loss: 0.0550 - acc: 0.5921 - val_loss: 0.0564 - val_acc: 0.5717\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 224us/step - loss: 0.0548 - acc: 0.5928 - val_loss: 0.0560 - val_acc: 0.5817\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0545 - acc: 0.5981 - val_loss: 0.0549 - val_acc: 0.5873\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 220us/step - loss: 0.0543 - acc: 0.5946 - val_loss: 0.0555 - val_acc: 0.5792\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0542 - acc: 0.5968 - val_loss: 0.0552 - val_acc: 0.5829\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0542 - acc: 0.5949 - val_loss: 0.0561 - val_acc: 0.5729\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 225us/step - loss: 0.0539 - acc: 0.5965 - val_loss: 0.0564 - val_acc: 0.5723\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0544 - acc: 0.5903 - val_loss: 0.0560 - val_acc: 0.5773\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0537 - acc: 0.6006 - val_loss: 0.0560 - val_acc: 0.5761\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 224us/step - loss: 0.0535 - acc: 0.5993 - val_loss: 0.0561 - val_acc: 0.5729\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 224us/step - loss: 0.0533 - acc: 0.6006 - val_loss: 0.0562 - val_acc: 0.5742\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 224us/step - loss: 0.0535 - acc: 0.6012 - val_loss: 0.0567 - val_acc: 0.5648\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 224us/step - loss: 0.0533 - acc: 0.6006 - val_loss: 0.0566 - val_acc: 0.5667\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 224us/step - loss: 0.0531 - acc: 0.6009 - val_loss: 0.0571 - val_acc: 0.5630\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 224us/step - loss: 0.0533 - acc: 0.6034 - val_loss: 0.0572 - val_acc: 0.5611\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0533 - acc: 0.6056 - val_loss: 0.0571 - val_acc: 0.5555\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 220us/step - loss: 0.0531 - acc: 0.6021 - val_loss: 0.0570 - val_acc: 0.5680\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0532 - acc: 0.6027 - val_loss: 0.0572 - val_acc: 0.5648\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0530 - acc: 0.6037 - val_loss: 0.0579 - val_acc: 0.5574\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0534 - acc: 0.5990 - val_loss: 0.0577 - val_acc: 0.5555\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0531 - acc: 0.6056 - val_loss: 0.0576 - val_acc: 0.5599\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0531 - acc: 0.6056 - val_loss: 0.0574 - val_acc: 0.5617\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0528 - acc: 0.6074 - val_loss: 0.0592 - val_acc: 0.5493\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0529 - acc: 0.6068 - val_loss: 0.0576 - val_acc: 0.5605\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0529 - acc: 0.6074 - val_loss: 0.0577 - val_acc: 0.5630\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0528 - acc: 0.6084 - val_loss: 0.0579 - val_acc: 0.5555\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0527 - acc: 0.6096 - val_loss: 0.0585 - val_acc: 0.5524\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0534 - acc: 0.6052 - val_loss: 0.0584 - val_acc: 0.5623\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0529 - acc: 0.6090 - val_loss: 0.0585 - val_acc: 0.5636\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0530 - acc: 0.6056 - val_loss: 0.0584 - val_acc: 0.5505\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0529 - acc: 0.6071 - val_loss: 0.0580 - val_acc: 0.5586\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0526 - acc: 0.6102 - val_loss: 0.0580 - val_acc: 0.5567\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0530 - acc: 0.6077 - val_loss: 0.0579 - val_acc: 0.5574\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0526 - acc: 0.6105 - val_loss: 0.0581 - val_acc: 0.5561\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0527 - acc: 0.6087 - val_loss: 0.0590 - val_acc: 0.5530\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0529 - acc: 0.6071 - val_loss: 0.0580 - val_acc: 0.5561\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0529 - acc: 0.6093 - val_loss: 0.0580 - val_acc: 0.5555\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0523 - acc: 0.6127 - val_loss: 0.0582 - val_acc: 0.5549\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0526 - acc: 0.6105 - val_loss: 0.0583 - val_acc: 0.5524\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0523 - acc: 0.6162 - val_loss: 0.0588 - val_acc: 0.5524\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0525 - acc: 0.6090 - val_loss: 0.0585 - val_acc: 0.5530\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 225us/step - loss: 0.0527 - acc: 0.6105 - val_loss: 0.0587 - val_acc: 0.5536\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0522 - acc: 0.6143 - val_loss: 0.0581 - val_acc: 0.5536\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0522 - acc: 0.6127 - val_loss: 0.0588 - val_acc: 0.5486\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0523 - acc: 0.6158 - val_loss: 0.0600 - val_acc: 0.5393\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 226us/step - loss: 0.0524 - acc: 0.6109 - val_loss: 0.0597 - val_acc: 0.5380\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0521 - acc: 0.6177 - val_loss: 0.0591 - val_acc: 0.5480\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0525 - acc: 0.6146 - val_loss: 0.0597 - val_acc: 0.5449\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0524 - acc: 0.6124 - val_loss: 0.0586 - val_acc: 0.5505\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0525 - acc: 0.6115 - val_loss: 0.0604 - val_acc: 0.5436\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0533 - acc: 0.6093 - val_loss: 0.0595 - val_acc: 0.5430\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0522 - acc: 0.6124 - val_loss: 0.0590 - val_acc: 0.5449\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0522 - acc: 0.6162 - val_loss: 0.0594 - val_acc: 0.5461\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0523 - acc: 0.6099 - val_loss: 0.0591 - val_acc: 0.5480\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0522 - acc: 0.6155 - val_loss: 0.0597 - val_acc: 0.5443\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0518 - acc: 0.6205 - val_loss: 0.0607 - val_acc: 0.5312\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 264us/step - loss: 0.0523 - acc: 0.6124 - val_loss: 0.0596 - val_acc: 0.5424\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 289us/step - loss: 0.0520 - acc: 0.6155 - val_loss: 0.0586 - val_acc: 0.5567\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 291us/step - loss: 0.0518 - acc: 0.6155 - val_loss: 0.0591 - val_acc: 0.5499\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 287us/step - loss: 0.0519 - acc: 0.6162 - val_loss: 0.0597 - val_acc: 0.5424\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 291us/step - loss: 0.0519 - acc: 0.6202 - val_loss: 0.0600 - val_acc: 0.5355\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 294us/step - loss: 0.0519 - acc: 0.6196 - val_loss: 0.0600 - val_acc: 0.5387\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0521 - acc: 0.6146 - val_loss: 0.0595 - val_acc: 0.5355\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0523 - acc: 0.6127 - val_loss: 0.0596 - val_acc: 0.5374\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0523 - acc: 0.6130 - val_loss: 0.0614 - val_acc: 0.5293\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0526 - acc: 0.6124 - val_loss: 0.0594 - val_acc: 0.5418\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0521 - acc: 0.6155 - val_loss: 0.0597 - val_acc: 0.5387\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0520 - acc: 0.6174 - val_loss: 0.0598 - val_acc: 0.5349\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 238us/step - loss: 0.0522 - acc: 0.6137 - val_loss: 0.0605 - val_acc: 0.5374\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0520 - acc: 0.6143 - val_loss: 0.0599 - val_acc: 0.5380\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0519 - acc: 0.6183 - val_loss: 0.0596 - val_acc: 0.5393\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0517 - acc: 0.6155 - val_loss: 0.0596 - val_acc: 0.5374\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0518 - acc: 0.6146 - val_loss: 0.0601 - val_acc: 0.5349\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0517 - acc: 0.6205 - val_loss: 0.0595 - val_acc: 0.5393\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0517 - acc: 0.6196 - val_loss: 0.0600 - val_acc: 0.5374\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0514 - acc: 0.6233 - val_loss: 0.0607 - val_acc: 0.5324\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0519 - acc: 0.6174 - val_loss: 0.0592 - val_acc: 0.5474\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0521 - acc: 0.6127 - val_loss: 0.0598 - val_acc: 0.5405\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0514 - acc: 0.6199 - val_loss: 0.0597 - val_acc: 0.5418\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0516 - acc: 0.6199 - val_loss: 0.0603 - val_acc: 0.5405\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0515 - acc: 0.6199 - val_loss: 0.0597 - val_acc: 0.5436\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0515 - acc: 0.6239 - val_loss: 0.0592 - val_acc: 0.5430\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0517 - acc: 0.6218 - val_loss: 0.0605 - val_acc: 0.5305\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0514 - acc: 0.6174 - val_loss: 0.0605 - val_acc: 0.5374\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0514 - acc: 0.6186 - val_loss: 0.0595 - val_acc: 0.5399\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0520 - acc: 0.6186 - val_loss: 0.0598 - val_acc: 0.5424\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0513 - acc: 0.6249 - val_loss: 0.0602 - val_acc: 0.5380\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0516 - acc: 0.6158 - val_loss: 0.0597 - val_acc: 0.5493\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 238us/step - loss: 0.0513 - acc: 0.6215 - val_loss: 0.0604 - val_acc: 0.5424\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0513 - acc: 0.6218 - val_loss: 0.0596 - val_acc: 0.5349\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0515 - acc: 0.6205 - val_loss: 0.0604 - val_acc: 0.5368\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0513 - acc: 0.6236 - val_loss: 0.0602 - val_acc: 0.5343\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0514 - acc: 0.6211 - val_loss: 0.0597 - val_acc: 0.5399\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0515 - acc: 0.6190 - val_loss: 0.0601 - val_acc: 0.5393\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0517 - acc: 0.6183 - val_loss: 0.0595 - val_acc: 0.5468\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0515 - acc: 0.6233 - val_loss: 0.0607 - val_acc: 0.5343\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 224us/step - loss: 0.0512 - acc: 0.6252 - val_loss: 0.0603 - val_acc: 0.5399\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0513 - acc: 0.6190 - val_loss: 0.0609 - val_acc: 0.5349\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0512 - acc: 0.6221 - val_loss: 0.0603 - val_acc: 0.5399\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0516 - acc: 0.6193 - val_loss: 0.0604 - val_acc: 0.5349\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0515 - acc: 0.6180 - val_loss: 0.0601 - val_acc: 0.5405\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0513 - acc: 0.6218 - val_loss: 0.0603 - val_acc: 0.5461\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0511 - acc: 0.6249 - val_loss: 0.0601 - val_acc: 0.5430\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 225us/step - loss: 0.0512 - acc: 0.6215 - val_loss: 0.0599 - val_acc: 0.5380\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0512 - acc: 0.6246 - val_loss: 0.0608 - val_acc: 0.5337\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0509 - acc: 0.6236 - val_loss: 0.0604 - val_acc: 0.5387\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0514 - acc: 0.6224 - val_loss: 0.0608 - val_acc: 0.5380\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0513 - acc: 0.6230 - val_loss: 0.0604 - val_acc: 0.5387\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0509 - acc: 0.6274 - val_loss: 0.0605 - val_acc: 0.5374\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0511 - acc: 0.6218 - val_loss: 0.0603 - val_acc: 0.5424\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0510 - acc: 0.6211 - val_loss: 0.0604 - val_acc: 0.5374\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0511 - acc: 0.6264 - val_loss: 0.0605 - val_acc: 0.5337\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0512 - acc: 0.6283 - val_loss: 0.0617 - val_acc: 0.5281\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 238us/step - loss: 0.0510 - acc: 0.6236 - val_loss: 0.0601 - val_acc: 0.5436\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0511 - acc: 0.6230 - val_loss: 0.0606 - val_acc: 0.5305\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0510 - acc: 0.6261 - val_loss: 0.0602 - val_acc: 0.5337\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0505 - acc: 0.6283 - val_loss: 0.0610 - val_acc: 0.5256\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0511 - acc: 0.6243 - val_loss: 0.0608 - val_acc: 0.5312\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0509 - acc: 0.6289 - val_loss: 0.0602 - val_acc: 0.5436\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0512 - acc: 0.6199 - val_loss: 0.0610 - val_acc: 0.5324\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0512 - acc: 0.6236 - val_loss: 0.0608 - val_acc: 0.5380\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0505 - acc: 0.6330 - val_loss: 0.0613 - val_acc: 0.5468\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0505 - acc: 0.6299 - val_loss: 0.0609 - val_acc: 0.5374\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0507 - acc: 0.6277 - val_loss: 0.0620 - val_acc: 0.5224\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0507 - acc: 0.6268 - val_loss: 0.0602 - val_acc: 0.5411\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0510 - acc: 0.6261 - val_loss: 0.0607 - val_acc: 0.5399\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0510 - acc: 0.6218 - val_loss: 0.0605 - val_acc: 0.5355\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0507 - acc: 0.6243 - val_loss: 0.0604 - val_acc: 0.5349\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0511 - acc: 0.6236 - val_loss: 0.0605 - val_acc: 0.5380\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0506 - acc: 0.6249 - val_loss: 0.0613 - val_acc: 0.5249\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0506 - acc: 0.6286 - val_loss: 0.0608 - val_acc: 0.5380\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0506 - acc: 0.6299 - val_loss: 0.0611 - val_acc: 0.5337\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0503 - acc: 0.6277 - val_loss: 0.0614 - val_acc: 0.5287\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0503 - acc: 0.6292 - val_loss: 0.0613 - val_acc: 0.5399\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0510 - acc: 0.6230 - val_loss: 0.0603 - val_acc: 0.5411\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0510 - acc: 0.6274 - val_loss: 0.0609 - val_acc: 0.5318\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0504 - acc: 0.6268 - val_loss: 0.0606 - val_acc: 0.5355\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0509 - acc: 0.6243 - val_loss: 0.0607 - val_acc: 0.5337\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0505 - acc: 0.6296 - val_loss: 0.0615 - val_acc: 0.5268\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 241us/step - loss: 0.0506 - acc: 0.6255 - val_loss: 0.0609 - val_acc: 0.5374\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0506 - acc: 0.6289 - val_loss: 0.0612 - val_acc: 0.5337\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0506 - acc: 0.6289 - val_loss: 0.0603 - val_acc: 0.5405\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 238us/step - loss: 0.0503 - acc: 0.6255 - val_loss: 0.0611 - val_acc: 0.5343\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0508 - acc: 0.6280 - val_loss: 0.0607 - val_acc: 0.5349\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0503 - acc: 0.6299 - val_loss: 0.0607 - val_acc: 0.5461\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0502 - acc: 0.6296 - val_loss: 0.0605 - val_acc: 0.5399\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0505 - acc: 0.6271 - val_loss: 0.0615 - val_acc: 0.5393\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0505 - acc: 0.6211 - val_loss: 0.0617 - val_acc: 0.5368\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 238us/step - loss: 0.0506 - acc: 0.6264 - val_loss: 0.0615 - val_acc: 0.5355\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0501 - acc: 0.6280 - val_loss: 0.0605 - val_acc: 0.5393\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0505 - acc: 0.6255 - val_loss: 0.0614 - val_acc: 0.5349\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0501 - acc: 0.6345 - val_loss: 0.0610 - val_acc: 0.5330\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0516 - acc: 0.6255 - val_loss: 0.0615 - val_acc: 0.5324\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0504 - acc: 0.6299 - val_loss: 0.0608 - val_acc: 0.5343\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0503 - acc: 0.6255 - val_loss: 0.0616 - val_acc: 0.5318\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0497 - acc: 0.6370 - val_loss: 0.0613 - val_acc: 0.5337\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0499 - acc: 0.6361 - val_loss: 0.0612 - val_acc: 0.5274\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0498 - acc: 0.6321 - val_loss: 0.0613 - val_acc: 0.5312\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0499 - acc: 0.6327 - val_loss: 0.0618 - val_acc: 0.5312\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0496 - acc: 0.6374 - val_loss: 0.0609 - val_acc: 0.5374\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0500 - acc: 0.6327 - val_loss: 0.0613 - val_acc: 0.5362\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0499 - acc: 0.6364 - val_loss: 0.0616 - val_acc: 0.5374\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 227us/step - loss: 0.0498 - acc: 0.6370 - val_loss: 0.0614 - val_acc: 0.5424\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0498 - acc: 0.6324 - val_loss: 0.0621 - val_acc: 0.5281\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0500 - acc: 0.6345 - val_loss: 0.0631 - val_acc: 0.5137\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0498 - acc: 0.6370 - val_loss: 0.0608 - val_acc: 0.5405\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0497 - acc: 0.6342 - val_loss: 0.0625 - val_acc: 0.5262\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0500 - acc: 0.6314 - val_loss: 0.0615 - val_acc: 0.5374\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0499 - acc: 0.6333 - val_loss: 0.0616 - val_acc: 0.5374\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 229us/step - loss: 0.0499 - acc: 0.6361 - val_loss: 0.0622 - val_acc: 0.5324\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0504 - acc: 0.6274 - val_loss: 0.0610 - val_acc: 0.5330\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0498 - acc: 0.6374 - val_loss: 0.0620 - val_acc: 0.5287\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0501 - acc: 0.6352 - val_loss: 0.0616 - val_acc: 0.5318\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0498 - acc: 0.6380 - val_loss: 0.0614 - val_acc: 0.5312\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0500 - acc: 0.6349 - val_loss: 0.0622 - val_acc: 0.5243\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0496 - acc: 0.6367 - val_loss: 0.0620 - val_acc: 0.5387\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 231us/step - loss: 0.0500 - acc: 0.6330 - val_loss: 0.0625 - val_acc: 0.5299\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0497 - acc: 0.6370 - val_loss: 0.0613 - val_acc: 0.5299\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0499 - acc: 0.6342 - val_loss: 0.0630 - val_acc: 0.5168\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0502 - acc: 0.6349 - val_loss: 0.0615 - val_acc: 0.5362\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0495 - acc: 0.6345 - val_loss: 0.0625 - val_acc: 0.5262\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0493 - acc: 0.6430 - val_loss: 0.0617 - val_acc: 0.5387\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0502 - acc: 0.6327 - val_loss: 0.0619 - val_acc: 0.5349\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0496 - acc: 0.6330 - val_loss: 0.0619 - val_acc: 0.5349\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 228us/step - loss: 0.0502 - acc: 0.6305 - val_loss: 0.0617 - val_acc: 0.5337\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0502 - acc: 0.6286 - val_loss: 0.0618 - val_acc: 0.5318\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0496 - acc: 0.6405 - val_loss: 0.0614 - val_acc: 0.5337\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0492 - acc: 0.6392 - val_loss: 0.0618 - val_acc: 0.5343\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 239us/step - loss: 0.0495 - acc: 0.6405 - val_loss: 0.0618 - val_acc: 0.5312\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0500 - acc: 0.6411 - val_loss: 0.0618 - val_acc: 0.5387\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0494 - acc: 0.6399 - val_loss: 0.0616 - val_acc: 0.5374\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0494 - acc: 0.6408 - val_loss: 0.0618 - val_acc: 0.5305\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0495 - acc: 0.6395 - val_loss: 0.0626 - val_acc: 0.5249\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0496 - acc: 0.6386 - val_loss: 0.0622 - val_acc: 0.5274\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0496 - acc: 0.6374 - val_loss: 0.0625 - val_acc: 0.5193\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0495 - acc: 0.6386 - val_loss: 0.0618 - val_acc: 0.5224\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0493 - acc: 0.6433 - val_loss: 0.0624 - val_acc: 0.5343\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0490 - acc: 0.6442 - val_loss: 0.0625 - val_acc: 0.5287\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 232us/step - loss: 0.0493 - acc: 0.6364 - val_loss: 0.0618 - val_acc: 0.5281\n",
      "Train on 3208 samples, validate on 1603 samples\n",
      "Epoch 1/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0564 - acc: 0.5686 - val_loss: 0.0504 - val_acc: 0.6369\n",
      "Epoch 2/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0552 - acc: 0.5814 - val_loss: 0.0520 - val_acc: 0.6257\n",
      "Epoch 3/200\n",
      "3208/3208 [==============================] - 1s 240us/step - loss: 0.0548 - acc: 0.5879 - val_loss: 0.0499 - val_acc: 0.6463\n",
      "Epoch 4/200\n",
      "3208/3208 [==============================] - 1s 229us/step - loss: 0.0544 - acc: 0.5898 - val_loss: 0.0524 - val_acc: 0.6163\n",
      "Epoch 5/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0543 - acc: 0.5913 - val_loss: 0.0517 - val_acc: 0.6263\n",
      "Epoch 6/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0547 - acc: 0.5854 - val_loss: 0.0508 - val_acc: 0.6313\n",
      "Epoch 7/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0539 - acc: 0.5988 - val_loss: 0.0509 - val_acc: 0.6307\n",
      "Epoch 8/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0544 - acc: 0.5892 - val_loss: 0.0509 - val_acc: 0.6294\n",
      "Epoch 9/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0541 - acc: 0.5938 - val_loss: 0.0516 - val_acc: 0.6245\n",
      "Epoch 10/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0544 - acc: 0.5960 - val_loss: 0.0522 - val_acc: 0.6188\n",
      "Epoch 11/200\n",
      "3208/3208 [==============================] - 1s 240us/step - loss: 0.0541 - acc: 0.5913 - val_loss: 0.0509 - val_acc: 0.6288\n",
      "Epoch 12/200\n",
      "3208/3208 [==============================] - 1s 228us/step - loss: 0.0536 - acc: 0.5945 - val_loss: 0.0514 - val_acc: 0.6288\n",
      "Epoch 13/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0535 - acc: 0.5954 - val_loss: 0.0511 - val_acc: 0.6263\n",
      "Epoch 14/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0534 - acc: 0.6001 - val_loss: 0.0515 - val_acc: 0.6213\n",
      "Epoch 15/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0536 - acc: 0.5982 - val_loss: 0.0519 - val_acc: 0.6195\n",
      "Epoch 16/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0534 - acc: 0.6007 - val_loss: 0.0530 - val_acc: 0.6145\n",
      "Epoch 17/200\n",
      "3208/3208 [==============================] - 1s 240us/step - loss: 0.0531 - acc: 0.6054 - val_loss: 0.0522 - val_acc: 0.6207\n",
      "Epoch 18/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0536 - acc: 0.6047 - val_loss: 0.0531 - val_acc: 0.6195\n",
      "Epoch 19/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0531 - acc: 0.6001 - val_loss: 0.0522 - val_acc: 0.6195\n",
      "Epoch 20/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0528 - acc: 0.6082 - val_loss: 0.0527 - val_acc: 0.6107\n",
      "Epoch 21/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0527 - acc: 0.6072 - val_loss: 0.0528 - val_acc: 0.6157\n",
      "Epoch 22/200\n",
      "3208/3208 [==============================] - 1s 240us/step - loss: 0.0533 - acc: 0.6041 - val_loss: 0.0519 - val_acc: 0.6188\n",
      "Epoch 23/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0530 - acc: 0.6050 - val_loss: 0.0530 - val_acc: 0.6001\n",
      "Epoch 24/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0525 - acc: 0.6057 - val_loss: 0.0529 - val_acc: 0.6057\n",
      "Epoch 25/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0529 - acc: 0.6054 - val_loss: 0.0520 - val_acc: 0.6201\n",
      "Epoch 26/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0527 - acc: 0.6032 - val_loss: 0.0528 - val_acc: 0.6163\n",
      "Epoch 27/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0526 - acc: 0.6035 - val_loss: 0.0536 - val_acc: 0.5951\n",
      "Epoch 28/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0530 - acc: 0.6029 - val_loss: 0.0523 - val_acc: 0.6176\n",
      "Epoch 29/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0524 - acc: 0.6075 - val_loss: 0.0537 - val_acc: 0.6101\n",
      "Epoch 30/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0528 - acc: 0.6054 - val_loss: 0.0526 - val_acc: 0.6151\n",
      "Epoch 31/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0523 - acc: 0.6072 - val_loss: 0.0527 - val_acc: 0.6138\n",
      "Epoch 32/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0527 - acc: 0.6094 - val_loss: 0.0536 - val_acc: 0.6095\n",
      "Epoch 33/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0529 - acc: 0.6019 - val_loss: 0.0524 - val_acc: 0.6195\n",
      "Epoch 34/200\n",
      "3208/3208 [==============================] - 1s 230us/step - loss: 0.0525 - acc: 0.6082 - val_loss: 0.0534 - val_acc: 0.6089\n",
      "Epoch 35/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0525 - acc: 0.6085 - val_loss: 0.0535 - val_acc: 0.6145\n",
      "Epoch 36/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0523 - acc: 0.6072 - val_loss: 0.0532 - val_acc: 0.6126\n",
      "Epoch 37/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0524 - acc: 0.6072 - val_loss: 0.0531 - val_acc: 0.6120\n",
      "Epoch 38/200\n",
      "3208/3208 [==============================] - 1s 226us/step - loss: 0.0525 - acc: 0.6066 - val_loss: 0.0528 - val_acc: 0.6163\n",
      "Epoch 39/200\n",
      "3208/3208 [==============================] - 1s 230us/step - loss: 0.0524 - acc: 0.6125 - val_loss: 0.0529 - val_acc: 0.6132\n",
      "Epoch 40/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0522 - acc: 0.6122 - val_loss: 0.0538 - val_acc: 0.6070\n",
      "Epoch 41/200\n",
      "3208/3208 [==============================] - 1s 229us/step - loss: 0.0522 - acc: 0.6097 - val_loss: 0.0549 - val_acc: 0.5976\n",
      "Epoch 42/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0523 - acc: 0.6085 - val_loss: 0.0542 - val_acc: 0.6026\n",
      "Epoch 43/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0522 - acc: 0.6172 - val_loss: 0.0531 - val_acc: 0.6138\n",
      "Epoch 44/200\n",
      "3208/3208 [==============================] - 1s 228us/step - loss: 0.0518 - acc: 0.6141 - val_loss: 0.0531 - val_acc: 0.6076\n",
      "Epoch 45/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0521 - acc: 0.6135 - val_loss: 0.0536 - val_acc: 0.6020\n",
      "Epoch 46/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0519 - acc: 0.6169 - val_loss: 0.0540 - val_acc: 0.6007\n",
      "Epoch 47/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0520 - acc: 0.6113 - val_loss: 0.0540 - val_acc: 0.6020\n",
      "Epoch 48/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0521 - acc: 0.6100 - val_loss: 0.0544 - val_acc: 0.5995\n",
      "Epoch 49/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0518 - acc: 0.6122 - val_loss: 0.0538 - val_acc: 0.6057\n",
      "Epoch 50/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0517 - acc: 0.6128 - val_loss: 0.0541 - val_acc: 0.6076\n",
      "Epoch 51/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0518 - acc: 0.6213 - val_loss: 0.0537 - val_acc: 0.6095\n",
      "Epoch 52/200\n",
      "3208/3208 [==============================] - 1s 226us/step - loss: 0.0520 - acc: 0.6107 - val_loss: 0.0535 - val_acc: 0.6120\n",
      "Epoch 53/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0519 - acc: 0.6141 - val_loss: 0.0538 - val_acc: 0.6082\n",
      "Epoch 54/200\n",
      "3208/3208 [==============================] - 1s 230us/step - loss: 0.0515 - acc: 0.6194 - val_loss: 0.0552 - val_acc: 0.5926\n",
      "Epoch 55/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0520 - acc: 0.6079 - val_loss: 0.0552 - val_acc: 0.5964\n",
      "Epoch 56/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0520 - acc: 0.6135 - val_loss: 0.0535 - val_acc: 0.6089\n",
      "Epoch 57/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0515 - acc: 0.6181 - val_loss: 0.0540 - val_acc: 0.6057\n",
      "Epoch 58/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0520 - acc: 0.6116 - val_loss: 0.0544 - val_acc: 0.5976\n",
      "Epoch 59/200\n",
      "3208/3208 [==============================] - 1s 230us/step - loss: 0.0517 - acc: 0.6116 - val_loss: 0.0540 - val_acc: 0.6039\n",
      "Epoch 60/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0516 - acc: 0.6185 - val_loss: 0.0545 - val_acc: 0.6039\n",
      "Epoch 61/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0515 - acc: 0.6138 - val_loss: 0.0536 - val_acc: 0.6120\n",
      "Epoch 62/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0515 - acc: 0.6141 - val_loss: 0.0538 - val_acc: 0.5983\n",
      "Epoch 63/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0516 - acc: 0.6153 - val_loss: 0.0550 - val_acc: 0.6014\n",
      "Epoch 64/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0518 - acc: 0.6213 - val_loss: 0.0548 - val_acc: 0.5901\n",
      "Epoch 65/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0512 - acc: 0.6200 - val_loss: 0.0536 - val_acc: 0.6076\n",
      "Epoch 66/200\n",
      "3208/3208 [==============================] - 1s 226us/step - loss: 0.0517 - acc: 0.6160 - val_loss: 0.0541 - val_acc: 0.6082\n",
      "Epoch 67/200\n",
      "3208/3208 [==============================] - 1s 230us/step - loss: 0.0513 - acc: 0.6169 - val_loss: 0.0548 - val_acc: 0.5976\n",
      "Epoch 68/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0511 - acc: 0.6172 - val_loss: 0.0542 - val_acc: 0.6007\n",
      "Epoch 69/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0513 - acc: 0.6160 - val_loss: 0.0561 - val_acc: 0.5814\n",
      "Epoch 70/200\n",
      "3208/3208 [==============================] - 1s 230us/step - loss: 0.0508 - acc: 0.6256 - val_loss: 0.0548 - val_acc: 0.5883\n",
      "Epoch 71/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0510 - acc: 0.6231 - val_loss: 0.0549 - val_acc: 0.5970\n",
      "Epoch 72/200\n",
      "3208/3208 [==============================] - 1s 230us/step - loss: 0.0512 - acc: 0.6213 - val_loss: 0.0540 - val_acc: 0.6114\n",
      "Epoch 73/200\n",
      "3208/3208 [==============================] - 1s 229us/step - loss: 0.0514 - acc: 0.6169 - val_loss: 0.0541 - val_acc: 0.6076\n",
      "Epoch 74/200\n",
      "3208/3208 [==============================] - 1s 226us/step - loss: 0.0518 - acc: 0.6163 - val_loss: 0.0546 - val_acc: 0.6014\n",
      "Epoch 75/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0516 - acc: 0.6138 - val_loss: 0.0559 - val_acc: 0.5939\n",
      "Epoch 76/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0518 - acc: 0.6169 - val_loss: 0.0545 - val_acc: 0.6020\n",
      "Epoch 77/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0510 - acc: 0.6185 - val_loss: 0.0553 - val_acc: 0.5864\n",
      "Epoch 78/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0508 - acc: 0.6244 - val_loss: 0.0543 - val_acc: 0.6007\n",
      "Epoch 79/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0509 - acc: 0.6216 - val_loss: 0.0549 - val_acc: 0.5976\n",
      "Epoch 80/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0509 - acc: 0.6238 - val_loss: 0.0552 - val_acc: 0.5895\n",
      "Epoch 81/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0507 - acc: 0.6262 - val_loss: 0.0555 - val_acc: 0.5920\n",
      "Epoch 82/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0510 - acc: 0.6234 - val_loss: 0.0545 - val_acc: 0.6014\n",
      "Epoch 83/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0513 - acc: 0.6206 - val_loss: 0.0545 - val_acc: 0.6001\n",
      "Epoch 84/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0506 - acc: 0.6216 - val_loss: 0.0558 - val_acc: 0.5889\n",
      "Epoch 85/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0516 - acc: 0.6116 - val_loss: 0.0549 - val_acc: 0.6026\n",
      "Epoch 86/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0513 - acc: 0.6156 - val_loss: 0.0557 - val_acc: 0.5964\n",
      "Epoch 87/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0508 - acc: 0.6216 - val_loss: 0.0553 - val_acc: 0.5976\n",
      "Epoch 88/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0508 - acc: 0.6203 - val_loss: 0.0555 - val_acc: 0.5876\n",
      "Epoch 89/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0508 - acc: 0.6188 - val_loss: 0.0548 - val_acc: 0.5951\n",
      "Epoch 90/200\n",
      "3208/3208 [==============================] - 1s 240us/step - loss: 0.0510 - acc: 0.6200 - val_loss: 0.0556 - val_acc: 0.5858\n",
      "Epoch 91/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0509 - acc: 0.6231 - val_loss: 0.0545 - val_acc: 0.6051\n",
      "Epoch 92/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0504 - acc: 0.6272 - val_loss: 0.0549 - val_acc: 0.5983\n",
      "Epoch 93/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0503 - acc: 0.6303 - val_loss: 0.0553 - val_acc: 0.6026\n",
      "Epoch 94/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0503 - acc: 0.6256 - val_loss: 0.0554 - val_acc: 0.6007\n",
      "Epoch 95/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0502 - acc: 0.6287 - val_loss: 0.0544 - val_acc: 0.6064\n",
      "Epoch 96/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0505 - acc: 0.6231 - val_loss: 0.0558 - val_acc: 0.5908\n",
      "Epoch 97/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0506 - acc: 0.6256 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 98/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0507 - acc: 0.6238 - val_loss: 0.0553 - val_acc: 0.5933\n",
      "Epoch 99/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0508 - acc: 0.6234 - val_loss: 0.0549 - val_acc: 0.5983\n",
      "Epoch 100/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0502 - acc: 0.6247 - val_loss: 0.0550 - val_acc: 0.5995\n",
      "Epoch 101/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0503 - acc: 0.6275 - val_loss: 0.0556 - val_acc: 0.5814\n",
      "Epoch 102/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0507 - acc: 0.6228 - val_loss: 0.0548 - val_acc: 0.6001\n",
      "Epoch 103/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0506 - acc: 0.6241 - val_loss: 0.0549 - val_acc: 0.6014\n",
      "Epoch 104/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0505 - acc: 0.6300 - val_loss: 0.0548 - val_acc: 0.5951\n",
      "Epoch 105/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0505 - acc: 0.6244 - val_loss: 0.0556 - val_acc: 0.5870\n",
      "Epoch 106/200\n",
      "3208/3208 [==============================] - 1s 240us/step - loss: 0.0499 - acc: 0.6291 - val_loss: 0.0570 - val_acc: 0.5802\n",
      "Epoch 107/200\n",
      "3208/3208 [==============================] - 1s 240us/step - loss: 0.0501 - acc: 0.6306 - val_loss: 0.0551 - val_acc: 0.5920\n",
      "Epoch 108/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0498 - acc: 0.6291 - val_loss: 0.0572 - val_acc: 0.5739\n",
      "Epoch 109/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0505 - acc: 0.6238 - val_loss: 0.0553 - val_acc: 0.6026\n",
      "Epoch 110/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0508 - acc: 0.6228 - val_loss: 0.0560 - val_acc: 0.5970\n",
      "Epoch 111/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0503 - acc: 0.6266 - val_loss: 0.0552 - val_acc: 0.5933\n",
      "Epoch 112/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0507 - acc: 0.6222 - val_loss: 0.0570 - val_acc: 0.5802\n",
      "Epoch 113/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0500 - acc: 0.6259 - val_loss: 0.0550 - val_acc: 0.5926\n",
      "Epoch 114/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0502 - acc: 0.6309 - val_loss: 0.0559 - val_acc: 0.5914\n",
      "Epoch 115/200\n",
      "3208/3208 [==============================] - 1s 229us/step - loss: 0.0504 - acc: 0.6241 - val_loss: 0.0558 - val_acc: 0.5933\n",
      "Epoch 116/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0508 - acc: 0.6228 - val_loss: 0.0560 - val_acc: 0.5901\n",
      "Epoch 117/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0501 - acc: 0.6325 - val_loss: 0.0559 - val_acc: 0.5883\n",
      "Epoch 118/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0499 - acc: 0.6284 - val_loss: 0.0561 - val_acc: 0.5876\n",
      "Epoch 119/200\n",
      "3208/3208 [==============================] - 1s 230us/step - loss: 0.0500 - acc: 0.6300 - val_loss: 0.0560 - val_acc: 0.5833\n",
      "Epoch 120/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0501 - acc: 0.6275 - val_loss: 0.0558 - val_acc: 0.5858\n",
      "Epoch 121/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0502 - acc: 0.6291 - val_loss: 0.0557 - val_acc: 0.5976\n",
      "Epoch 122/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0504 - acc: 0.6209 - val_loss: 0.0551 - val_acc: 0.5926\n",
      "Epoch 123/200\n",
      "3208/3208 [==============================] - 1s 228us/step - loss: 0.0497 - acc: 0.6315 - val_loss: 0.0558 - val_acc: 0.5914\n",
      "Epoch 124/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0504 - acc: 0.6262 - val_loss: 0.0554 - val_acc: 0.5983\n",
      "Epoch 125/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0500 - acc: 0.6250 - val_loss: 0.0553 - val_acc: 0.5989\n",
      "Epoch 126/200\n",
      "3208/3208 [==============================] - 1s 225us/step - loss: 0.0496 - acc: 0.6315 - val_loss: 0.0555 - val_acc: 0.5951\n",
      "Epoch 127/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0498 - acc: 0.6356 - val_loss: 0.0551 - val_acc: 0.6032\n",
      "Epoch 128/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0500 - acc: 0.6294 - val_loss: 0.0555 - val_acc: 0.5920\n",
      "Epoch 129/200\n",
      "3208/3208 [==============================] - 1s 226us/step - loss: 0.0500 - acc: 0.6281 - val_loss: 0.0566 - val_acc: 0.5858\n",
      "Epoch 130/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0495 - acc: 0.6328 - val_loss: 0.0555 - val_acc: 0.5920\n",
      "Epoch 131/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0496 - acc: 0.6347 - val_loss: 0.0568 - val_acc: 0.5795\n",
      "Epoch 132/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0502 - acc: 0.6231 - val_loss: 0.0560 - val_acc: 0.5914\n",
      "Epoch 133/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0497 - acc: 0.6340 - val_loss: 0.0565 - val_acc: 0.5852\n",
      "Epoch 134/200\n",
      "3208/3208 [==============================] - 1s 227us/step - loss: 0.0496 - acc: 0.6284 - val_loss: 0.0559 - val_acc: 0.5895\n",
      "Epoch 135/200\n",
      "3208/3208 [==============================] - 1s 230us/step - loss: 0.0517 - acc: 0.6132 - val_loss: 0.0557 - val_acc: 0.5908\n",
      "Epoch 136/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0500 - acc: 0.6294 - val_loss: 0.0560 - val_acc: 0.5914\n",
      "Epoch 137/200\n",
      "3208/3208 [==============================] - 1s 227us/step - loss: 0.0498 - acc: 0.6344 - val_loss: 0.0559 - val_acc: 0.5901\n",
      "Epoch 138/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0497 - acc: 0.6322 - val_loss: 0.0559 - val_acc: 0.5883\n",
      "Epoch 139/200\n",
      "3208/3208 [==============================] - 1s 230us/step - loss: 0.0503 - acc: 0.6250 - val_loss: 0.0558 - val_acc: 0.5908\n",
      "Epoch 140/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0502 - acc: 0.6294 - val_loss: 0.0561 - val_acc: 0.5964\n",
      "Epoch 141/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0501 - acc: 0.6294 - val_loss: 0.0568 - val_acc: 0.5852\n",
      "Epoch 142/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0495 - acc: 0.6353 - val_loss: 0.0558 - val_acc: 0.5933\n",
      "Epoch 143/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0497 - acc: 0.6347 - val_loss: 0.0556 - val_acc: 0.5883\n",
      "Epoch 144/200\n",
      "3208/3208 [==============================] - 1s 229us/step - loss: 0.0499 - acc: 0.6247 - val_loss: 0.0557 - val_acc: 0.5914\n",
      "Epoch 145/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0498 - acc: 0.6297 - val_loss: 0.0581 - val_acc: 0.5614\n",
      "Epoch 146/200\n",
      "3208/3208 [==============================] - 1s 229us/step - loss: 0.0499 - acc: 0.6284 - val_loss: 0.0563 - val_acc: 0.5852\n",
      "Epoch 147/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0497 - acc: 0.6322 - val_loss: 0.0565 - val_acc: 0.5852\n",
      "Epoch 148/200\n",
      "3208/3208 [==============================] - 1s 227us/step - loss: 0.0499 - acc: 0.6275 - val_loss: 0.0568 - val_acc: 0.5852\n",
      "Epoch 149/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0497 - acc: 0.6334 - val_loss: 0.0563 - val_acc: 0.5845\n",
      "Epoch 150/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0492 - acc: 0.6297 - val_loss: 0.0563 - val_acc: 0.5858\n",
      "Epoch 151/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0498 - acc: 0.6340 - val_loss: 0.0572 - val_acc: 0.5852\n",
      "Epoch 152/200\n",
      "3208/3208 [==============================] - 1s 229us/step - loss: 0.0498 - acc: 0.6328 - val_loss: 0.0579 - val_acc: 0.5783\n",
      "Epoch 153/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0497 - acc: 0.6294 - val_loss: 0.0571 - val_acc: 0.5852\n",
      "Epoch 154/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0494 - acc: 0.6393 - val_loss: 0.0569 - val_acc: 0.5858\n",
      "Epoch 155/200\n",
      "3208/3208 [==============================] - 1s 229us/step - loss: 0.0498 - acc: 0.6306 - val_loss: 0.0569 - val_acc: 0.5908\n",
      "Epoch 156/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0493 - acc: 0.6325 - val_loss: 0.0560 - val_acc: 0.5889\n",
      "Epoch 157/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0492 - acc: 0.6365 - val_loss: 0.0557 - val_acc: 0.5908\n",
      "Epoch 158/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0499 - acc: 0.6328 - val_loss: 0.0557 - val_acc: 0.6045\n",
      "Epoch 159/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0492 - acc: 0.6368 - val_loss: 0.0562 - val_acc: 0.5976\n",
      "Epoch 160/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0491 - acc: 0.6347 - val_loss: 0.0577 - val_acc: 0.5789\n",
      "Epoch 161/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0491 - acc: 0.6387 - val_loss: 0.0560 - val_acc: 0.5989\n",
      "Epoch 162/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0490 - acc: 0.6378 - val_loss: 0.0567 - val_acc: 0.5901\n",
      "Epoch 163/200\n",
      "3208/3208 [==============================] - 1s 240us/step - loss: 0.0492 - acc: 0.6337 - val_loss: 0.0564 - val_acc: 0.5895\n",
      "Epoch 164/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0487 - acc: 0.6359 - val_loss: 0.0567 - val_acc: 0.5864\n",
      "Epoch 165/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0496 - acc: 0.6291 - val_loss: 0.0571 - val_acc: 0.5839\n",
      "Epoch 166/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0500 - acc: 0.6234 - val_loss: 0.0556 - val_acc: 0.6001\n",
      "Epoch 167/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0491 - acc: 0.6337 - val_loss: 0.0568 - val_acc: 0.5820\n",
      "Epoch 168/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0488 - acc: 0.6378 - val_loss: 0.0564 - val_acc: 0.5914\n",
      "Epoch 169/200\n",
      "3208/3208 [==============================] - 1s 240us/step - loss: 0.0488 - acc: 0.6428 - val_loss: 0.0571 - val_acc: 0.5876\n",
      "Epoch 170/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0497 - acc: 0.6334 - val_loss: 0.0569 - val_acc: 0.5839\n",
      "Epoch 171/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0493 - acc: 0.6384 - val_loss: 0.0572 - val_acc: 0.5827\n",
      "Epoch 172/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0492 - acc: 0.6359 - val_loss: 0.0568 - val_acc: 0.5845\n",
      "Epoch 173/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0494 - acc: 0.6337 - val_loss: 0.0568 - val_acc: 0.5833\n",
      "Epoch 174/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0491 - acc: 0.6378 - val_loss: 0.0571 - val_acc: 0.5827\n",
      "Epoch 175/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0494 - acc: 0.6337 - val_loss: 0.0566 - val_acc: 0.5858\n",
      "Epoch 176/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0492 - acc: 0.6340 - val_loss: 0.0588 - val_acc: 0.5727\n",
      "Epoch 177/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0490 - acc: 0.6353 - val_loss: 0.0563 - val_acc: 0.5839\n",
      "Epoch 178/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0488 - acc: 0.6400 - val_loss: 0.0570 - val_acc: 0.5852\n",
      "Epoch 179/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0488 - acc: 0.6359 - val_loss: 0.0571 - val_acc: 0.5789\n",
      "Epoch 180/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0490 - acc: 0.6362 - val_loss: 0.0565 - val_acc: 0.5883\n",
      "Epoch 181/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0491 - acc: 0.6372 - val_loss: 0.0571 - val_acc: 0.5876\n",
      "Epoch 182/200\n",
      "3208/3208 [==============================] - 1s 274us/step - loss: 0.0498 - acc: 0.6322 - val_loss: 0.0572 - val_acc: 0.5920\n",
      "Epoch 183/200\n",
      "3208/3208 [==============================] - 1s 286us/step - loss: 0.0495 - acc: 0.6372 - val_loss: 0.0567 - val_acc: 0.5852\n",
      "Epoch 184/200\n",
      "3208/3208 [==============================] - 1s 286us/step - loss: 0.0491 - acc: 0.6350 - val_loss: 0.0570 - val_acc: 0.5870\n",
      "Epoch 185/200\n",
      "3208/3208 [==============================] - 1s 288us/step - loss: 0.0497 - acc: 0.6284 - val_loss: 0.0574 - val_acc: 0.5895\n",
      "Epoch 186/200\n",
      "3208/3208 [==============================] - 1s 286us/step - loss: 0.0499 - acc: 0.6284 - val_loss: 0.0571 - val_acc: 0.5883\n",
      "Epoch 187/200\n",
      "3208/3208 [==============================] - 1s 285us/step - loss: 0.0489 - acc: 0.6434 - val_loss: 0.0580 - val_acc: 0.5783\n",
      "Epoch 188/200\n",
      "3208/3208 [==============================] - 1s 284us/step - loss: 0.0489 - acc: 0.6390 - val_loss: 0.0571 - val_acc: 0.5901\n",
      "Epoch 189/200\n",
      "3208/3208 [==============================] - 1s 287us/step - loss: 0.0488 - acc: 0.6431 - val_loss: 0.0577 - val_acc: 0.5820\n",
      "Epoch 190/200\n",
      "3208/3208 [==============================] - 1s 281us/step - loss: 0.0491 - acc: 0.6372 - val_loss: 0.0569 - val_acc: 0.5958\n",
      "Epoch 191/200\n",
      "3208/3208 [==============================] - 1s 285us/step - loss: 0.0489 - acc: 0.6406 - val_loss: 0.0569 - val_acc: 0.5895\n",
      "Epoch 192/200\n",
      "3208/3208 [==============================] - 1s 287us/step - loss: 0.0491 - acc: 0.6359 - val_loss: 0.0566 - val_acc: 0.5901\n",
      "Epoch 193/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0488 - acc: 0.6412 - val_loss: 0.0571 - val_acc: 0.5883\n",
      "Epoch 194/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0497 - acc: 0.6331 - val_loss: 0.0566 - val_acc: 0.5920\n",
      "Epoch 195/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0486 - acc: 0.6365 - val_loss: 0.0569 - val_acc: 0.5908\n",
      "Epoch 196/200\n",
      "3208/3208 [==============================] - 1s 233us/step - loss: 0.0490 - acc: 0.6375 - val_loss: 0.0575 - val_acc: 0.5926\n",
      "Epoch 197/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0489 - acc: 0.6418 - val_loss: 0.0579 - val_acc: 0.5814\n",
      "Epoch 198/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0487 - acc: 0.6387 - val_loss: 0.0571 - val_acc: 0.5939\n",
      "Epoch 199/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0487 - acc: 0.6406 - val_loss: 0.0579 - val_acc: 0.5845\n",
      "Epoch 200/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0496 - acc: 0.6319 - val_loss: 0.0571 - val_acc: 0.5901\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=20, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=16, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=12, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=200, validation_data=(dev_valid_x, dev_valid_y))\n",
    "  \n",
    "pred_y = nn_model.predict(valid_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result_cv = result_cv.append({'detail':'kfold=3', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqbbFgAoXt7j"
   },
   "source": [
    "Now I tried different optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 20811
    },
    "colab_type": "code",
    "id": "PlMsM8SHPFkQ",
    "outputId": "4c776302-5938-4229-932e-7e56b1f5f4d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_62 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 12)                204       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 2,386\n",
      "Trainable params: 2,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 1s 415us/step - loss: 0.0884 - acc: 0.2304 - val_loss: 0.0886 - val_acc: 0.2157\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0883 - acc: 0.2304 - val_loss: 0.0884 - val_acc: 0.2157\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0881 - acc: 0.2304 - val_loss: 0.0883 - val_acc: 0.2157\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0879 - acc: 0.2304 - val_loss: 0.0881 - val_acc: 0.2157\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0878 - acc: 0.2304 - val_loss: 0.0880 - val_acc: 0.2157\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0876 - acc: 0.2304 - val_loss: 0.0878 - val_acc: 0.2157\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0874 - acc: 0.2304 - val_loss: 0.0877 - val_acc: 0.2157\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0873 - acc: 0.2304 - val_loss: 0.0875 - val_acc: 0.2157\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0871 - acc: 0.2304 - val_loss: 0.0874 - val_acc: 0.2157\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0870 - acc: 0.2304 - val_loss: 0.0872 - val_acc: 0.2157\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0868 - acc: 0.2304 - val_loss: 0.0871 - val_acc: 0.2157\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0867 - acc: 0.2304 - val_loss: 0.0869 - val_acc: 0.2157\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0866 - acc: 0.2304 - val_loss: 0.0868 - val_acc: 0.2157\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0864 - acc: 0.2304 - val_loss: 0.0866 - val_acc: 0.2157\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0863 - acc: 0.2304 - val_loss: 0.0865 - val_acc: 0.2157\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0861 - acc: 0.2304 - val_loss: 0.0864 - val_acc: 0.2157\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0860 - acc: 0.2304 - val_loss: 0.0862 - val_acc: 0.2157\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0859 - acc: 0.2304 - val_loss: 0.0861 - val_acc: 0.2157\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0858 - acc: 0.2304 - val_loss: 0.0860 - val_acc: 0.2157\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0856 - acc: 0.2304 - val_loss: 0.0859 - val_acc: 0.2157\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0855 - acc: 0.2304 - val_loss: 0.0857 - val_acc: 0.2157\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0854 - acc: 0.2304 - val_loss: 0.0856 - val_acc: 0.2157\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0853 - acc: 0.2304 - val_loss: 0.0855 - val_acc: 0.2157\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0852 - acc: 0.2304 - val_loss: 0.0854 - val_acc: 0.2157\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0850 - acc: 0.2304 - val_loss: 0.0853 - val_acc: 0.2157\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0849 - acc: 0.2304 - val_loss: 0.0852 - val_acc: 0.2157\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0848 - acc: 0.2304 - val_loss: 0.0851 - val_acc: 0.2157\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0847 - acc: 0.2304 - val_loss: 0.0850 - val_acc: 0.2157\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0846 - acc: 0.2304 - val_loss: 0.0849 - val_acc: 0.2157\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0845 - acc: 0.2304 - val_loss: 0.0848 - val_acc: 0.2157\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0844 - acc: 0.2304 - val_loss: 0.0847 - val_acc: 0.2157\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0843 - acc: 0.2304 - val_loss: 0.0846 - val_acc: 0.2157\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0843 - acc: 0.2304 - val_loss: 0.0845 - val_acc: 0.2157\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0842 - acc: 0.2304 - val_loss: 0.0844 - val_acc: 0.2157\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0841 - acc: 0.2304 - val_loss: 0.0843 - val_acc: 0.2157\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0840 - acc: 0.2304 - val_loss: 0.0842 - val_acc: 0.2157\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0839 - acc: 0.2304 - val_loss: 0.0842 - val_acc: 0.2157\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0838 - acc: 0.2304 - val_loss: 0.0841 - val_acc: 0.2157\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0838 - acc: 0.2304 - val_loss: 0.0840 - val_acc: 0.2157\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0837 - acc: 0.2304 - val_loss: 0.0839 - val_acc: 0.2157\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0836 - acc: 0.2304 - val_loss: 0.0839 - val_acc: 0.2157\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0836 - acc: 0.2304 - val_loss: 0.0838 - val_acc: 0.2157\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0835 - acc: 0.2295 - val_loss: 0.0837 - val_acc: 0.1234\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0834 - acc: 0.2413 - val_loss: 0.0837 - val_acc: 0.2999\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0834 - acc: 0.2922 - val_loss: 0.0836 - val_acc: 0.2999\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0833 - acc: 0.2922 - val_loss: 0.0836 - val_acc: 0.2999\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0833 - acc: 0.2922 - val_loss: 0.0835 - val_acc: 0.2999\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0832 - acc: 0.2922 - val_loss: 0.0835 - val_acc: 0.2999\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0832 - acc: 0.2922 - val_loss: 0.0834 - val_acc: 0.2999\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0831 - acc: 0.2922 - val_loss: 0.0834 - val_acc: 0.2999\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0831 - acc: 0.2922 - val_loss: 0.0833 - val_acc: 0.2999\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0830 - acc: 0.2922 - val_loss: 0.0833 - val_acc: 0.2999\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0830 - acc: 0.2922 - val_loss: 0.0832 - val_acc: 0.2999\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0829 - acc: 0.2922 - val_loss: 0.0832 - val_acc: 0.2999\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0829 - acc: 0.2922 - val_loss: 0.0831 - val_acc: 0.2999\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0829 - acc: 0.2922 - val_loss: 0.0831 - val_acc: 0.2999\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0828 - acc: 0.2922 - val_loss: 0.0831 - val_acc: 0.2999\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0828 - acc: 0.2922 - val_loss: 0.0830 - val_acc: 0.2999\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0828 - acc: 0.2922 - val_loss: 0.0830 - val_acc: 0.2999\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0827 - acc: 0.2922 - val_loss: 0.0830 - val_acc: 0.2999\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0827 - acc: 0.2922 - val_loss: 0.0829 - val_acc: 0.2999\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0827 - acc: 0.2922 - val_loss: 0.0829 - val_acc: 0.2999\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0826 - acc: 0.2922 - val_loss: 0.0829 - val_acc: 0.2999\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0826 - acc: 0.2922 - val_loss: 0.0829 - val_acc: 0.2999\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0826 - acc: 0.2922 - val_loss: 0.0828 - val_acc: 0.2999\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0826 - acc: 0.2922 - val_loss: 0.0828 - val_acc: 0.2999\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0825 - acc: 0.2922 - val_loss: 0.0828 - val_acc: 0.2999\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0825 - acc: 0.2922 - val_loss: 0.0828 - val_acc: 0.2999\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0825 - acc: 0.2922 - val_loss: 0.0827 - val_acc: 0.2999\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0825 - acc: 0.2922 - val_loss: 0.0827 - val_acc: 0.2999\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0824 - acc: 0.2922 - val_loss: 0.0827 - val_acc: 0.2999\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0824 - acc: 0.2922 - val_loss: 0.0827 - val_acc: 0.2999\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0824 - acc: 0.2922 - val_loss: 0.0826 - val_acc: 0.2999\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0824 - acc: 0.2922 - val_loss: 0.0826 - val_acc: 0.2999\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0823 - acc: 0.2922 - val_loss: 0.0826 - val_acc: 0.2999\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0823 - acc: 0.2922 - val_loss: 0.0826 - val_acc: 0.2999\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0823 - acc: 0.2922 - val_loss: 0.0826 - val_acc: 0.2999\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0823 - acc: 0.2922 - val_loss: 0.0825 - val_acc: 0.2999\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0823 - acc: 0.2922 - val_loss: 0.0825 - val_acc: 0.2999\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0822 - acc: 0.2922 - val_loss: 0.0825 - val_acc: 0.2999\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0822 - acc: 0.2922 - val_loss: 0.0825 - val_acc: 0.2999\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0822 - acc: 0.2922 - val_loss: 0.0825 - val_acc: 0.2999\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0822 - acc: 0.2922 - val_loss: 0.0825 - val_acc: 0.2999\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 207us/step - loss: 0.0822 - acc: 0.2922 - val_loss: 0.0824 - val_acc: 0.2999\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 212us/step - loss: 0.0822 - acc: 0.2922 - val_loss: 0.0824 - val_acc: 0.2999\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 213us/step - loss: 0.0821 - acc: 0.2922 - val_loss: 0.0824 - val_acc: 0.2999\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 208us/step - loss: 0.0821 - acc: 0.2922 - val_loss: 0.0824 - val_acc: 0.2999\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 214us/step - loss: 0.0821 - acc: 0.2922 - val_loss: 0.0824 - val_acc: 0.2999\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 212us/step - loss: 0.0821 - acc: 0.2922 - val_loss: 0.0824 - val_acc: 0.2999\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 208us/step - loss: 0.0821 - acc: 0.2922 - val_loss: 0.0824 - val_acc: 0.2999\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0821 - acc: 0.2922 - val_loss: 0.0824 - val_acc: 0.2999\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0820 - acc: 0.2922 - val_loss: 0.0823 - val_acc: 0.2999\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0820 - acc: 0.2922 - val_loss: 0.0823 - val_acc: 0.2999\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0820 - acc: 0.2922 - val_loss: 0.0823 - val_acc: 0.2999\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0820 - acc: 0.2922 - val_loss: 0.0823 - val_acc: 0.2999\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0820 - acc: 0.2922 - val_loss: 0.0823 - val_acc: 0.2999\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0820 - acc: 0.2922 - val_loss: 0.0823 - val_acc: 0.2999\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0820 - acc: 0.2922 - val_loss: 0.0823 - val_acc: 0.2999\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0820 - acc: 0.2922 - val_loss: 0.0823 - val_acc: 0.2999\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0819 - acc: 0.2922 - val_loss: 0.0822 - val_acc: 0.2999\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0819 - acc: 0.2922 - val_loss: 0.0822 - val_acc: 0.2999\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0819 - acc: 0.2922 - val_loss: 0.0822 - val_acc: 0.2999\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0819 - acc: 0.2922 - val_loss: 0.0822 - val_acc: 0.2999\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0819 - acc: 0.2922 - val_loss: 0.0822 - val_acc: 0.2999\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0819 - acc: 0.2922 - val_loss: 0.0822 - val_acc: 0.2999\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0819 - acc: 0.2922 - val_loss: 0.0822 - val_acc: 0.2999\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0819 - acc: 0.2922 - val_loss: 0.0822 - val_acc: 0.2999\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0818 - acc: 0.2922 - val_loss: 0.0822 - val_acc: 0.2999\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0818 - acc: 0.2922 - val_loss: 0.0822 - val_acc: 0.2999\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0818 - acc: 0.2922 - val_loss: 0.0821 - val_acc: 0.2999\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0818 - acc: 0.2922 - val_loss: 0.0821 - val_acc: 0.2999\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0818 - acc: 0.2922 - val_loss: 0.0821 - val_acc: 0.2999\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0818 - acc: 0.2922 - val_loss: 0.0821 - val_acc: 0.2999\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0818 - acc: 0.2922 - val_loss: 0.0821 - val_acc: 0.2999\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0818 - acc: 0.2922 - val_loss: 0.0821 - val_acc: 0.2999\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0818 - acc: 0.2922 - val_loss: 0.0821 - val_acc: 0.2999\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0817 - acc: 0.2922 - val_loss: 0.0821 - val_acc: 0.2999\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0817 - acc: 0.2922 - val_loss: 0.0821 - val_acc: 0.2999\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0817 - acc: 0.2922 - val_loss: 0.0821 - val_acc: 0.2999\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0817 - acc: 0.2922 - val_loss: 0.0821 - val_acc: 0.2999\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0817 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0817 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0817 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0817 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0817 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0817 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0820 - val_acc: 0.2999\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0816 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0815 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 175us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0814 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0812 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 175us/step - loss: 0.0812 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0812 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0812 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0812 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0812 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0812 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0812 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0812 - acc: 0.2922 - val_loss: 0.0817 - val_acc: 0.2999\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0813 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0814 - val_acc: 0.2874\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 172us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0812 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0813 - val_acc: 0.2874\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 157us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 157us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 175us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 172us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0811 - acc: 0.2984 - val_loss: 0.0812 - val_acc: 0.2874\n",
      "Train on 3208 samples, validate on 1603 samples\n",
      "Epoch 1/200\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 2/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 3/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 4/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 5/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 6/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 7/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 8/200\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 9/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 10/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 11/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 12/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 13/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 14/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 15/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 16/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 17/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 18/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 19/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 20/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 21/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 22/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 23/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 24/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 25/200\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 26/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 27/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 28/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 29/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 30/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 31/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 32/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 33/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 34/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 35/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 36/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 37/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 38/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 39/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 40/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 41/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 42/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 43/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 44/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 45/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 46/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 47/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 48/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 49/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 50/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 51/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 52/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 53/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 54/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 55/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 56/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 57/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 58/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 59/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 60/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 61/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 62/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 63/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 64/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 65/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 66/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 67/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 68/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 69/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 70/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 71/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 72/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 73/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 74/200\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 75/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 76/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 77/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 78/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 79/200\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 80/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 81/200\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 82/200\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 83/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 84/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 85/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 86/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 87/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 88/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 89/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 90/200\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 91/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 92/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 93/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 94/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 95/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 96/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 97/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 98/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 99/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 100/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 101/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 102/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 103/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 104/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 105/200\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 106/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 107/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 108/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 109/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 110/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 111/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 112/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 113/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 114/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 115/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 116/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 117/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 118/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 119/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 120/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 121/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 122/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 123/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 124/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 125/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 126/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 127/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 128/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 129/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 130/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 131/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 132/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 133/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 134/200\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 135/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 136/200\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 137/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 138/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 139/200\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 140/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 141/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 142/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 143/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 144/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 145/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 146/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 147/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 148/200\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 149/200\n",
      "3208/3208 [==============================] - 1s 211us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 150/200\n",
      "3208/3208 [==============================] - 1s 210us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 151/200\n",
      "3208/3208 [==============================] - 1s 207us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 152/200\n",
      "3208/3208 [==============================] - 1s 210us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 153/200\n",
      "3208/3208 [==============================] - 1s 209us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 154/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 155/200\n",
      "3208/3208 [==============================] - 1s 209us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 156/200\n",
      "3208/3208 [==============================] - 1s 209us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 157/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 158/200\n",
      "3208/3208 [==============================] - 1s 208us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 159/200\n",
      "3208/3208 [==============================] - 1s 204us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 160/200\n",
      "3208/3208 [==============================] - 1s 208us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 161/200\n",
      "3208/3208 [==============================] - 1s 209us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 162/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 163/200\n",
      "3208/3208 [==============================] - 1s 204us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 164/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 165/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 166/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 167/200\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 168/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 169/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 170/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 171/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 172/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 173/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 174/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 175/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 176/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 177/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 178/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 179/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 180/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 181/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 182/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 183/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 184/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 185/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 186/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 187/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 188/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 189/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 190/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 191/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 192/200\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 193/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 194/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 195/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 196/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 197/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 198/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 199/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n",
      "Epoch 200/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0813 - acc: 0.2936 - val_loss: 0.0807 - val_acc: 0.2969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=20, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=16, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=12, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "sgd = keras.optimizers.SGD()\n",
    "nn_model.compile(optimizer=sgd, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=200, validation_data=(dev_valid_x, dev_valid_y))\n",
    "  \n",
    "pred_y = nn_model.predict(valid_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result_cv = result_cv.append({'detail':'sgd', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 20811
    },
    "colab_type": "code",
    "id": "0Kowhr2TPkBr",
    "outputId": "7cfda8a0-630a-4a94-e0f3-bdf96711cedd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_67 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 12)                204       \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 2,386\n",
      "Trainable params: 2,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 2s 469us/step - loss: 0.0869 - acc: 0.1568 - val_loss: 0.0827 - val_acc: 0.2999\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0805 - acc: 0.2978 - val_loss: 0.0806 - val_acc: 0.2999\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0792 - acc: 0.4004 - val_loss: 0.0784 - val_acc: 0.4539\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0759 - acc: 0.4428 - val_loss: 0.0741 - val_acc: 0.4570\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0717 - acc: 0.4475 - val_loss: 0.0702 - val_acc: 0.4551\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 207us/step - loss: 0.0694 - acc: 0.4468 - val_loss: 0.0690 - val_acc: 0.4551\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 203us/step - loss: 0.0687 - acc: 0.4475 - val_loss: 0.0685 - val_acc: 0.4557\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 205us/step - loss: 0.0684 - acc: 0.4478 - val_loss: 0.0684 - val_acc: 0.4576\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0682 - acc: 0.4481 - val_loss: 0.0682 - val_acc: 0.4582\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 205us/step - loss: 0.0680 - acc: 0.4493 - val_loss: 0.0679 - val_acc: 0.4557\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0678 - acc: 0.4506 - val_loss: 0.0676 - val_acc: 0.4557\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0676 - acc: 0.4509 - val_loss: 0.0674 - val_acc: 0.4589\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 203us/step - loss: 0.0675 - acc: 0.4521 - val_loss: 0.0673 - val_acc: 0.4607\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0673 - acc: 0.4521 - val_loss: 0.0671 - val_acc: 0.4607\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0671 - acc: 0.4528 - val_loss: 0.0668 - val_acc: 0.4595\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0670 - acc: 0.4524 - val_loss: 0.0669 - val_acc: 0.4632\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0668 - acc: 0.4546 - val_loss: 0.0665 - val_acc: 0.4607\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 204us/step - loss: 0.0667 - acc: 0.4549 - val_loss: 0.0663 - val_acc: 0.4589\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 206us/step - loss: 0.0665 - acc: 0.4528 - val_loss: 0.0663 - val_acc: 0.4626\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0664 - acc: 0.4528 - val_loss: 0.0662 - val_acc: 0.4632\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 205us/step - loss: 0.0662 - acc: 0.4531 - val_loss: 0.0659 - val_acc: 0.4601\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0660 - acc: 0.4565 - val_loss: 0.0656 - val_acc: 0.4601\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0659 - acc: 0.4546 - val_loss: 0.0655 - val_acc: 0.4607\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0657 - acc: 0.4515 - val_loss: 0.0656 - val_acc: 0.4638\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0656 - acc: 0.4509 - val_loss: 0.0652 - val_acc: 0.4613\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0654 - acc: 0.4515 - val_loss: 0.0650 - val_acc: 0.4620\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0652 - acc: 0.4549 - val_loss: 0.0649 - val_acc: 0.4626\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0651 - acc: 0.4553 - val_loss: 0.0646 - val_acc: 0.4638\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0649 - acc: 0.4559 - val_loss: 0.0644 - val_acc: 0.4645\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0647 - acc: 0.4549 - val_loss: 0.0642 - val_acc: 0.4663\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0646 - acc: 0.4596 - val_loss: 0.0641 - val_acc: 0.4657\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0644 - acc: 0.4587 - val_loss: 0.0646 - val_acc: 0.4707\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0643 - acc: 0.4596 - val_loss: 0.0638 - val_acc: 0.4701\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0642 - acc: 0.4643 - val_loss: 0.0636 - val_acc: 0.4676\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0639 - acc: 0.4627 - val_loss: 0.0645 - val_acc: 0.4819\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 205us/step - loss: 0.0638 - acc: 0.4671 - val_loss: 0.0634 - val_acc: 0.4719\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 203us/step - loss: 0.0637 - acc: 0.4643 - val_loss: 0.0649 - val_acc: 0.4913\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 230us/step - loss: 0.0636 - acc: 0.4662 - val_loss: 0.0635 - val_acc: 0.4838\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0634 - acc: 0.4721 - val_loss: 0.0633 - val_acc: 0.4863\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0633 - acc: 0.4724 - val_loss: 0.0631 - val_acc: 0.4844\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0632 - acc: 0.4727 - val_loss: 0.0628 - val_acc: 0.4757\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0630 - acc: 0.4783 - val_loss: 0.0627 - val_acc: 0.4776\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0630 - acc: 0.4805 - val_loss: 0.0627 - val_acc: 0.4863\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0628 - acc: 0.4818 - val_loss: 0.0631 - val_acc: 0.4931\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0627 - acc: 0.4908 - val_loss: 0.0623 - val_acc: 0.4713\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0626 - acc: 0.4867 - val_loss: 0.0624 - val_acc: 0.4938\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0624 - acc: 0.4914 - val_loss: 0.0622 - val_acc: 0.4950\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0623 - acc: 0.4927 - val_loss: 0.0624 - val_acc: 0.4931\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0622 - acc: 0.4896 - val_loss: 0.0627 - val_acc: 0.4913\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0621 - acc: 0.4970 - val_loss: 0.0618 - val_acc: 0.4819\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0620 - acc: 0.4983 - val_loss: 0.0617 - val_acc: 0.4913\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0618 - acc: 0.4992 - val_loss: 0.0622 - val_acc: 0.4950\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0617 - acc: 0.5039 - val_loss: 0.0623 - val_acc: 0.4975\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0616 - acc: 0.4998 - val_loss: 0.0613 - val_acc: 0.4931\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 203us/step - loss: 0.0614 - acc: 0.5048 - val_loss: 0.0625 - val_acc: 0.4906\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0613 - acc: 0.5017 - val_loss: 0.0612 - val_acc: 0.4975\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0612 - acc: 0.5039 - val_loss: 0.0612 - val_acc: 0.4931\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0610 - acc: 0.5036 - val_loss: 0.0609 - val_acc: 0.4919\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0609 - acc: 0.5051 - val_loss: 0.0612 - val_acc: 0.4925\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0607 - acc: 0.5092 - val_loss: 0.0617 - val_acc: 0.4944\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0606 - acc: 0.5070 - val_loss: 0.0602 - val_acc: 0.5293\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0603 - acc: 0.5276 - val_loss: 0.0620 - val_acc: 0.4969\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0602 - acc: 0.5329 - val_loss: 0.0603 - val_acc: 0.5050\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0600 - acc: 0.5360 - val_loss: 0.0597 - val_acc: 0.5349\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0598 - acc: 0.5401 - val_loss: 0.0595 - val_acc: 0.5349\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0598 - acc: 0.5423 - val_loss: 0.0593 - val_acc: 0.5380\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0596 - acc: 0.5416 - val_loss: 0.0595 - val_acc: 0.5393\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0595 - acc: 0.5419 - val_loss: 0.0591 - val_acc: 0.5355\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0594 - acc: 0.5426 - val_loss: 0.0590 - val_acc: 0.5362\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0593 - acc: 0.5416 - val_loss: 0.0588 - val_acc: 0.5380\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0592 - acc: 0.5429 - val_loss: 0.0589 - val_acc: 0.5393\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0592 - acc: 0.5457 - val_loss: 0.0587 - val_acc: 0.5362\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0591 - acc: 0.5407 - val_loss: 0.0590 - val_acc: 0.5374\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0591 - acc: 0.5419 - val_loss: 0.0585 - val_acc: 0.5374\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0590 - acc: 0.5432 - val_loss: 0.0585 - val_acc: 0.5374\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0589 - acc: 0.5404 - val_loss: 0.0585 - val_acc: 0.5368\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0589 - acc: 0.5432 - val_loss: 0.0587 - val_acc: 0.5374\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0589 - acc: 0.5438 - val_loss: 0.0584 - val_acc: 0.5355\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0588 - acc: 0.5435 - val_loss: 0.0593 - val_acc: 0.5362\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0588 - acc: 0.5423 - val_loss: 0.0597 - val_acc: 0.5330\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0588 - acc: 0.5429 - val_loss: 0.0590 - val_acc: 0.5337\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0588 - acc: 0.5407 - val_loss: 0.0587 - val_acc: 0.5362\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0587 - acc: 0.5438 - val_loss: 0.0583 - val_acc: 0.5355\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0587 - acc: 0.5438 - val_loss: 0.0587 - val_acc: 0.5374\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0586 - acc: 0.5407 - val_loss: 0.0600 - val_acc: 0.5193\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0587 - acc: 0.5429 - val_loss: 0.0582 - val_acc: 0.5362\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0586 - acc: 0.5435 - val_loss: 0.0582 - val_acc: 0.5362\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0586 - acc: 0.5423 - val_loss: 0.0582 - val_acc: 0.5355\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0586 - acc: 0.5419 - val_loss: 0.0606 - val_acc: 0.5281\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0585 - acc: 0.5407 - val_loss: 0.0583 - val_acc: 0.5355\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0585 - acc: 0.5435 - val_loss: 0.0583 - val_acc: 0.5349\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0585 - acc: 0.5438 - val_loss: 0.0585 - val_acc: 0.5380\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0585 - acc: 0.5472 - val_loss: 0.0581 - val_acc: 0.5368\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0585 - acc: 0.5444 - val_loss: 0.0582 - val_acc: 0.5349\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0585 - acc: 0.5426 - val_loss: 0.0582 - val_acc: 0.5368\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0584 - acc: 0.5429 - val_loss: 0.0597 - val_acc: 0.5312\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0585 - acc: 0.5451 - val_loss: 0.0586 - val_acc: 0.5330\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0584 - acc: 0.5447 - val_loss: 0.0582 - val_acc: 0.5368\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0583 - acc: 0.5429 - val_loss: 0.0581 - val_acc: 0.5349\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0584 - acc: 0.5441 - val_loss: 0.0580 - val_acc: 0.5362\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0583 - acc: 0.5438 - val_loss: 0.0581 - val_acc: 0.5355\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0583 - acc: 0.5447 - val_loss: 0.0588 - val_acc: 0.5362\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0583 - acc: 0.5457 - val_loss: 0.0585 - val_acc: 0.5362\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0583 - acc: 0.5451 - val_loss: 0.0590 - val_acc: 0.5368\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0582 - acc: 0.5432 - val_loss: 0.0579 - val_acc: 0.5362\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0583 - acc: 0.5441 - val_loss: 0.0583 - val_acc: 0.5393\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0583 - acc: 0.5469 - val_loss: 0.0579 - val_acc: 0.5343\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0582 - acc: 0.5447 - val_loss: 0.0582 - val_acc: 0.5387\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0581 - acc: 0.5485 - val_loss: 0.0613 - val_acc: 0.5293\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0582 - acc: 0.5482 - val_loss: 0.0592 - val_acc: 0.5318\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0582 - acc: 0.5444 - val_loss: 0.0579 - val_acc: 0.5337\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0581 - acc: 0.5482 - val_loss: 0.0583 - val_acc: 0.5355\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0582 - acc: 0.5432 - val_loss: 0.0589 - val_acc: 0.5337\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0581 - acc: 0.5451 - val_loss: 0.0579 - val_acc: 0.5330\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0581 - acc: 0.5423 - val_loss: 0.0590 - val_acc: 0.5368\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0580 - acc: 0.5454 - val_loss: 0.0578 - val_acc: 0.5343\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0581 - acc: 0.5466 - val_loss: 0.0577 - val_acc: 0.5399\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0581 - acc: 0.5472 - val_loss: 0.0585 - val_acc: 0.5424\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0579 - acc: 0.5444 - val_loss: 0.0582 - val_acc: 0.5424\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0581 - acc: 0.5472 - val_loss: 0.0582 - val_acc: 0.5436\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0580 - acc: 0.5469 - val_loss: 0.0577 - val_acc: 0.5411\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0580 - acc: 0.5469 - val_loss: 0.0577 - val_acc: 0.5380\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 203us/step - loss: 0.0580 - acc: 0.5476 - val_loss: 0.0578 - val_acc: 0.5418\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0579 - acc: 0.5485 - val_loss: 0.0581 - val_acc: 0.5449\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0578 - acc: 0.5494 - val_loss: 0.0577 - val_acc: 0.5436\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0579 - acc: 0.5463 - val_loss: 0.0578 - val_acc: 0.5411\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0579 - acc: 0.5500 - val_loss: 0.0586 - val_acc: 0.5480\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0578 - acc: 0.5497 - val_loss: 0.0581 - val_acc: 0.5418\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0578 - acc: 0.5491 - val_loss: 0.0575 - val_acc: 0.5411\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0578 - acc: 0.5507 - val_loss: 0.0604 - val_acc: 0.5399\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 207us/step - loss: 0.0578 - acc: 0.5485 - val_loss: 0.0575 - val_acc: 0.5411\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0577 - acc: 0.5491 - val_loss: 0.0583 - val_acc: 0.5499\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0577 - acc: 0.5513 - val_loss: 0.0583 - val_acc: 0.5505\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0577 - acc: 0.5500 - val_loss: 0.0575 - val_acc: 0.5449\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0577 - acc: 0.5504 - val_loss: 0.0574 - val_acc: 0.5430\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0577 - acc: 0.5516 - val_loss: 0.0574 - val_acc: 0.5468\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0577 - acc: 0.5507 - val_loss: 0.0581 - val_acc: 0.5549\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0576 - acc: 0.5522 - val_loss: 0.0574 - val_acc: 0.5418\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0576 - acc: 0.5491 - val_loss: 0.0578 - val_acc: 0.5486\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0576 - acc: 0.5507 - val_loss: 0.0574 - val_acc: 0.5443\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0576 - acc: 0.5494 - val_loss: 0.0573 - val_acc: 0.5461\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0576 - acc: 0.5535 - val_loss: 0.0576 - val_acc: 0.5455\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0576 - acc: 0.5482 - val_loss: 0.0574 - val_acc: 0.5411\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0576 - acc: 0.5513 - val_loss: 0.0573 - val_acc: 0.5424\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0576 - acc: 0.5513 - val_loss: 0.0577 - val_acc: 0.5555\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0576 - acc: 0.5557 - val_loss: 0.0574 - val_acc: 0.5449\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0575 - acc: 0.5538 - val_loss: 0.0581 - val_acc: 0.5474\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0576 - acc: 0.5507 - val_loss: 0.0574 - val_acc: 0.5474\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0575 - acc: 0.5522 - val_loss: 0.0580 - val_acc: 0.5461\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0575 - acc: 0.5535 - val_loss: 0.0574 - val_acc: 0.5411\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0575 - acc: 0.5519 - val_loss: 0.0574 - val_acc: 0.5418\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0576 - acc: 0.5513 - val_loss: 0.0573 - val_acc: 0.5468\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0574 - acc: 0.5504 - val_loss: 0.0572 - val_acc: 0.5480\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0574 - acc: 0.5541 - val_loss: 0.0572 - val_acc: 0.5468\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0574 - acc: 0.5535 - val_loss: 0.0579 - val_acc: 0.5505\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0575 - acc: 0.5541 - val_loss: 0.0593 - val_acc: 0.5411\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0574 - acc: 0.5550 - val_loss: 0.0575 - val_acc: 0.5574\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0575 - acc: 0.5575 - val_loss: 0.0578 - val_acc: 0.5493\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0574 - acc: 0.5535 - val_loss: 0.0574 - val_acc: 0.5461\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0575 - acc: 0.5557 - val_loss: 0.0579 - val_acc: 0.5549\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0574 - acc: 0.5541 - val_loss: 0.0578 - val_acc: 0.5461\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0574 - acc: 0.5563 - val_loss: 0.0573 - val_acc: 0.5493\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0574 - acc: 0.5553 - val_loss: 0.0573 - val_acc: 0.5499\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0574 - acc: 0.5569 - val_loss: 0.0578 - val_acc: 0.5542\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0574 - acc: 0.5547 - val_loss: 0.0575 - val_acc: 0.5524\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0574 - acc: 0.5569 - val_loss: 0.0572 - val_acc: 0.5486\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0574 - acc: 0.5550 - val_loss: 0.0574 - val_acc: 0.5524\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0573 - acc: 0.5566 - val_loss: 0.0573 - val_acc: 0.5480\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0573 - acc: 0.5553 - val_loss: 0.0575 - val_acc: 0.5493\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0574 - acc: 0.5525 - val_loss: 0.0572 - val_acc: 0.5511\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0573 - acc: 0.5553 - val_loss: 0.0574 - val_acc: 0.5586\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0573 - acc: 0.5557 - val_loss: 0.0574 - val_acc: 0.5561\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0574 - acc: 0.5550 - val_loss: 0.0572 - val_acc: 0.5542\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0573 - acc: 0.5560 - val_loss: 0.0573 - val_acc: 0.5499\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0573 - acc: 0.5563 - val_loss: 0.0573 - val_acc: 0.5493\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0573 - acc: 0.5572 - val_loss: 0.0572 - val_acc: 0.5505\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0573 - acc: 0.5529 - val_loss: 0.0572 - val_acc: 0.5524\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0573 - acc: 0.5560 - val_loss: 0.0581 - val_acc: 0.5530\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0572 - acc: 0.5588 - val_loss: 0.0575 - val_acc: 0.5561\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0573 - acc: 0.5560 - val_loss: 0.0572 - val_acc: 0.5542\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0573 - acc: 0.5560 - val_loss: 0.0582 - val_acc: 0.5517\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0573 - acc: 0.5557 - val_loss: 0.0571 - val_acc: 0.5517\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0573 - acc: 0.5557 - val_loss: 0.0572 - val_acc: 0.5530\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0573 - acc: 0.5591 - val_loss: 0.0573 - val_acc: 0.5530\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0573 - acc: 0.5541 - val_loss: 0.0575 - val_acc: 0.5567\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0572 - acc: 0.5594 - val_loss: 0.0593 - val_acc: 0.5436\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0572 - acc: 0.5575 - val_loss: 0.0585 - val_acc: 0.5511\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0572 - acc: 0.5563 - val_loss: 0.0592 - val_acc: 0.5411\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0572 - acc: 0.5569 - val_loss: 0.0571 - val_acc: 0.5592\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0572 - acc: 0.5591 - val_loss: 0.0573 - val_acc: 0.5461\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0573 - acc: 0.5588 - val_loss: 0.0575 - val_acc: 0.5567\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0572 - acc: 0.5575 - val_loss: 0.0573 - val_acc: 0.5574\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0572 - acc: 0.5588 - val_loss: 0.0599 - val_acc: 0.5156\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 203us/step - loss: 0.0572 - acc: 0.5597 - val_loss: 0.0578 - val_acc: 0.5542\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0572 - acc: 0.5591 - val_loss: 0.0572 - val_acc: 0.5549\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0572 - acc: 0.5569 - val_loss: 0.0572 - val_acc: 0.5580\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0572 - acc: 0.5594 - val_loss: 0.0575 - val_acc: 0.5555\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0572 - acc: 0.5550 - val_loss: 0.0572 - val_acc: 0.5586\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0572 - acc: 0.5613 - val_loss: 0.0575 - val_acc: 0.5567\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0565 - acc: 0.5675 - val_loss: 0.0584 - val_acc: 0.5393\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0565 - acc: 0.5659 - val_loss: 0.0591 - val_acc: 0.5343\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0563 - acc: 0.5647 - val_loss: 0.0587 - val_acc: 0.5362\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0564 - acc: 0.5659 - val_loss: 0.0586 - val_acc: 0.5399\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0564 - acc: 0.5656 - val_loss: 0.0585 - val_acc: 0.5393\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0563 - acc: 0.5663 - val_loss: 0.0586 - val_acc: 0.5411\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0563 - acc: 0.5681 - val_loss: 0.0587 - val_acc: 0.5393\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0563 - acc: 0.5675 - val_loss: 0.0587 - val_acc: 0.5362\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0563 - acc: 0.5672 - val_loss: 0.0589 - val_acc: 0.5411\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0563 - acc: 0.5650 - val_loss: 0.0588 - val_acc: 0.5337\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0563 - acc: 0.5691 - val_loss: 0.0591 - val_acc: 0.5318\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0562 - acc: 0.5663 - val_loss: 0.0587 - val_acc: 0.5349\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0562 - acc: 0.5678 - val_loss: 0.0594 - val_acc: 0.5324\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0563 - acc: 0.5684 - val_loss: 0.0618 - val_acc: 0.4981\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0563 - acc: 0.5641 - val_loss: 0.0591 - val_acc: 0.5349\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0562 - acc: 0.5669 - val_loss: 0.0590 - val_acc: 0.5374\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0562 - acc: 0.5675 - val_loss: 0.0596 - val_acc: 0.5293\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0561 - acc: 0.5678 - val_loss: 0.0589 - val_acc: 0.5343\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0562 - acc: 0.5691 - val_loss: 0.0589 - val_acc: 0.5349\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0562 - acc: 0.5684 - val_loss: 0.0590 - val_acc: 0.5324\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0562 - acc: 0.5700 - val_loss: 0.0593 - val_acc: 0.5312\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0562 - acc: 0.5663 - val_loss: 0.0588 - val_acc: 0.5349\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0562 - acc: 0.5675 - val_loss: 0.0589 - val_acc: 0.5343\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0561 - acc: 0.5716 - val_loss: 0.0589 - val_acc: 0.5330\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0561 - acc: 0.5697 - val_loss: 0.0589 - val_acc: 0.5355\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0561 - acc: 0.5694 - val_loss: 0.0589 - val_acc: 0.5355\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0562 - acc: 0.5675 - val_loss: 0.0592 - val_acc: 0.5287\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0560 - acc: 0.5706 - val_loss: 0.0598 - val_acc: 0.5256\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0561 - acc: 0.5681 - val_loss: 0.0593 - val_acc: 0.5337\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0561 - acc: 0.5697 - val_loss: 0.0591 - val_acc: 0.5355\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0561 - acc: 0.5697 - val_loss: 0.0591 - val_acc: 0.5330\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0561 - acc: 0.5659 - val_loss: 0.0589 - val_acc: 0.5343\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0560 - acc: 0.5706 - val_loss: 0.0589 - val_acc: 0.5349\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0561 - acc: 0.5697 - val_loss: 0.0592 - val_acc: 0.5305\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0561 - acc: 0.5681 - val_loss: 0.0591 - val_acc: 0.5324\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0560 - acc: 0.5706 - val_loss: 0.0592 - val_acc: 0.5299\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0560 - acc: 0.5709 - val_loss: 0.0589 - val_acc: 0.5362\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0561 - acc: 0.5675 - val_loss: 0.0592 - val_acc: 0.5337\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0560 - acc: 0.5684 - val_loss: 0.0590 - val_acc: 0.5337\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0560 - acc: 0.5719 - val_loss: 0.0591 - val_acc: 0.5299\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0560 - acc: 0.5706 - val_loss: 0.0592 - val_acc: 0.5305\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0560 - acc: 0.5684 - val_loss: 0.0590 - val_acc: 0.5337\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0560 - acc: 0.5722 - val_loss: 0.0590 - val_acc: 0.5330\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0559 - acc: 0.5697 - val_loss: 0.0602 - val_acc: 0.5175\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0559 - acc: 0.5700 - val_loss: 0.0593 - val_acc: 0.5324\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0559 - acc: 0.5734 - val_loss: 0.0591 - val_acc: 0.5343\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0559 - acc: 0.5694 - val_loss: 0.0599 - val_acc: 0.5274\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0559 - acc: 0.5691 - val_loss: 0.0590 - val_acc: 0.5318\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0559 - acc: 0.5694 - val_loss: 0.0590 - val_acc: 0.5330\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0559 - acc: 0.5719 - val_loss: 0.0592 - val_acc: 0.5299\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0559 - acc: 0.5713 - val_loss: 0.0591 - val_acc: 0.5349\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0559 - acc: 0.5713 - val_loss: 0.0593 - val_acc: 0.5299\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0558 - acc: 0.5694 - val_loss: 0.0593 - val_acc: 0.5337\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0558 - acc: 0.5722 - val_loss: 0.0592 - val_acc: 0.5305\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0559 - acc: 0.5669 - val_loss: 0.0591 - val_acc: 0.5355\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0559 - acc: 0.5753 - val_loss: 0.0591 - val_acc: 0.5318\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0558 - acc: 0.5728 - val_loss: 0.0595 - val_acc: 0.5293\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0558 - acc: 0.5750 - val_loss: 0.0596 - val_acc: 0.5312\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0559 - acc: 0.5706 - val_loss: 0.0593 - val_acc: 0.5349\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0558 - acc: 0.5737 - val_loss: 0.0591 - val_acc: 0.5343\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0558 - acc: 0.5672 - val_loss: 0.0591 - val_acc: 0.5349\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0558 - acc: 0.5713 - val_loss: 0.0602 - val_acc: 0.5187\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0558 - acc: 0.5716 - val_loss: 0.0601 - val_acc: 0.5237\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0558 - acc: 0.5688 - val_loss: 0.0591 - val_acc: 0.5330\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0557 - acc: 0.5722 - val_loss: 0.0592 - val_acc: 0.5324\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0557 - acc: 0.5709 - val_loss: 0.0594 - val_acc: 0.5318\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0558 - acc: 0.5719 - val_loss: 0.0592 - val_acc: 0.5337\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0559 - acc: 0.5713 - val_loss: 0.0591 - val_acc: 0.5380\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0557 - acc: 0.5713 - val_loss: 0.0593 - val_acc: 0.5337\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0557 - acc: 0.5719 - val_loss: 0.0596 - val_acc: 0.5293\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0557 - acc: 0.5728 - val_loss: 0.0592 - val_acc: 0.5355\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0557 - acc: 0.5737 - val_loss: 0.0594 - val_acc: 0.5337\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0558 - acc: 0.5756 - val_loss: 0.0599 - val_acc: 0.5281\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0558 - acc: 0.5728 - val_loss: 0.0592 - val_acc: 0.5343\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0557 - acc: 0.5737 - val_loss: 0.0592 - val_acc: 0.5349\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0557 - acc: 0.5706 - val_loss: 0.0593 - val_acc: 0.5343\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0557 - acc: 0.5741 - val_loss: 0.0592 - val_acc: 0.5362\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0558 - acc: 0.5731 - val_loss: 0.0591 - val_acc: 0.5362\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0558 - acc: 0.5719 - val_loss: 0.0592 - val_acc: 0.5374\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0557 - acc: 0.5766 - val_loss: 0.0593 - val_acc: 0.5343\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0557 - acc: 0.5750 - val_loss: 0.0594 - val_acc: 0.5299\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0557 - acc: 0.5772 - val_loss: 0.0594 - val_acc: 0.5281\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0556 - acc: 0.5759 - val_loss: 0.0594 - val_acc: 0.5312\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0557 - acc: 0.5716 - val_loss: 0.0603 - val_acc: 0.5193\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0557 - acc: 0.5722 - val_loss: 0.0591 - val_acc: 0.5349\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0556 - acc: 0.5722 - val_loss: 0.0592 - val_acc: 0.5355\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0557 - acc: 0.5719 - val_loss: 0.0591 - val_acc: 0.5343\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0556 - acc: 0.5753 - val_loss: 0.0593 - val_acc: 0.5349\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0556 - acc: 0.5766 - val_loss: 0.0594 - val_acc: 0.5305\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0556 - acc: 0.5766 - val_loss: 0.0596 - val_acc: 0.5299\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0555 - acc: 0.5766 - val_loss: 0.0597 - val_acc: 0.5274\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0555 - acc: 0.5769 - val_loss: 0.0594 - val_acc: 0.5324\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0555 - acc: 0.5759 - val_loss: 0.0625 - val_acc: 0.4931\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0556 - acc: 0.5728 - val_loss: 0.0591 - val_acc: 0.5343\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0555 - acc: 0.5756 - val_loss: 0.0590 - val_acc: 0.5362\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0555 - acc: 0.5728 - val_loss: 0.0593 - val_acc: 0.5318\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0555 - acc: 0.5703 - val_loss: 0.0601 - val_acc: 0.5224\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0555 - acc: 0.5716 - val_loss: 0.0591 - val_acc: 0.5368\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0555 - acc: 0.5734 - val_loss: 0.0590 - val_acc: 0.5387\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0554 - acc: 0.5716 - val_loss: 0.0590 - val_acc: 0.5362\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0554 - acc: 0.5744 - val_loss: 0.0589 - val_acc: 0.5368\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0554 - acc: 0.5747 - val_loss: 0.0593 - val_acc: 0.5330\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0554 - acc: 0.5737 - val_loss: 0.0593 - val_acc: 0.5324\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0554 - acc: 0.5737 - val_loss: 0.0589 - val_acc: 0.5368\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0554 - acc: 0.5737 - val_loss: 0.0589 - val_acc: 0.5399\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0553 - acc: 0.5737 - val_loss: 0.0589 - val_acc: 0.5368\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0553 - acc: 0.5737 - val_loss: 0.0612 - val_acc: 0.5162\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0553 - acc: 0.5762 - val_loss: 0.0591 - val_acc: 0.5337\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0553 - acc: 0.5734 - val_loss: 0.0590 - val_acc: 0.5380\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0553 - acc: 0.5775 - val_loss: 0.0590 - val_acc: 0.5399\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0553 - acc: 0.5741 - val_loss: 0.0590 - val_acc: 0.5380\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0553 - acc: 0.5759 - val_loss: 0.0596 - val_acc: 0.5293\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0552 - acc: 0.5731 - val_loss: 0.0599 - val_acc: 0.5237\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0552 - acc: 0.5744 - val_loss: 0.0592 - val_acc: 0.5343\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0551 - acc: 0.5778 - val_loss: 0.0593 - val_acc: 0.5330\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0552 - acc: 0.5731 - val_loss: 0.0589 - val_acc: 0.5399\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0552 - acc: 0.5741 - val_loss: 0.0589 - val_acc: 0.5368\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0552 - acc: 0.5737 - val_loss: 0.0592 - val_acc: 0.5343\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0551 - acc: 0.5756 - val_loss: 0.0591 - val_acc: 0.5343\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0552 - acc: 0.5691 - val_loss: 0.0590 - val_acc: 0.5368\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0551 - acc: 0.5725 - val_loss: 0.0590 - val_acc: 0.5368\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0551 - acc: 0.5769 - val_loss: 0.0618 - val_acc: 0.5025\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0551 - acc: 0.5737 - val_loss: 0.0589 - val_acc: 0.5343\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0551 - acc: 0.5750 - val_loss: 0.0590 - val_acc: 0.5324\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0551 - acc: 0.5753 - val_loss: 0.0597 - val_acc: 0.5312\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0550 - acc: 0.5744 - val_loss: 0.0588 - val_acc: 0.5368\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0551 - acc: 0.5737 - val_loss: 0.0589 - val_acc: 0.5362\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0550 - acc: 0.5747 - val_loss: 0.0598 - val_acc: 0.5243\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0551 - acc: 0.5734 - val_loss: 0.0589 - val_acc: 0.5374\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0550 - acc: 0.5766 - val_loss: 0.0589 - val_acc: 0.5362\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0550 - acc: 0.5759 - val_loss: 0.0588 - val_acc: 0.5343\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0550 - acc: 0.5750 - val_loss: 0.0594 - val_acc: 0.5268\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0549 - acc: 0.5753 - val_loss: 0.0600 - val_acc: 0.5281\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0550 - acc: 0.5741 - val_loss: 0.0588 - val_acc: 0.5374\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0550 - acc: 0.5772 - val_loss: 0.0589 - val_acc: 0.5349\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0550 - acc: 0.5741 - val_loss: 0.0589 - val_acc: 0.5368\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0549 - acc: 0.5800 - val_loss: 0.0590 - val_acc: 0.5318\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0550 - acc: 0.5762 - val_loss: 0.0595 - val_acc: 0.5318\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0549 - acc: 0.5750 - val_loss: 0.0595 - val_acc: 0.5293\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0549 - acc: 0.5737 - val_loss: 0.0604 - val_acc: 0.5212\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0549 - acc: 0.5744 - val_loss: 0.0587 - val_acc: 0.5374\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0549 - acc: 0.5753 - val_loss: 0.0591 - val_acc: 0.5337\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0548 - acc: 0.5762 - val_loss: 0.0588 - val_acc: 0.5393\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0549 - acc: 0.5762 - val_loss: 0.0591 - val_acc: 0.5355\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0548 - acc: 0.5753 - val_loss: 0.0590 - val_acc: 0.5349\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0549 - acc: 0.5775 - val_loss: 0.0593 - val_acc: 0.5293\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0549 - acc: 0.5737 - val_loss: 0.0596 - val_acc: 0.5281\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0549 - acc: 0.5747 - val_loss: 0.0597 - val_acc: 0.5262\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0549 - acc: 0.5759 - val_loss: 0.0589 - val_acc: 0.5343\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0548 - acc: 0.5794 - val_loss: 0.0595 - val_acc: 0.5318\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0548 - acc: 0.5787 - val_loss: 0.0590 - val_acc: 0.5349\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0549 - acc: 0.5769 - val_loss: 0.0588 - val_acc: 0.5362\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0548 - acc: 0.5769 - val_loss: 0.0591 - val_acc: 0.5362\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0547 - acc: 0.5794 - val_loss: 0.0588 - val_acc: 0.5374\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0547 - acc: 0.5815 - val_loss: 0.0595 - val_acc: 0.5293\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0548 - acc: 0.5744 - val_loss: 0.0596 - val_acc: 0.5268\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0548 - acc: 0.5806 - val_loss: 0.0588 - val_acc: 0.5399\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0548 - acc: 0.5809 - val_loss: 0.0587 - val_acc: 0.5362\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0548 - acc: 0.5794 - val_loss: 0.0589 - val_acc: 0.5343\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0548 - acc: 0.5781 - val_loss: 0.0595 - val_acc: 0.5312\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0547 - acc: 0.5766 - val_loss: 0.0589 - val_acc: 0.5362\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0547 - acc: 0.5837 - val_loss: 0.0591 - val_acc: 0.5330\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0547 - acc: 0.5772 - val_loss: 0.0589 - val_acc: 0.5387\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0547 - acc: 0.5812 - val_loss: 0.0588 - val_acc: 0.5380\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0548 - acc: 0.5806 - val_loss: 0.0588 - val_acc: 0.5393\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0546 - acc: 0.5856 - val_loss: 0.0589 - val_acc: 0.5405\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0547 - acc: 0.5825 - val_loss: 0.0588 - val_acc: 0.5405\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0545 - acc: 0.5815 - val_loss: 0.0593 - val_acc: 0.5355\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0547 - acc: 0.5803 - val_loss: 0.0594 - val_acc: 0.5368\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0546 - acc: 0.5812 - val_loss: 0.0589 - val_acc: 0.5418\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0546 - acc: 0.5840 - val_loss: 0.0598 - val_acc: 0.5324\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0546 - acc: 0.5828 - val_loss: 0.0600 - val_acc: 0.5349\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0545 - acc: 0.5834 - val_loss: 0.0597 - val_acc: 0.5318\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0546 - acc: 0.5809 - val_loss: 0.0589 - val_acc: 0.5374\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0547 - acc: 0.5806 - val_loss: 0.0592 - val_acc: 0.5343\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 202us/step - loss: 0.0546 - acc: 0.5862 - val_loss: 0.0588 - val_acc: 0.5380\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0546 - acc: 0.5809 - val_loss: 0.0597 - val_acc: 0.5324\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0547 - acc: 0.5800 - val_loss: 0.0588 - val_acc: 0.5387\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0546 - acc: 0.5850 - val_loss: 0.0592 - val_acc: 0.5399\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0547 - acc: 0.5822 - val_loss: 0.0589 - val_acc: 0.5380\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0546 - acc: 0.5806 - val_loss: 0.0590 - val_acc: 0.5387\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0546 - acc: 0.5822 - val_loss: 0.0591 - val_acc: 0.5405\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0546 - acc: 0.5806 - val_loss: 0.0589 - val_acc: 0.5380\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0546 - acc: 0.5856 - val_loss: 0.0591 - val_acc: 0.5387\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0547 - acc: 0.5803 - val_loss: 0.0589 - val_acc: 0.5405\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 206us/step - loss: 0.0546 - acc: 0.5853 - val_loss: 0.0590 - val_acc: 0.5362\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0546 - acc: 0.5819 - val_loss: 0.0590 - val_acc: 0.5368\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0546 - acc: 0.5815 - val_loss: 0.0593 - val_acc: 0.5405\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0546 - acc: 0.5868 - val_loss: 0.0601 - val_acc: 0.5262\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0545 - acc: 0.5837 - val_loss: 0.0590 - val_acc: 0.5362\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0546 - acc: 0.5834 - val_loss: 0.0589 - val_acc: 0.5362\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0545 - acc: 0.5847 - val_loss: 0.0593 - val_acc: 0.5337\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0546 - acc: 0.5834 - val_loss: 0.0591 - val_acc: 0.5455\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0545 - acc: 0.5875 - val_loss: 0.0589 - val_acc: 0.5387\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0546 - acc: 0.5840 - val_loss: 0.0590 - val_acc: 0.5374\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0546 - acc: 0.5840 - val_loss: 0.0590 - val_acc: 0.5362\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0546 - acc: 0.5847 - val_loss: 0.0588 - val_acc: 0.5387\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0545 - acc: 0.5865 - val_loss: 0.0591 - val_acc: 0.5305\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0545 - acc: 0.5834 - val_loss: 0.0590 - val_acc: 0.5368\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0546 - acc: 0.5834 - val_loss: 0.0594 - val_acc: 0.5362\n",
      "Train on 3208 samples, validate on 1603 samples\n",
      "Epoch 1/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0569 - acc: 0.5536 - val_loss: 0.0540 - val_acc: 0.5964\n",
      "Epoch 2/200\n",
      "3208/3208 [==============================] - 1s 198us/step - loss: 0.0568 - acc: 0.5577 - val_loss: 0.0543 - val_acc: 0.5951\n",
      "Epoch 3/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0568 - acc: 0.5620 - val_loss: 0.0543 - val_acc: 0.5939\n",
      "Epoch 4/200\n",
      "3208/3208 [==============================] - 1s 197us/step - loss: 0.0567 - acc: 0.5627 - val_loss: 0.0556 - val_acc: 0.5870\n",
      "Epoch 5/200\n",
      "3208/3208 [==============================] - 1s 196us/step - loss: 0.0567 - acc: 0.5645 - val_loss: 0.0544 - val_acc: 0.5926\n",
      "Epoch 6/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0567 - acc: 0.5623 - val_loss: 0.0543 - val_acc: 0.5939\n",
      "Epoch 7/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0566 - acc: 0.5648 - val_loss: 0.0548 - val_acc: 0.5951\n",
      "Epoch 8/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0566 - acc: 0.5651 - val_loss: 0.0549 - val_acc: 0.5845\n",
      "Epoch 9/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0566 - acc: 0.5614 - val_loss: 0.0546 - val_acc: 0.5914\n",
      "Epoch 10/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0566 - acc: 0.5673 - val_loss: 0.0544 - val_acc: 0.5908\n",
      "Epoch 11/200\n",
      "3208/3208 [==============================] - 1s 195us/step - loss: 0.0565 - acc: 0.5655 - val_loss: 0.0550 - val_acc: 0.5814\n",
      "Epoch 12/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0566 - acc: 0.5651 - val_loss: 0.0548 - val_acc: 0.5908\n",
      "Epoch 13/200\n",
      "3208/3208 [==============================] - 1s 196us/step - loss: 0.0565 - acc: 0.5655 - val_loss: 0.0550 - val_acc: 0.5901\n",
      "Epoch 14/200\n",
      "3208/3208 [==============================] - 1s 198us/step - loss: 0.0565 - acc: 0.5680 - val_loss: 0.0546 - val_acc: 0.5876\n",
      "Epoch 15/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0565 - acc: 0.5655 - val_loss: 0.0547 - val_acc: 0.5926\n",
      "Epoch 16/200\n",
      "3208/3208 [==============================] - 1s 197us/step - loss: 0.0565 - acc: 0.5670 - val_loss: 0.0545 - val_acc: 0.5839\n",
      "Epoch 17/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0565 - acc: 0.5636 - val_loss: 0.0552 - val_acc: 0.5839\n",
      "Epoch 18/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0565 - acc: 0.5661 - val_loss: 0.0556 - val_acc: 0.5764\n",
      "Epoch 19/200\n",
      "3208/3208 [==============================] - 1s 196us/step - loss: 0.0565 - acc: 0.5623 - val_loss: 0.0547 - val_acc: 0.5845\n",
      "Epoch 20/200\n",
      "3208/3208 [==============================] - 1s 198us/step - loss: 0.0564 - acc: 0.5642 - val_loss: 0.0548 - val_acc: 0.5858\n",
      "Epoch 21/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0564 - acc: 0.5701 - val_loss: 0.0547 - val_acc: 0.5852\n",
      "Epoch 22/200\n",
      "3208/3208 [==============================] - 1s 198us/step - loss: 0.0564 - acc: 0.5676 - val_loss: 0.0545 - val_acc: 0.5883\n",
      "Epoch 23/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0564 - acc: 0.5648 - val_loss: 0.0546 - val_acc: 0.5883\n",
      "Epoch 24/200\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0562 - acc: 0.5695 - val_loss: 0.0546 - val_acc: 0.5889\n",
      "Epoch 25/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0564 - acc: 0.5667 - val_loss: 0.0548 - val_acc: 0.5858\n",
      "Epoch 26/200\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0563 - acc: 0.5661 - val_loss: 0.0557 - val_acc: 0.5770\n",
      "Epoch 27/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0563 - acc: 0.5683 - val_loss: 0.0553 - val_acc: 0.5833\n",
      "Epoch 28/200\n",
      "3208/3208 [==============================] - 1s 197us/step - loss: 0.0564 - acc: 0.5686 - val_loss: 0.0552 - val_acc: 0.5839\n",
      "Epoch 29/200\n",
      "3208/3208 [==============================] - 1s 192us/step - loss: 0.0563 - acc: 0.5658 - val_loss: 0.0560 - val_acc: 0.5764\n",
      "Epoch 30/200\n",
      "3208/3208 [==============================] - 1s 195us/step - loss: 0.0563 - acc: 0.5683 - val_loss: 0.0552 - val_acc: 0.5820\n",
      "Epoch 31/200\n",
      "3208/3208 [==============================] - 1s 195us/step - loss: 0.0563 - acc: 0.5658 - val_loss: 0.0551 - val_acc: 0.5783\n",
      "Epoch 32/200\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0563 - acc: 0.5664 - val_loss: 0.0548 - val_acc: 0.5914\n",
      "Epoch 33/200\n",
      "3208/3208 [==============================] - 1s 197us/step - loss: 0.0563 - acc: 0.5655 - val_loss: 0.0554 - val_acc: 0.5814\n",
      "Epoch 34/200\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0563 - acc: 0.5673 - val_loss: 0.0548 - val_acc: 0.5814\n",
      "Epoch 35/200\n",
      "3208/3208 [==============================] - 1s 198us/step - loss: 0.0563 - acc: 0.5704 - val_loss: 0.0551 - val_acc: 0.5827\n",
      "Epoch 36/200\n",
      "3208/3208 [==============================] - 1s 198us/step - loss: 0.0563 - acc: 0.5692 - val_loss: 0.0565 - val_acc: 0.5602\n",
      "Epoch 37/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0562 - acc: 0.5708 - val_loss: 0.0550 - val_acc: 0.5839\n",
      "Epoch 38/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0563 - acc: 0.5676 - val_loss: 0.0559 - val_acc: 0.5752\n",
      "Epoch 39/200\n",
      "3208/3208 [==============================] - 1s 196us/step - loss: 0.0563 - acc: 0.5673 - val_loss: 0.0547 - val_acc: 0.5883\n",
      "Epoch 40/200\n",
      "3208/3208 [==============================] - 1s 224us/step - loss: 0.0563 - acc: 0.5680 - val_loss: 0.0548 - val_acc: 0.5870\n",
      "Epoch 41/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0562 - acc: 0.5673 - val_loss: 0.0549 - val_acc: 0.5876\n",
      "Epoch 42/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0561 - acc: 0.5733 - val_loss: 0.0549 - val_acc: 0.5827\n",
      "Epoch 43/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0562 - acc: 0.5717 - val_loss: 0.0548 - val_acc: 0.5876\n",
      "Epoch 44/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0562 - acc: 0.5670 - val_loss: 0.0551 - val_acc: 0.5852\n",
      "Epoch 45/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0562 - acc: 0.5711 - val_loss: 0.0550 - val_acc: 0.5802\n",
      "Epoch 46/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0562 - acc: 0.5692 - val_loss: 0.0560 - val_acc: 0.5590\n",
      "Epoch 47/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0562 - acc: 0.5680 - val_loss: 0.0553 - val_acc: 0.5845\n",
      "Epoch 48/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0562 - acc: 0.5667 - val_loss: 0.0550 - val_acc: 0.5827\n",
      "Epoch 49/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0563 - acc: 0.5686 - val_loss: 0.0552 - val_acc: 0.5827\n",
      "Epoch 50/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0562 - acc: 0.5664 - val_loss: 0.0551 - val_acc: 0.5833\n",
      "Epoch 51/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0562 - acc: 0.5714 - val_loss: 0.0559 - val_acc: 0.5739\n",
      "Epoch 52/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0562 - acc: 0.5658 - val_loss: 0.0565 - val_acc: 0.5490\n",
      "Epoch 53/200\n",
      "3208/3208 [==============================] - 1s 207us/step - loss: 0.0562 - acc: 0.5648 - val_loss: 0.0549 - val_acc: 0.5870\n",
      "Epoch 54/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0562 - acc: 0.5658 - val_loss: 0.0550 - val_acc: 0.5808\n",
      "Epoch 55/200\n",
      "3208/3208 [==============================] - 1s 194us/step - loss: 0.0562 - acc: 0.5717 - val_loss: 0.0550 - val_acc: 0.5864\n",
      "Epoch 56/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0561 - acc: 0.5670 - val_loss: 0.0550 - val_acc: 0.5802\n",
      "Epoch 57/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0561 - acc: 0.5689 - val_loss: 0.0549 - val_acc: 0.5901\n",
      "Epoch 58/200\n",
      "3208/3208 [==============================] - 1s 196us/step - loss: 0.0561 - acc: 0.5698 - val_loss: 0.0550 - val_acc: 0.5876\n",
      "Epoch 59/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0562 - acc: 0.5667 - val_loss: 0.0553 - val_acc: 0.5789\n",
      "Epoch 60/200\n",
      "3208/3208 [==============================] - 1s 197us/step - loss: 0.0562 - acc: 0.5692 - val_loss: 0.0556 - val_acc: 0.5739\n",
      "Epoch 61/200\n",
      "3208/3208 [==============================] - 1s 198us/step - loss: 0.0561 - acc: 0.5701 - val_loss: 0.0549 - val_acc: 0.5852\n",
      "Epoch 62/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0561 - acc: 0.5676 - val_loss: 0.0552 - val_acc: 0.5876\n",
      "Epoch 63/200\n",
      "3208/3208 [==============================] - 1s 192us/step - loss: 0.0561 - acc: 0.5726 - val_loss: 0.0550 - val_acc: 0.5839\n",
      "Epoch 64/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0561 - acc: 0.5686 - val_loss: 0.0551 - val_acc: 0.5839\n",
      "Epoch 65/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0561 - acc: 0.5698 - val_loss: 0.0549 - val_acc: 0.5883\n",
      "Epoch 66/200\n",
      "3208/3208 [==============================] - 1s 272us/step - loss: 0.0561 - acc: 0.5692 - val_loss: 0.0562 - val_acc: 0.5671\n",
      "Epoch 67/200\n",
      "3208/3208 [==============================] - 2s 497us/step - loss: 0.0561 - acc: 0.5680 - val_loss: 0.0558 - val_acc: 0.5671\n",
      "Epoch 68/200\n",
      "3208/3208 [==============================] - 2s 537us/step - loss: 0.0562 - acc: 0.5676 - val_loss: 0.0550 - val_acc: 0.5852\n",
      "Epoch 69/200\n",
      "3208/3208 [==============================] - 1s 419us/step - loss: 0.0560 - acc: 0.5661 - val_loss: 0.0557 - val_acc: 0.5739\n",
      "Epoch 70/200\n",
      "3208/3208 [==============================] - 1s 422us/step - loss: 0.0561 - acc: 0.5689 - val_loss: 0.0555 - val_acc: 0.5802\n",
      "Epoch 71/200\n",
      "3208/3208 [==============================] - 1s 386us/step - loss: 0.0561 - acc: 0.5720 - val_loss: 0.0551 - val_acc: 0.5858\n",
      "Epoch 72/200\n",
      "3208/3208 [==============================] - 1s 370us/step - loss: 0.0561 - acc: 0.5686 - val_loss: 0.0563 - val_acc: 0.5614\n",
      "Epoch 73/200\n",
      "3208/3208 [==============================] - 2s 499us/step - loss: 0.0561 - acc: 0.5670 - val_loss: 0.0554 - val_acc: 0.5783\n",
      "Epoch 74/200\n",
      "3208/3208 [==============================] - 1s 365us/step - loss: 0.0560 - acc: 0.5704 - val_loss: 0.0550 - val_acc: 0.5870\n",
      "Epoch 75/200\n",
      "3208/3208 [==============================] - 1s 208us/step - loss: 0.0560 - acc: 0.5723 - val_loss: 0.0555 - val_acc: 0.5764\n",
      "Epoch 76/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0559 - acc: 0.5689 - val_loss: 0.0550 - val_acc: 0.5889\n",
      "Epoch 77/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0561 - acc: 0.5689 - val_loss: 0.0550 - val_acc: 0.5870\n",
      "Epoch 78/200\n",
      "3208/3208 [==============================] - 1s 203us/step - loss: 0.0560 - acc: 0.5683 - val_loss: 0.0556 - val_acc: 0.5733\n",
      "Epoch 79/200\n",
      "3208/3208 [==============================] - 1s 198us/step - loss: 0.0560 - acc: 0.5689 - val_loss: 0.0551 - val_acc: 0.5889\n",
      "Epoch 80/200\n",
      "3208/3208 [==============================] - 1s 204us/step - loss: 0.0560 - acc: 0.5686 - val_loss: 0.0552 - val_acc: 0.5889\n",
      "Epoch 81/200\n",
      "3208/3208 [==============================] - 1s 207us/step - loss: 0.0560 - acc: 0.5698 - val_loss: 0.0553 - val_acc: 0.5777\n",
      "Epoch 82/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0560 - acc: 0.5720 - val_loss: 0.0554 - val_acc: 0.5727\n",
      "Epoch 83/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0561 - acc: 0.5651 - val_loss: 0.0552 - val_acc: 0.5820\n",
      "Epoch 84/200\n",
      "3208/3208 [==============================] - 1s 202us/step - loss: 0.0561 - acc: 0.5683 - val_loss: 0.0552 - val_acc: 0.5783\n",
      "Epoch 85/200\n",
      "3208/3208 [==============================] - 1s 203us/step - loss: 0.0561 - acc: 0.5683 - val_loss: 0.0558 - val_acc: 0.5708\n",
      "Epoch 86/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0560 - acc: 0.5708 - val_loss: 0.0554 - val_acc: 0.5783\n",
      "Epoch 87/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0560 - acc: 0.5673 - val_loss: 0.0552 - val_acc: 0.5852\n",
      "Epoch 88/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0560 - acc: 0.5655 - val_loss: 0.0555 - val_acc: 0.5783\n",
      "Epoch 89/200\n",
      "3208/3208 [==============================] - 1s 204us/step - loss: 0.0560 - acc: 0.5686 - val_loss: 0.0551 - val_acc: 0.5827\n",
      "Epoch 90/200\n",
      "3208/3208 [==============================] - 1s 197us/step - loss: 0.0560 - acc: 0.5714 - val_loss: 0.0559 - val_acc: 0.5708\n",
      "Epoch 91/200\n",
      "3208/3208 [==============================] - 1s 208us/step - loss: 0.0560 - acc: 0.5704 - val_loss: 0.0552 - val_acc: 0.5845\n",
      "Epoch 92/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0560 - acc: 0.5723 - val_loss: 0.0554 - val_acc: 0.5814\n",
      "Epoch 93/200\n",
      "3208/3208 [==============================] - 1s 198us/step - loss: 0.0560 - acc: 0.5726 - val_loss: 0.0555 - val_acc: 0.5820\n",
      "Epoch 94/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0560 - acc: 0.5661 - val_loss: 0.0557 - val_acc: 0.5739\n",
      "Epoch 95/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0560 - acc: 0.5689 - val_loss: 0.0551 - val_acc: 0.5889\n",
      "Epoch 96/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0560 - acc: 0.5683 - val_loss: 0.0552 - val_acc: 0.5833\n",
      "Epoch 97/200\n",
      "3208/3208 [==============================] - 1s 202us/step - loss: 0.0559 - acc: 0.5733 - val_loss: 0.0553 - val_acc: 0.5814\n",
      "Epoch 98/200\n",
      "3208/3208 [==============================] - 1s 197us/step - loss: 0.0560 - acc: 0.5745 - val_loss: 0.0556 - val_acc: 0.5770\n",
      "Epoch 99/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0559 - acc: 0.5701 - val_loss: 0.0556 - val_acc: 0.5783\n",
      "Epoch 100/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0560 - acc: 0.5676 - val_loss: 0.0552 - val_acc: 0.5808\n",
      "Epoch 101/200\n",
      "3208/3208 [==============================] - 1s 195us/step - loss: 0.0559 - acc: 0.5739 - val_loss: 0.0551 - val_acc: 0.5864\n",
      "Epoch 102/200\n",
      "3208/3208 [==============================] - 1s 204us/step - loss: 0.0560 - acc: 0.5686 - val_loss: 0.0554 - val_acc: 0.5789\n",
      "Epoch 103/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0560 - acc: 0.5729 - val_loss: 0.0553 - val_acc: 0.5820\n",
      "Epoch 104/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0560 - acc: 0.5726 - val_loss: 0.0552 - val_acc: 0.5752\n",
      "Epoch 105/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0559 - acc: 0.5711 - val_loss: 0.0551 - val_acc: 0.5889\n",
      "Epoch 106/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0559 - acc: 0.5692 - val_loss: 0.0552 - val_acc: 0.5858\n",
      "Epoch 107/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0560 - acc: 0.5701 - val_loss: 0.0552 - val_acc: 0.5870\n",
      "Epoch 108/200\n",
      "3208/3208 [==============================] - 1s 256us/step - loss: 0.0560 - acc: 0.5692 - val_loss: 0.0562 - val_acc: 0.5646\n",
      "Epoch 109/200\n",
      "3208/3208 [==============================] - 1s 261us/step - loss: 0.0559 - acc: 0.5708 - val_loss: 0.0565 - val_acc: 0.5546\n",
      "Epoch 110/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0559 - acc: 0.5695 - val_loss: 0.0553 - val_acc: 0.5789\n",
      "Epoch 111/200\n",
      "3208/3208 [==============================] - 1s 260us/step - loss: 0.0559 - acc: 0.5701 - val_loss: 0.0557 - val_acc: 0.5789\n",
      "Epoch 112/200\n",
      "3208/3208 [==============================] - 1s 256us/step - loss: 0.0560 - acc: 0.5711 - val_loss: 0.0555 - val_acc: 0.5789\n",
      "Epoch 113/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0560 - acc: 0.5711 - val_loss: 0.0558 - val_acc: 0.5714\n",
      "Epoch 114/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0559 - acc: 0.5723 - val_loss: 0.0557 - val_acc: 0.5789\n",
      "Epoch 115/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0559 - acc: 0.5683 - val_loss: 0.0559 - val_acc: 0.5714\n",
      "Epoch 116/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0560 - acc: 0.5711 - val_loss: 0.0556 - val_acc: 0.5739\n",
      "Epoch 117/200\n",
      "3208/3208 [==============================] - 1s 203us/step - loss: 0.0559 - acc: 0.5704 - val_loss: 0.0553 - val_acc: 0.5833\n",
      "Epoch 118/200\n",
      "3208/3208 [==============================] - 1s 215us/step - loss: 0.0559 - acc: 0.5692 - val_loss: 0.0554 - val_acc: 0.5783\n",
      "Epoch 119/200\n",
      "3208/3208 [==============================] - 1s 208us/step - loss: 0.0559 - acc: 0.5708 - val_loss: 0.0555 - val_acc: 0.5808\n",
      "Epoch 120/200\n",
      "3208/3208 [==============================] - 1s 202us/step - loss: 0.0559 - acc: 0.5739 - val_loss: 0.0555 - val_acc: 0.5770\n",
      "Epoch 121/200\n",
      "3208/3208 [==============================] - 1s 204us/step - loss: 0.0559 - acc: 0.5711 - val_loss: 0.0564 - val_acc: 0.5652\n",
      "Epoch 122/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0560 - acc: 0.5714 - val_loss: 0.0564 - val_acc: 0.5540\n",
      "Epoch 123/200\n",
      "3208/3208 [==============================] - 1s 197us/step - loss: 0.0559 - acc: 0.5720 - val_loss: 0.0553 - val_acc: 0.5789\n",
      "Epoch 124/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0559 - acc: 0.5708 - val_loss: 0.0553 - val_acc: 0.5845\n",
      "Epoch 125/200\n",
      "3208/3208 [==============================] - 1s 197us/step - loss: 0.0558 - acc: 0.5726 - val_loss: 0.0561 - val_acc: 0.5658\n",
      "Epoch 126/200\n",
      "3208/3208 [==============================] - 1s 203us/step - loss: 0.0559 - acc: 0.5664 - val_loss: 0.0557 - val_acc: 0.5696\n",
      "Epoch 127/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0559 - acc: 0.5720 - val_loss: 0.0560 - val_acc: 0.5683\n",
      "Epoch 128/200\n",
      "3208/3208 [==============================] - 1s 196us/step - loss: 0.0559 - acc: 0.5723 - val_loss: 0.0553 - val_acc: 0.5802\n",
      "Epoch 129/200\n",
      "3208/3208 [==============================] - 1s 203us/step - loss: 0.0559 - acc: 0.5726 - val_loss: 0.0555 - val_acc: 0.5777\n",
      "Epoch 130/200\n",
      "3208/3208 [==============================] - 1s 202us/step - loss: 0.0558 - acc: 0.5773 - val_loss: 0.0559 - val_acc: 0.5739\n",
      "Epoch 131/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0558 - acc: 0.5701 - val_loss: 0.0564 - val_acc: 0.5677\n",
      "Epoch 132/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0560 - acc: 0.5708 - val_loss: 0.0553 - val_acc: 0.5770\n",
      "Epoch 133/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0559 - acc: 0.5736 - val_loss: 0.0558 - val_acc: 0.5752\n",
      "Epoch 134/200\n",
      "3208/3208 [==============================] - 1s 208us/step - loss: 0.0558 - acc: 0.5726 - val_loss: 0.0553 - val_acc: 0.5814\n",
      "Epoch 135/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0558 - acc: 0.5754 - val_loss: 0.0555 - val_acc: 0.5783\n",
      "Epoch 136/200\n",
      "3208/3208 [==============================] - 1s 200us/step - loss: 0.0559 - acc: 0.5714 - val_loss: 0.0558 - val_acc: 0.5727\n",
      "Epoch 137/200\n",
      "3208/3208 [==============================] - 1s 204us/step - loss: 0.0559 - acc: 0.5745 - val_loss: 0.0561 - val_acc: 0.5639\n",
      "Epoch 138/200\n",
      "3208/3208 [==============================] - 1s 207us/step - loss: 0.0559 - acc: 0.5664 - val_loss: 0.0555 - val_acc: 0.5814\n",
      "Epoch 139/200\n",
      "3208/3208 [==============================] - 1s 203us/step - loss: 0.0558 - acc: 0.5704 - val_loss: 0.0558 - val_acc: 0.5777\n",
      "Epoch 140/200\n",
      "3208/3208 [==============================] - 1s 203us/step - loss: 0.0559 - acc: 0.5704 - val_loss: 0.0555 - val_acc: 0.5770\n",
      "Epoch 141/200\n",
      "3208/3208 [==============================] - 1s 204us/step - loss: 0.0558 - acc: 0.5726 - val_loss: 0.0554 - val_acc: 0.5833\n",
      "Epoch 142/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0559 - acc: 0.5692 - val_loss: 0.0559 - val_acc: 0.5770\n",
      "Epoch 143/200\n",
      "3208/3208 [==============================] - 1s 203us/step - loss: 0.0558 - acc: 0.5711 - val_loss: 0.0552 - val_acc: 0.5858\n",
      "Epoch 144/200\n",
      "3208/3208 [==============================] - 1s 203us/step - loss: 0.0559 - acc: 0.5692 - val_loss: 0.0557 - val_acc: 0.5739\n",
      "Epoch 145/200\n",
      "3208/3208 [==============================] - 1s 207us/step - loss: 0.0558 - acc: 0.5723 - val_loss: 0.0554 - val_acc: 0.5870\n",
      "Epoch 146/200\n",
      "3208/3208 [==============================] - 1s 204us/step - loss: 0.0559 - acc: 0.5720 - val_loss: 0.0560 - val_acc: 0.5733\n",
      "Epoch 147/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0558 - acc: 0.5717 - val_loss: 0.0559 - val_acc: 0.5770\n",
      "Epoch 148/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0558 - acc: 0.5686 - val_loss: 0.0554 - val_acc: 0.5789\n",
      "Epoch 149/200\n",
      "3208/3208 [==============================] - 1s 204us/step - loss: 0.0558 - acc: 0.5736 - val_loss: 0.0556 - val_acc: 0.5764\n",
      "Epoch 150/200\n",
      "3208/3208 [==============================] - 1s 204us/step - loss: 0.0558 - acc: 0.5711 - val_loss: 0.0556 - val_acc: 0.5739\n",
      "Epoch 151/200\n",
      "3208/3208 [==============================] - 1s 210us/step - loss: 0.0558 - acc: 0.5711 - val_loss: 0.0553 - val_acc: 0.5833\n",
      "Epoch 152/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0558 - acc: 0.5739 - val_loss: 0.0552 - val_acc: 0.5827\n",
      "Epoch 153/200\n",
      "3208/3208 [==============================] - 1s 203us/step - loss: 0.0558 - acc: 0.5748 - val_loss: 0.0554 - val_acc: 0.5783\n",
      "Epoch 154/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0558 - acc: 0.5686 - val_loss: 0.0556 - val_acc: 0.5770\n",
      "Epoch 155/200\n",
      "3208/3208 [==============================] - 1s 201us/step - loss: 0.0559 - acc: 0.5692 - val_loss: 0.0560 - val_acc: 0.5721\n",
      "Epoch 156/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0558 - acc: 0.5736 - val_loss: 0.0555 - val_acc: 0.5833\n",
      "Epoch 157/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0557 - acc: 0.5720 - val_loss: 0.0553 - val_acc: 0.5820\n",
      "Epoch 158/200\n",
      "3208/3208 [==============================] - 1s 202us/step - loss: 0.0558 - acc: 0.5748 - val_loss: 0.0560 - val_acc: 0.5702\n",
      "Epoch 159/200\n",
      "3208/3208 [==============================] - 1s 207us/step - loss: 0.0558 - acc: 0.5726 - val_loss: 0.0557 - val_acc: 0.5721\n",
      "Epoch 160/200\n",
      "3208/3208 [==============================] - 1s 208us/step - loss: 0.0557 - acc: 0.5717 - val_loss: 0.0557 - val_acc: 0.5739\n",
      "Epoch 161/200\n",
      "3208/3208 [==============================] - 1s 215us/step - loss: 0.0558 - acc: 0.5751 - val_loss: 0.0556 - val_acc: 0.5745\n",
      "Epoch 162/200\n",
      "3208/3208 [==============================] - 1s 208us/step - loss: 0.0558 - acc: 0.5773 - val_loss: 0.0553 - val_acc: 0.5820\n",
      "Epoch 163/200\n",
      "3208/3208 [==============================] - 1s 211us/step - loss: 0.0558 - acc: 0.5729 - val_loss: 0.0558 - val_acc: 0.5727\n",
      "Epoch 164/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0558 - acc: 0.5720 - val_loss: 0.0563 - val_acc: 0.5671\n",
      "Epoch 165/200\n",
      "3208/3208 [==============================] - 1s 216us/step - loss: 0.0557 - acc: 0.5723 - val_loss: 0.0556 - val_acc: 0.5733\n",
      "Epoch 166/200\n",
      "3208/3208 [==============================] - 1s 215us/step - loss: 0.0558 - acc: 0.5748 - val_loss: 0.0555 - val_acc: 0.5833\n",
      "Epoch 167/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0558 - acc: 0.5729 - val_loss: 0.0559 - val_acc: 0.5770\n",
      "Epoch 168/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0556 - acc: 0.5736 - val_loss: 0.0553 - val_acc: 0.5814\n",
      "Epoch 169/200\n",
      "3208/3208 [==============================] - 1s 209us/step - loss: 0.0558 - acc: 0.5736 - val_loss: 0.0560 - val_acc: 0.5652\n",
      "Epoch 170/200\n",
      "3208/3208 [==============================] - 1s 208us/step - loss: 0.0558 - acc: 0.5717 - val_loss: 0.0553 - val_acc: 0.5845\n",
      "Epoch 171/200\n",
      "3208/3208 [==============================] - 1s 214us/step - loss: 0.0558 - acc: 0.5748 - val_loss: 0.0558 - val_acc: 0.5696\n",
      "Epoch 172/200\n",
      "3208/3208 [==============================] - 1s 210us/step - loss: 0.0558 - acc: 0.5726 - val_loss: 0.0558 - val_acc: 0.5689\n",
      "Epoch 173/200\n",
      "3208/3208 [==============================] - 1s 207us/step - loss: 0.0557 - acc: 0.5726 - val_loss: 0.0554 - val_acc: 0.5833\n",
      "Epoch 174/200\n",
      "3208/3208 [==============================] - 1s 215us/step - loss: 0.0558 - acc: 0.5714 - val_loss: 0.0553 - val_acc: 0.5814\n",
      "Epoch 175/200\n",
      "3208/3208 [==============================] - 1s 207us/step - loss: 0.0557 - acc: 0.5739 - val_loss: 0.0554 - val_acc: 0.5833\n",
      "Epoch 176/200\n",
      "3208/3208 [==============================] - 1s 210us/step - loss: 0.0557 - acc: 0.5742 - val_loss: 0.0561 - val_acc: 0.5664\n",
      "Epoch 177/200\n",
      "3208/3208 [==============================] - 1s 215us/step - loss: 0.0557 - acc: 0.5729 - val_loss: 0.0564 - val_acc: 0.5540\n",
      "Epoch 178/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0557 - acc: 0.5729 - val_loss: 0.0557 - val_acc: 0.5770\n",
      "Epoch 179/200\n",
      "3208/3208 [==============================] - 1s 213us/step - loss: 0.0557 - acc: 0.5723 - val_loss: 0.0555 - val_acc: 0.5721\n",
      "Epoch 180/200\n",
      "3208/3208 [==============================] - 1s 219us/step - loss: 0.0557 - acc: 0.5742 - val_loss: 0.0553 - val_acc: 0.5852\n",
      "Epoch 181/200\n",
      "3208/3208 [==============================] - 1s 211us/step - loss: 0.0557 - acc: 0.5748 - val_loss: 0.0552 - val_acc: 0.5814\n",
      "Epoch 182/200\n",
      "3208/3208 [==============================] - 1s 210us/step - loss: 0.0557 - acc: 0.5717 - val_loss: 0.0557 - val_acc: 0.5770\n",
      "Epoch 183/200\n",
      "3208/3208 [==============================] - 1s 215us/step - loss: 0.0556 - acc: 0.5723 - val_loss: 0.0554 - val_acc: 0.5789\n",
      "Epoch 184/200\n",
      "3208/3208 [==============================] - 1s 212us/step - loss: 0.0557 - acc: 0.5698 - val_loss: 0.0560 - val_acc: 0.5639\n",
      "Epoch 185/200\n",
      "3208/3208 [==============================] - 1s 214us/step - loss: 0.0558 - acc: 0.5726 - val_loss: 0.0559 - val_acc: 0.5739\n",
      "Epoch 186/200\n",
      "3208/3208 [==============================] - 1s 214us/step - loss: 0.0557 - acc: 0.5714 - val_loss: 0.0554 - val_acc: 0.5814\n",
      "Epoch 187/200\n",
      "3208/3208 [==============================] - 1s 209us/step - loss: 0.0557 - acc: 0.5745 - val_loss: 0.0556 - val_acc: 0.5814\n",
      "Epoch 188/200\n",
      "3208/3208 [==============================] - 1s 216us/step - loss: 0.0557 - acc: 0.5742 - val_loss: 0.0553 - val_acc: 0.5764\n",
      "Epoch 189/200\n",
      "3208/3208 [==============================] - 1s 211us/step - loss: 0.0557 - acc: 0.5739 - val_loss: 0.0558 - val_acc: 0.5739\n",
      "Epoch 190/200\n",
      "3208/3208 [==============================] - 1s 212us/step - loss: 0.0556 - acc: 0.5742 - val_loss: 0.0562 - val_acc: 0.5677\n",
      "Epoch 191/200\n",
      "3208/3208 [==============================] - 1s 215us/step - loss: 0.0557 - acc: 0.5754 - val_loss: 0.0554 - val_acc: 0.5789\n",
      "Epoch 192/200\n",
      "3208/3208 [==============================] - 1s 215us/step - loss: 0.0556 - acc: 0.5714 - val_loss: 0.0555 - val_acc: 0.5795\n",
      "Epoch 193/200\n",
      "3208/3208 [==============================] - 1s 210us/step - loss: 0.0557 - acc: 0.5733 - val_loss: 0.0560 - val_acc: 0.5671\n",
      "Epoch 194/200\n",
      "3208/3208 [==============================] - 1s 211us/step - loss: 0.0556 - acc: 0.5751 - val_loss: 0.0555 - val_acc: 0.5820\n",
      "Epoch 195/200\n",
      "3208/3208 [==============================] - 1s 207us/step - loss: 0.0556 - acc: 0.5729 - val_loss: 0.0555 - val_acc: 0.5814\n",
      "Epoch 196/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0557 - acc: 0.5742 - val_loss: 0.0568 - val_acc: 0.5614\n",
      "Epoch 197/200\n",
      "3208/3208 [==============================] - 1s 207us/step - loss: 0.0557 - acc: 0.5720 - val_loss: 0.0558 - val_acc: 0.5752\n",
      "Epoch 198/200\n",
      "3208/3208 [==============================] - 1s 210us/step - loss: 0.0557 - acc: 0.5761 - val_loss: 0.0555 - val_acc: 0.5795\n",
      "Epoch 199/200\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0557 - acc: 0.5726 - val_loss: 0.0555 - val_acc: 0.5845\n",
      "Epoch 200/200\n",
      "3208/3208 [==============================] - 1s 206us/step - loss: 0.0557 - acc: 0.5667 - val_loss: 0.0554 - val_acc: 0.5827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=20, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=16, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=12, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "rms = keras.optimizers.RMSprop()\n",
    "nn_model.compile(optimizer=rms, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=200, validation_data=(dev_valid_x, dev_valid_y))\n",
    "  \n",
    "pred_y = nn_model.predict(valid_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result_cv = result_cv.append({'detail':'RMSprop', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 20811
    },
    "colab_type": "code",
    "id": "n7okBy-iXWxs",
    "outputId": "08238ae7-66bd-4d2e-c346-73f111ff910f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 12)                204       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 2,386\n",
      "Trainable params: 2,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 1s 465us/step - loss: 0.0836 - acc: 0.2498 - val_loss: 0.0818 - val_acc: 0.2999\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0810 - acc: 0.2922 - val_loss: 0.0812 - val_acc: 0.2999\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0802 - acc: 0.3084 - val_loss: 0.0800 - val_acc: 0.3335\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0781 - acc: 0.4287 - val_loss: 0.0767 - val_acc: 0.4539\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0744 - acc: 0.4456 - val_loss: 0.0731 - val_acc: 0.4539\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0718 - acc: 0.4450 - val_loss: 0.0712 - val_acc: 0.4545\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0705 - acc: 0.4434 - val_loss: 0.0703 - val_acc: 0.4551\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0698 - acc: 0.4428 - val_loss: 0.0697 - val_acc: 0.4551\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 206us/step - loss: 0.0695 - acc: 0.4428 - val_loss: 0.0694 - val_acc: 0.4545\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0692 - acc: 0.4422 - val_loss: 0.0692 - val_acc: 0.4551\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0691 - acc: 0.4431 - val_loss: 0.0691 - val_acc: 0.4551\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0690 - acc: 0.4428 - val_loss: 0.0689 - val_acc: 0.4545\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0689 - acc: 0.4422 - val_loss: 0.0688 - val_acc: 0.4545\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0688 - acc: 0.4415 - val_loss: 0.0688 - val_acc: 0.4551\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0687 - acc: 0.4425 - val_loss: 0.0688 - val_acc: 0.4545\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0687 - acc: 0.4434 - val_loss: 0.0687 - val_acc: 0.4551\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0686 - acc: 0.4425 - val_loss: 0.0686 - val_acc: 0.4551\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0686 - acc: 0.4440 - val_loss: 0.0685 - val_acc: 0.4551\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0685 - acc: 0.4425 - val_loss: 0.0685 - val_acc: 0.4551\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0685 - acc: 0.4456 - val_loss: 0.0685 - val_acc: 0.4545\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0684 - acc: 0.4453 - val_loss: 0.0684 - val_acc: 0.4545\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0683 - acc: 0.4456 - val_loss: 0.0683 - val_acc: 0.4545\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0683 - acc: 0.4462 - val_loss: 0.0683 - val_acc: 0.4551\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 197us/step - loss: 0.0682 - acc: 0.4459 - val_loss: 0.0682 - val_acc: 0.4551\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0682 - acc: 0.4456 - val_loss: 0.0682 - val_acc: 0.4557\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0681 - acc: 0.4459 - val_loss: 0.0681 - val_acc: 0.4557\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0681 - acc: 0.4462 - val_loss: 0.0681 - val_acc: 0.4557\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0680 - acc: 0.4465 - val_loss: 0.0681 - val_acc: 0.4564\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0680 - acc: 0.4471 - val_loss: 0.0680 - val_acc: 0.4564\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0679 - acc: 0.4478 - val_loss: 0.0679 - val_acc: 0.4564\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0679 - acc: 0.4478 - val_loss: 0.0679 - val_acc: 0.4564\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0678 - acc: 0.4475 - val_loss: 0.0678 - val_acc: 0.4564\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0678 - acc: 0.4471 - val_loss: 0.0678 - val_acc: 0.4564\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0677 - acc: 0.4484 - val_loss: 0.0677 - val_acc: 0.4564\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0677 - acc: 0.4484 - val_loss: 0.0677 - val_acc: 0.4576\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0676 - acc: 0.4500 - val_loss: 0.0676 - val_acc: 0.4564\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0676 - acc: 0.4487 - val_loss: 0.0675 - val_acc: 0.4589\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0675 - acc: 0.4496 - val_loss: 0.0675 - val_acc: 0.4595\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0675 - acc: 0.4500 - val_loss: 0.0674 - val_acc: 0.4589\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0674 - acc: 0.4496 - val_loss: 0.0674 - val_acc: 0.4589\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0674 - acc: 0.4500 - val_loss: 0.0673 - val_acc: 0.4595\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0673 - acc: 0.4512 - val_loss: 0.0672 - val_acc: 0.4582\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0673 - acc: 0.4496 - val_loss: 0.0673 - val_acc: 0.4589\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0672 - acc: 0.4524 - val_loss: 0.0672 - val_acc: 0.4595\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0672 - acc: 0.4518 - val_loss: 0.0672 - val_acc: 0.4589\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0672 - acc: 0.4521 - val_loss: 0.0671 - val_acc: 0.4582\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0671 - acc: 0.4537 - val_loss: 0.0671 - val_acc: 0.4582\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0671 - acc: 0.4528 - val_loss: 0.0670 - val_acc: 0.4595\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0670 - acc: 0.4534 - val_loss: 0.0669 - val_acc: 0.4595\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0670 - acc: 0.4531 - val_loss: 0.0669 - val_acc: 0.4589\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0669 - acc: 0.4534 - val_loss: 0.0668 - val_acc: 0.4589\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0669 - acc: 0.4537 - val_loss: 0.0669 - val_acc: 0.4607\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0669 - acc: 0.4540 - val_loss: 0.0668 - val_acc: 0.4601\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0668 - acc: 0.4543 - val_loss: 0.0668 - val_acc: 0.4607\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0668 - acc: 0.4534 - val_loss: 0.0667 - val_acc: 0.4607\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0667 - acc: 0.4537 - val_loss: 0.0666 - val_acc: 0.4607\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0667 - acc: 0.4540 - val_loss: 0.0666 - val_acc: 0.4607\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0666 - acc: 0.4534 - val_loss: 0.0666 - val_acc: 0.4607\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 201us/step - loss: 0.0666 - acc: 0.4549 - val_loss: 0.0665 - val_acc: 0.4613\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0666 - acc: 0.4537 - val_loss: 0.0664 - val_acc: 0.4607\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0665 - acc: 0.4540 - val_loss: 0.0664 - val_acc: 0.4613\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0665 - acc: 0.4534 - val_loss: 0.0663 - val_acc: 0.4607\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0664 - acc: 0.4543 - val_loss: 0.0663 - val_acc: 0.4607\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0664 - acc: 0.4546 - val_loss: 0.0663 - val_acc: 0.4607\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0664 - acc: 0.4540 - val_loss: 0.0662 - val_acc: 0.4607\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0663 - acc: 0.4546 - val_loss: 0.0662 - val_acc: 0.4620\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0663 - acc: 0.4553 - val_loss: 0.0661 - val_acc: 0.4613\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0662 - acc: 0.4543 - val_loss: 0.0661 - val_acc: 0.4613\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0662 - acc: 0.4549 - val_loss: 0.0661 - val_acc: 0.4626\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0662 - acc: 0.4549 - val_loss: 0.0660 - val_acc: 0.4613\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0661 - acc: 0.4549 - val_loss: 0.0660 - val_acc: 0.4626\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0661 - acc: 0.4556 - val_loss: 0.0659 - val_acc: 0.4626\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0661 - acc: 0.4556 - val_loss: 0.0659 - val_acc: 0.4620\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0660 - acc: 0.4549 - val_loss: 0.0658 - val_acc: 0.4620\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0660 - acc: 0.4553 - val_loss: 0.0658 - val_acc: 0.4626\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0659 - acc: 0.4559 - val_loss: 0.0658 - val_acc: 0.4620\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0659 - acc: 0.4562 - val_loss: 0.0657 - val_acc: 0.4613\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0659 - acc: 0.4559 - val_loss: 0.0657 - val_acc: 0.4613\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0658 - acc: 0.4553 - val_loss: 0.0657 - val_acc: 0.4626\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0658 - acc: 0.4568 - val_loss: 0.0656 - val_acc: 0.4626\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0658 - acc: 0.4559 - val_loss: 0.0656 - val_acc: 0.4626\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0657 - acc: 0.4568 - val_loss: 0.0655 - val_acc: 0.4620\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0657 - acc: 0.4577 - val_loss: 0.0654 - val_acc: 0.4626\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0657 - acc: 0.4559 - val_loss: 0.0655 - val_acc: 0.4638\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0656 - acc: 0.4577 - val_loss: 0.0655 - val_acc: 0.4632\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0656 - acc: 0.4562 - val_loss: 0.0654 - val_acc: 0.4626\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0655 - acc: 0.4577 - val_loss: 0.0654 - val_acc: 0.4638\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0655 - acc: 0.4581 - val_loss: 0.0653 - val_acc: 0.4632\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0655 - acc: 0.4568 - val_loss: 0.0654 - val_acc: 0.4657\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0654 - acc: 0.4581 - val_loss: 0.0652 - val_acc: 0.4638\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0654 - acc: 0.4562 - val_loss: 0.0652 - val_acc: 0.4651\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0654 - acc: 0.4571 - val_loss: 0.0651 - val_acc: 0.4638\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0653 - acc: 0.4568 - val_loss: 0.0651 - val_acc: 0.4638\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0653 - acc: 0.4568 - val_loss: 0.0651 - val_acc: 0.4638\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0653 - acc: 0.4556 - val_loss: 0.0650 - val_acc: 0.4626\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0652 - acc: 0.4553 - val_loss: 0.0650 - val_acc: 0.4657\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0652 - acc: 0.4571 - val_loss: 0.0650 - val_acc: 0.4651\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0652 - acc: 0.4565 - val_loss: 0.0650 - val_acc: 0.4657\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0651 - acc: 0.4559 - val_loss: 0.0650 - val_acc: 0.4657\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0651 - acc: 0.4556 - val_loss: 0.0649 - val_acc: 0.4651\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0651 - acc: 0.4546 - val_loss: 0.0648 - val_acc: 0.4638\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0650 - acc: 0.4556 - val_loss: 0.0648 - val_acc: 0.4645\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0650 - acc: 0.4543 - val_loss: 0.0649 - val_acc: 0.4645\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0650 - acc: 0.4556 - val_loss: 0.0648 - val_acc: 0.4645\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0650 - acc: 0.4553 - val_loss: 0.0647 - val_acc: 0.4645\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0649 - acc: 0.4546 - val_loss: 0.0647 - val_acc: 0.4626\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0649 - acc: 0.4553 - val_loss: 0.0646 - val_acc: 0.4638\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0648 - acc: 0.4553 - val_loss: 0.0647 - val_acc: 0.4620\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0648 - acc: 0.4556 - val_loss: 0.0647 - val_acc: 0.4613\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0648 - acc: 0.4577 - val_loss: 0.0646 - val_acc: 0.4632\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0648 - acc: 0.4565 - val_loss: 0.0646 - val_acc: 0.4613\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0647 - acc: 0.4571 - val_loss: 0.0645 - val_acc: 0.4607\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0647 - acc: 0.4574 - val_loss: 0.0645 - val_acc: 0.4626\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0647 - acc: 0.4565 - val_loss: 0.0645 - val_acc: 0.4607\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0647 - acc: 0.4556 - val_loss: 0.0644 - val_acc: 0.4613\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0646 - acc: 0.4568 - val_loss: 0.0645 - val_acc: 0.4613\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0646 - acc: 0.4581 - val_loss: 0.0644 - val_acc: 0.4632\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0646 - acc: 0.4584 - val_loss: 0.0644 - val_acc: 0.4626\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0645 - acc: 0.4581 - val_loss: 0.0643 - val_acc: 0.4626\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0645 - acc: 0.4593 - val_loss: 0.0643 - val_acc: 0.4651\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0645 - acc: 0.4590 - val_loss: 0.0643 - val_acc: 0.4638\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0645 - acc: 0.4599 - val_loss: 0.0643 - val_acc: 0.4645\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0644 - acc: 0.4584 - val_loss: 0.0643 - val_acc: 0.4638\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0644 - acc: 0.4599 - val_loss: 0.0642 - val_acc: 0.4651\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0644 - acc: 0.4599 - val_loss: 0.0642 - val_acc: 0.4645\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0644 - acc: 0.4609 - val_loss: 0.0642 - val_acc: 0.4663\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0643 - acc: 0.4599 - val_loss: 0.0641 - val_acc: 0.4651\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0643 - acc: 0.4593 - val_loss: 0.0642 - val_acc: 0.4713\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0643 - acc: 0.4621 - val_loss: 0.0642 - val_acc: 0.4719\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0643 - acc: 0.4602 - val_loss: 0.0641 - val_acc: 0.4713\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0643 - acc: 0.4621 - val_loss: 0.0641 - val_acc: 0.4701\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0642 - acc: 0.4627 - val_loss: 0.0640 - val_acc: 0.4713\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0642 - acc: 0.4618 - val_loss: 0.0640 - val_acc: 0.4713\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0642 - acc: 0.4652 - val_loss: 0.0640 - val_acc: 0.4719\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0642 - acc: 0.4643 - val_loss: 0.0640 - val_acc: 0.4713\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0641 - acc: 0.4634 - val_loss: 0.0640 - val_acc: 0.4719\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0641 - acc: 0.4634 - val_loss: 0.0639 - val_acc: 0.4719\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0641 - acc: 0.4640 - val_loss: 0.0639 - val_acc: 0.4726\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0641 - acc: 0.4630 - val_loss: 0.0639 - val_acc: 0.4732\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0640 - acc: 0.4640 - val_loss: 0.0639 - val_acc: 0.4707\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0640 - acc: 0.4646 - val_loss: 0.0639 - val_acc: 0.4744\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0640 - acc: 0.4640 - val_loss: 0.0638 - val_acc: 0.4738\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0640 - acc: 0.4652 - val_loss: 0.0638 - val_acc: 0.4738\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0640 - acc: 0.4652 - val_loss: 0.0638 - val_acc: 0.4695\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0639 - acc: 0.4665 - val_loss: 0.0638 - val_acc: 0.4738\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0639 - acc: 0.4668 - val_loss: 0.0637 - val_acc: 0.4713\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0639 - acc: 0.4646 - val_loss: 0.0637 - val_acc: 0.4707\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0639 - acc: 0.4655 - val_loss: 0.0637 - val_acc: 0.4757\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0639 - acc: 0.4659 - val_loss: 0.0636 - val_acc: 0.4719\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0638 - acc: 0.4643 - val_loss: 0.0636 - val_acc: 0.4732\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0638 - acc: 0.4643 - val_loss: 0.0636 - val_acc: 0.4732\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 199us/step - loss: 0.0638 - acc: 0.4652 - val_loss: 0.0636 - val_acc: 0.4769\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0638 - acc: 0.4655 - val_loss: 0.0636 - val_acc: 0.4776\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0638 - acc: 0.4668 - val_loss: 0.0636 - val_acc: 0.4769\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0637 - acc: 0.4677 - val_loss: 0.0636 - val_acc: 0.4776\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0637 - acc: 0.4665 - val_loss: 0.0636 - val_acc: 0.4763\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0637 - acc: 0.4671 - val_loss: 0.0635 - val_acc: 0.4763\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0637 - acc: 0.4671 - val_loss: 0.0635 - val_acc: 0.4769\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0637 - acc: 0.4659 - val_loss: 0.0635 - val_acc: 0.4769\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0637 - acc: 0.4674 - val_loss: 0.0635 - val_acc: 0.4769\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0636 - acc: 0.4680 - val_loss: 0.0636 - val_acc: 0.4763\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0636 - acc: 0.4668 - val_loss: 0.0635 - val_acc: 0.4757\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0636 - acc: 0.4671 - val_loss: 0.0634 - val_acc: 0.4776\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0636 - acc: 0.4668 - val_loss: 0.0634 - val_acc: 0.4776\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0636 - acc: 0.4668 - val_loss: 0.0634 - val_acc: 0.4751\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0636 - acc: 0.4668 - val_loss: 0.0634 - val_acc: 0.4757\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0635 - acc: 0.4659 - val_loss: 0.0634 - val_acc: 0.4763\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0635 - acc: 0.4693 - val_loss: 0.0634 - val_acc: 0.4757\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0635 - acc: 0.4677 - val_loss: 0.0633 - val_acc: 0.4763\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0635 - acc: 0.4671 - val_loss: 0.0634 - val_acc: 0.4776\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0635 - acc: 0.4677 - val_loss: 0.0634 - val_acc: 0.4782\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0635 - acc: 0.4696 - val_loss: 0.0633 - val_acc: 0.4763\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0634 - acc: 0.4696 - val_loss: 0.0633 - val_acc: 0.4782\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0634 - acc: 0.4690 - val_loss: 0.0633 - val_acc: 0.4788\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0634 - acc: 0.4718 - val_loss: 0.0633 - val_acc: 0.4776\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0634 - acc: 0.4708 - val_loss: 0.0633 - val_acc: 0.4807\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0634 - acc: 0.4743 - val_loss: 0.0633 - val_acc: 0.4794\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0634 - acc: 0.4727 - val_loss: 0.0632 - val_acc: 0.4776\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0634 - acc: 0.4715 - val_loss: 0.0633 - val_acc: 0.4807\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0633 - acc: 0.4730 - val_loss: 0.0632 - val_acc: 0.4788\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0633 - acc: 0.4755 - val_loss: 0.0632 - val_acc: 0.4819\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0633 - acc: 0.4761 - val_loss: 0.0632 - val_acc: 0.4800\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0633 - acc: 0.4755 - val_loss: 0.0632 - val_acc: 0.4807\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0633 - acc: 0.4724 - val_loss: 0.0632 - val_acc: 0.4819\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0633 - acc: 0.4737 - val_loss: 0.0632 - val_acc: 0.4819\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0633 - acc: 0.4765 - val_loss: 0.0631 - val_acc: 0.4800\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0632 - acc: 0.4755 - val_loss: 0.0631 - val_acc: 0.4819\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0632 - acc: 0.4780 - val_loss: 0.0631 - val_acc: 0.4819\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0632 - acc: 0.4733 - val_loss: 0.0631 - val_acc: 0.4819\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0632 - acc: 0.4793 - val_loss: 0.0631 - val_acc: 0.4819\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0632 - acc: 0.4771 - val_loss: 0.0631 - val_acc: 0.4832\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0632 - acc: 0.4765 - val_loss: 0.0632 - val_acc: 0.4875\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0632 - acc: 0.4818 - val_loss: 0.0631 - val_acc: 0.4825\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0631 - acc: 0.4786 - val_loss: 0.0631 - val_acc: 0.4832\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0631 - acc: 0.4808 - val_loss: 0.0630 - val_acc: 0.4825\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0631 - acc: 0.4783 - val_loss: 0.0630 - val_acc: 0.4832\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0631 - acc: 0.4796 - val_loss: 0.0630 - val_acc: 0.4825\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0631 - acc: 0.4802 - val_loss: 0.0630 - val_acc: 0.4857\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0631 - acc: 0.4814 - val_loss: 0.0630 - val_acc: 0.4869\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0631 - acc: 0.4824 - val_loss: 0.0630 - val_acc: 0.4857\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0625 - acc: 0.4843 - val_loss: 0.0641 - val_acc: 0.4657\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0625 - acc: 0.4821 - val_loss: 0.0641 - val_acc: 0.4682\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0624 - acc: 0.4849 - val_loss: 0.0641 - val_acc: 0.4620\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0624 - acc: 0.4808 - val_loss: 0.0641 - val_acc: 0.4651\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0624 - acc: 0.4843 - val_loss: 0.0641 - val_acc: 0.4632\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0624 - acc: 0.4814 - val_loss: 0.0641 - val_acc: 0.4663\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0624 - acc: 0.4830 - val_loss: 0.0640 - val_acc: 0.4670\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0623 - acc: 0.4852 - val_loss: 0.0640 - val_acc: 0.4657\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0623 - acc: 0.4867 - val_loss: 0.0641 - val_acc: 0.4589\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0623 - acc: 0.4802 - val_loss: 0.0640 - val_acc: 0.4645\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0623 - acc: 0.4830 - val_loss: 0.0640 - val_acc: 0.4651\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0623 - acc: 0.4836 - val_loss: 0.0641 - val_acc: 0.4613\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0623 - acc: 0.4818 - val_loss: 0.0640 - val_acc: 0.4645\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0623 - acc: 0.4846 - val_loss: 0.0640 - val_acc: 0.4626\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0622 - acc: 0.4824 - val_loss: 0.0640 - val_acc: 0.4676\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0622 - acc: 0.4874 - val_loss: 0.0640 - val_acc: 0.4638\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0622 - acc: 0.4855 - val_loss: 0.0640 - val_acc: 0.4626\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0622 - acc: 0.4849 - val_loss: 0.0640 - val_acc: 0.4657\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0622 - acc: 0.4846 - val_loss: 0.0639 - val_acc: 0.4688\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0622 - acc: 0.4849 - val_loss: 0.0639 - val_acc: 0.4657\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0622 - acc: 0.4833 - val_loss: 0.0639 - val_acc: 0.4695\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0621 - acc: 0.4899 - val_loss: 0.0640 - val_acc: 0.4626\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0621 - acc: 0.4839 - val_loss: 0.0639 - val_acc: 0.4638\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0621 - acc: 0.4864 - val_loss: 0.0640 - val_acc: 0.4632\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0621 - acc: 0.4852 - val_loss: 0.0639 - val_acc: 0.4688\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0621 - acc: 0.4867 - val_loss: 0.0639 - val_acc: 0.4676\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0621 - acc: 0.4880 - val_loss: 0.0640 - val_acc: 0.4620\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0621 - acc: 0.4889 - val_loss: 0.0639 - val_acc: 0.4638\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0621 - acc: 0.4852 - val_loss: 0.0639 - val_acc: 0.4732\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0620 - acc: 0.4902 - val_loss: 0.0639 - val_acc: 0.4651\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0620 - acc: 0.4880 - val_loss: 0.0639 - val_acc: 0.4719\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0620 - acc: 0.4936 - val_loss: 0.0639 - val_acc: 0.4651\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0620 - acc: 0.4911 - val_loss: 0.0639 - val_acc: 0.4651\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0620 - acc: 0.4886 - val_loss: 0.0638 - val_acc: 0.4719\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0620 - acc: 0.4936 - val_loss: 0.0639 - val_acc: 0.4657\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0620 - acc: 0.4899 - val_loss: 0.0638 - val_acc: 0.4726\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0620 - acc: 0.4920 - val_loss: 0.0638 - val_acc: 0.4769\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0620 - acc: 0.4952 - val_loss: 0.0638 - val_acc: 0.4707\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0619 - acc: 0.4967 - val_loss: 0.0639 - val_acc: 0.4638\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0619 - acc: 0.4896 - val_loss: 0.0638 - val_acc: 0.4707\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0619 - acc: 0.4973 - val_loss: 0.0638 - val_acc: 0.4688\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0619 - acc: 0.4952 - val_loss: 0.0638 - val_acc: 0.4688\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0619 - acc: 0.4952 - val_loss: 0.0638 - val_acc: 0.4701\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0619 - acc: 0.4970 - val_loss: 0.0638 - val_acc: 0.4701\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0619 - acc: 0.4970 - val_loss: 0.0638 - val_acc: 0.4695\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0619 - acc: 0.4955 - val_loss: 0.0638 - val_acc: 0.4701\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0619 - acc: 0.4983 - val_loss: 0.0638 - val_acc: 0.4695\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0618 - acc: 0.4967 - val_loss: 0.0638 - val_acc: 0.4707\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0618 - acc: 0.4986 - val_loss: 0.0638 - val_acc: 0.4751\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0618 - acc: 0.5008 - val_loss: 0.0638 - val_acc: 0.4695\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0618 - acc: 0.4964 - val_loss: 0.0638 - val_acc: 0.4701\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0618 - acc: 0.4942 - val_loss: 0.0637 - val_acc: 0.4788\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0618 - acc: 0.5020 - val_loss: 0.0638 - val_acc: 0.4719\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0618 - acc: 0.5030 - val_loss: 0.0638 - val_acc: 0.4676\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0618 - acc: 0.4986 - val_loss: 0.0638 - val_acc: 0.4726\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0618 - acc: 0.5002 - val_loss: 0.0637 - val_acc: 0.4757\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0618 - acc: 0.5002 - val_loss: 0.0637 - val_acc: 0.4769\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0617 - acc: 0.5020 - val_loss: 0.0637 - val_acc: 0.4800\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0617 - acc: 0.5058 - val_loss: 0.0637 - val_acc: 0.4751\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0617 - acc: 0.5039 - val_loss: 0.0637 - val_acc: 0.4713\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0617 - acc: 0.5017 - val_loss: 0.0637 - val_acc: 0.4719\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0617 - acc: 0.5023 - val_loss: 0.0637 - val_acc: 0.4788\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0617 - acc: 0.5027 - val_loss: 0.0637 - val_acc: 0.4825\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0617 - acc: 0.5039 - val_loss: 0.0637 - val_acc: 0.4788\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0617 - acc: 0.5076 - val_loss: 0.0637 - val_acc: 0.4719\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0617 - acc: 0.5027 - val_loss: 0.0637 - val_acc: 0.4744\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0617 - acc: 0.5039 - val_loss: 0.0637 - val_acc: 0.4800\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0617 - acc: 0.5067 - val_loss: 0.0637 - val_acc: 0.4732\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0616 - acc: 0.5048 - val_loss: 0.0637 - val_acc: 0.4813\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0616 - acc: 0.5067 - val_loss: 0.0637 - val_acc: 0.4807\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0616 - acc: 0.5051 - val_loss: 0.0636 - val_acc: 0.4850\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0616 - acc: 0.5070 - val_loss: 0.0637 - val_acc: 0.4813\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0616 - acc: 0.5086 - val_loss: 0.0637 - val_acc: 0.4794\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0616 - acc: 0.5095 - val_loss: 0.0637 - val_acc: 0.4719\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0616 - acc: 0.5039 - val_loss: 0.0637 - val_acc: 0.4794\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0616 - acc: 0.5058 - val_loss: 0.0636 - val_acc: 0.4807\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0616 - acc: 0.5092 - val_loss: 0.0636 - val_acc: 0.4807\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0616 - acc: 0.5092 - val_loss: 0.0637 - val_acc: 0.4776\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0616 - acc: 0.5083 - val_loss: 0.0636 - val_acc: 0.4819\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0616 - acc: 0.5089 - val_loss: 0.0636 - val_acc: 0.4807\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0615 - acc: 0.5089 - val_loss: 0.0636 - val_acc: 0.4807\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0615 - acc: 0.5067 - val_loss: 0.0636 - val_acc: 0.4844\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0615 - acc: 0.5104 - val_loss: 0.0636 - val_acc: 0.4769\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0615 - acc: 0.5089 - val_loss: 0.0636 - val_acc: 0.4825\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0615 - acc: 0.5067 - val_loss: 0.0636 - val_acc: 0.4857\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0615 - acc: 0.5086 - val_loss: 0.0636 - val_acc: 0.4850\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0615 - acc: 0.5098 - val_loss: 0.0636 - val_acc: 0.4857\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0615 - acc: 0.5086 - val_loss: 0.0636 - val_acc: 0.4813\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0615 - acc: 0.5108 - val_loss: 0.0636 - val_acc: 0.4813\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0615 - acc: 0.5089 - val_loss: 0.0636 - val_acc: 0.4788\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0615 - acc: 0.5083 - val_loss: 0.0636 - val_acc: 0.4863\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0615 - acc: 0.5089 - val_loss: 0.0636 - val_acc: 0.4844\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0615 - acc: 0.5080 - val_loss: 0.0636 - val_acc: 0.4857\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0614 - acc: 0.5098 - val_loss: 0.0636 - val_acc: 0.4850\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0614 - acc: 0.5089 - val_loss: 0.0635 - val_acc: 0.4832\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0614 - acc: 0.5095 - val_loss: 0.0635 - val_acc: 0.4844\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0614 - acc: 0.5092 - val_loss: 0.0635 - val_acc: 0.4844\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0614 - acc: 0.5092 - val_loss: 0.0635 - val_acc: 0.4850\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0614 - acc: 0.5111 - val_loss: 0.0636 - val_acc: 0.4844\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0614 - acc: 0.5089 - val_loss: 0.0635 - val_acc: 0.4857\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0614 - acc: 0.5073 - val_loss: 0.0635 - val_acc: 0.4844\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0614 - acc: 0.5098 - val_loss: 0.0635 - val_acc: 0.4832\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0614 - acc: 0.5086 - val_loss: 0.0635 - val_acc: 0.4832\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0614 - acc: 0.5098 - val_loss: 0.0635 - val_acc: 0.4838\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0614 - acc: 0.5095 - val_loss: 0.0635 - val_acc: 0.4832\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0614 - acc: 0.5076 - val_loss: 0.0635 - val_acc: 0.4844\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0613 - acc: 0.5114 - val_loss: 0.0635 - val_acc: 0.4832\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0613 - acc: 0.5095 - val_loss: 0.0635 - val_acc: 0.4832\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0613 - acc: 0.5083 - val_loss: 0.0635 - val_acc: 0.4838\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0613 - acc: 0.5083 - val_loss: 0.0635 - val_acc: 0.4825\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0613 - acc: 0.5086 - val_loss: 0.0635 - val_acc: 0.4844\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0613 - acc: 0.5095 - val_loss: 0.0635 - val_acc: 0.4857\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0613 - acc: 0.5092 - val_loss: 0.0635 - val_acc: 0.4857\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0613 - acc: 0.5080 - val_loss: 0.0635 - val_acc: 0.4838\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0613 - acc: 0.5089 - val_loss: 0.0635 - val_acc: 0.4850\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0613 - acc: 0.5092 - val_loss: 0.0635 - val_acc: 0.4844\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0613 - acc: 0.5092 - val_loss: 0.0635 - val_acc: 0.4825\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0613 - acc: 0.5080 - val_loss: 0.0635 - val_acc: 0.4838\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0613 - acc: 0.5098 - val_loss: 0.0634 - val_acc: 0.4819\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0613 - acc: 0.5089 - val_loss: 0.0634 - val_acc: 0.4813\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0612 - acc: 0.5104 - val_loss: 0.0634 - val_acc: 0.4800\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0612 - acc: 0.5101 - val_loss: 0.0634 - val_acc: 0.4800\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0612 - acc: 0.5083 - val_loss: 0.0634 - val_acc: 0.4813\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0612 - acc: 0.5080 - val_loss: 0.0634 - val_acc: 0.4819\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 239us/step - loss: 0.0612 - acc: 0.5086 - val_loss: 0.0634 - val_acc: 0.4844\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0612 - acc: 0.5089 - val_loss: 0.0634 - val_acc: 0.4825\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 238us/step - loss: 0.0612 - acc: 0.5083 - val_loss: 0.0634 - val_acc: 0.4825\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 239us/step - loss: 0.0612 - acc: 0.5083 - val_loss: 0.0634 - val_acc: 0.4807\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0612 - acc: 0.5086 - val_loss: 0.0634 - val_acc: 0.4807\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0612 - acc: 0.5083 - val_loss: 0.0634 - val_acc: 0.4813\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 236us/step - loss: 0.0612 - acc: 0.5073 - val_loss: 0.0634 - val_acc: 0.4813\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 235us/step - loss: 0.0612 - acc: 0.5083 - val_loss: 0.0634 - val_acc: 0.4832\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 234us/step - loss: 0.0612 - acc: 0.5089 - val_loss: 0.0634 - val_acc: 0.4863\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 238us/step - loss: 0.0612 - acc: 0.5073 - val_loss: 0.0634 - val_acc: 0.4825\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0612 - acc: 0.5086 - val_loss: 0.0634 - val_acc: 0.4819\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0611 - acc: 0.5083 - val_loss: 0.0634 - val_acc: 0.4819\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0612 - acc: 0.5095 - val_loss: 0.0634 - val_acc: 0.4825\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 203us/step - loss: 0.0611 - acc: 0.5076 - val_loss: 0.0634 - val_acc: 0.4813\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0611 - acc: 0.5086 - val_loss: 0.0634 - val_acc: 0.4825\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0611 - acc: 0.5092 - val_loss: 0.0634 - val_acc: 0.4832\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0611 - acc: 0.5073 - val_loss: 0.0634 - val_acc: 0.4813\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0611 - acc: 0.5080 - val_loss: 0.0634 - val_acc: 0.4819\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0611 - acc: 0.5080 - val_loss: 0.0634 - val_acc: 0.4819\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0611 - acc: 0.5080 - val_loss: 0.0633 - val_acc: 0.4850\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0611 - acc: 0.5080 - val_loss: 0.0633 - val_acc: 0.4857\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0611 - acc: 0.5083 - val_loss: 0.0633 - val_acc: 0.4863\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0611 - acc: 0.5089 - val_loss: 0.0634 - val_acc: 0.4825\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0611 - acc: 0.5089 - val_loss: 0.0633 - val_acc: 0.4819\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0611 - acc: 0.5092 - val_loss: 0.0633 - val_acc: 0.4819\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0611 - acc: 0.5111 - val_loss: 0.0633 - val_acc: 0.4863\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0611 - acc: 0.5070 - val_loss: 0.0633 - val_acc: 0.4838\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0611 - acc: 0.5073 - val_loss: 0.0633 - val_acc: 0.4819\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0611 - acc: 0.5070 - val_loss: 0.0633 - val_acc: 0.4825\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0610 - acc: 0.5089 - val_loss: 0.0633 - val_acc: 0.4832\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0610 - acc: 0.5073 - val_loss: 0.0633 - val_acc: 0.4850\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0610 - acc: 0.5083 - val_loss: 0.0633 - val_acc: 0.4844\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0610 - acc: 0.5086 - val_loss: 0.0633 - val_acc: 0.4838\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0610 - acc: 0.5098 - val_loss: 0.0633 - val_acc: 0.4825\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0610 - acc: 0.5070 - val_loss: 0.0633 - val_acc: 0.4844\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0610 - acc: 0.5089 - val_loss: 0.0633 - val_acc: 0.4850\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0610 - acc: 0.5083 - val_loss: 0.0633 - val_acc: 0.4850\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0610 - acc: 0.5104 - val_loss: 0.0633 - val_acc: 0.4857\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0610 - acc: 0.5076 - val_loss: 0.0633 - val_acc: 0.4850\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0610 - acc: 0.5098 - val_loss: 0.0633 - val_acc: 0.4850\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0610 - acc: 0.5083 - val_loss: 0.0633 - val_acc: 0.4857\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0610 - acc: 0.5092 - val_loss: 0.0633 - val_acc: 0.4863\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0610 - acc: 0.5092 - val_loss: 0.0633 - val_acc: 0.4857\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0610 - acc: 0.5098 - val_loss: 0.0633 - val_acc: 0.4844\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0610 - acc: 0.5104 - val_loss: 0.0633 - val_acc: 0.4857\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0610 - acc: 0.5086 - val_loss: 0.0633 - val_acc: 0.4850\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0610 - acc: 0.5086 - val_loss: 0.0632 - val_acc: 0.4844\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0609 - acc: 0.5098 - val_loss: 0.0632 - val_acc: 0.4844\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0609 - acc: 0.5089 - val_loss: 0.0632 - val_acc: 0.4844\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0609 - acc: 0.5108 - val_loss: 0.0632 - val_acc: 0.4850\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0609 - acc: 0.5114 - val_loss: 0.0632 - val_acc: 0.4844\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0609 - acc: 0.5104 - val_loss: 0.0633 - val_acc: 0.4838\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0609 - acc: 0.5101 - val_loss: 0.0632 - val_acc: 0.4838\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0609 - acc: 0.5095 - val_loss: 0.0632 - val_acc: 0.4838\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0609 - acc: 0.5104 - val_loss: 0.0632 - val_acc: 0.4844\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0609 - acc: 0.5101 - val_loss: 0.0632 - val_acc: 0.4844\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0609 - acc: 0.5095 - val_loss: 0.0632 - val_acc: 0.4832\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0609 - acc: 0.5108 - val_loss: 0.0632 - val_acc: 0.4844\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0609 - acc: 0.5104 - val_loss: 0.0632 - val_acc: 0.4844\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0609 - acc: 0.5120 - val_loss: 0.0632 - val_acc: 0.4850\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0609 - acc: 0.5120 - val_loss: 0.0632 - val_acc: 0.4850\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0609 - acc: 0.5117 - val_loss: 0.0632 - val_acc: 0.4838\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0609 - acc: 0.5104 - val_loss: 0.0632 - val_acc: 0.4857\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0608 - acc: 0.5111 - val_loss: 0.0632 - val_acc: 0.4863\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0609 - acc: 0.5108 - val_loss: 0.0632 - val_acc: 0.4857\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0608 - acc: 0.5108 - val_loss: 0.0632 - val_acc: 0.4857\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0608 - acc: 0.5111 - val_loss: 0.0632 - val_acc: 0.4863\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0608 - acc: 0.5108 - val_loss: 0.0632 - val_acc: 0.4863\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0608 - acc: 0.5114 - val_loss: 0.0632 - val_acc: 0.4857\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0608 - acc: 0.5111 - val_loss: 0.0632 - val_acc: 0.4850\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0608 - acc: 0.5111 - val_loss: 0.0632 - val_acc: 0.4838\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0608 - acc: 0.5114 - val_loss: 0.0632 - val_acc: 0.4844\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0608 - acc: 0.5108 - val_loss: 0.0632 - val_acc: 0.4857\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0608 - acc: 0.5120 - val_loss: 0.0632 - val_acc: 0.4844\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0608 - acc: 0.5111 - val_loss: 0.0632 - val_acc: 0.4844\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 224us/step - loss: 0.0608 - acc: 0.5129 - val_loss: 0.0631 - val_acc: 0.4857\n",
      "Train on 3208 samples, validate on 1603 samples\n",
      "Epoch 1/200\n",
      "3208/3208 [==============================] - 1s 234us/step - loss: 0.0622 - acc: 0.4935 - val_loss: 0.0603 - val_acc: 0.5259\n",
      "Epoch 2/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0622 - acc: 0.4925 - val_loss: 0.0603 - val_acc: 0.5240\n",
      "Epoch 3/200\n",
      "3208/3208 [==============================] - 1s 241us/step - loss: 0.0622 - acc: 0.4931 - val_loss: 0.0603 - val_acc: 0.5265\n",
      "Epoch 4/200\n",
      "3208/3208 [==============================] - 1s 241us/step - loss: 0.0622 - acc: 0.4925 - val_loss: 0.0603 - val_acc: 0.5240\n",
      "Epoch 5/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0622 - acc: 0.4913 - val_loss: 0.0603 - val_acc: 0.5259\n",
      "Epoch 6/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0621 - acc: 0.4928 - val_loss: 0.0603 - val_acc: 0.5265\n",
      "Epoch 7/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0621 - acc: 0.4925 - val_loss: 0.0603 - val_acc: 0.5284\n",
      "Epoch 8/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0621 - acc: 0.4922 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 9/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0621 - acc: 0.4941 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 10/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0621 - acc: 0.4931 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 11/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0621 - acc: 0.4935 - val_loss: 0.0603 - val_acc: 0.5271\n",
      "Epoch 12/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0621 - acc: 0.4931 - val_loss: 0.0603 - val_acc: 0.5265\n",
      "Epoch 13/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0621 - acc: 0.4928 - val_loss: 0.0603 - val_acc: 0.5265\n",
      "Epoch 14/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0621 - acc: 0.4928 - val_loss: 0.0603 - val_acc: 0.5284\n",
      "Epoch 15/200\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0621 - acc: 0.4928 - val_loss: 0.0603 - val_acc: 0.5271\n",
      "Epoch 16/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0621 - acc: 0.4935 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 17/200\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0621 - acc: 0.4938 - val_loss: 0.0603 - val_acc: 0.5290\n",
      "Epoch 18/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0621 - acc: 0.4953 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 19/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0620 - acc: 0.4956 - val_loss: 0.0603 - val_acc: 0.5296\n",
      "Epoch 20/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0620 - acc: 0.4959 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 21/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0620 - acc: 0.4950 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 22/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0620 - acc: 0.4944 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 23/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0620 - acc: 0.4938 - val_loss: 0.0603 - val_acc: 0.5284\n",
      "Epoch 24/200\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0620 - acc: 0.4956 - val_loss: 0.0603 - val_acc: 0.5271\n",
      "Epoch 25/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0620 - acc: 0.4944 - val_loss: 0.0603 - val_acc: 0.5271\n",
      "Epoch 26/200\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0620 - acc: 0.4972 - val_loss: 0.0603 - val_acc: 0.5259\n",
      "Epoch 27/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0620 - acc: 0.4956 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 28/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0620 - acc: 0.4963 - val_loss: 0.0603 - val_acc: 0.5265\n",
      "Epoch 29/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0620 - acc: 0.4959 - val_loss: 0.0603 - val_acc: 0.5215\n",
      "Epoch 30/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0620 - acc: 0.4950 - val_loss: 0.0603 - val_acc: 0.5271\n",
      "Epoch 31/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0620 - acc: 0.4953 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 32/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0620 - acc: 0.4959 - val_loss: 0.0603 - val_acc: 0.5271\n",
      "Epoch 33/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0620 - acc: 0.4963 - val_loss: 0.0603 - val_acc: 0.5271\n",
      "Epoch 34/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0620 - acc: 0.4950 - val_loss: 0.0603 - val_acc: 0.5284\n",
      "Epoch 35/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0619 - acc: 0.4966 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 36/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0619 - acc: 0.4956 - val_loss: 0.0603 - val_acc: 0.5215\n",
      "Epoch 37/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0619 - acc: 0.4950 - val_loss: 0.0603 - val_acc: 0.5246\n",
      "Epoch 38/200\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0619 - acc: 0.4941 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 39/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0619 - acc: 0.4959 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 40/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0619 - acc: 0.4950 - val_loss: 0.0603 - val_acc: 0.5296\n",
      "Epoch 41/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0619 - acc: 0.4969 - val_loss: 0.0603 - val_acc: 0.5296\n",
      "Epoch 42/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0619 - acc: 0.4959 - val_loss: 0.0603 - val_acc: 0.5284\n",
      "Epoch 43/200\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0619 - acc: 0.4956 - val_loss: 0.0603 - val_acc: 0.5290\n",
      "Epoch 44/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0619 - acc: 0.4956 - val_loss: 0.0603 - val_acc: 0.5290\n",
      "Epoch 45/200\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0619 - acc: 0.4953 - val_loss: 0.0603 - val_acc: 0.5290\n",
      "Epoch 46/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0619 - acc: 0.4950 - val_loss: 0.0603 - val_acc: 0.5290\n",
      "Epoch 47/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0619 - acc: 0.4953 - val_loss: 0.0603 - val_acc: 0.5290\n",
      "Epoch 48/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0619 - acc: 0.4959 - val_loss: 0.0603 - val_acc: 0.5203\n",
      "Epoch 49/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0619 - acc: 0.4944 - val_loss: 0.0603 - val_acc: 0.5290\n",
      "Epoch 50/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0619 - acc: 0.4959 - val_loss: 0.0603 - val_acc: 0.5290\n",
      "Epoch 51/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0619 - acc: 0.4956 - val_loss: 0.0603 - val_acc: 0.5278\n",
      "Epoch 52/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0619 - acc: 0.4950 - val_loss: 0.0603 - val_acc: 0.5290\n",
      "Epoch 53/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0618 - acc: 0.4947 - val_loss: 0.0603 - val_acc: 0.5290\n",
      "Epoch 54/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0618 - acc: 0.4953 - val_loss: 0.0603 - val_acc: 0.5265\n",
      "Epoch 55/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0618 - acc: 0.4953 - val_loss: 0.0603 - val_acc: 0.5290\n",
      "Epoch 56/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0618 - acc: 0.4963 - val_loss: 0.0603 - val_acc: 0.5259\n",
      "Epoch 57/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0618 - acc: 0.4947 - val_loss: 0.0603 - val_acc: 0.5253\n",
      "Epoch 58/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0618 - acc: 0.4953 - val_loss: 0.0603 - val_acc: 0.5284\n",
      "Epoch 59/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0618 - acc: 0.4966 - val_loss: 0.0602 - val_acc: 0.5290\n",
      "Epoch 60/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0618 - acc: 0.4953 - val_loss: 0.0602 - val_acc: 0.5284\n",
      "Epoch 61/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0618 - acc: 0.4956 - val_loss: 0.0602 - val_acc: 0.5278\n",
      "Epoch 62/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0618 - acc: 0.4953 - val_loss: 0.0602 - val_acc: 0.5278\n",
      "Epoch 63/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0618 - acc: 0.4963 - val_loss: 0.0602 - val_acc: 0.5271\n",
      "Epoch 64/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0618 - acc: 0.4950 - val_loss: 0.0602 - val_acc: 0.5278\n",
      "Epoch 65/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0618 - acc: 0.4959 - val_loss: 0.0602 - val_acc: 0.5278\n",
      "Epoch 66/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0618 - acc: 0.4947 - val_loss: 0.0602 - val_acc: 0.5278\n",
      "Epoch 67/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0618 - acc: 0.4956 - val_loss: 0.0602 - val_acc: 0.5278\n",
      "Epoch 68/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0618 - acc: 0.4953 - val_loss: 0.0602 - val_acc: 0.5271\n",
      "Epoch 69/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0618 - acc: 0.4963 - val_loss: 0.0602 - val_acc: 0.5259\n",
      "Epoch 70/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0618 - acc: 0.4953 - val_loss: 0.0602 - val_acc: 0.5278\n",
      "Epoch 71/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0617 - acc: 0.4959 - val_loss: 0.0602 - val_acc: 0.5271\n",
      "Epoch 72/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0617 - acc: 0.4953 - val_loss: 0.0602 - val_acc: 0.5234\n",
      "Epoch 73/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0617 - acc: 0.4963 - val_loss: 0.0602 - val_acc: 0.5278\n",
      "Epoch 74/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0617 - acc: 0.4956 - val_loss: 0.0602 - val_acc: 0.5265\n",
      "Epoch 75/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0617 - acc: 0.4944 - val_loss: 0.0602 - val_acc: 0.5271\n",
      "Epoch 76/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0617 - acc: 0.4956 - val_loss: 0.0602 - val_acc: 0.5265\n",
      "Epoch 77/200\n",
      "3208/3208 [==============================] - 1s 195us/step - loss: 0.0617 - acc: 0.4966 - val_loss: 0.0602 - val_acc: 0.5265\n",
      "Epoch 78/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0617 - acc: 0.4959 - val_loss: 0.0602 - val_acc: 0.5259\n",
      "Epoch 79/200\n",
      "3208/3208 [==============================] - 1s 195us/step - loss: 0.0617 - acc: 0.4959 - val_loss: 0.0602 - val_acc: 0.5259\n",
      "Epoch 80/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0617 - acc: 0.4956 - val_loss: 0.0602 - val_acc: 0.5265\n",
      "Epoch 81/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0617 - acc: 0.4972 - val_loss: 0.0602 - val_acc: 0.5265\n",
      "Epoch 82/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0617 - acc: 0.4959 - val_loss: 0.0602 - val_acc: 0.5259\n",
      "Epoch 83/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0617 - acc: 0.4963 - val_loss: 0.0602 - val_acc: 0.5265\n",
      "Epoch 84/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0617 - acc: 0.4956 - val_loss: 0.0602 - val_acc: 0.5265\n",
      "Epoch 85/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0617 - acc: 0.4959 - val_loss: 0.0602 - val_acc: 0.5265\n",
      "Epoch 86/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0617 - acc: 0.4963 - val_loss: 0.0602 - val_acc: 0.5265\n",
      "Epoch 87/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0617 - acc: 0.4953 - val_loss: 0.0602 - val_acc: 0.5265\n",
      "Epoch 88/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0617 - acc: 0.4966 - val_loss: 0.0602 - val_acc: 0.5253\n",
      "Epoch 89/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0616 - acc: 0.4953 - val_loss: 0.0601 - val_acc: 0.5271\n",
      "Epoch 90/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0616 - acc: 0.4972 - val_loss: 0.0601 - val_acc: 0.5265\n",
      "Epoch 91/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0616 - acc: 0.4963 - val_loss: 0.0601 - val_acc: 0.5265\n",
      "Epoch 92/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0616 - acc: 0.4959 - val_loss: 0.0601 - val_acc: 0.5265\n",
      "Epoch 93/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0616 - acc: 0.4966 - val_loss: 0.0601 - val_acc: 0.5253\n",
      "Epoch 94/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0616 - acc: 0.4966 - val_loss: 0.0601 - val_acc: 0.5265\n",
      "Epoch 95/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0616 - acc: 0.4963 - val_loss: 0.0601 - val_acc: 0.5259\n",
      "Epoch 96/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0616 - acc: 0.4963 - val_loss: 0.0601 - val_acc: 0.5271\n",
      "Epoch 97/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0616 - acc: 0.4966 - val_loss: 0.0601 - val_acc: 0.5265\n",
      "Epoch 98/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0616 - acc: 0.4963 - val_loss: 0.0601 - val_acc: 0.5265\n",
      "Epoch 99/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0616 - acc: 0.4963 - val_loss: 0.0601 - val_acc: 0.5253\n",
      "Epoch 100/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0616 - acc: 0.4966 - val_loss: 0.0601 - val_acc: 0.5259\n",
      "Epoch 101/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0616 - acc: 0.4953 - val_loss: 0.0601 - val_acc: 0.5259\n",
      "Epoch 102/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0616 - acc: 0.4959 - val_loss: 0.0601 - val_acc: 0.5271\n",
      "Epoch 103/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0616 - acc: 0.4966 - val_loss: 0.0601 - val_acc: 0.5253\n",
      "Epoch 104/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0616 - acc: 0.4956 - val_loss: 0.0601 - val_acc: 0.5253\n",
      "Epoch 105/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0616 - acc: 0.4963 - val_loss: 0.0601 - val_acc: 0.5259\n",
      "Epoch 106/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0616 - acc: 0.4956 - val_loss: 0.0601 - val_acc: 0.5265\n",
      "Epoch 107/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0615 - acc: 0.4975 - val_loss: 0.0601 - val_acc: 0.5259\n",
      "Epoch 108/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0615 - acc: 0.4966 - val_loss: 0.0601 - val_acc: 0.5259\n",
      "Epoch 109/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0615 - acc: 0.4966 - val_loss: 0.0601 - val_acc: 0.5259\n",
      "Epoch 110/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0615 - acc: 0.4969 - val_loss: 0.0601 - val_acc: 0.5253\n",
      "Epoch 111/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0615 - acc: 0.4963 - val_loss: 0.0601 - val_acc: 0.5271\n",
      "Epoch 112/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0615 - acc: 0.4966 - val_loss: 0.0601 - val_acc: 0.5259\n",
      "Epoch 113/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0615 - acc: 0.4950 - val_loss: 0.0600 - val_acc: 0.5284\n",
      "Epoch 114/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0615 - acc: 0.4966 - val_loss: 0.0601 - val_acc: 0.5253\n",
      "Epoch 115/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0615 - acc: 0.4959 - val_loss: 0.0600 - val_acc: 0.5290\n",
      "Epoch 116/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0615 - acc: 0.4966 - val_loss: 0.0600 - val_acc: 0.5290\n",
      "Epoch 117/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0615 - acc: 0.4959 - val_loss: 0.0600 - val_acc: 0.5271\n",
      "Epoch 118/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0615 - acc: 0.4956 - val_loss: 0.0600 - val_acc: 0.5278\n",
      "Epoch 119/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0615 - acc: 0.4972 - val_loss: 0.0600 - val_acc: 0.5240\n",
      "Epoch 120/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0615 - acc: 0.4969 - val_loss: 0.0600 - val_acc: 0.5284\n",
      "Epoch 121/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0615 - acc: 0.4969 - val_loss: 0.0600 - val_acc: 0.5290\n",
      "Epoch 122/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0615 - acc: 0.4953 - val_loss: 0.0600 - val_acc: 0.5259\n",
      "Epoch 123/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0615 - acc: 0.4963 - val_loss: 0.0600 - val_acc: 0.5253\n",
      "Epoch 124/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0614 - acc: 0.4975 - val_loss: 0.0600 - val_acc: 0.5246\n",
      "Epoch 125/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0614 - acc: 0.4969 - val_loss: 0.0600 - val_acc: 0.5265\n",
      "Epoch 126/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0614 - acc: 0.4963 - val_loss: 0.0600 - val_acc: 0.5271\n",
      "Epoch 127/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0614 - acc: 0.4966 - val_loss: 0.0600 - val_acc: 0.5265\n",
      "Epoch 128/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0614 - acc: 0.4963 - val_loss: 0.0600 - val_acc: 0.5246\n",
      "Epoch 129/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0614 - acc: 0.4959 - val_loss: 0.0600 - val_acc: 0.5265\n",
      "Epoch 130/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0614 - acc: 0.4966 - val_loss: 0.0600 - val_acc: 0.5284\n",
      "Epoch 131/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0614 - acc: 0.4963 - val_loss: 0.0600 - val_acc: 0.5278\n",
      "Epoch 132/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0614 - acc: 0.4966 - val_loss: 0.0600 - val_acc: 0.5271\n",
      "Epoch 133/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0614 - acc: 0.4963 - val_loss: 0.0600 - val_acc: 0.5246\n",
      "Epoch 134/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0614 - acc: 0.4969 - val_loss: 0.0599 - val_acc: 0.5271\n",
      "Epoch 135/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0614 - acc: 0.4969 - val_loss: 0.0600 - val_acc: 0.5253\n",
      "Epoch 136/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0614 - acc: 0.4963 - val_loss: 0.0599 - val_acc: 0.5271\n",
      "Epoch 137/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0614 - acc: 0.4969 - val_loss: 0.0599 - val_acc: 0.5265\n",
      "Epoch 138/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0614 - acc: 0.4959 - val_loss: 0.0599 - val_acc: 0.5271\n",
      "Epoch 139/200\n",
      "3208/3208 [==============================] - 1s 192us/step - loss: 0.0614 - acc: 0.4947 - val_loss: 0.0599 - val_acc: 0.5271\n",
      "Epoch 140/200\n",
      "3208/3208 [==============================] - 1s 195us/step - loss: 0.0613 - acc: 0.4953 - val_loss: 0.0599 - val_acc: 0.5265\n",
      "Epoch 141/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0613 - acc: 0.4950 - val_loss: 0.0599 - val_acc: 0.5271\n",
      "Epoch 142/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0613 - acc: 0.4972 - val_loss: 0.0599 - val_acc: 0.5265\n",
      "Epoch 143/200\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0613 - acc: 0.4959 - val_loss: 0.0599 - val_acc: 0.5253\n",
      "Epoch 144/200\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0613 - acc: 0.4959 - val_loss: 0.0599 - val_acc: 0.5259\n",
      "Epoch 145/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0613 - acc: 0.4959 - val_loss: 0.0599 - val_acc: 0.5259\n",
      "Epoch 146/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0613 - acc: 0.4963 - val_loss: 0.0599 - val_acc: 0.5259\n",
      "Epoch 147/200\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0613 - acc: 0.4966 - val_loss: 0.0599 - val_acc: 0.5265\n",
      "Epoch 148/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0613 - acc: 0.4963 - val_loss: 0.0599 - val_acc: 0.5265\n",
      "Epoch 149/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0613 - acc: 0.4959 - val_loss: 0.0599 - val_acc: 0.5271\n",
      "Epoch 150/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0613 - acc: 0.4950 - val_loss: 0.0599 - val_acc: 0.5265\n",
      "Epoch 151/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0613 - acc: 0.4953 - val_loss: 0.0599 - val_acc: 0.5265\n",
      "Epoch 152/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0613 - acc: 0.4959 - val_loss: 0.0599 - val_acc: 0.5265\n",
      "Epoch 153/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0613 - acc: 0.4950 - val_loss: 0.0599 - val_acc: 0.5265\n",
      "Epoch 154/200\n",
      "3208/3208 [==============================] - 1s 194us/step - loss: 0.0613 - acc: 0.4956 - val_loss: 0.0599 - val_acc: 0.5271\n",
      "Epoch 155/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0613 - acc: 0.4947 - val_loss: 0.0599 - val_acc: 0.5265\n",
      "Epoch 156/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0612 - acc: 0.4953 - val_loss: 0.0599 - val_acc: 0.5271\n",
      "Epoch 157/200\n",
      "3208/3208 [==============================] - 1s 195us/step - loss: 0.0612 - acc: 0.4950 - val_loss: 0.0598 - val_acc: 0.5265\n",
      "Epoch 158/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0612 - acc: 0.4953 - val_loss: 0.0598 - val_acc: 0.5259\n",
      "Epoch 159/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0612 - acc: 0.4950 - val_loss: 0.0598 - val_acc: 0.5265\n",
      "Epoch 160/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0612 - acc: 0.4947 - val_loss: 0.0598 - val_acc: 0.5265\n",
      "Epoch 161/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0612 - acc: 0.4947 - val_loss: 0.0598 - val_acc: 0.5271\n",
      "Epoch 162/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0612 - acc: 0.4953 - val_loss: 0.0598 - val_acc: 0.5259\n",
      "Epoch 163/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0612 - acc: 0.4978 - val_loss: 0.0598 - val_acc: 0.5259\n",
      "Epoch 164/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0612 - acc: 0.4947 - val_loss: 0.0598 - val_acc: 0.5259\n",
      "Epoch 165/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0612 - acc: 0.4947 - val_loss: 0.0598 - val_acc: 0.5265\n",
      "Epoch 166/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0612 - acc: 0.4950 - val_loss: 0.0598 - val_acc: 0.5259\n",
      "Epoch 167/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0612 - acc: 0.4966 - val_loss: 0.0598 - val_acc: 0.5240\n",
      "Epoch 168/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0612 - acc: 0.4959 - val_loss: 0.0598 - val_acc: 0.5246\n",
      "Epoch 169/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0612 - acc: 0.4956 - val_loss: 0.0598 - val_acc: 0.5271\n",
      "Epoch 170/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0611 - acc: 0.4966 - val_loss: 0.0598 - val_acc: 0.5265\n",
      "Epoch 171/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0611 - acc: 0.4984 - val_loss: 0.0598 - val_acc: 0.5228\n",
      "Epoch 172/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0611 - acc: 0.4959 - val_loss: 0.0598 - val_acc: 0.5271\n",
      "Epoch 173/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0611 - acc: 0.4994 - val_loss: 0.0597 - val_acc: 0.5246\n",
      "Epoch 174/200\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0611 - acc: 0.4972 - val_loss: 0.0597 - val_acc: 0.5246\n",
      "Epoch 175/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0611 - acc: 0.4950 - val_loss: 0.0597 - val_acc: 0.5265\n",
      "Epoch 176/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0611 - acc: 0.4975 - val_loss: 0.0597 - val_acc: 0.5284\n",
      "Epoch 177/200\n",
      "3208/3208 [==============================] - 1s 194us/step - loss: 0.0611 - acc: 0.5000 - val_loss: 0.0597 - val_acc: 0.5271\n",
      "Epoch 178/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0611 - acc: 0.4991 - val_loss: 0.0597 - val_acc: 0.5284\n",
      "Epoch 179/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0611 - acc: 0.5016 - val_loss: 0.0597 - val_acc: 0.5265\n",
      "Epoch 180/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0611 - acc: 0.4984 - val_loss: 0.0597 - val_acc: 0.5271\n",
      "Epoch 181/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0611 - acc: 0.4981 - val_loss: 0.0597 - val_acc: 0.5284\n",
      "Epoch 182/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0611 - acc: 0.5016 - val_loss: 0.0597 - val_acc: 0.5259\n",
      "Epoch 183/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0611 - acc: 0.5000 - val_loss: 0.0597 - val_acc: 0.5246\n",
      "Epoch 184/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0611 - acc: 0.5025 - val_loss: 0.0597 - val_acc: 0.5259\n",
      "Epoch 185/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0611 - acc: 0.5019 - val_loss: 0.0597 - val_acc: 0.5246\n",
      "Epoch 186/200\n",
      "3208/3208 [==============================] - 1s 195us/step - loss: 0.0610 - acc: 0.5022 - val_loss: 0.0597 - val_acc: 0.5234\n",
      "Epoch 187/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0610 - acc: 0.4963 - val_loss: 0.0597 - val_acc: 0.5253\n",
      "Epoch 188/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0610 - acc: 0.4981 - val_loss: 0.0597 - val_acc: 0.5234\n",
      "Epoch 189/200\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0610 - acc: 0.4991 - val_loss: 0.0597 - val_acc: 0.5234\n",
      "Epoch 190/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0610 - acc: 0.5012 - val_loss: 0.0597 - val_acc: 0.5246\n",
      "Epoch 191/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0610 - acc: 0.4972 - val_loss: 0.0597 - val_acc: 0.5228\n",
      "Epoch 192/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0610 - acc: 0.5009 - val_loss: 0.0596 - val_acc: 0.5234\n",
      "Epoch 193/200\n",
      "3208/3208 [==============================] - 1s 196us/step - loss: 0.0610 - acc: 0.5003 - val_loss: 0.0596 - val_acc: 0.5246\n",
      "Epoch 194/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0610 - acc: 0.4991 - val_loss: 0.0596 - val_acc: 0.5259\n",
      "Epoch 195/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0610 - acc: 0.5025 - val_loss: 0.0596 - val_acc: 0.5234\n",
      "Epoch 196/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0610 - acc: 0.5016 - val_loss: 0.0596 - val_acc: 0.5246\n",
      "Epoch 197/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0610 - acc: 0.5037 - val_loss: 0.0596 - val_acc: 0.5234\n",
      "Epoch 198/200\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0610 - acc: 0.5031 - val_loss: 0.0596 - val_acc: 0.5234\n",
      "Epoch 199/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0610 - acc: 0.5022 - val_loss: 0.0596 - val_acc: 0.5240\n",
      "Epoch 200/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0609 - acc: 0.5044 - val_loss: 0.0596 - val_acc: 0.5265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=20, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=16, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=12, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adagrad = keras.optimizers.Adagrad()\n",
    "nn_model.compile(optimizer=adagrad, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=200, validation_data=(dev_valid_x, dev_valid_y))\n",
    "  \n",
    "pred_y = nn_model.predict(valid_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result_cv = result_cv.append({'detail':'Adagrad', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 20811
    },
    "colab_type": "code",
    "id": "xbPHMHOsXgem",
    "outputId": "2dfaf35e-9e19-430e-aff8-4babe769f93d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_77 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 12)                204       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 2,386\n",
      "Trainable params: 2,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 2s 551us/step - loss: 0.0841 - acc: 0.2922 - val_loss: 0.0828 - val_acc: 0.2999\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0819 - acc: 0.2922 - val_loss: 0.0819 - val_acc: 0.2999\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0813 - acc: 0.2922 - val_loss: 0.0816 - val_acc: 0.2999\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0811 - acc: 0.2922 - val_loss: 0.0816 - val_acc: 0.2999\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0810 - acc: 0.2922 - val_loss: 0.0815 - val_acc: 0.2999\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0810 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0809 - acc: 0.2922 - val_loss: 0.0815 - val_acc: 0.2999\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0809 - acc: 0.2922 - val_loss: 0.0815 - val_acc: 0.2999\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0809 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0809 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 259us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0809 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 259us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 259us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 258us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 259us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0814 - val_acc: 0.2999\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 258us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 259us/step - loss: 0.0807 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0807 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0808 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0807 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0807 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0807 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0807 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 259us/step - loss: 0.0807 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0807 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0806 - acc: 0.2922 - val_loss: 0.0813 - val_acc: 0.2999\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0806 - acc: 0.2922 - val_loss: 0.0812 - val_acc: 0.2999\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0806 - acc: 0.2922 - val_loss: 0.0812 - val_acc: 0.2999\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0806 - acc: 0.2922 - val_loss: 0.0812 - val_acc: 0.2999\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0806 - acc: 0.2922 - val_loss: 0.0811 - val_acc: 0.2999\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0805 - acc: 0.2922 - val_loss: 0.0811 - val_acc: 0.2999\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0805 - acc: 0.2922 - val_loss: 0.0811 - val_acc: 0.2999\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0805 - acc: 0.2922 - val_loss: 0.0811 - val_acc: 0.2999\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0804 - acc: 0.2922 - val_loss: 0.0810 - val_acc: 0.2999\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0804 - acc: 0.2922 - val_loss: 0.0810 - val_acc: 0.2999\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0804 - acc: 0.2922 - val_loss: 0.0809 - val_acc: 0.2999\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0803 - acc: 0.3025 - val_loss: 0.0809 - val_acc: 0.2999\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0802 - acc: 0.2922 - val_loss: 0.0808 - val_acc: 0.2999\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0801 - acc: 0.2928 - val_loss: 0.0807 - val_acc: 0.2999\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0800 - acc: 0.2934 - val_loss: 0.0806 - val_acc: 0.3086\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0799 - acc: 0.3277 - val_loss: 0.0805 - val_acc: 0.3753\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0798 - acc: 0.3801 - val_loss: 0.0802 - val_acc: 0.2999\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0796 - acc: 0.3714 - val_loss: 0.0800 - val_acc: 0.4476\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0793 - acc: 0.4213 - val_loss: 0.0798 - val_acc: 0.4507\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0790 - acc: 0.4378 - val_loss: 0.0794 - val_acc: 0.4507\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0786 - acc: 0.4384 - val_loss: 0.0789 - val_acc: 0.4507\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0780 - acc: 0.4400 - val_loss: 0.0782 - val_acc: 0.4489\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0773 - acc: 0.4394 - val_loss: 0.0774 - val_acc: 0.4539\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0764 - acc: 0.4422 - val_loss: 0.0763 - val_acc: 0.4532\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0753 - acc: 0.4425 - val_loss: 0.0751 - val_acc: 0.4545\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0739 - acc: 0.4437 - val_loss: 0.0736 - val_acc: 0.4539\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0726 - acc: 0.4431 - val_loss: 0.0723 - val_acc: 0.4539\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0714 - acc: 0.4434 - val_loss: 0.0712 - val_acc: 0.4564\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0705 - acc: 0.4428 - val_loss: 0.0702 - val_acc: 0.4539\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0698 - acc: 0.4431 - val_loss: 0.0696 - val_acc: 0.4539\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0693 - acc: 0.4428 - val_loss: 0.0692 - val_acc: 0.4539\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0690 - acc: 0.4425 - val_loss: 0.0689 - val_acc: 0.4539\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0687 - acc: 0.4425 - val_loss: 0.0688 - val_acc: 0.4551\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0686 - acc: 0.4434 - val_loss: 0.0685 - val_acc: 0.4539\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0684 - acc: 0.4428 - val_loss: 0.0684 - val_acc: 0.4545\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0684 - acc: 0.4428 - val_loss: 0.0683 - val_acc: 0.4539\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0683 - acc: 0.4434 - val_loss: 0.0682 - val_acc: 0.4545\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0682 - acc: 0.4437 - val_loss: 0.0682 - val_acc: 0.4545\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 242us/step - loss: 0.0682 - acc: 0.4443 - val_loss: 0.0680 - val_acc: 0.4539\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0682 - acc: 0.4434 - val_loss: 0.0680 - val_acc: 0.4545\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0681 - acc: 0.4437 - val_loss: 0.0680 - val_acc: 0.4539\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0681 - acc: 0.4437 - val_loss: 0.0681 - val_acc: 0.4570\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 242us/step - loss: 0.0681 - acc: 0.4437 - val_loss: 0.0679 - val_acc: 0.4545\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0680 - acc: 0.4440 - val_loss: 0.0681 - val_acc: 0.4570\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0680 - acc: 0.4443 - val_loss: 0.0678 - val_acc: 0.4564\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0680 - acc: 0.4447 - val_loss: 0.0679 - val_acc: 0.4570\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0679 - acc: 0.4443 - val_loss: 0.0678 - val_acc: 0.4570\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0679 - acc: 0.4459 - val_loss: 0.0677 - val_acc: 0.4570\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0679 - acc: 0.4453 - val_loss: 0.0679 - val_acc: 0.4570\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0679 - acc: 0.4453 - val_loss: 0.0677 - val_acc: 0.4564\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0678 - acc: 0.4462 - val_loss: 0.0676 - val_acc: 0.4564\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0678 - acc: 0.4453 - val_loss: 0.0676 - val_acc: 0.4564\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0678 - acc: 0.4450 - val_loss: 0.0677 - val_acc: 0.4564\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0678 - acc: 0.4468 - val_loss: 0.0678 - val_acc: 0.4557\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0677 - acc: 0.4475 - val_loss: 0.0676 - val_acc: 0.4570\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0677 - acc: 0.4465 - val_loss: 0.0677 - val_acc: 0.4564\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0677 - acc: 0.4468 - val_loss: 0.0675 - val_acc: 0.4570\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0677 - acc: 0.4468 - val_loss: 0.0675 - val_acc: 0.4570\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0676 - acc: 0.4471 - val_loss: 0.0675 - val_acc: 0.4564\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 240us/step - loss: 0.0676 - acc: 0.4475 - val_loss: 0.0678 - val_acc: 0.4576\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0676 - acc: 0.4484 - val_loss: 0.0675 - val_acc: 0.4557\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0675 - acc: 0.4487 - val_loss: 0.0674 - val_acc: 0.4564\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0675 - acc: 0.4487 - val_loss: 0.0673 - val_acc: 0.4564\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0675 - acc: 0.4490 - val_loss: 0.0674 - val_acc: 0.4570\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 240us/step - loss: 0.0675 - acc: 0.4487 - val_loss: 0.0673 - val_acc: 0.4570\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 242us/step - loss: 0.0675 - acc: 0.4484 - val_loss: 0.0673 - val_acc: 0.4564\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0674 - acc: 0.4490 - val_loss: 0.0676 - val_acc: 0.4607\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0674 - acc: 0.4503 - val_loss: 0.0672 - val_acc: 0.4576\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0674 - acc: 0.4490 - val_loss: 0.0672 - val_acc: 0.4570\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0674 - acc: 0.4496 - val_loss: 0.0673 - val_acc: 0.4576\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0673 - acc: 0.4509 - val_loss: 0.0671 - val_acc: 0.4570\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0673 - acc: 0.4487 - val_loss: 0.0673 - val_acc: 0.4607\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0673 - acc: 0.4496 - val_loss: 0.0671 - val_acc: 0.4564\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0673 - acc: 0.4512 - val_loss: 0.0671 - val_acc: 0.4576\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0672 - acc: 0.4503 - val_loss: 0.0671 - val_acc: 0.4576\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0672 - acc: 0.4503 - val_loss: 0.0670 - val_acc: 0.4576\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0672 - acc: 0.4512 - val_loss: 0.0675 - val_acc: 0.4613\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0672 - acc: 0.4524 - val_loss: 0.0673 - val_acc: 0.4620\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0672 - acc: 0.4518 - val_loss: 0.0669 - val_acc: 0.4576\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0671 - acc: 0.4524 - val_loss: 0.0669 - val_acc: 0.4570\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0671 - acc: 0.4524 - val_loss: 0.0669 - val_acc: 0.4576\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0671 - acc: 0.4509 - val_loss: 0.0669 - val_acc: 0.4570\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0671 - acc: 0.4540 - val_loss: 0.0670 - val_acc: 0.4595\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0670 - acc: 0.4512 - val_loss: 0.0668 - val_acc: 0.4570\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0671 - acc: 0.4524 - val_loss: 0.0668 - val_acc: 0.4564\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 240us/step - loss: 0.0670 - acc: 0.4524 - val_loss: 0.0669 - val_acc: 0.4582\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0670 - acc: 0.4509 - val_loss: 0.0667 - val_acc: 0.4564\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0669 - acc: 0.4534 - val_loss: 0.0669 - val_acc: 0.4620\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0670 - acc: 0.4534 - val_loss: 0.0667 - val_acc: 0.4570\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0670 - acc: 0.4512 - val_loss: 0.0667 - val_acc: 0.4564\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0669 - acc: 0.4521 - val_loss: 0.0682 - val_acc: 0.4657\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0669 - acc: 0.4531 - val_loss: 0.0667 - val_acc: 0.4582\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0669 - acc: 0.4515 - val_loss: 0.0668 - val_acc: 0.4632\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0668 - acc: 0.4528 - val_loss: 0.0667 - val_acc: 0.4601\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0668 - acc: 0.4524 - val_loss: 0.0666 - val_acc: 0.4582\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0668 - acc: 0.4490 - val_loss: 0.0666 - val_acc: 0.4582\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0667 - acc: 0.4496 - val_loss: 0.0668 - val_acc: 0.4663\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0668 - acc: 0.4521 - val_loss: 0.0668 - val_acc: 0.4657\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0668 - acc: 0.4496 - val_loss: 0.0666 - val_acc: 0.4613\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0667 - acc: 0.4506 - val_loss: 0.0665 - val_acc: 0.4576\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0667 - acc: 0.4487 - val_loss: 0.0673 - val_acc: 0.4645\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0667 - acc: 0.4493 - val_loss: 0.0666 - val_acc: 0.4657\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0667 - acc: 0.4521 - val_loss: 0.0669 - val_acc: 0.4676\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0667 - acc: 0.4503 - val_loss: 0.0664 - val_acc: 0.4582\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0667 - acc: 0.4500 - val_loss: 0.0664 - val_acc: 0.4613\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0667 - acc: 0.4515 - val_loss: 0.0665 - val_acc: 0.4632\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0666 - acc: 0.4490 - val_loss: 0.0668 - val_acc: 0.4670\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0666 - acc: 0.4509 - val_loss: 0.0665 - val_acc: 0.4589\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0666 - acc: 0.4506 - val_loss: 0.0663 - val_acc: 0.4601\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0666 - acc: 0.4512 - val_loss: 0.0668 - val_acc: 0.4645\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0665 - acc: 0.4496 - val_loss: 0.0667 - val_acc: 0.4645\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0665 - acc: 0.4503 - val_loss: 0.0665 - val_acc: 0.4638\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0665 - acc: 0.4509 - val_loss: 0.0664 - val_acc: 0.4645\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0665 - acc: 0.4528 - val_loss: 0.0672 - val_acc: 0.4645\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0665 - acc: 0.4503 - val_loss: 0.0666 - val_acc: 0.4651\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0665 - acc: 0.4515 - val_loss: 0.0663 - val_acc: 0.4626\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0664 - acc: 0.4515 - val_loss: 0.0663 - val_acc: 0.4632\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0665 - acc: 0.4515 - val_loss: 0.0664 - val_acc: 0.4651\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0664 - acc: 0.4512 - val_loss: 0.0662 - val_acc: 0.4645\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0664 - acc: 0.4506 - val_loss: 0.0672 - val_acc: 0.4744\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0664 - acc: 0.4493 - val_loss: 0.0663 - val_acc: 0.4638\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0663 - acc: 0.4540 - val_loss: 0.0661 - val_acc: 0.4645\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0663 - acc: 0.4500 - val_loss: 0.0662 - val_acc: 0.4651\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0663 - acc: 0.4512 - val_loss: 0.0662 - val_acc: 0.4632\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0663 - acc: 0.4500 - val_loss: 0.0662 - val_acc: 0.4638\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 242us/step - loss: 0.0663 - acc: 0.4534 - val_loss: 0.0661 - val_acc: 0.4645\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0663 - acc: 0.4506 - val_loss: 0.0661 - val_acc: 0.4651\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0663 - acc: 0.4512 - val_loss: 0.0662 - val_acc: 0.4638\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0663 - acc: 0.4500 - val_loss: 0.0663 - val_acc: 0.4638\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0663 - acc: 0.4506 - val_loss: 0.0661 - val_acc: 0.4645\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0662 - acc: 0.4524 - val_loss: 0.0661 - val_acc: 0.4632\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0662 - acc: 0.4515 - val_loss: 0.0660 - val_acc: 0.4651\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0662 - acc: 0.4496 - val_loss: 0.0660 - val_acc: 0.4651\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0661 - acc: 0.4553 - val_loss: 0.0660 - val_acc: 0.4651\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0661 - acc: 0.4518 - val_loss: 0.0663 - val_acc: 0.4638\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0661 - acc: 0.4528 - val_loss: 0.0666 - val_acc: 0.4663\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 311us/step - loss: 0.0661 - acc: 0.4537 - val_loss: 0.0663 - val_acc: 0.4645\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 317us/step - loss: 0.0661 - acc: 0.4521 - val_loss: 0.0663 - val_acc: 0.4676\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 315us/step - loss: 0.0661 - acc: 0.4531 - val_loss: 0.0660 - val_acc: 0.4632\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 317us/step - loss: 0.0661 - acc: 0.4528 - val_loss: 0.0672 - val_acc: 0.4751\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 316us/step - loss: 0.0661 - acc: 0.4500 - val_loss: 0.0659 - val_acc: 0.4632\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 317us/step - loss: 0.0660 - acc: 0.4518 - val_loss: 0.0660 - val_acc: 0.4651\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 316us/step - loss: 0.0660 - acc: 0.4518 - val_loss: 0.0658 - val_acc: 0.4651\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 318us/step - loss: 0.0659 - acc: 0.4537 - val_loss: 0.0660 - val_acc: 0.4632\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 321us/step - loss: 0.0660 - acc: 0.4534 - val_loss: 0.0660 - val_acc: 0.4682\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 315us/step - loss: 0.0659 - acc: 0.4565 - val_loss: 0.0660 - val_acc: 0.4676\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 259us/step - loss: 0.0660 - acc: 0.4556 - val_loss: 0.0658 - val_acc: 0.4651\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 261us/step - loss: 0.0659 - acc: 0.4549 - val_loss: 0.0658 - val_acc: 0.4657\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0659 - acc: 0.4568 - val_loss: 0.0661 - val_acc: 0.4670\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0659 - acc: 0.4553 - val_loss: 0.0662 - val_acc: 0.4682\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0658 - acc: 0.4549 - val_loss: 0.0673 - val_acc: 0.4763\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0659 - acc: 0.4553 - val_loss: 0.0657 - val_acc: 0.4645\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0658 - acc: 0.4553 - val_loss: 0.0657 - val_acc: 0.4651\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0658 - acc: 0.4531 - val_loss: 0.0657 - val_acc: 0.4657\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0658 - acc: 0.4540 - val_loss: 0.0656 - val_acc: 0.4651\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0658 - acc: 0.4559 - val_loss: 0.0657 - val_acc: 0.4651\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0658 - acc: 0.4574 - val_loss: 0.0661 - val_acc: 0.4688\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0657 - acc: 0.4574 - val_loss: 0.0658 - val_acc: 0.4676\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0657 - acc: 0.4571 - val_loss: 0.0659 - val_acc: 0.4695\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0657 - acc: 0.4574 - val_loss: 0.0656 - val_acc: 0.4651\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0656 - acc: 0.4556 - val_loss: 0.0667 - val_acc: 0.4769\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0651 - acc: 0.4655 - val_loss: 0.0668 - val_acc: 0.4389\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0651 - acc: 0.4640 - val_loss: 0.0667 - val_acc: 0.4401\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0650 - acc: 0.4674 - val_loss: 0.0666 - val_acc: 0.4445\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0650 - acc: 0.4680 - val_loss: 0.0672 - val_acc: 0.4377\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 261us/step - loss: 0.0650 - acc: 0.4646 - val_loss: 0.0668 - val_acc: 0.4389\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0649 - acc: 0.4668 - val_loss: 0.0666 - val_acc: 0.4476\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0649 - acc: 0.4687 - val_loss: 0.0667 - val_acc: 0.4395\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0649 - acc: 0.4665 - val_loss: 0.0675 - val_acc: 0.4377\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0649 - acc: 0.4668 - val_loss: 0.0671 - val_acc: 0.4551\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0648 - acc: 0.4699 - val_loss: 0.0667 - val_acc: 0.4439\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0648 - acc: 0.4677 - val_loss: 0.0670 - val_acc: 0.4389\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0647 - acc: 0.4699 - val_loss: 0.0666 - val_acc: 0.4433\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0647 - acc: 0.4702 - val_loss: 0.0666 - val_acc: 0.4433\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0647 - acc: 0.4677 - val_loss: 0.0665 - val_acc: 0.4433\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0647 - acc: 0.4684 - val_loss: 0.0669 - val_acc: 0.4377\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0647 - acc: 0.4693 - val_loss: 0.0668 - val_acc: 0.4389\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 259us/step - loss: 0.0646 - acc: 0.4718 - val_loss: 0.0665 - val_acc: 0.4458\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0646 - acc: 0.4727 - val_loss: 0.0669 - val_acc: 0.4370\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0645 - acc: 0.4715 - val_loss: 0.0666 - val_acc: 0.4439\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 258us/step - loss: 0.0645 - acc: 0.4693 - val_loss: 0.0664 - val_acc: 0.4439\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0644 - acc: 0.4712 - val_loss: 0.0676 - val_acc: 0.4726\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0645 - acc: 0.4721 - val_loss: 0.0663 - val_acc: 0.4445\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0644 - acc: 0.4718 - val_loss: 0.0663 - val_acc: 0.4445\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 283us/step - loss: 0.0644 - acc: 0.4705 - val_loss: 0.0663 - val_acc: 0.4420\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 316us/step - loss: 0.0644 - acc: 0.4705 - val_loss: 0.0663 - val_acc: 0.4439\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 319us/step - loss: 0.0643 - acc: 0.4727 - val_loss: 0.0663 - val_acc: 0.4439\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 319us/step - loss: 0.0642 - acc: 0.4746 - val_loss: 0.0665 - val_acc: 0.4476\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 317us/step - loss: 0.0643 - acc: 0.4724 - val_loss: 0.0665 - val_acc: 0.4439\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 301us/step - loss: 0.0643 - acc: 0.4749 - val_loss: 0.0662 - val_acc: 0.4439\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0642 - acc: 0.4727 - val_loss: 0.0663 - val_acc: 0.4451\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0642 - acc: 0.4730 - val_loss: 0.0662 - val_acc: 0.4445\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0641 - acc: 0.4727 - val_loss: 0.0664 - val_acc: 0.4439\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0641 - acc: 0.4761 - val_loss: 0.0662 - val_acc: 0.4464\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0641 - acc: 0.4771 - val_loss: 0.0662 - val_acc: 0.4445\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0641 - acc: 0.4743 - val_loss: 0.0661 - val_acc: 0.4439\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0640 - acc: 0.4749 - val_loss: 0.0663 - val_acc: 0.4433\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0640 - acc: 0.4737 - val_loss: 0.0660 - val_acc: 0.4433\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0639 - acc: 0.4749 - val_loss: 0.0663 - val_acc: 0.4626\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0639 - acc: 0.4765 - val_loss: 0.0660 - val_acc: 0.4451\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0639 - acc: 0.4752 - val_loss: 0.0666 - val_acc: 0.4408\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0638 - acc: 0.4758 - val_loss: 0.0661 - val_acc: 0.4620\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0638 - acc: 0.4721 - val_loss: 0.0659 - val_acc: 0.4426\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0637 - acc: 0.4777 - val_loss: 0.0670 - val_acc: 0.4389\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0637 - acc: 0.4758 - val_loss: 0.0660 - val_acc: 0.4626\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0637 - acc: 0.4774 - val_loss: 0.0658 - val_acc: 0.4464\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0636 - acc: 0.4755 - val_loss: 0.0659 - val_acc: 0.4420\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0636 - acc: 0.4774 - val_loss: 0.0660 - val_acc: 0.4433\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0636 - acc: 0.4761 - val_loss: 0.0657 - val_acc: 0.4445\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0635 - acc: 0.4765 - val_loss: 0.0657 - val_acc: 0.4470\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0635 - acc: 0.4796 - val_loss: 0.0657 - val_acc: 0.4451\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0634 - acc: 0.4765 - val_loss: 0.0667 - val_acc: 0.4701\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0635 - acc: 0.4774 - val_loss: 0.0663 - val_acc: 0.4414\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0634 - acc: 0.4774 - val_loss: 0.0658 - val_acc: 0.4420\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0634 - acc: 0.4780 - val_loss: 0.0656 - val_acc: 0.4570\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 260us/step - loss: 0.0634 - acc: 0.4755 - val_loss: 0.0656 - val_acc: 0.4632\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0633 - acc: 0.4755 - val_loss: 0.0656 - val_acc: 0.4433\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0633 - acc: 0.4746 - val_loss: 0.0655 - val_acc: 0.4707\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0633 - acc: 0.4761 - val_loss: 0.0656 - val_acc: 0.4420\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0631 - acc: 0.4805 - val_loss: 0.0655 - val_acc: 0.4445\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0631 - acc: 0.4830 - val_loss: 0.0659 - val_acc: 0.4433\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0631 - acc: 0.4793 - val_loss: 0.0654 - val_acc: 0.4439\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0630 - acc: 0.4799 - val_loss: 0.0655 - val_acc: 0.4426\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 258us/step - loss: 0.0630 - acc: 0.4780 - val_loss: 0.0657 - val_acc: 0.4426\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0630 - acc: 0.4799 - val_loss: 0.0654 - val_acc: 0.4426\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0629 - acc: 0.4818 - val_loss: 0.0655 - val_acc: 0.4414\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 258us/step - loss: 0.0629 - acc: 0.4821 - val_loss: 0.0652 - val_acc: 0.4582\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0629 - acc: 0.4811 - val_loss: 0.0652 - val_acc: 0.4445\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0628 - acc: 0.4830 - val_loss: 0.0659 - val_acc: 0.4426\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0628 - acc: 0.4836 - val_loss: 0.0655 - val_acc: 0.4426\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0627 - acc: 0.4896 - val_loss: 0.0651 - val_acc: 0.4476\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0626 - acc: 0.4930 - val_loss: 0.0652 - val_acc: 0.4433\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0626 - acc: 0.4877 - val_loss: 0.0651 - val_acc: 0.4483\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0626 - acc: 0.4889 - val_loss: 0.0651 - val_acc: 0.4458\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0626 - acc: 0.4871 - val_loss: 0.0650 - val_acc: 0.4495\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0625 - acc: 0.4917 - val_loss: 0.0649 - val_acc: 0.4557\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0624 - acc: 0.4871 - val_loss: 0.0650 - val_acc: 0.4433\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0624 - acc: 0.4911 - val_loss: 0.0649 - val_acc: 0.4507\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0624 - acc: 0.4886 - val_loss: 0.0647 - val_acc: 0.4719\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0623 - acc: 0.4927 - val_loss: 0.0647 - val_acc: 0.4807\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0623 - acc: 0.4914 - val_loss: 0.0647 - val_acc: 0.4807\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0622 - acc: 0.4967 - val_loss: 0.0647 - val_acc: 0.4825\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0622 - acc: 0.4920 - val_loss: 0.0647 - val_acc: 0.4782\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0621 - acc: 0.4933 - val_loss: 0.0647 - val_acc: 0.4526\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0621 - acc: 0.4942 - val_loss: 0.0645 - val_acc: 0.4782\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0620 - acc: 0.5008 - val_loss: 0.0645 - val_acc: 0.4751\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0620 - acc: 0.4989 - val_loss: 0.0644 - val_acc: 0.4769\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0619 - acc: 0.5011 - val_loss: 0.0646 - val_acc: 0.4532\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0619 - acc: 0.5017 - val_loss: 0.0646 - val_acc: 0.4501\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0619 - acc: 0.4980 - val_loss: 0.0643 - val_acc: 0.4701\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 260us/step - loss: 0.0618 - acc: 0.5030 - val_loss: 0.0645 - val_acc: 0.4476\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0617 - acc: 0.4964 - val_loss: 0.0643 - val_acc: 0.4613\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0617 - acc: 0.5017 - val_loss: 0.0649 - val_acc: 0.4420\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0617 - acc: 0.5011 - val_loss: 0.0648 - val_acc: 0.4451\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0616 - acc: 0.4995 - val_loss: 0.0642 - val_acc: 0.4663\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0615 - acc: 0.5014 - val_loss: 0.0647 - val_acc: 0.4458\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0615 - acc: 0.4964 - val_loss: 0.0640 - val_acc: 0.4776\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0614 - acc: 0.5033 - val_loss: 0.0640 - val_acc: 0.4751\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0614 - acc: 0.5014 - val_loss: 0.0640 - val_acc: 0.4782\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0613 - acc: 0.5048 - val_loss: 0.0646 - val_acc: 0.4439\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 256us/step - loss: 0.0613 - acc: 0.5027 - val_loss: 0.0638 - val_acc: 0.4738\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0612 - acc: 0.5058 - val_loss: 0.0638 - val_acc: 0.4744\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0612 - acc: 0.5083 - val_loss: 0.0641 - val_acc: 0.4800\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 263us/step - loss: 0.0612 - acc: 0.5036 - val_loss: 0.0651 - val_acc: 0.4682\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0611 - acc: 0.5064 - val_loss: 0.0637 - val_acc: 0.4794\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0610 - acc: 0.5136 - val_loss: 0.0642 - val_acc: 0.4956\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0610 - acc: 0.5257 - val_loss: 0.0638 - val_acc: 0.4794\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0609 - acc: 0.5148 - val_loss: 0.0642 - val_acc: 0.4857\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0608 - acc: 0.5276 - val_loss: 0.0637 - val_acc: 0.4944\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0608 - acc: 0.5292 - val_loss: 0.0637 - val_acc: 0.4800\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0607 - acc: 0.5301 - val_loss: 0.0634 - val_acc: 0.4882\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 253us/step - loss: 0.0606 - acc: 0.5341 - val_loss: 0.0634 - val_acc: 0.4894\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0606 - acc: 0.5323 - val_loss: 0.0633 - val_acc: 0.4925\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0605 - acc: 0.5341 - val_loss: 0.0632 - val_acc: 0.5081\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0605 - acc: 0.5429 - val_loss: 0.0639 - val_acc: 0.4807\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0605 - acc: 0.5363 - val_loss: 0.0635 - val_acc: 0.5031\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 241us/step - loss: 0.0604 - acc: 0.5376 - val_loss: 0.0638 - val_acc: 0.4825\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0604 - acc: 0.5416 - val_loss: 0.0631 - val_acc: 0.5075\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0603 - acc: 0.5454 - val_loss: 0.0631 - val_acc: 0.4988\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0602 - acc: 0.5416 - val_loss: 0.0630 - val_acc: 0.5062\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 259us/step - loss: 0.0602 - acc: 0.5419 - val_loss: 0.0632 - val_acc: 0.5062\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 242us/step - loss: 0.0601 - acc: 0.5451 - val_loss: 0.0631 - val_acc: 0.5087\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0602 - acc: 0.5441 - val_loss: 0.0631 - val_acc: 0.5006\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0600 - acc: 0.5466 - val_loss: 0.0628 - val_acc: 0.5106\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0600 - acc: 0.5447 - val_loss: 0.0628 - val_acc: 0.5100\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 241us/step - loss: 0.0599 - acc: 0.5441 - val_loss: 0.0638 - val_acc: 0.4838\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0599 - acc: 0.5432 - val_loss: 0.0627 - val_acc: 0.5087\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0599 - acc: 0.5438 - val_loss: 0.0629 - val_acc: 0.5044\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0598 - acc: 0.5441 - val_loss: 0.0638 - val_acc: 0.4919\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0598 - acc: 0.5438 - val_loss: 0.0636 - val_acc: 0.5025\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0598 - acc: 0.5482 - val_loss: 0.0631 - val_acc: 0.5056\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0596 - acc: 0.5463 - val_loss: 0.0626 - val_acc: 0.5131\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0596 - acc: 0.5466 - val_loss: 0.0626 - val_acc: 0.5062\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0596 - acc: 0.5469 - val_loss: 0.0640 - val_acc: 0.4875\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0596 - acc: 0.5454 - val_loss: 0.0634 - val_acc: 0.5056\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0596 - acc: 0.5482 - val_loss: 0.0629 - val_acc: 0.5112\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0595 - acc: 0.5469 - val_loss: 0.0628 - val_acc: 0.5118\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0595 - acc: 0.5479 - val_loss: 0.0624 - val_acc: 0.5131\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0594 - acc: 0.5482 - val_loss: 0.0627 - val_acc: 0.5125\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0594 - acc: 0.5491 - val_loss: 0.0623 - val_acc: 0.5112\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 254us/step - loss: 0.0595 - acc: 0.5454 - val_loss: 0.0626 - val_acc: 0.5125\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0593 - acc: 0.5472 - val_loss: 0.0624 - val_acc: 0.5125\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0593 - acc: 0.5460 - val_loss: 0.0643 - val_acc: 0.4900\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0593 - acc: 0.5472 - val_loss: 0.0624 - val_acc: 0.5112\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0593 - acc: 0.5491 - val_loss: 0.0637 - val_acc: 0.4888\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0592 - acc: 0.5463 - val_loss: 0.0623 - val_acc: 0.5100\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 257us/step - loss: 0.0592 - acc: 0.5432 - val_loss: 0.0623 - val_acc: 0.5118\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0592 - acc: 0.5488 - val_loss: 0.0645 - val_acc: 0.4900\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0592 - acc: 0.5476 - val_loss: 0.0630 - val_acc: 0.5118\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 255us/step - loss: 0.0592 - acc: 0.5485 - val_loss: 0.0623 - val_acc: 0.5087\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0591 - acc: 0.5460 - val_loss: 0.0621 - val_acc: 0.5131\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0590 - acc: 0.5491 - val_loss: 0.0636 - val_acc: 0.4950\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0590 - acc: 0.5510 - val_loss: 0.0622 - val_acc: 0.5118\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0590 - acc: 0.5476 - val_loss: 0.0623 - val_acc: 0.5112\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0589 - acc: 0.5469 - val_loss: 0.0624 - val_acc: 0.5075\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0591 - acc: 0.5479 - val_loss: 0.0633 - val_acc: 0.5106\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0590 - acc: 0.5491 - val_loss: 0.0621 - val_acc: 0.5137\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0589 - acc: 0.5494 - val_loss: 0.0634 - val_acc: 0.5012\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0589 - acc: 0.5494 - val_loss: 0.0624 - val_acc: 0.5118\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0589 - acc: 0.5494 - val_loss: 0.0664 - val_acc: 0.4451\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0589 - acc: 0.5472 - val_loss: 0.0626 - val_acc: 0.5112\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0588 - acc: 0.5488 - val_loss: 0.0621 - val_acc: 0.5137\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0588 - acc: 0.5482 - val_loss: 0.0621 - val_acc: 0.5137\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0588 - acc: 0.5497 - val_loss: 0.0620 - val_acc: 0.5143\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0588 - acc: 0.5472 - val_loss: 0.0653 - val_acc: 0.4707\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0588 - acc: 0.5476 - val_loss: 0.0626 - val_acc: 0.5143\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0587 - acc: 0.5457 - val_loss: 0.0624 - val_acc: 0.5118\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0589 - acc: 0.5488 - val_loss: 0.0622 - val_acc: 0.5125\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0587 - acc: 0.5497 - val_loss: 0.0620 - val_acc: 0.5106\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0588 - acc: 0.5476 - val_loss: 0.0620 - val_acc: 0.5131\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 241us/step - loss: 0.0587 - acc: 0.5491 - val_loss: 0.0626 - val_acc: 0.5118\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0586 - acc: 0.5504 - val_loss: 0.0629 - val_acc: 0.5081\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0587 - acc: 0.5472 - val_loss: 0.0621 - val_acc: 0.5118\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0587 - acc: 0.5485 - val_loss: 0.0629 - val_acc: 0.5081\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0586 - acc: 0.5457 - val_loss: 0.0626 - val_acc: 0.5100\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0586 - acc: 0.5485 - val_loss: 0.0622 - val_acc: 0.5112\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0586 - acc: 0.5491 - val_loss: 0.0619 - val_acc: 0.5125\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0586 - acc: 0.5488 - val_loss: 0.0619 - val_acc: 0.5125\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0586 - acc: 0.5485 - val_loss: 0.0624 - val_acc: 0.5118\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0586 - acc: 0.5463 - val_loss: 0.0620 - val_acc: 0.5137\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 250us/step - loss: 0.0586 - acc: 0.5491 - val_loss: 0.0620 - val_acc: 0.5137\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0586 - acc: 0.5472 - val_loss: 0.0620 - val_acc: 0.5131\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0586 - acc: 0.5479 - val_loss: 0.0620 - val_acc: 0.5143\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 242us/step - loss: 0.0585 - acc: 0.5457 - val_loss: 0.0618 - val_acc: 0.5118\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0586 - acc: 0.5476 - val_loss: 0.0619 - val_acc: 0.5143\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 252us/step - loss: 0.0585 - acc: 0.5447 - val_loss: 0.0622 - val_acc: 0.5106\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0584 - acc: 0.5457 - val_loss: 0.0618 - val_acc: 0.5118\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0585 - acc: 0.5466 - val_loss: 0.0619 - val_acc: 0.5125\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 248us/step - loss: 0.0585 - acc: 0.5472 - val_loss: 0.0618 - val_acc: 0.5131\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 251us/step - loss: 0.0585 - acc: 0.5463 - val_loss: 0.0618 - val_acc: 0.5131\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 249us/step - loss: 0.0584 - acc: 0.5479 - val_loss: 0.0618 - val_acc: 0.5125\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 242us/step - loss: 0.0584 - acc: 0.5460 - val_loss: 0.0625 - val_acc: 0.5081\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 243us/step - loss: 0.0584 - acc: 0.5482 - val_loss: 0.0619 - val_acc: 0.5131\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0585 - acc: 0.5494 - val_loss: 0.0618 - val_acc: 0.5131\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0584 - acc: 0.5466 - val_loss: 0.0619 - val_acc: 0.5143\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0583 - acc: 0.5469 - val_loss: 0.0619 - val_acc: 0.5106\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 246us/step - loss: 0.0583 - acc: 0.5472 - val_loss: 0.0633 - val_acc: 0.5012\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 247us/step - loss: 0.0583 - acc: 0.5463 - val_loss: 0.0619 - val_acc: 0.5118\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 244us/step - loss: 0.0584 - acc: 0.5482 - val_loss: 0.0619 - val_acc: 0.5118\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 245us/step - loss: 0.0584 - acc: 0.5441 - val_loss: 0.0618 - val_acc: 0.5106\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 241us/step - loss: 0.0583 - acc: 0.5463 - val_loss: 0.0623 - val_acc: 0.5118\n",
      "Train on 3208 samples, validate on 1603 samples\n",
      "Epoch 1/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0604 - acc: 0.5237 - val_loss: 0.0576 - val_acc: 0.5671\n",
      "Epoch 2/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0604 - acc: 0.5246 - val_loss: 0.0585 - val_acc: 0.5652\n",
      "Epoch 3/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0603 - acc: 0.5246 - val_loss: 0.0577 - val_acc: 0.5683\n",
      "Epoch 4/200\n",
      "3208/3208 [==============================] - 1s 255us/step - loss: 0.0603 - acc: 0.5246 - val_loss: 0.0577 - val_acc: 0.5677\n",
      "Epoch 5/200\n",
      "3208/3208 [==============================] - 1s 258us/step - loss: 0.0602 - acc: 0.5237 - val_loss: 0.0576 - val_acc: 0.5696\n",
      "Epoch 6/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0602 - acc: 0.5252 - val_loss: 0.0585 - val_acc: 0.5602\n",
      "Epoch 7/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0603 - acc: 0.5274 - val_loss: 0.0577 - val_acc: 0.5677\n",
      "Epoch 8/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0602 - acc: 0.5259 - val_loss: 0.0600 - val_acc: 0.5502\n",
      "Epoch 9/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0602 - acc: 0.5249 - val_loss: 0.0577 - val_acc: 0.5671\n",
      "Epoch 10/200\n",
      "3208/3208 [==============================] - 1s 256us/step - loss: 0.0601 - acc: 0.5252 - val_loss: 0.0577 - val_acc: 0.5639\n",
      "Epoch 11/200\n",
      "3208/3208 [==============================] - 1s 254us/step - loss: 0.0602 - acc: 0.5274 - val_loss: 0.0586 - val_acc: 0.5602\n",
      "Epoch 12/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0601 - acc: 0.5284 - val_loss: 0.0580 - val_acc: 0.5658\n",
      "Epoch 13/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0601 - acc: 0.5271 - val_loss: 0.0581 - val_acc: 0.5652\n",
      "Epoch 14/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0600 - acc: 0.5330 - val_loss: 0.0578 - val_acc: 0.5639\n",
      "Epoch 15/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0601 - acc: 0.5265 - val_loss: 0.0578 - val_acc: 0.5652\n",
      "Epoch 16/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0600 - acc: 0.5271 - val_loss: 0.0582 - val_acc: 0.5652\n",
      "Epoch 17/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0600 - acc: 0.5293 - val_loss: 0.0580 - val_acc: 0.5664\n",
      "Epoch 18/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0600 - acc: 0.5281 - val_loss: 0.0579 - val_acc: 0.5633\n",
      "Epoch 19/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0601 - acc: 0.5293 - val_loss: 0.0578 - val_acc: 0.5646\n",
      "Epoch 20/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0599 - acc: 0.5271 - val_loss: 0.0578 - val_acc: 0.5639\n",
      "Epoch 21/200\n",
      "3208/3208 [==============================] - 1s 254us/step - loss: 0.0600 - acc: 0.5318 - val_loss: 0.0583 - val_acc: 0.5646\n",
      "Epoch 22/200\n",
      "3208/3208 [==============================] - 1s 255us/step - loss: 0.0599 - acc: 0.5312 - val_loss: 0.0578 - val_acc: 0.5627\n",
      "Epoch 23/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0600 - acc: 0.5315 - val_loss: 0.0583 - val_acc: 0.5633\n",
      "Epoch 24/200\n",
      "3208/3208 [==============================] - 1s 256us/step - loss: 0.0600 - acc: 0.5287 - val_loss: 0.0579 - val_acc: 0.5633\n",
      "Epoch 25/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0599 - acc: 0.5340 - val_loss: 0.0583 - val_acc: 0.5646\n",
      "Epoch 26/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0599 - acc: 0.5296 - val_loss: 0.0580 - val_acc: 0.5633\n",
      "Epoch 27/200\n",
      "3208/3208 [==============================] - 1s 255us/step - loss: 0.0600 - acc: 0.5327 - val_loss: 0.0578 - val_acc: 0.5633\n",
      "Epoch 28/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0599 - acc: 0.5327 - val_loss: 0.0582 - val_acc: 0.5664\n",
      "Epoch 29/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0598 - acc: 0.5327 - val_loss: 0.0582 - val_acc: 0.5677\n",
      "Epoch 30/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0598 - acc: 0.5334 - val_loss: 0.0578 - val_acc: 0.5639\n",
      "Epoch 31/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0598 - acc: 0.5330 - val_loss: 0.0578 - val_acc: 0.5627\n",
      "Epoch 32/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0598 - acc: 0.5340 - val_loss: 0.0590 - val_acc: 0.5371\n",
      "Epoch 33/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0597 - acc: 0.5349 - val_loss: 0.0579 - val_acc: 0.5639\n",
      "Epoch 34/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0598 - acc: 0.5337 - val_loss: 0.0588 - val_acc: 0.5577\n",
      "Epoch 35/200\n",
      "3208/3208 [==============================] - 1s 258us/step - loss: 0.0598 - acc: 0.5352 - val_loss: 0.0579 - val_acc: 0.5639\n",
      "Epoch 36/200\n",
      "3208/3208 [==============================] - 1s 256us/step - loss: 0.0598 - acc: 0.5343 - val_loss: 0.0582 - val_acc: 0.5565\n",
      "Epoch 37/200\n",
      "3208/3208 [==============================] - 1s 257us/step - loss: 0.0598 - acc: 0.5362 - val_loss: 0.0590 - val_acc: 0.5459\n",
      "Epoch 38/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0597 - acc: 0.5334 - val_loss: 0.0584 - val_acc: 0.5664\n",
      "Epoch 39/200\n",
      "3208/3208 [==============================] - 1s 256us/step - loss: 0.0597 - acc: 0.5349 - val_loss: 0.0585 - val_acc: 0.5552\n",
      "Epoch 40/200\n",
      "3208/3208 [==============================] - 1s 255us/step - loss: 0.0599 - acc: 0.5352 - val_loss: 0.0579 - val_acc: 0.5621\n",
      "Epoch 41/200\n",
      "3208/3208 [==============================] - 1s 262us/step - loss: 0.0598 - acc: 0.5337 - val_loss: 0.0579 - val_acc: 0.5633\n",
      "Epoch 42/200\n",
      "3208/3208 [==============================] - 1s 256us/step - loss: 0.0597 - acc: 0.5371 - val_loss: 0.0586 - val_acc: 0.5546\n",
      "Epoch 43/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0597 - acc: 0.5371 - val_loss: 0.0587 - val_acc: 0.5471\n",
      "Epoch 44/200\n",
      "3208/3208 [==============================] - 1s 255us/step - loss: 0.0597 - acc: 0.5358 - val_loss: 0.0583 - val_acc: 0.5646\n",
      "Epoch 45/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0597 - acc: 0.5387 - val_loss: 0.0578 - val_acc: 0.5627\n",
      "Epoch 46/200\n",
      "3208/3208 [==============================] - 1s 259us/step - loss: 0.0597 - acc: 0.5349 - val_loss: 0.0579 - val_acc: 0.5646\n",
      "Epoch 47/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0598 - acc: 0.5358 - val_loss: 0.0583 - val_acc: 0.5602\n",
      "Epoch 48/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0597 - acc: 0.5362 - val_loss: 0.0579 - val_acc: 0.5633\n",
      "Epoch 49/200\n",
      "3208/3208 [==============================] - 1s 257us/step - loss: 0.0596 - acc: 0.5346 - val_loss: 0.0578 - val_acc: 0.5633\n",
      "Epoch 50/200\n",
      "3208/3208 [==============================] - 1s 254us/step - loss: 0.0596 - acc: 0.5377 - val_loss: 0.0579 - val_acc: 0.5639\n",
      "Epoch 51/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0596 - acc: 0.5343 - val_loss: 0.0591 - val_acc: 0.5627\n",
      "Epoch 52/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0597 - acc: 0.5362 - val_loss: 0.0595 - val_acc: 0.5515\n",
      "Epoch 53/200\n",
      "3208/3208 [==============================] - 1s 255us/step - loss: 0.0596 - acc: 0.5343 - val_loss: 0.0578 - val_acc: 0.5621\n",
      "Epoch 54/200\n",
      "3208/3208 [==============================] - 1s 254us/step - loss: 0.0597 - acc: 0.5343 - val_loss: 0.0578 - val_acc: 0.5621\n",
      "Epoch 55/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0597 - acc: 0.5362 - val_loss: 0.0580 - val_acc: 0.5633\n",
      "Epoch 56/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0596 - acc: 0.5355 - val_loss: 0.0579 - val_acc: 0.5627\n",
      "Epoch 57/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0596 - acc: 0.5365 - val_loss: 0.0584 - val_acc: 0.5633\n",
      "Epoch 58/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0595 - acc: 0.5358 - val_loss: 0.0580 - val_acc: 0.5639\n",
      "Epoch 59/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0595 - acc: 0.5349 - val_loss: 0.0580 - val_acc: 0.5633\n",
      "Epoch 60/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0595 - acc: 0.5337 - val_loss: 0.0579 - val_acc: 0.5639\n",
      "Epoch 61/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0595 - acc: 0.5362 - val_loss: 0.0605 - val_acc: 0.5515\n",
      "Epoch 62/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0597 - acc: 0.5358 - val_loss: 0.0581 - val_acc: 0.5627\n",
      "Epoch 63/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0595 - acc: 0.5377 - val_loss: 0.0588 - val_acc: 0.5533\n",
      "Epoch 64/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0595 - acc: 0.5349 - val_loss: 0.0582 - val_acc: 0.5633\n",
      "Epoch 65/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0595 - acc: 0.5355 - val_loss: 0.0595 - val_acc: 0.5508\n",
      "Epoch 66/200\n",
      "3208/3208 [==============================] - 1s 243us/step - loss: 0.0595 - acc: 0.5362 - val_loss: 0.0578 - val_acc: 0.5633\n",
      "Epoch 67/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0596 - acc: 0.5374 - val_loss: 0.0579 - val_acc: 0.5627\n",
      "Epoch 68/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0595 - acc: 0.5349 - val_loss: 0.0581 - val_acc: 0.5621\n",
      "Epoch 69/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0595 - acc: 0.5343 - val_loss: 0.0581 - val_acc: 0.5633\n",
      "Epoch 70/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0595 - acc: 0.5352 - val_loss: 0.0578 - val_acc: 0.5639\n",
      "Epoch 71/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0595 - acc: 0.5374 - val_loss: 0.0584 - val_acc: 0.5621\n",
      "Epoch 72/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0594 - acc: 0.5371 - val_loss: 0.0579 - val_acc: 0.5633\n",
      "Epoch 73/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0594 - acc: 0.5371 - val_loss: 0.0578 - val_acc: 0.5627\n",
      "Epoch 74/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0595 - acc: 0.5365 - val_loss: 0.0579 - val_acc: 0.5633\n",
      "Epoch 75/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0594 - acc: 0.5355 - val_loss: 0.0579 - val_acc: 0.5627\n",
      "Epoch 76/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0595 - acc: 0.5358 - val_loss: 0.0578 - val_acc: 0.5621\n",
      "Epoch 77/200\n",
      "3208/3208 [==============================] - 1s 254us/step - loss: 0.0595 - acc: 0.5362 - val_loss: 0.0580 - val_acc: 0.5614\n",
      "Epoch 78/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0594 - acc: 0.5380 - val_loss: 0.0579 - val_acc: 0.5621\n",
      "Epoch 79/200\n",
      "3208/3208 [==============================] - 1s 254us/step - loss: 0.0595 - acc: 0.5380 - val_loss: 0.0579 - val_acc: 0.5633\n",
      "Epoch 80/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0595 - acc: 0.5380 - val_loss: 0.0579 - val_acc: 0.5621\n",
      "Epoch 81/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0595 - acc: 0.5368 - val_loss: 0.0585 - val_acc: 0.5571\n",
      "Epoch 82/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0594 - acc: 0.5377 - val_loss: 0.0579 - val_acc: 0.5608\n",
      "Epoch 83/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0594 - acc: 0.5380 - val_loss: 0.0584 - val_acc: 0.5558\n",
      "Epoch 84/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0594 - acc: 0.5387 - val_loss: 0.0578 - val_acc: 0.5621\n",
      "Epoch 85/200\n",
      "3208/3208 [==============================] - 1s 254us/step - loss: 0.0594 - acc: 0.5365 - val_loss: 0.0578 - val_acc: 0.5621\n",
      "Epoch 86/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0594 - acc: 0.5371 - val_loss: 0.0587 - val_acc: 0.5596\n",
      "Epoch 87/200\n",
      "3208/3208 [==============================] - 1s 253us/step - loss: 0.0594 - acc: 0.5377 - val_loss: 0.0579 - val_acc: 0.5608\n",
      "Epoch 88/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0594 - acc: 0.5371 - val_loss: 0.0582 - val_acc: 0.5614\n",
      "Epoch 89/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0594 - acc: 0.5368 - val_loss: 0.0579 - val_acc: 0.5621\n",
      "Epoch 90/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0593 - acc: 0.5380 - val_loss: 0.0578 - val_acc: 0.5608\n",
      "Epoch 91/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0594 - acc: 0.5365 - val_loss: 0.0578 - val_acc: 0.5608\n",
      "Epoch 92/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0593 - acc: 0.5371 - val_loss: 0.0579 - val_acc: 0.5621\n",
      "Epoch 93/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0593 - acc: 0.5380 - val_loss: 0.0587 - val_acc: 0.5608\n",
      "Epoch 94/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0593 - acc: 0.5371 - val_loss: 0.0585 - val_acc: 0.5602\n",
      "Epoch 95/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0593 - acc: 0.5380 - val_loss: 0.0583 - val_acc: 0.5596\n",
      "Epoch 96/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0594 - acc: 0.5374 - val_loss: 0.0579 - val_acc: 0.5621\n",
      "Epoch 97/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0594 - acc: 0.5365 - val_loss: 0.0580 - val_acc: 0.5602\n",
      "Epoch 98/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0593 - acc: 0.5383 - val_loss: 0.0579 - val_acc: 0.5608\n",
      "Epoch 99/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0593 - acc: 0.5380 - val_loss: 0.0584 - val_acc: 0.5602\n",
      "Epoch 100/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0593 - acc: 0.5393 - val_loss: 0.0581 - val_acc: 0.5608\n",
      "Epoch 101/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0593 - acc: 0.5393 - val_loss: 0.0593 - val_acc: 0.5602\n",
      "Epoch 102/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0593 - acc: 0.5383 - val_loss: 0.0578 - val_acc: 0.5608\n",
      "Epoch 103/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0593 - acc: 0.5380 - val_loss: 0.0582 - val_acc: 0.5583\n",
      "Epoch 104/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0593 - acc: 0.5349 - val_loss: 0.0579 - val_acc: 0.5602\n",
      "Epoch 105/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0593 - acc: 0.5365 - val_loss: 0.0579 - val_acc: 0.5590\n",
      "Epoch 106/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0593 - acc: 0.5365 - val_loss: 0.0584 - val_acc: 0.5552\n",
      "Epoch 107/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0593 - acc: 0.5371 - val_loss: 0.0579 - val_acc: 0.5608\n",
      "Epoch 108/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0593 - acc: 0.5377 - val_loss: 0.0593 - val_acc: 0.5278\n",
      "Epoch 109/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0593 - acc: 0.5358 - val_loss: 0.0579 - val_acc: 0.5602\n",
      "Epoch 110/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0593 - acc: 0.5362 - val_loss: 0.0583 - val_acc: 0.5558\n",
      "Epoch 111/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0592 - acc: 0.5402 - val_loss: 0.0581 - val_acc: 0.5583\n",
      "Epoch 112/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0593 - acc: 0.5374 - val_loss: 0.0579 - val_acc: 0.5602\n",
      "Epoch 113/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0593 - acc: 0.5368 - val_loss: 0.0581 - val_acc: 0.5583\n",
      "Epoch 114/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0592 - acc: 0.5387 - val_loss: 0.0607 - val_acc: 0.5240\n",
      "Epoch 115/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0592 - acc: 0.5355 - val_loss: 0.0579 - val_acc: 0.5590\n",
      "Epoch 116/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0592 - acc: 0.5368 - val_loss: 0.0583 - val_acc: 0.5571\n",
      "Epoch 117/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0592 - acc: 0.5371 - val_loss: 0.0579 - val_acc: 0.5590\n",
      "Epoch 118/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0592 - acc: 0.5371 - val_loss: 0.0585 - val_acc: 0.5546\n",
      "Epoch 119/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0592 - acc: 0.5374 - val_loss: 0.0591 - val_acc: 0.5577\n",
      "Epoch 120/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0592 - acc: 0.5377 - val_loss: 0.0580 - val_acc: 0.5565\n",
      "Epoch 121/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0592 - acc: 0.5362 - val_loss: 0.0579 - val_acc: 0.5602\n",
      "Epoch 122/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0591 - acc: 0.5383 - val_loss: 0.0588 - val_acc: 0.5527\n",
      "Epoch 123/200\n",
      "3208/3208 [==============================] - 1s 240us/step - loss: 0.0592 - acc: 0.5371 - val_loss: 0.0581 - val_acc: 0.5596\n",
      "Epoch 124/200\n",
      "3208/3208 [==============================] - 1s 243us/step - loss: 0.0592 - acc: 0.5377 - val_loss: 0.0586 - val_acc: 0.5558\n",
      "Epoch 125/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0592 - acc: 0.5380 - val_loss: 0.0578 - val_acc: 0.5602\n",
      "Epoch 126/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0592 - acc: 0.5383 - val_loss: 0.0597 - val_acc: 0.5427\n",
      "Epoch 127/200\n",
      "3208/3208 [==============================] - 1s 241us/step - loss: 0.0591 - acc: 0.5387 - val_loss: 0.0585 - val_acc: 0.5596\n",
      "Epoch 128/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0592 - acc: 0.5390 - val_loss: 0.0578 - val_acc: 0.5602\n",
      "Epoch 129/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0592 - acc: 0.5365 - val_loss: 0.0578 - val_acc: 0.5583\n",
      "Epoch 130/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0592 - acc: 0.5368 - val_loss: 0.0584 - val_acc: 0.5540\n",
      "Epoch 131/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0592 - acc: 0.5362 - val_loss: 0.0580 - val_acc: 0.5583\n",
      "Epoch 132/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0592 - acc: 0.5374 - val_loss: 0.0579 - val_acc: 0.5577\n",
      "Epoch 133/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0592 - acc: 0.5358 - val_loss: 0.0578 - val_acc: 0.5590\n",
      "Epoch 134/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0592 - acc: 0.5368 - val_loss: 0.0579 - val_acc: 0.5590\n",
      "Epoch 135/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0592 - acc: 0.5368 - val_loss: 0.0580 - val_acc: 0.5583\n",
      "Epoch 136/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0593 - acc: 0.5405 - val_loss: 0.0579 - val_acc: 0.5590\n",
      "Epoch 137/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0591 - acc: 0.5380 - val_loss: 0.0579 - val_acc: 0.5583\n",
      "Epoch 138/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0591 - acc: 0.5402 - val_loss: 0.0585 - val_acc: 0.5577\n",
      "Epoch 139/200\n",
      "3208/3208 [==============================] - 1s 243us/step - loss: 0.0592 - acc: 0.5374 - val_loss: 0.0578 - val_acc: 0.5571\n",
      "Epoch 140/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0592 - acc: 0.5390 - val_loss: 0.0583 - val_acc: 0.5558\n",
      "Epoch 141/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0592 - acc: 0.5383 - val_loss: 0.0580 - val_acc: 0.5577\n",
      "Epoch 142/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0590 - acc: 0.5402 - val_loss: 0.0580 - val_acc: 0.5577\n",
      "Epoch 143/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0591 - acc: 0.5374 - val_loss: 0.0582 - val_acc: 0.5571\n",
      "Epoch 144/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0591 - acc: 0.5380 - val_loss: 0.0586 - val_acc: 0.5502\n",
      "Epoch 145/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0591 - acc: 0.5377 - val_loss: 0.0580 - val_acc: 0.5596\n",
      "Epoch 146/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0591 - acc: 0.5380 - val_loss: 0.0578 - val_acc: 0.5583\n",
      "Epoch 147/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0591 - acc: 0.5383 - val_loss: 0.0582 - val_acc: 0.5565\n",
      "Epoch 148/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0591 - acc: 0.5390 - val_loss: 0.0578 - val_acc: 0.5577\n",
      "Epoch 149/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0592 - acc: 0.5399 - val_loss: 0.0583 - val_acc: 0.5533\n",
      "Epoch 150/200\n",
      "3208/3208 [==============================] - 1s 243us/step - loss: 0.0592 - acc: 0.5380 - val_loss: 0.0578 - val_acc: 0.5583\n",
      "Epoch 151/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0592 - acc: 0.5393 - val_loss: 0.0596 - val_acc: 0.5471\n",
      "Epoch 152/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0591 - acc: 0.5377 - val_loss: 0.0582 - val_acc: 0.5571\n",
      "Epoch 153/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0592 - acc: 0.5402 - val_loss: 0.0580 - val_acc: 0.5546\n",
      "Epoch 154/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0591 - acc: 0.5387 - val_loss: 0.0579 - val_acc: 0.5552\n",
      "Epoch 155/200\n",
      "3208/3208 [==============================] - 1s 270us/step - loss: 0.0591 - acc: 0.5411 - val_loss: 0.0578 - val_acc: 0.5565\n",
      "Epoch 156/200\n",
      "3208/3208 [==============================] - 1s 256us/step - loss: 0.0591 - acc: 0.5371 - val_loss: 0.0586 - val_acc: 0.5496\n",
      "Epoch 157/200\n",
      "3208/3208 [==============================] - 1s 263us/step - loss: 0.0591 - acc: 0.5377 - val_loss: 0.0579 - val_acc: 0.5577\n",
      "Epoch 158/200\n",
      "3208/3208 [==============================] - 1s 316us/step - loss: 0.0591 - acc: 0.5408 - val_loss: 0.0580 - val_acc: 0.5571\n",
      "Epoch 159/200\n",
      "3208/3208 [==============================] - 1s 317us/step - loss: 0.0590 - acc: 0.5387 - val_loss: 0.0589 - val_acc: 0.5471\n",
      "Epoch 160/200\n",
      "3208/3208 [==============================] - 1s 317us/step - loss: 0.0591 - acc: 0.5383 - val_loss: 0.0587 - val_acc: 0.5577\n",
      "Epoch 161/200\n",
      "3208/3208 [==============================] - 1s 315us/step - loss: 0.0591 - acc: 0.5415 - val_loss: 0.0578 - val_acc: 0.5583\n",
      "Epoch 162/200\n",
      "3208/3208 [==============================] - 1s 317us/step - loss: 0.0591 - acc: 0.5387 - val_loss: 0.0578 - val_acc: 0.5565\n",
      "Epoch 163/200\n",
      "3208/3208 [==============================] - 1s 315us/step - loss: 0.0592 - acc: 0.5371 - val_loss: 0.0578 - val_acc: 0.5565\n",
      "Epoch 164/200\n",
      "3208/3208 [==============================] - 1s 314us/step - loss: 0.0591 - acc: 0.5365 - val_loss: 0.0578 - val_acc: 0.5577\n",
      "Epoch 165/200\n",
      "3208/3208 [==============================] - 1s 318us/step - loss: 0.0590 - acc: 0.5402 - val_loss: 0.0579 - val_acc: 0.5583\n",
      "Epoch 166/200\n",
      "3208/3208 [==============================] - 1s 317us/step - loss: 0.0591 - acc: 0.5399 - val_loss: 0.0578 - val_acc: 0.5577\n",
      "Epoch 167/200\n",
      "3208/3208 [==============================] - 1s 299us/step - loss: 0.0591 - acc: 0.5408 - val_loss: 0.0578 - val_acc: 0.5565\n",
      "Epoch 168/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0590 - acc: 0.5402 - val_loss: 0.0580 - val_acc: 0.5565\n",
      "Epoch 169/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0591 - acc: 0.5387 - val_loss: 0.0578 - val_acc: 0.5558\n",
      "Epoch 170/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0590 - acc: 0.5371 - val_loss: 0.0582 - val_acc: 0.5540\n",
      "Epoch 171/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0591 - acc: 0.5390 - val_loss: 0.0605 - val_acc: 0.5421\n",
      "Epoch 172/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0590 - acc: 0.5405 - val_loss: 0.0578 - val_acc: 0.5577\n",
      "Epoch 173/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0590 - acc: 0.5408 - val_loss: 0.0578 - val_acc: 0.5577\n",
      "Epoch 174/200\n",
      "3208/3208 [==============================] - 1s 243us/step - loss: 0.0591 - acc: 0.5383 - val_loss: 0.0583 - val_acc: 0.5533\n",
      "Epoch 175/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0590 - acc: 0.5405 - val_loss: 0.0579 - val_acc: 0.5565\n",
      "Epoch 176/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0591 - acc: 0.5393 - val_loss: 0.0577 - val_acc: 0.5558\n",
      "Epoch 177/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0591 - acc: 0.5402 - val_loss: 0.0579 - val_acc: 0.5546\n",
      "Epoch 178/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0590 - acc: 0.5393 - val_loss: 0.0582 - val_acc: 0.5558\n",
      "Epoch 179/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0590 - acc: 0.5399 - val_loss: 0.0577 - val_acc: 0.5546\n",
      "Epoch 180/200\n",
      "3208/3208 [==============================] - 1s 243us/step - loss: 0.0590 - acc: 0.5411 - val_loss: 0.0577 - val_acc: 0.5571\n",
      "Epoch 181/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0590 - acc: 0.5408 - val_loss: 0.0594 - val_acc: 0.5452\n",
      "Epoch 182/200\n",
      "3208/3208 [==============================] - 1s 244us/step - loss: 0.0590 - acc: 0.5383 - val_loss: 0.0578 - val_acc: 0.5552\n",
      "Epoch 183/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0589 - acc: 0.5393 - val_loss: 0.0578 - val_acc: 0.5571\n",
      "Epoch 184/200\n",
      "3208/3208 [==============================] - 1s 243us/step - loss: 0.0591 - acc: 0.5387 - val_loss: 0.0579 - val_acc: 0.5558\n",
      "Epoch 185/200\n",
      "3208/3208 [==============================] - 1s 250us/step - loss: 0.0589 - acc: 0.5399 - val_loss: 0.0593 - val_acc: 0.5471\n",
      "Epoch 186/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0590 - acc: 0.5408 - val_loss: 0.0581 - val_acc: 0.5577\n",
      "Epoch 187/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0591 - acc: 0.5393 - val_loss: 0.0577 - val_acc: 0.5565\n",
      "Epoch 188/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0590 - acc: 0.5418 - val_loss: 0.0605 - val_acc: 0.5427\n",
      "Epoch 189/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0590 - acc: 0.5380 - val_loss: 0.0595 - val_acc: 0.5471\n",
      "Epoch 190/200\n",
      "3208/3208 [==============================] - 1s 247us/step - loss: 0.0591 - acc: 0.5421 - val_loss: 0.0578 - val_acc: 0.5583\n",
      "Epoch 191/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0589 - acc: 0.5411 - val_loss: 0.0589 - val_acc: 0.5552\n",
      "Epoch 192/200\n",
      "3208/3208 [==============================] - 1s 248us/step - loss: 0.0590 - acc: 0.5411 - val_loss: 0.0579 - val_acc: 0.5558\n",
      "Epoch 193/200\n",
      "3208/3208 [==============================] - 1s 243us/step - loss: 0.0589 - acc: 0.5396 - val_loss: 0.0578 - val_acc: 0.5583\n",
      "Epoch 194/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0590 - acc: 0.5402 - val_loss: 0.0577 - val_acc: 0.5577\n",
      "Epoch 195/200\n",
      "3208/3208 [==============================] - 1s 252us/step - loss: 0.0589 - acc: 0.5390 - val_loss: 0.0583 - val_acc: 0.5546\n",
      "Epoch 196/200\n",
      "3208/3208 [==============================] - 1s 242us/step - loss: 0.0590 - acc: 0.5408 - val_loss: 0.0577 - val_acc: 0.5571\n",
      "Epoch 197/200\n",
      "3208/3208 [==============================] - 1s 246us/step - loss: 0.0590 - acc: 0.5405 - val_loss: 0.0581 - val_acc: 0.5546\n",
      "Epoch 198/200\n",
      "3208/3208 [==============================] - 1s 245us/step - loss: 0.0589 - acc: 0.5396 - val_loss: 0.0577 - val_acc: 0.5577\n",
      "Epoch 199/200\n",
      "3208/3208 [==============================] - 1s 249us/step - loss: 0.0590 - acc: 0.5411 - val_loss: 0.0578 - val_acc: 0.5571\n",
      "Epoch 200/200\n",
      "3208/3208 [==============================] - 1s 251us/step - loss: 0.0590 - acc: 0.5424 - val_loss: 0.0578 - val_acc: 0.5590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=20, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=16, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=12, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adadelta = keras.optimizers.Adadelta()\n",
    "nn_model.compile(optimizer=adadelta, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=200, validation_data=(dev_valid_x, dev_valid_y))\n",
    "  \n",
    "pred_y = nn_model.predict(valid_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result_cv = result_cv.append({'detail':'Adadelta', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 20689
    },
    "colab_type": "code",
    "id": "6OewT6p8ftTF",
    "outputId": "378bacab-f3fd-416e-a463-338a7b900e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_82 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 1,754\n",
      "Trainable params: 1,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 2s 596us/step - loss: 0.0748 - acc: 0.3923 - val_loss: 0.0678 - val_acc: 0.4551\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 233us/step - loss: 0.0677 - acc: 0.4515 - val_loss: 0.0664 - val_acc: 0.4576\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 238us/step - loss: 0.0665 - acc: 0.4496 - val_loss: 0.0656 - val_acc: 0.4595\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 237us/step - loss: 0.0656 - acc: 0.4593 - val_loss: 0.0644 - val_acc: 0.4763\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 212us/step - loss: 0.0641 - acc: 0.4696 - val_loss: 0.0665 - val_acc: 0.4289\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0644 - acc: 0.4737 - val_loss: 0.0632 - val_acc: 0.5012\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0625 - acc: 0.4849 - val_loss: 0.0621 - val_acc: 0.4963\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0616 - acc: 0.4977 - val_loss: 0.0609 - val_acc: 0.4751\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0606 - acc: 0.5161 - val_loss: 0.0602 - val_acc: 0.4950\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0603 - acc: 0.5173 - val_loss: 0.0604 - val_acc: 0.5343\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0597 - acc: 0.5326 - val_loss: 0.0590 - val_acc: 0.5405\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0592 - acc: 0.5482 - val_loss: 0.0611 - val_acc: 0.5287\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0597 - acc: 0.5304 - val_loss: 0.0602 - val_acc: 0.5200\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0589 - acc: 0.5401 - val_loss: 0.0588 - val_acc: 0.5418\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0580 - acc: 0.5625 - val_loss: 0.0584 - val_acc: 0.5468\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0579 - acc: 0.5578 - val_loss: 0.0580 - val_acc: 0.5455\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0578 - acc: 0.5641 - val_loss: 0.0591 - val_acc: 0.5530\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0579 - acc: 0.5622 - val_loss: 0.0580 - val_acc: 0.5574\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0577 - acc: 0.5600 - val_loss: 0.0583 - val_acc: 0.5449\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0574 - acc: 0.5666 - val_loss: 0.0594 - val_acc: 0.5511\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0572 - acc: 0.5644 - val_loss: 0.0594 - val_acc: 0.5343\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0570 - acc: 0.5688 - val_loss: 0.0587 - val_acc: 0.5405\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0574 - acc: 0.5650 - val_loss: 0.0580 - val_acc: 0.5574\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0567 - acc: 0.5753 - val_loss: 0.0580 - val_acc: 0.5536\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0571 - acc: 0.5672 - val_loss: 0.0580 - val_acc: 0.5493\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0566 - acc: 0.5762 - val_loss: 0.0580 - val_acc: 0.5580\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0564 - acc: 0.5781 - val_loss: 0.0577 - val_acc: 0.5599\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0564 - acc: 0.5800 - val_loss: 0.0587 - val_acc: 0.5461\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0565 - acc: 0.5778 - val_loss: 0.0587 - val_acc: 0.5387\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0566 - acc: 0.5825 - val_loss: 0.0589 - val_acc: 0.5449\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0564 - acc: 0.5756 - val_loss: 0.0578 - val_acc: 0.5542\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0561 - acc: 0.5753 - val_loss: 0.0579 - val_acc: 0.5605\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0561 - acc: 0.5803 - val_loss: 0.0578 - val_acc: 0.5536\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0559 - acc: 0.5828 - val_loss: 0.0574 - val_acc: 0.5549\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0563 - acc: 0.5790 - val_loss: 0.0584 - val_acc: 0.5536\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0561 - acc: 0.5800 - val_loss: 0.0583 - val_acc: 0.5574\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0558 - acc: 0.5778 - val_loss: 0.0575 - val_acc: 0.5630\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0558 - acc: 0.5806 - val_loss: 0.0605 - val_acc: 0.5355\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0559 - acc: 0.5812 - val_loss: 0.0576 - val_acc: 0.5630\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0556 - acc: 0.5784 - val_loss: 0.0579 - val_acc: 0.5536\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0559 - acc: 0.5822 - val_loss: 0.0635 - val_acc: 0.4950\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0560 - acc: 0.5806 - val_loss: 0.0587 - val_acc: 0.5468\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0556 - acc: 0.5843 - val_loss: 0.0594 - val_acc: 0.5455\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0552 - acc: 0.5903 - val_loss: 0.0587 - val_acc: 0.5499\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0552 - acc: 0.5859 - val_loss: 0.0575 - val_acc: 0.5555\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0557 - acc: 0.5825 - val_loss: 0.0591 - val_acc: 0.5387\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0550 - acc: 0.5887 - val_loss: 0.0578 - val_acc: 0.5555\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0551 - acc: 0.5890 - val_loss: 0.0582 - val_acc: 0.5499\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0551 - acc: 0.5884 - val_loss: 0.0591 - val_acc: 0.5436\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0551 - acc: 0.5925 - val_loss: 0.0581 - val_acc: 0.5580\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0550 - acc: 0.5934 - val_loss: 0.0581 - val_acc: 0.5542\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0550 - acc: 0.5925 - val_loss: 0.0579 - val_acc: 0.5555\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0549 - acc: 0.5921 - val_loss: 0.0580 - val_acc: 0.5555\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0548 - acc: 0.5971 - val_loss: 0.0586 - val_acc: 0.5505\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0548 - acc: 0.5884 - val_loss: 0.0582 - val_acc: 0.5499\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0548 - acc: 0.5912 - val_loss: 0.0583 - val_acc: 0.5443\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0545 - acc: 0.5999 - val_loss: 0.0595 - val_acc: 0.5436\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0547 - acc: 0.5987 - val_loss: 0.0585 - val_acc: 0.5524\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0549 - acc: 0.5915 - val_loss: 0.0581 - val_acc: 0.5542\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0546 - acc: 0.5990 - val_loss: 0.0578 - val_acc: 0.5561\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0545 - acc: 0.5909 - val_loss: 0.0578 - val_acc: 0.5524\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0548 - acc: 0.5946 - val_loss: 0.0583 - val_acc: 0.5461\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0544 - acc: 0.5984 - val_loss: 0.0578 - val_acc: 0.5599\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0543 - acc: 0.5984 - val_loss: 0.0584 - val_acc: 0.5486\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0545 - acc: 0.5918 - val_loss: 0.0580 - val_acc: 0.5493\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0542 - acc: 0.5968 - val_loss: 0.0582 - val_acc: 0.5468\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0541 - acc: 0.5987 - val_loss: 0.0579 - val_acc: 0.5517\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0546 - acc: 0.5965 - val_loss: 0.0580 - val_acc: 0.5493\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0544 - acc: 0.5956 - val_loss: 0.0580 - val_acc: 0.5561\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0541 - acc: 0.5971 - val_loss: 0.0587 - val_acc: 0.5517\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0542 - acc: 0.5981 - val_loss: 0.0595 - val_acc: 0.5436\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0539 - acc: 0.6006 - val_loss: 0.0585 - val_acc: 0.5536\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0540 - acc: 0.6012 - val_loss: 0.0580 - val_acc: 0.5542\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0542 - acc: 0.6015 - val_loss: 0.0587 - val_acc: 0.5493\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0541 - acc: 0.6027 - val_loss: 0.0586 - val_acc: 0.5511\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0540 - acc: 0.6021 - val_loss: 0.0587 - val_acc: 0.5486\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0538 - acc: 0.6071 - val_loss: 0.0581 - val_acc: 0.5524\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0537 - acc: 0.6068 - val_loss: 0.0585 - val_acc: 0.5493\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0539 - acc: 0.6031 - val_loss: 0.0589 - val_acc: 0.5449\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0537 - acc: 0.5996 - val_loss: 0.0593 - val_acc: 0.5486\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0536 - acc: 0.6046 - val_loss: 0.0590 - val_acc: 0.5449\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0539 - acc: 0.6062 - val_loss: 0.0583 - val_acc: 0.5536\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0538 - acc: 0.6024 - val_loss: 0.0580 - val_acc: 0.5511\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0535 - acc: 0.6040 - val_loss: 0.0586 - val_acc: 0.5480\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0537 - acc: 0.6046 - val_loss: 0.0585 - val_acc: 0.5499\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0534 - acc: 0.6087 - val_loss: 0.0583 - val_acc: 0.5480\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0534 - acc: 0.6052 - val_loss: 0.0587 - val_acc: 0.5499\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0531 - acc: 0.6090 - val_loss: 0.0585 - val_acc: 0.5517\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0537 - acc: 0.6018 - val_loss: 0.0585 - val_acc: 0.5474\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0532 - acc: 0.6068 - val_loss: 0.0585 - val_acc: 0.5480\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0534 - acc: 0.6031 - val_loss: 0.0583 - val_acc: 0.5493\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0535 - acc: 0.6080 - val_loss: 0.0589 - val_acc: 0.5424\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0532 - acc: 0.6087 - val_loss: 0.0586 - val_acc: 0.5468\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0532 - acc: 0.6087 - val_loss: 0.0585 - val_acc: 0.5424\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0530 - acc: 0.6074 - val_loss: 0.0593 - val_acc: 0.5474\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0531 - acc: 0.6105 - val_loss: 0.0582 - val_acc: 0.5536\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0532 - acc: 0.6071 - val_loss: 0.0588 - val_acc: 0.5449\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0529 - acc: 0.6118 - val_loss: 0.0599 - val_acc: 0.5505\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0529 - acc: 0.6130 - val_loss: 0.0590 - val_acc: 0.5449\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0532 - acc: 0.6109 - val_loss: 0.0588 - val_acc: 0.5411\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0528 - acc: 0.6155 - val_loss: 0.0597 - val_acc: 0.5443\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0528 - acc: 0.6127 - val_loss: 0.0606 - val_acc: 0.5387\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0530 - acc: 0.6102 - val_loss: 0.0587 - val_acc: 0.5517\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0530 - acc: 0.6080 - val_loss: 0.0588 - val_acc: 0.5517\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0527 - acc: 0.6149 - val_loss: 0.0589 - val_acc: 0.5474\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0530 - acc: 0.6149 - val_loss: 0.0600 - val_acc: 0.5387\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0529 - acc: 0.6112 - val_loss: 0.0588 - val_acc: 0.5436\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0530 - acc: 0.6037 - val_loss: 0.0594 - val_acc: 0.5449\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0525 - acc: 0.6152 - val_loss: 0.0593 - val_acc: 0.5480\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0526 - acc: 0.6171 - val_loss: 0.0593 - val_acc: 0.5405\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0523 - acc: 0.6162 - val_loss: 0.0597 - val_acc: 0.5474\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0526 - acc: 0.6149 - val_loss: 0.0606 - val_acc: 0.5343\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0524 - acc: 0.6152 - val_loss: 0.0597 - val_acc: 0.5468\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0530 - acc: 0.6096 - val_loss: 0.0595 - val_acc: 0.5411\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0523 - acc: 0.6168 - val_loss: 0.0599 - val_acc: 0.5443\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0524 - acc: 0.6211 - val_loss: 0.0605 - val_acc: 0.5393\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0522 - acc: 0.6215 - val_loss: 0.0592 - val_acc: 0.5418\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0521 - acc: 0.6168 - val_loss: 0.0597 - val_acc: 0.5418\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0522 - acc: 0.6177 - val_loss: 0.0598 - val_acc: 0.5436\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0523 - acc: 0.6168 - val_loss: 0.0595 - val_acc: 0.5493\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0524 - acc: 0.6155 - val_loss: 0.0596 - val_acc: 0.5436\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0521 - acc: 0.6239 - val_loss: 0.0621 - val_acc: 0.5393\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0530 - acc: 0.6112 - val_loss: 0.0603 - val_acc: 0.5461\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0523 - acc: 0.6186 - val_loss: 0.0596 - val_acc: 0.5480\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0520 - acc: 0.6208 - val_loss: 0.0599 - val_acc: 0.5424\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0520 - acc: 0.6211 - val_loss: 0.0594 - val_acc: 0.5468\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0519 - acc: 0.6224 - val_loss: 0.0605 - val_acc: 0.5480\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0521 - acc: 0.6177 - val_loss: 0.0601 - val_acc: 0.5374\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0519 - acc: 0.6218 - val_loss: 0.0599 - val_acc: 0.5430\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0518 - acc: 0.6239 - val_loss: 0.0599 - val_acc: 0.5461\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0521 - acc: 0.6152 - val_loss: 0.0603 - val_acc: 0.5436\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0518 - acc: 0.6202 - val_loss: 0.0599 - val_acc: 0.5493\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0519 - acc: 0.6168 - val_loss: 0.0595 - val_acc: 0.5499\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 208us/step - loss: 0.0518 - acc: 0.6236 - val_loss: 0.0602 - val_acc: 0.5461\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0514 - acc: 0.6271 - val_loss: 0.0600 - val_acc: 0.5461\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0516 - acc: 0.6227 - val_loss: 0.0641 - val_acc: 0.5112\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0517 - acc: 0.6202 - val_loss: 0.0622 - val_acc: 0.5337\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0515 - acc: 0.6230 - val_loss: 0.0602 - val_acc: 0.5424\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0515 - acc: 0.6239 - val_loss: 0.0625 - val_acc: 0.5287\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0514 - acc: 0.6264 - val_loss: 0.0620 - val_acc: 0.5374\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0517 - acc: 0.6215 - val_loss: 0.0615 - val_acc: 0.5305\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0517 - acc: 0.6224 - val_loss: 0.0619 - val_acc: 0.5318\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0514 - acc: 0.6246 - val_loss: 0.0601 - val_acc: 0.5474\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0516 - acc: 0.6239 - val_loss: 0.0607 - val_acc: 0.5424\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0513 - acc: 0.6243 - val_loss: 0.0601 - val_acc: 0.5505\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0512 - acc: 0.6283 - val_loss: 0.0600 - val_acc: 0.5511\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0513 - acc: 0.6243 - val_loss: 0.0600 - val_acc: 0.5474\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0511 - acc: 0.6339 - val_loss: 0.0597 - val_acc: 0.5468\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0512 - acc: 0.6317 - val_loss: 0.0612 - val_acc: 0.5368\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0514 - acc: 0.6308 - val_loss: 0.0609 - val_acc: 0.5511\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0511 - acc: 0.6299 - val_loss: 0.0604 - val_acc: 0.5493\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0509 - acc: 0.6299 - val_loss: 0.0608 - val_acc: 0.5436\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0512 - acc: 0.6321 - val_loss: 0.0603 - val_acc: 0.5474\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0508 - acc: 0.6330 - val_loss: 0.0609 - val_acc: 0.5449\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0512 - acc: 0.6177 - val_loss: 0.0605 - val_acc: 0.5480\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0509 - acc: 0.6342 - val_loss: 0.0612 - val_acc: 0.5455\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0517 - acc: 0.6218 - val_loss: 0.0606 - val_acc: 0.5461\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0514 - acc: 0.6261 - val_loss: 0.0604 - val_acc: 0.5436\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0508 - acc: 0.6314 - val_loss: 0.0604 - val_acc: 0.5486\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0511 - acc: 0.6277 - val_loss: 0.0600 - val_acc: 0.5480\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0507 - acc: 0.6302 - val_loss: 0.0607 - val_acc: 0.5505\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0507 - acc: 0.6336 - val_loss: 0.0602 - val_acc: 0.5474\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0509 - acc: 0.6286 - val_loss: 0.0612 - val_acc: 0.5449\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0507 - acc: 0.6324 - val_loss: 0.0605 - val_acc: 0.5505\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0509 - acc: 0.6283 - val_loss: 0.0608 - val_acc: 0.5480\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0503 - acc: 0.6342 - val_loss: 0.0619 - val_acc: 0.5374\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0506 - acc: 0.6352 - val_loss: 0.0604 - val_acc: 0.5499\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0505 - acc: 0.6358 - val_loss: 0.0611 - val_acc: 0.5499\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0508 - acc: 0.6330 - val_loss: 0.0615 - val_acc: 0.5349\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0507 - acc: 0.6302 - val_loss: 0.0611 - val_acc: 0.5499\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0506 - acc: 0.6314 - val_loss: 0.0607 - val_acc: 0.5443\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0503 - acc: 0.6377 - val_loss: 0.0613 - val_acc: 0.5511\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0504 - acc: 0.6361 - val_loss: 0.0604 - val_acc: 0.5486\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0503 - acc: 0.6389 - val_loss: 0.0605 - val_acc: 0.5430\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0506 - acc: 0.6345 - val_loss: 0.0605 - val_acc: 0.5436\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0504 - acc: 0.6367 - val_loss: 0.0607 - val_acc: 0.5505\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0506 - acc: 0.6292 - val_loss: 0.0609 - val_acc: 0.5517\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0504 - acc: 0.6370 - val_loss: 0.0610 - val_acc: 0.5486\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0502 - acc: 0.6364 - val_loss: 0.0608 - val_acc: 0.5443\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0501 - acc: 0.6355 - val_loss: 0.0606 - val_acc: 0.5524\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0502 - acc: 0.6342 - val_loss: 0.0617 - val_acc: 0.5449\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0503 - acc: 0.6349 - val_loss: 0.0606 - val_acc: 0.5468\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0505 - acc: 0.6330 - val_loss: 0.0617 - val_acc: 0.5387\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0501 - acc: 0.6392 - val_loss: 0.0616 - val_acc: 0.5418\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0503 - acc: 0.6336 - val_loss: 0.0613 - val_acc: 0.5449\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0504 - acc: 0.6342 - val_loss: 0.0622 - val_acc: 0.5424\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0501 - acc: 0.6370 - val_loss: 0.0613 - val_acc: 0.5430\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0499 - acc: 0.6377 - val_loss: 0.0612 - val_acc: 0.5530\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0501 - acc: 0.6377 - val_loss: 0.0611 - val_acc: 0.5411\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 193us/step - loss: 0.0500 - acc: 0.6399 - val_loss: 0.0611 - val_acc: 0.5486\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0498 - acc: 0.6405 - val_loss: 0.0611 - val_acc: 0.5536\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0501 - acc: 0.6336 - val_loss: 0.0618 - val_acc: 0.5443\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0503 - acc: 0.6339 - val_loss: 0.0619 - val_acc: 0.5411\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0498 - acc: 0.6389 - val_loss: 0.0618 - val_acc: 0.5480\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0497 - acc: 0.6399 - val_loss: 0.0617 - val_acc: 0.5449\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0499 - acc: 0.6374 - val_loss: 0.0618 - val_acc: 0.5455\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0498 - acc: 0.6417 - val_loss: 0.0615 - val_acc: 0.5449\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0496 - acc: 0.6405 - val_loss: 0.0616 - val_acc: 0.5411\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0496 - acc: 0.6473 - val_loss: 0.0618 - val_acc: 0.5436\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0497 - acc: 0.6402 - val_loss: 0.0612 - val_acc: 0.5436\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0553 - acc: 0.5965 - val_loss: 0.0512 - val_acc: 0.6228\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0542 - acc: 0.6059 - val_loss: 0.0529 - val_acc: 0.6147\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0541 - acc: 0.6034 - val_loss: 0.0526 - val_acc: 0.6128\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0534 - acc: 0.6109 - val_loss: 0.0521 - val_acc: 0.6147\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0536 - acc: 0.6105 - val_loss: 0.0534 - val_acc: 0.5960\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0532 - acc: 0.6140 - val_loss: 0.0530 - val_acc: 0.6054\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0529 - acc: 0.6155 - val_loss: 0.0538 - val_acc: 0.5910\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0526 - acc: 0.6162 - val_loss: 0.0540 - val_acc: 0.5904\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0525 - acc: 0.6155 - val_loss: 0.0543 - val_acc: 0.5910\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0528 - acc: 0.6140 - val_loss: 0.0542 - val_acc: 0.5973\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0525 - acc: 0.6180 - val_loss: 0.0547 - val_acc: 0.5867\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0522 - acc: 0.6196 - val_loss: 0.0540 - val_acc: 0.5979\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0523 - acc: 0.6158 - val_loss: 0.0544 - val_acc: 0.5923\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0523 - acc: 0.6171 - val_loss: 0.0550 - val_acc: 0.5798\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0519 - acc: 0.6211 - val_loss: 0.0556 - val_acc: 0.5916\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0518 - acc: 0.6230 - val_loss: 0.0544 - val_acc: 0.5941\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0516 - acc: 0.6249 - val_loss: 0.0551 - val_acc: 0.5848\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0516 - acc: 0.6196 - val_loss: 0.0558 - val_acc: 0.5704\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0519 - acc: 0.6218 - val_loss: 0.0557 - val_acc: 0.5829\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0514 - acc: 0.6274 - val_loss: 0.0552 - val_acc: 0.5804\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0517 - acc: 0.6243 - val_loss: 0.0560 - val_acc: 0.5804\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0517 - acc: 0.6243 - val_loss: 0.0567 - val_acc: 0.5742\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0515 - acc: 0.6283 - val_loss: 0.0563 - val_acc: 0.5761\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0513 - acc: 0.6255 - val_loss: 0.0563 - val_acc: 0.5686\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0514 - acc: 0.6302 - val_loss: 0.0562 - val_acc: 0.5655\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0513 - acc: 0.6268 - val_loss: 0.0562 - val_acc: 0.5667\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0513 - acc: 0.6261 - val_loss: 0.0569 - val_acc: 0.5717\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0512 - acc: 0.6314 - val_loss: 0.0569 - val_acc: 0.5599\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0513 - acc: 0.6296 - val_loss: 0.0576 - val_acc: 0.5748\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0513 - acc: 0.6271 - val_loss: 0.0575 - val_acc: 0.5673\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0510 - acc: 0.6314 - val_loss: 0.0572 - val_acc: 0.5692\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0512 - acc: 0.6296 - val_loss: 0.0566 - val_acc: 0.5692\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0510 - acc: 0.6314 - val_loss: 0.0582 - val_acc: 0.5599\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0505 - acc: 0.6327 - val_loss: 0.0576 - val_acc: 0.5704\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0509 - acc: 0.6274 - val_loss: 0.0580 - val_acc: 0.5555\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0509 - acc: 0.6283 - val_loss: 0.0579 - val_acc: 0.5586\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0506 - acc: 0.6280 - val_loss: 0.0573 - val_acc: 0.5648\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0506 - acc: 0.6327 - val_loss: 0.0576 - val_acc: 0.5623\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0507 - acc: 0.6380 - val_loss: 0.0581 - val_acc: 0.5642\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0505 - acc: 0.6349 - val_loss: 0.0574 - val_acc: 0.5580\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0503 - acc: 0.6333 - val_loss: 0.0574 - val_acc: 0.5648\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0505 - acc: 0.6324 - val_loss: 0.0581 - val_acc: 0.5561\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0504 - acc: 0.6355 - val_loss: 0.0580 - val_acc: 0.5648\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0502 - acc: 0.6374 - val_loss: 0.0576 - val_acc: 0.5567\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0501 - acc: 0.6383 - val_loss: 0.0579 - val_acc: 0.5561\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0499 - acc: 0.6380 - val_loss: 0.0585 - val_acc: 0.5630\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0506 - acc: 0.6311 - val_loss: 0.0583 - val_acc: 0.5493\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0505 - acc: 0.6349 - val_loss: 0.0588 - val_acc: 0.5555\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0504 - acc: 0.6386 - val_loss: 0.0580 - val_acc: 0.5592\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0503 - acc: 0.6358 - val_loss: 0.0583 - val_acc: 0.5567\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0499 - acc: 0.6417 - val_loss: 0.0591 - val_acc: 0.5599\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0497 - acc: 0.6395 - val_loss: 0.0592 - val_acc: 0.5455\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0498 - acc: 0.6392 - val_loss: 0.0593 - val_acc: 0.5511\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0506 - acc: 0.6364 - val_loss: 0.0583 - val_acc: 0.5623\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0497 - acc: 0.6402 - val_loss: 0.0590 - val_acc: 0.5443\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0498 - acc: 0.6389 - val_loss: 0.0593 - val_acc: 0.5468\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0502 - acc: 0.6314 - val_loss: 0.0594 - val_acc: 0.5443\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0496 - acc: 0.6442 - val_loss: 0.0591 - val_acc: 0.5449\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0499 - acc: 0.6389 - val_loss: 0.0596 - val_acc: 0.5524\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0496 - acc: 0.6495 - val_loss: 0.0592 - val_acc: 0.5474\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0498 - acc: 0.6427 - val_loss: 0.0593 - val_acc: 0.5449\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0494 - acc: 0.6458 - val_loss: 0.0591 - val_acc: 0.5536\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0496 - acc: 0.6455 - val_loss: 0.0596 - val_acc: 0.5474\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0493 - acc: 0.6405 - val_loss: 0.0594 - val_acc: 0.5461\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0493 - acc: 0.6470 - val_loss: 0.0597 - val_acc: 0.5430\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0497 - acc: 0.6464 - val_loss: 0.0602 - val_acc: 0.5368\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0495 - acc: 0.6411 - val_loss: 0.0591 - val_acc: 0.5480\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0493 - acc: 0.6483 - val_loss: 0.0595 - val_acc: 0.5443\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0495 - acc: 0.6445 - val_loss: 0.0593 - val_acc: 0.5474\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0489 - acc: 0.6464 - val_loss: 0.0604 - val_acc: 0.5474\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0494 - acc: 0.6452 - val_loss: 0.0598 - val_acc: 0.5517\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0492 - acc: 0.6473 - val_loss: 0.0606 - val_acc: 0.5461\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0495 - acc: 0.6464 - val_loss: 0.0591 - val_acc: 0.5499\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0493 - acc: 0.6420 - val_loss: 0.0607 - val_acc: 0.5443\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0490 - acc: 0.6489 - val_loss: 0.0597 - val_acc: 0.5567\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0493 - acc: 0.6470 - val_loss: 0.0601 - val_acc: 0.5330\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0492 - acc: 0.6489 - val_loss: 0.0594 - val_acc: 0.5449\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0490 - acc: 0.6489 - val_loss: 0.0600 - val_acc: 0.5293\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0493 - acc: 0.6433 - val_loss: 0.0603 - val_acc: 0.5449\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0488 - acc: 0.6483 - val_loss: 0.0596 - val_acc: 0.5393\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0489 - acc: 0.6445 - val_loss: 0.0607 - val_acc: 0.5324\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0490 - acc: 0.6486 - val_loss: 0.0601 - val_acc: 0.5380\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0488 - acc: 0.6514 - val_loss: 0.0607 - val_acc: 0.5362\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0487 - acc: 0.6526 - val_loss: 0.0619 - val_acc: 0.5281\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0490 - acc: 0.6498 - val_loss: 0.0613 - val_acc: 0.5393\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0489 - acc: 0.6464 - val_loss: 0.0601 - val_acc: 0.5455\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0488 - acc: 0.6498 - val_loss: 0.0610 - val_acc: 0.5293\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0486 - acc: 0.6554 - val_loss: 0.0607 - val_acc: 0.5405\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0486 - acc: 0.6533 - val_loss: 0.0607 - val_acc: 0.5449\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0488 - acc: 0.6498 - val_loss: 0.0622 - val_acc: 0.5287\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0487 - acc: 0.6501 - val_loss: 0.0611 - val_acc: 0.5355\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0486 - acc: 0.6517 - val_loss: 0.0607 - val_acc: 0.5455\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0489 - acc: 0.6520 - val_loss: 0.0606 - val_acc: 0.5330\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0482 - acc: 0.6539 - val_loss: 0.0620 - val_acc: 0.5287\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0485 - acc: 0.6564 - val_loss: 0.0609 - val_acc: 0.5343\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0483 - acc: 0.6564 - val_loss: 0.0621 - val_acc: 0.5262\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0484 - acc: 0.6539 - val_loss: 0.0610 - val_acc: 0.5362\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0483 - acc: 0.6548 - val_loss: 0.0612 - val_acc: 0.5312\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0486 - acc: 0.6501 - val_loss: 0.0614 - val_acc: 0.5362\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0486 - acc: 0.6533 - val_loss: 0.0612 - val_acc: 0.5330\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0481 - acc: 0.6545 - val_loss: 0.0616 - val_acc: 0.5318\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0482 - acc: 0.6579 - val_loss: 0.0622 - val_acc: 0.5343\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0482 - acc: 0.6505 - val_loss: 0.0614 - val_acc: 0.5293\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0482 - acc: 0.6564 - val_loss: 0.0625 - val_acc: 0.5293\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0481 - acc: 0.6529 - val_loss: 0.0632 - val_acc: 0.5393\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0484 - acc: 0.6548 - val_loss: 0.0620 - val_acc: 0.5305\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0483 - acc: 0.6576 - val_loss: 0.0620 - val_acc: 0.5424\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0479 - acc: 0.6576 - val_loss: 0.0624 - val_acc: 0.5343\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0480 - acc: 0.6570 - val_loss: 0.0617 - val_acc: 0.5449\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0480 - acc: 0.6523 - val_loss: 0.0615 - val_acc: 0.5318\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0483 - acc: 0.6529 - val_loss: 0.0614 - val_acc: 0.5324\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0481 - acc: 0.6533 - val_loss: 0.0615 - val_acc: 0.5305\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0476 - acc: 0.6564 - val_loss: 0.0620 - val_acc: 0.5349\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0479 - acc: 0.6548 - val_loss: 0.0621 - val_acc: 0.5343\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0483 - acc: 0.6511 - val_loss: 0.0628 - val_acc: 0.5224\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0479 - acc: 0.6545 - val_loss: 0.0615 - val_acc: 0.5349\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0483 - acc: 0.6520 - val_loss: 0.0613 - val_acc: 0.5387\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0478 - acc: 0.6635 - val_loss: 0.0631 - val_acc: 0.5224\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0480 - acc: 0.6632 - val_loss: 0.0622 - val_acc: 0.5343\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0478 - acc: 0.6570 - val_loss: 0.0619 - val_acc: 0.5312\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0482 - acc: 0.6529 - val_loss: 0.0630 - val_acc: 0.5237\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0482 - acc: 0.6576 - val_loss: 0.0620 - val_acc: 0.5305\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0479 - acc: 0.6595 - val_loss: 0.0623 - val_acc: 0.5249\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0477 - acc: 0.6629 - val_loss: 0.0623 - val_acc: 0.5287\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0475 - acc: 0.6614 - val_loss: 0.0622 - val_acc: 0.5337\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0482 - acc: 0.6573 - val_loss: 0.0629 - val_acc: 0.5249\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0478 - acc: 0.6576 - val_loss: 0.0617 - val_acc: 0.5312\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0479 - acc: 0.6589 - val_loss: 0.0624 - val_acc: 0.5405\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0476 - acc: 0.6614 - val_loss: 0.0623 - val_acc: 0.5305\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0476 - acc: 0.6660 - val_loss: 0.0629 - val_acc: 0.5237\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0478 - acc: 0.6592 - val_loss: 0.0629 - val_acc: 0.5256\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0476 - acc: 0.6611 - val_loss: 0.0634 - val_acc: 0.5281\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0473 - acc: 0.6654 - val_loss: 0.0635 - val_acc: 0.5293\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0474 - acc: 0.6607 - val_loss: 0.0636 - val_acc: 0.5318\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 203us/step - loss: 0.0477 - acc: 0.6620 - val_loss: 0.0623 - val_acc: 0.5299\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0473 - acc: 0.6642 - val_loss: 0.0626 - val_acc: 0.5281\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0475 - acc: 0.6651 - val_loss: 0.0631 - val_acc: 0.5224\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0469 - acc: 0.6664 - val_loss: 0.0628 - val_acc: 0.5293\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0472 - acc: 0.6664 - val_loss: 0.0629 - val_acc: 0.5281\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0472 - acc: 0.6635 - val_loss: 0.0633 - val_acc: 0.5281\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0473 - acc: 0.6657 - val_loss: 0.0635 - val_acc: 0.5299\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0472 - acc: 0.6614 - val_loss: 0.0626 - val_acc: 0.5274\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0471 - acc: 0.6645 - val_loss: 0.0627 - val_acc: 0.5287\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0473 - acc: 0.6623 - val_loss: 0.0638 - val_acc: 0.5274\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0472 - acc: 0.6614 - val_loss: 0.0635 - val_acc: 0.5305\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0470 - acc: 0.6648 - val_loss: 0.0632 - val_acc: 0.5287\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0471 - acc: 0.6648 - val_loss: 0.0632 - val_acc: 0.5243\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0471 - acc: 0.6635 - val_loss: 0.0642 - val_acc: 0.5237\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0474 - acc: 0.6657 - val_loss: 0.0647 - val_acc: 0.5175\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0470 - acc: 0.6660 - val_loss: 0.0629 - val_acc: 0.5237\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0470 - acc: 0.6654 - val_loss: 0.0639 - val_acc: 0.5237\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0472 - acc: 0.6607 - val_loss: 0.0634 - val_acc: 0.5312\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0468 - acc: 0.6685 - val_loss: 0.0633 - val_acc: 0.5293\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0466 - acc: 0.6682 - val_loss: 0.0634 - val_acc: 0.5237\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0467 - acc: 0.6676 - val_loss: 0.0640 - val_acc: 0.5200\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0468 - acc: 0.6701 - val_loss: 0.0637 - val_acc: 0.5224\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0469 - acc: 0.6651 - val_loss: 0.0638 - val_acc: 0.5262\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0463 - acc: 0.6742 - val_loss: 0.0633 - val_acc: 0.5312\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0468 - acc: 0.6667 - val_loss: 0.0643 - val_acc: 0.5262\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0467 - acc: 0.6679 - val_loss: 0.0632 - val_acc: 0.5337\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0473 - acc: 0.6629 - val_loss: 0.0639 - val_acc: 0.5212\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0469 - acc: 0.6642 - val_loss: 0.0636 - val_acc: 0.5262\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0468 - acc: 0.6645 - val_loss: 0.0642 - val_acc: 0.5249\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0465 - acc: 0.6685 - val_loss: 0.0645 - val_acc: 0.5187\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0465 - acc: 0.6698 - val_loss: 0.0634 - val_acc: 0.5212\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0465 - acc: 0.6688 - val_loss: 0.0639 - val_acc: 0.5249\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0465 - acc: 0.6695 - val_loss: 0.0642 - val_acc: 0.5193\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0463 - acc: 0.6726 - val_loss: 0.0638 - val_acc: 0.5305\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0462 - acc: 0.6735 - val_loss: 0.0633 - val_acc: 0.5305\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0464 - acc: 0.6729 - val_loss: 0.0641 - val_acc: 0.5268\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0467 - acc: 0.6645 - val_loss: 0.0636 - val_acc: 0.5293\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0467 - acc: 0.6695 - val_loss: 0.0649 - val_acc: 0.5212\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0464 - acc: 0.6704 - val_loss: 0.0643 - val_acc: 0.5330\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0463 - acc: 0.6685 - val_loss: 0.0643 - val_acc: 0.5237\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0467 - acc: 0.6738 - val_loss: 0.0647 - val_acc: 0.5293\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0464 - acc: 0.6710 - val_loss: 0.0642 - val_acc: 0.5231\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0465 - acc: 0.6682 - val_loss: 0.0633 - val_acc: 0.5318\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0467 - acc: 0.6667 - val_loss: 0.0657 - val_acc: 0.5125\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0465 - acc: 0.6667 - val_loss: 0.0638 - val_acc: 0.5262\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0463 - acc: 0.6707 - val_loss: 0.0639 - val_acc: 0.5237\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0463 - acc: 0.6692 - val_loss: 0.0647 - val_acc: 0.5175\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0461 - acc: 0.6754 - val_loss: 0.0647 - val_acc: 0.5206\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0466 - acc: 0.6679 - val_loss: 0.0647 - val_acc: 0.5200\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0461 - acc: 0.6698 - val_loss: 0.0650 - val_acc: 0.5200\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0462 - acc: 0.6710 - val_loss: 0.0637 - val_acc: 0.5305\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0461 - acc: 0.6745 - val_loss: 0.0639 - val_acc: 0.5224\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0460 - acc: 0.6726 - val_loss: 0.0656 - val_acc: 0.5168\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0461 - acc: 0.6745 - val_loss: 0.0653 - val_acc: 0.5137\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0458 - acc: 0.6713 - val_loss: 0.0639 - val_acc: 0.5231\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0461 - acc: 0.6729 - val_loss: 0.0651 - val_acc: 0.5231\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0466 - acc: 0.6710 - val_loss: 0.0644 - val_acc: 0.5218\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0462 - acc: 0.6688 - val_loss: 0.0641 - val_acc: 0.5274\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0467 - acc: 0.6670 - val_loss: 0.0640 - val_acc: 0.5224\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0460 - acc: 0.6745 - val_loss: 0.0645 - val_acc: 0.5249\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0460 - acc: 0.6723 - val_loss: 0.0642 - val_acc: 0.5318\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0461 - acc: 0.6782 - val_loss: 0.0642 - val_acc: 0.5218\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0458 - acc: 0.6798 - val_loss: 0.0651 - val_acc: 0.5305\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0461 - acc: 0.6735 - val_loss: 0.0647 - val_acc: 0.5224\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0459 - acc: 0.6742 - val_loss: 0.0658 - val_acc: 0.5187\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0458 - acc: 0.6760 - val_loss: 0.0641 - val_acc: 0.5268\n",
      "Train on 3208 samples, validate on 1603 samples\n",
      "Epoch 1/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0565 - acc: 0.5857 - val_loss: 0.0449 - val_acc: 0.6856\n",
      "Epoch 2/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0554 - acc: 0.5935 - val_loss: 0.0461 - val_acc: 0.6725\n",
      "Epoch 3/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0539 - acc: 0.6016 - val_loss: 0.0487 - val_acc: 0.6488\n",
      "Epoch 4/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0533 - acc: 0.6072 - val_loss: 0.0488 - val_acc: 0.6463\n",
      "Epoch 5/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0529 - acc: 0.6125 - val_loss: 0.0490 - val_acc: 0.6488\n",
      "Epoch 6/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0528 - acc: 0.6047 - val_loss: 0.0489 - val_acc: 0.6494\n",
      "Epoch 7/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0525 - acc: 0.6125 - val_loss: 0.0494 - val_acc: 0.6438\n",
      "Epoch 8/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0526 - acc: 0.6122 - val_loss: 0.0498 - val_acc: 0.6407\n",
      "Epoch 9/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0523 - acc: 0.6166 - val_loss: 0.0502 - val_acc: 0.6326\n",
      "Epoch 10/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0522 - acc: 0.6172 - val_loss: 0.0502 - val_acc: 0.6338\n",
      "Epoch 11/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0520 - acc: 0.6128 - val_loss: 0.0504 - val_acc: 0.6382\n",
      "Epoch 12/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0520 - acc: 0.6141 - val_loss: 0.0506 - val_acc: 0.6294\n",
      "Epoch 13/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0517 - acc: 0.6222 - val_loss: 0.0520 - val_acc: 0.6188\n",
      "Epoch 14/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0517 - acc: 0.6181 - val_loss: 0.0508 - val_acc: 0.6332\n",
      "Epoch 15/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0513 - acc: 0.6209 - val_loss: 0.0511 - val_acc: 0.6257\n",
      "Epoch 16/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0512 - acc: 0.6244 - val_loss: 0.0507 - val_acc: 0.6357\n",
      "Epoch 17/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0513 - acc: 0.6244 - val_loss: 0.0520 - val_acc: 0.6282\n",
      "Epoch 18/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0515 - acc: 0.6178 - val_loss: 0.0521 - val_acc: 0.6251\n",
      "Epoch 19/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0513 - acc: 0.6194 - val_loss: 0.0518 - val_acc: 0.6220\n",
      "Epoch 20/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0509 - acc: 0.6253 - val_loss: 0.0525 - val_acc: 0.6232\n",
      "Epoch 21/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0508 - acc: 0.6247 - val_loss: 0.0523 - val_acc: 0.6251\n",
      "Epoch 22/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0513 - acc: 0.6203 - val_loss: 0.0537 - val_acc: 0.6070\n",
      "Epoch 23/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0507 - acc: 0.6303 - val_loss: 0.0516 - val_acc: 0.6282\n",
      "Epoch 24/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0510 - acc: 0.6262 - val_loss: 0.0530 - val_acc: 0.6176\n",
      "Epoch 25/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0506 - acc: 0.6284 - val_loss: 0.0526 - val_acc: 0.6082\n",
      "Epoch 26/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0504 - acc: 0.6297 - val_loss: 0.0525 - val_acc: 0.6201\n",
      "Epoch 27/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0503 - acc: 0.6312 - val_loss: 0.0526 - val_acc: 0.6232\n",
      "Epoch 28/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0504 - acc: 0.6315 - val_loss: 0.0526 - val_acc: 0.6213\n",
      "Epoch 29/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0506 - acc: 0.6266 - val_loss: 0.0522 - val_acc: 0.6288\n",
      "Epoch 30/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0505 - acc: 0.6281 - val_loss: 0.0525 - val_acc: 0.6251\n",
      "Epoch 31/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0503 - acc: 0.6309 - val_loss: 0.0526 - val_acc: 0.6207\n",
      "Epoch 32/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0505 - acc: 0.6284 - val_loss: 0.0537 - val_acc: 0.6107\n",
      "Epoch 33/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0503 - acc: 0.6272 - val_loss: 0.0530 - val_acc: 0.6157\n",
      "Epoch 34/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0499 - acc: 0.6337 - val_loss: 0.0554 - val_acc: 0.5883\n",
      "Epoch 35/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0502 - acc: 0.6312 - val_loss: 0.0546 - val_acc: 0.6045\n",
      "Epoch 36/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0497 - acc: 0.6359 - val_loss: 0.0530 - val_acc: 0.6157\n",
      "Epoch 37/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0497 - acc: 0.6353 - val_loss: 0.0536 - val_acc: 0.6176\n",
      "Epoch 38/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0500 - acc: 0.6315 - val_loss: 0.0547 - val_acc: 0.5870\n",
      "Epoch 39/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0498 - acc: 0.6315 - val_loss: 0.0537 - val_acc: 0.6120\n",
      "Epoch 40/200\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0500 - acc: 0.6347 - val_loss: 0.0539 - val_acc: 0.6070\n",
      "Epoch 41/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0496 - acc: 0.6368 - val_loss: 0.0535 - val_acc: 0.6157\n",
      "Epoch 42/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0496 - acc: 0.6390 - val_loss: 0.0533 - val_acc: 0.6170\n",
      "Epoch 43/200\n",
      "3208/3208 [==============================] - 1s 197us/step - loss: 0.0497 - acc: 0.6344 - val_loss: 0.0542 - val_acc: 0.6138\n",
      "Epoch 44/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0495 - acc: 0.6378 - val_loss: 0.0541 - val_acc: 0.6089\n",
      "Epoch 45/200\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0495 - acc: 0.6356 - val_loss: 0.0540 - val_acc: 0.6120\n",
      "Epoch 46/200\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0493 - acc: 0.6368 - val_loss: 0.0539 - val_acc: 0.6120\n",
      "Epoch 47/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0496 - acc: 0.6368 - val_loss: 0.0548 - val_acc: 0.6089\n",
      "Epoch 48/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0492 - acc: 0.6412 - val_loss: 0.0544 - val_acc: 0.6089\n",
      "Epoch 49/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0498 - acc: 0.6347 - val_loss: 0.0540 - val_acc: 0.6138\n",
      "Epoch 50/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0501 - acc: 0.6322 - val_loss: 0.0540 - val_acc: 0.6120\n",
      "Epoch 51/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0493 - acc: 0.6403 - val_loss: 0.0542 - val_acc: 0.6095\n",
      "Epoch 52/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0494 - acc: 0.6400 - val_loss: 0.0551 - val_acc: 0.5901\n",
      "Epoch 53/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0494 - acc: 0.6381 - val_loss: 0.0546 - val_acc: 0.6138\n",
      "Epoch 54/200\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0490 - acc: 0.6428 - val_loss: 0.0547 - val_acc: 0.6064\n",
      "Epoch 55/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0493 - acc: 0.6403 - val_loss: 0.0543 - val_acc: 0.6082\n",
      "Epoch 56/200\n",
      "3208/3208 [==============================] - 1s 192us/step - loss: 0.0489 - acc: 0.6381 - val_loss: 0.0555 - val_acc: 0.6001\n",
      "Epoch 57/200\n",
      "3208/3208 [==============================] - 1s 238us/step - loss: 0.0495 - acc: 0.6365 - val_loss: 0.0545 - val_acc: 0.6101\n",
      "Epoch 58/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0488 - acc: 0.6400 - val_loss: 0.0559 - val_acc: 0.6026\n",
      "Epoch 59/200\n",
      "3208/3208 [==============================] - 1s 229us/step - loss: 0.0492 - acc: 0.6406 - val_loss: 0.0562 - val_acc: 0.5908\n",
      "Epoch 60/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0489 - acc: 0.6462 - val_loss: 0.0547 - val_acc: 0.6045\n",
      "Epoch 61/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0490 - acc: 0.6375 - val_loss: 0.0553 - val_acc: 0.6045\n",
      "Epoch 62/200\n",
      "3208/3208 [==============================] - 1s 235us/step - loss: 0.0490 - acc: 0.6384 - val_loss: 0.0553 - val_acc: 0.5958\n",
      "Epoch 63/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0497 - acc: 0.6387 - val_loss: 0.0561 - val_acc: 0.5951\n",
      "Epoch 64/200\n",
      "3208/3208 [==============================] - 1s 232us/step - loss: 0.0489 - acc: 0.6418 - val_loss: 0.0555 - val_acc: 0.5945\n",
      "Epoch 65/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0492 - acc: 0.6403 - val_loss: 0.0554 - val_acc: 0.6051\n",
      "Epoch 66/200\n",
      "3208/3208 [==============================] - 1s 230us/step - loss: 0.0487 - acc: 0.6412 - val_loss: 0.0551 - val_acc: 0.6089\n",
      "Epoch 67/200\n",
      "3208/3208 [==============================] - 1s 229us/step - loss: 0.0489 - acc: 0.6431 - val_loss: 0.0553 - val_acc: 0.6076\n",
      "Epoch 68/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0492 - acc: 0.6403 - val_loss: 0.0557 - val_acc: 0.5970\n",
      "Epoch 69/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0488 - acc: 0.6412 - val_loss: 0.0558 - val_acc: 0.5958\n",
      "Epoch 70/200\n",
      "3208/3208 [==============================] - 1s 196us/step - loss: 0.0487 - acc: 0.6409 - val_loss: 0.0566 - val_acc: 0.5908\n",
      "Epoch 71/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0490 - acc: 0.6412 - val_loss: 0.0560 - val_acc: 0.5970\n",
      "Epoch 72/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0485 - acc: 0.6434 - val_loss: 0.0563 - val_acc: 0.5989\n",
      "Epoch 73/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0482 - acc: 0.6506 - val_loss: 0.0565 - val_acc: 0.5883\n",
      "Epoch 74/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0483 - acc: 0.6506 - val_loss: 0.0561 - val_acc: 0.5901\n",
      "Epoch 75/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0482 - acc: 0.6531 - val_loss: 0.0569 - val_acc: 0.5901\n",
      "Epoch 76/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0484 - acc: 0.6425 - val_loss: 0.0556 - val_acc: 0.6039\n",
      "Epoch 77/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0485 - acc: 0.6415 - val_loss: 0.0558 - val_acc: 0.5964\n",
      "Epoch 78/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0489 - acc: 0.6428 - val_loss: 0.0560 - val_acc: 0.5983\n",
      "Epoch 79/200\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0482 - acc: 0.6496 - val_loss: 0.0559 - val_acc: 0.5926\n",
      "Epoch 80/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0479 - acc: 0.6534 - val_loss: 0.0559 - val_acc: 0.5983\n",
      "Epoch 81/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0484 - acc: 0.6481 - val_loss: 0.0566 - val_acc: 0.5901\n",
      "Epoch 82/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0484 - acc: 0.6496 - val_loss: 0.0580 - val_acc: 0.5789\n",
      "Epoch 83/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0483 - acc: 0.6499 - val_loss: 0.0570 - val_acc: 0.5820\n",
      "Epoch 84/200\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0482 - acc: 0.6493 - val_loss: 0.0567 - val_acc: 0.5914\n",
      "Epoch 85/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0480 - acc: 0.6474 - val_loss: 0.0571 - val_acc: 0.5883\n",
      "Epoch 86/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0485 - acc: 0.6474 - val_loss: 0.0572 - val_acc: 0.5908\n",
      "Epoch 87/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0483 - acc: 0.6468 - val_loss: 0.0567 - val_acc: 0.5989\n",
      "Epoch 88/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0480 - acc: 0.6474 - val_loss: 0.0564 - val_acc: 0.5901\n",
      "Epoch 89/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0481 - acc: 0.6434 - val_loss: 0.0565 - val_acc: 0.6007\n",
      "Epoch 90/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0479 - acc: 0.6490 - val_loss: 0.0562 - val_acc: 0.5976\n",
      "Epoch 91/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0479 - acc: 0.6481 - val_loss: 0.0578 - val_acc: 0.5864\n",
      "Epoch 92/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0477 - acc: 0.6512 - val_loss: 0.0568 - val_acc: 0.5939\n",
      "Epoch 93/200\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0484 - acc: 0.6481 - val_loss: 0.0577 - val_acc: 0.5820\n",
      "Epoch 94/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0476 - acc: 0.6571 - val_loss: 0.0570 - val_acc: 0.5889\n",
      "Epoch 95/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0479 - acc: 0.6502 - val_loss: 0.0572 - val_acc: 0.5939\n",
      "Epoch 96/200\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0478 - acc: 0.6499 - val_loss: 0.0566 - val_acc: 0.5983\n",
      "Epoch 97/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0478 - acc: 0.6518 - val_loss: 0.0570 - val_acc: 0.5908\n",
      "Epoch 98/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0479 - acc: 0.6506 - val_loss: 0.0567 - val_acc: 0.5951\n",
      "Epoch 99/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0477 - acc: 0.6521 - val_loss: 0.0570 - val_acc: 0.5914\n",
      "Epoch 100/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0477 - acc: 0.6512 - val_loss: 0.0570 - val_acc: 0.5914\n",
      "Epoch 101/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0479 - acc: 0.6481 - val_loss: 0.0571 - val_acc: 0.5876\n",
      "Epoch 102/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0478 - acc: 0.6531 - val_loss: 0.0575 - val_acc: 0.5895\n",
      "Epoch 103/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0480 - acc: 0.6534 - val_loss: 0.0589 - val_acc: 0.5702\n",
      "Epoch 104/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0480 - acc: 0.6487 - val_loss: 0.0571 - val_acc: 0.5933\n",
      "Epoch 105/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0477 - acc: 0.6565 - val_loss: 0.0571 - val_acc: 0.5889\n",
      "Epoch 106/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0477 - acc: 0.6524 - val_loss: 0.0579 - val_acc: 0.5827\n",
      "Epoch 107/200\n",
      "3208/3208 [==============================] - 1s 215us/step - loss: 0.0474 - acc: 0.6559 - val_loss: 0.0582 - val_acc: 0.5764\n",
      "Epoch 108/200\n",
      "3208/3208 [==============================] - 1s 239us/step - loss: 0.0478 - acc: 0.6484 - val_loss: 0.0569 - val_acc: 0.5939\n",
      "Epoch 109/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0479 - acc: 0.6509 - val_loss: 0.0577 - val_acc: 0.5876\n",
      "Epoch 110/200\n",
      "3208/3208 [==============================] - 1s 237us/step - loss: 0.0475 - acc: 0.6559 - val_loss: 0.0580 - val_acc: 0.5883\n",
      "Epoch 111/200\n",
      "3208/3208 [==============================] - 1s 231us/step - loss: 0.0471 - acc: 0.6546 - val_loss: 0.0584 - val_acc: 0.5777\n",
      "Epoch 112/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0479 - acc: 0.6496 - val_loss: 0.0580 - val_acc: 0.5814\n",
      "Epoch 113/200\n",
      "3208/3208 [==============================] - 1s 236us/step - loss: 0.0478 - acc: 0.6515 - val_loss: 0.0590 - val_acc: 0.5745\n",
      "Epoch 114/200\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0477 - acc: 0.6540 - val_loss: 0.0574 - val_acc: 0.5951\n",
      "Epoch 115/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0471 - acc: 0.6615 - val_loss: 0.0576 - val_acc: 0.5833\n",
      "Epoch 116/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0475 - acc: 0.6512 - val_loss: 0.0573 - val_acc: 0.5908\n",
      "Epoch 117/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0469 - acc: 0.6584 - val_loss: 0.0599 - val_acc: 0.5577\n",
      "Epoch 118/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0475 - acc: 0.6518 - val_loss: 0.0577 - val_acc: 0.5870\n",
      "Epoch 119/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0469 - acc: 0.6580 - val_loss: 0.0577 - val_acc: 0.5945\n",
      "Epoch 120/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0471 - acc: 0.6577 - val_loss: 0.0582 - val_acc: 0.5820\n",
      "Epoch 121/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0471 - acc: 0.6565 - val_loss: 0.0592 - val_acc: 0.5764\n",
      "Epoch 122/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0473 - acc: 0.6555 - val_loss: 0.0597 - val_acc: 0.5621\n",
      "Epoch 123/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0469 - acc: 0.6599 - val_loss: 0.0574 - val_acc: 0.5889\n",
      "Epoch 124/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0471 - acc: 0.6555 - val_loss: 0.0579 - val_acc: 0.5820\n",
      "Epoch 125/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0469 - acc: 0.6537 - val_loss: 0.0590 - val_acc: 0.5639\n",
      "Epoch 126/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0469 - acc: 0.6615 - val_loss: 0.0586 - val_acc: 0.5808\n",
      "Epoch 127/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0471 - acc: 0.6543 - val_loss: 0.0583 - val_acc: 0.5827\n",
      "Epoch 128/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0473 - acc: 0.6549 - val_loss: 0.0585 - val_acc: 0.5870\n",
      "Epoch 129/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0470 - acc: 0.6571 - val_loss: 0.0579 - val_acc: 0.5833\n",
      "Epoch 130/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0468 - acc: 0.6590 - val_loss: 0.0587 - val_acc: 0.5839\n",
      "Epoch 131/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0472 - acc: 0.6552 - val_loss: 0.0588 - val_acc: 0.5708\n",
      "Epoch 132/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0468 - acc: 0.6555 - val_loss: 0.0596 - val_acc: 0.5658\n",
      "Epoch 133/200\n",
      "3208/3208 [==============================] - 1s 199us/step - loss: 0.0467 - acc: 0.6605 - val_loss: 0.0590 - val_acc: 0.5745\n",
      "Epoch 134/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0468 - acc: 0.6640 - val_loss: 0.0586 - val_acc: 0.5839\n",
      "Epoch 135/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0471 - acc: 0.6521 - val_loss: 0.0590 - val_acc: 0.5827\n",
      "Epoch 136/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0471 - acc: 0.6584 - val_loss: 0.0584 - val_acc: 0.5814\n",
      "Epoch 137/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0466 - acc: 0.6637 - val_loss: 0.0590 - val_acc: 0.5858\n",
      "Epoch 138/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0469 - acc: 0.6565 - val_loss: 0.0589 - val_acc: 0.5733\n",
      "Epoch 139/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0467 - acc: 0.6559 - val_loss: 0.0586 - val_acc: 0.5808\n",
      "Epoch 140/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0470 - acc: 0.6568 - val_loss: 0.0585 - val_acc: 0.5876\n",
      "Epoch 141/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0466 - acc: 0.6546 - val_loss: 0.0584 - val_acc: 0.5883\n",
      "Epoch 142/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0467 - acc: 0.6565 - val_loss: 0.0583 - val_acc: 0.5908\n",
      "Epoch 143/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0464 - acc: 0.6621 - val_loss: 0.0586 - val_acc: 0.5752\n",
      "Epoch 144/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0470 - acc: 0.6596 - val_loss: 0.0590 - val_acc: 0.5752\n",
      "Epoch 145/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0465 - acc: 0.6593 - val_loss: 0.0603 - val_acc: 0.5565\n",
      "Epoch 146/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0465 - acc: 0.6621 - val_loss: 0.0601 - val_acc: 0.5577\n",
      "Epoch 147/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0472 - acc: 0.6568 - val_loss: 0.0587 - val_acc: 0.5833\n",
      "Epoch 148/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0466 - acc: 0.6621 - val_loss: 0.0584 - val_acc: 0.5758\n",
      "Epoch 149/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0467 - acc: 0.6618 - val_loss: 0.0590 - val_acc: 0.5721\n",
      "Epoch 150/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0467 - acc: 0.6584 - val_loss: 0.0588 - val_acc: 0.5920\n",
      "Epoch 151/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0465 - acc: 0.6605 - val_loss: 0.0590 - val_acc: 0.5802\n",
      "Epoch 152/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0464 - acc: 0.6615 - val_loss: 0.0590 - val_acc: 0.5770\n",
      "Epoch 153/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0467 - acc: 0.6587 - val_loss: 0.0591 - val_acc: 0.5845\n",
      "Epoch 154/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0467 - acc: 0.6580 - val_loss: 0.0596 - val_acc: 0.5727\n",
      "Epoch 155/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0463 - acc: 0.6633 - val_loss: 0.0604 - val_acc: 0.5702\n",
      "Epoch 156/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0467 - acc: 0.6599 - val_loss: 0.0592 - val_acc: 0.5777\n",
      "Epoch 157/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0463 - acc: 0.6633 - val_loss: 0.0586 - val_acc: 0.5895\n",
      "Epoch 158/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0466 - acc: 0.6568 - val_loss: 0.0592 - val_acc: 0.5770\n",
      "Epoch 159/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0467 - acc: 0.6599 - val_loss: 0.0600 - val_acc: 0.5608\n",
      "Epoch 160/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0460 - acc: 0.6649 - val_loss: 0.0590 - val_acc: 0.5802\n",
      "Epoch 161/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0463 - acc: 0.6627 - val_loss: 0.0588 - val_acc: 0.5876\n",
      "Epoch 162/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0461 - acc: 0.6652 - val_loss: 0.0594 - val_acc: 0.5845\n",
      "Epoch 163/200\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0462 - acc: 0.6686 - val_loss: 0.0588 - val_acc: 0.5758\n",
      "Epoch 164/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0464 - acc: 0.6587 - val_loss: 0.0597 - val_acc: 0.5664\n",
      "Epoch 165/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0463 - acc: 0.6624 - val_loss: 0.0585 - val_acc: 0.5901\n",
      "Epoch 166/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0463 - acc: 0.6624 - val_loss: 0.0602 - val_acc: 0.5646\n",
      "Epoch 167/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0467 - acc: 0.6593 - val_loss: 0.0593 - val_acc: 0.5777\n",
      "Epoch 168/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0459 - acc: 0.6683 - val_loss: 0.0590 - val_acc: 0.5764\n",
      "Epoch 169/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0463 - acc: 0.6655 - val_loss: 0.0597 - val_acc: 0.5677\n",
      "Epoch 170/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0461 - acc: 0.6671 - val_loss: 0.0593 - val_acc: 0.5802\n",
      "Epoch 171/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0459 - acc: 0.6649 - val_loss: 0.0612 - val_acc: 0.5446\n",
      "Epoch 172/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0462 - acc: 0.6655 - val_loss: 0.0594 - val_acc: 0.5789\n",
      "Epoch 173/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0459 - acc: 0.6677 - val_loss: 0.0593 - val_acc: 0.5758\n",
      "Epoch 174/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0458 - acc: 0.6677 - val_loss: 0.0585 - val_acc: 0.5895\n",
      "Epoch 175/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0461 - acc: 0.6668 - val_loss: 0.0599 - val_acc: 0.5683\n",
      "Epoch 176/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0459 - acc: 0.6680 - val_loss: 0.0591 - val_acc: 0.5795\n",
      "Epoch 177/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0460 - acc: 0.6693 - val_loss: 0.0595 - val_acc: 0.5758\n",
      "Epoch 178/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0462 - acc: 0.6646 - val_loss: 0.0597 - val_acc: 0.5714\n",
      "Epoch 179/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0466 - acc: 0.6612 - val_loss: 0.0601 - val_acc: 0.5658\n",
      "Epoch 180/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0463 - acc: 0.6658 - val_loss: 0.0612 - val_acc: 0.5558\n",
      "Epoch 181/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0465 - acc: 0.6674 - val_loss: 0.0590 - val_acc: 0.5833\n",
      "Epoch 182/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0457 - acc: 0.6705 - val_loss: 0.0602 - val_acc: 0.5721\n",
      "Epoch 183/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0460 - acc: 0.6627 - val_loss: 0.0595 - val_acc: 0.5739\n",
      "Epoch 184/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0462 - acc: 0.6640 - val_loss: 0.0586 - val_acc: 0.5820\n",
      "Epoch 185/200\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0460 - acc: 0.6643 - val_loss: 0.0596 - val_acc: 0.5783\n",
      "Epoch 186/200\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0462 - acc: 0.6652 - val_loss: 0.0594 - val_acc: 0.5845\n",
      "Epoch 187/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0457 - acc: 0.6655 - val_loss: 0.0601 - val_acc: 0.5664\n",
      "Epoch 188/200\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0460 - acc: 0.6655 - val_loss: 0.0597 - val_acc: 0.5783\n",
      "Epoch 189/200\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0459 - acc: 0.6686 - val_loss: 0.0606 - val_acc: 0.5627\n",
      "Epoch 190/200\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0457 - acc: 0.6693 - val_loss: 0.0598 - val_acc: 0.5876\n",
      "Epoch 191/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0457 - acc: 0.6652 - val_loss: 0.0597 - val_acc: 0.5745\n",
      "Epoch 192/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0461 - acc: 0.6624 - val_loss: 0.0598 - val_acc: 0.5702\n",
      "Epoch 193/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0456 - acc: 0.6674 - val_loss: 0.0597 - val_acc: 0.5714\n",
      "Epoch 194/200\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0457 - acc: 0.6696 - val_loss: 0.0595 - val_acc: 0.5833\n",
      "Epoch 195/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0457 - acc: 0.6711 - val_loss: 0.0605 - val_acc: 0.5627\n",
      "Epoch 196/200\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0454 - acc: 0.6733 - val_loss: 0.0595 - val_acc: 0.5783\n",
      "Epoch 197/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0458 - acc: 0.6665 - val_loss: 0.0598 - val_acc: 0.5721\n",
      "Epoch 198/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0456 - acc: 0.6724 - val_loss: 0.0592 - val_acc: 0.5883\n",
      "Epoch 199/200\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0458 - acc: 0.6643 - val_loss: 0.0600 - val_acc: 0.5845\n",
      "Epoch 200/200\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0462 - acc: 0.6699 - val_loss: 0.0608 - val_acc: 0.5596\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=16, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=200, validation_data=(dev_valid_x, dev_valid_y))\n",
    "  \n",
    "pred_y = nn_model.predict(valid_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result_cv = result_cv.append({'detail':'kfold=3 short model', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10489
    },
    "colab_type": "code",
    "id": "9-eMATijiQSh",
    "outputId": "6075a6eb-fe00-4c71-f95d-9b0c71b91c92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_85 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 1,754\n",
      "Trainable params: 1,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/100\n",
      "3207/3207 [==============================] - 2s 490us/step - loss: 0.0726 - acc: 0.4166 - val_loss: 0.0673 - val_acc: 0.4557\n",
      "Epoch 2/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0671 - acc: 0.4487 - val_loss: 0.0661 - val_acc: 0.4620\n",
      "Epoch 3/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0665 - acc: 0.4500 - val_loss: 0.0654 - val_acc: 0.4626\n",
      "Epoch 4/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0651 - acc: 0.4543 - val_loss: 0.0640 - val_acc: 0.4695\n",
      "Epoch 5/100\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0637 - acc: 0.4693 - val_loss: 0.0622 - val_acc: 0.4875\n",
      "Epoch 6/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0622 - acc: 0.4827 - val_loss: 0.0617 - val_acc: 0.4994\n",
      "Epoch 7/100\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0620 - acc: 0.4877 - val_loss: 0.0626 - val_acc: 0.4994\n",
      "Epoch 8/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0608 - acc: 0.5086 - val_loss: 0.0607 - val_acc: 0.5062\n",
      "Epoch 9/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0598 - acc: 0.5267 - val_loss: 0.0591 - val_acc: 0.5368\n",
      "Epoch 10/100\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0601 - acc: 0.5257 - val_loss: 0.0596 - val_acc: 0.5100\n",
      "Epoch 11/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0592 - acc: 0.5329 - val_loss: 0.0585 - val_acc: 0.5449\n",
      "Epoch 12/100\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0585 - acc: 0.5619 - val_loss: 0.0583 - val_acc: 0.5430\n",
      "Epoch 13/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0584 - acc: 0.5510 - val_loss: 0.0586 - val_acc: 0.5461\n",
      "Epoch 14/100\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0582 - acc: 0.5553 - val_loss: 0.0582 - val_acc: 0.5567\n",
      "Epoch 15/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0580 - acc: 0.5569 - val_loss: 0.0581 - val_acc: 0.5536\n",
      "Epoch 16/100\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0579 - acc: 0.5553 - val_loss: 0.0579 - val_acc: 0.5530\n",
      "Epoch 17/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0577 - acc: 0.5613 - val_loss: 0.0582 - val_acc: 0.5549\n",
      "Epoch 18/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0576 - acc: 0.5610 - val_loss: 0.0578 - val_acc: 0.5648\n",
      "Epoch 19/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0572 - acc: 0.5647 - val_loss: 0.0575 - val_acc: 0.5599\n",
      "Epoch 20/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0572 - acc: 0.5666 - val_loss: 0.0597 - val_acc: 0.5224\n",
      "Epoch 21/100\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0576 - acc: 0.5585 - val_loss: 0.0579 - val_acc: 0.5480\n",
      "Epoch 22/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0573 - acc: 0.5666 - val_loss: 0.0576 - val_acc: 0.5486\n",
      "Epoch 23/100\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0569 - acc: 0.5669 - val_loss: 0.0577 - val_acc: 0.5461\n",
      "Epoch 24/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0568 - acc: 0.5672 - val_loss: 0.0595 - val_acc: 0.5480\n",
      "Epoch 25/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0565 - acc: 0.5713 - val_loss: 0.0576 - val_acc: 0.5480\n",
      "Epoch 26/100\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0566 - acc: 0.5706 - val_loss: 0.0574 - val_acc: 0.5580\n",
      "Epoch 27/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0565 - acc: 0.5725 - val_loss: 0.0572 - val_acc: 0.5599\n",
      "Epoch 28/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0565 - acc: 0.5787 - val_loss: 0.0572 - val_acc: 0.5605\n",
      "Epoch 29/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0563 - acc: 0.5722 - val_loss: 0.0599 - val_acc: 0.5231\n",
      "Epoch 30/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0566 - acc: 0.5666 - val_loss: 0.0587 - val_acc: 0.5542\n",
      "Epoch 31/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0560 - acc: 0.5706 - val_loss: 0.0572 - val_acc: 0.5517\n",
      "Epoch 32/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0561 - acc: 0.5734 - val_loss: 0.0575 - val_acc: 0.5449\n",
      "Epoch 33/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0559 - acc: 0.5741 - val_loss: 0.0576 - val_acc: 0.5505\n",
      "Epoch 34/100\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0563 - acc: 0.5753 - val_loss: 0.0580 - val_acc: 0.5418\n",
      "Epoch 35/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0560 - acc: 0.5734 - val_loss: 0.0585 - val_acc: 0.5524\n",
      "Epoch 36/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0560 - acc: 0.5741 - val_loss: 0.0577 - val_acc: 0.5474\n",
      "Epoch 37/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0559 - acc: 0.5756 - val_loss: 0.0574 - val_acc: 0.5611\n",
      "Epoch 38/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0558 - acc: 0.5756 - val_loss: 0.0578 - val_acc: 0.5530\n",
      "Epoch 39/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0558 - acc: 0.5784 - val_loss: 0.0584 - val_acc: 0.5517\n",
      "Epoch 40/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0555 - acc: 0.5769 - val_loss: 0.0583 - val_acc: 0.5567\n",
      "Epoch 41/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0559 - acc: 0.5728 - val_loss: 0.0577 - val_acc: 0.5474\n",
      "Epoch 42/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0560 - acc: 0.5772 - val_loss: 0.0580 - val_acc: 0.5449\n",
      "Epoch 43/100\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0553 - acc: 0.5819 - val_loss: 0.0581 - val_acc: 0.5449\n",
      "Epoch 44/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0553 - acc: 0.5806 - val_loss: 0.0574 - val_acc: 0.5567\n",
      "Epoch 45/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0551 - acc: 0.5825 - val_loss: 0.0573 - val_acc: 0.5580\n",
      "Epoch 46/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0555 - acc: 0.5781 - val_loss: 0.0579 - val_acc: 0.5530\n",
      "Epoch 47/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0551 - acc: 0.5853 - val_loss: 0.0582 - val_acc: 0.5455\n",
      "Epoch 48/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0556 - acc: 0.5803 - val_loss: 0.0582 - val_acc: 0.5493\n",
      "Epoch 49/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0549 - acc: 0.5834 - val_loss: 0.0578 - val_acc: 0.5511\n",
      "Epoch 50/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0549 - acc: 0.5875 - val_loss: 0.0592 - val_acc: 0.5368\n",
      "Epoch 51/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0550 - acc: 0.5875 - val_loss: 0.0581 - val_acc: 0.5499\n",
      "Epoch 52/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0546 - acc: 0.5928 - val_loss: 0.0592 - val_acc: 0.5461\n",
      "Epoch 53/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0549 - acc: 0.5831 - val_loss: 0.0583 - val_acc: 0.5418\n",
      "Epoch 54/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0547 - acc: 0.5837 - val_loss: 0.0595 - val_acc: 0.5474\n",
      "Epoch 55/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0553 - acc: 0.5840 - val_loss: 0.0585 - val_acc: 0.5436\n",
      "Epoch 56/100\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0550 - acc: 0.5862 - val_loss: 0.0587 - val_acc: 0.5330\n",
      "Epoch 57/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0548 - acc: 0.5853 - val_loss: 0.0578 - val_acc: 0.5474\n",
      "Epoch 58/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0544 - acc: 0.5915 - val_loss: 0.0580 - val_acc: 0.5493\n",
      "Epoch 59/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0549 - acc: 0.5937 - val_loss: 0.0582 - val_acc: 0.5480\n",
      "Epoch 60/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0544 - acc: 0.5906 - val_loss: 0.0585 - val_acc: 0.5436\n",
      "Epoch 61/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0546 - acc: 0.5890 - val_loss: 0.0585 - val_acc: 0.5480\n",
      "Epoch 62/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0545 - acc: 0.5959 - val_loss: 0.0588 - val_acc: 0.5380\n",
      "Epoch 63/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0544 - acc: 0.5862 - val_loss: 0.0588 - val_acc: 0.5461\n",
      "Epoch 64/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0541 - acc: 0.5962 - val_loss: 0.0583 - val_acc: 0.5480\n",
      "Epoch 65/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0544 - acc: 0.5928 - val_loss: 0.0593 - val_acc: 0.5474\n",
      "Epoch 66/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0542 - acc: 0.5937 - val_loss: 0.0586 - val_acc: 0.5474\n",
      "Epoch 67/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0541 - acc: 0.5978 - val_loss: 0.0583 - val_acc: 0.5461\n",
      "Epoch 68/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0542 - acc: 0.5984 - val_loss: 0.0595 - val_acc: 0.5355\n",
      "Epoch 69/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0542 - acc: 0.5946 - val_loss: 0.0593 - val_acc: 0.5355\n",
      "Epoch 70/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0542 - acc: 0.5987 - val_loss: 0.0587 - val_acc: 0.5486\n",
      "Epoch 71/100\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0538 - acc: 0.5984 - val_loss: 0.0588 - val_acc: 0.5493\n",
      "Epoch 72/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0542 - acc: 0.5978 - val_loss: 0.0598 - val_acc: 0.5380\n",
      "Epoch 73/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0541 - acc: 0.5959 - val_loss: 0.0587 - val_acc: 0.5480\n",
      "Epoch 74/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0539 - acc: 0.6018 - val_loss: 0.0587 - val_acc: 0.5368\n",
      "Epoch 75/100\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0539 - acc: 0.5990 - val_loss: 0.0590 - val_acc: 0.5430\n",
      "Epoch 76/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0538 - acc: 0.6037 - val_loss: 0.0599 - val_acc: 0.5418\n",
      "Epoch 77/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0539 - acc: 0.6031 - val_loss: 0.0590 - val_acc: 0.5337\n",
      "Epoch 78/100\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0538 - acc: 0.6049 - val_loss: 0.0588 - val_acc: 0.5424\n",
      "Epoch 79/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0537 - acc: 0.6012 - val_loss: 0.0590 - val_acc: 0.5411\n",
      "Epoch 80/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0539 - acc: 0.5968 - val_loss: 0.0591 - val_acc: 0.5461\n",
      "Epoch 81/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0542 - acc: 0.6015 - val_loss: 0.0588 - val_acc: 0.5355\n",
      "Epoch 82/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0535 - acc: 0.6040 - val_loss: 0.0590 - val_acc: 0.5368\n",
      "Epoch 83/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0534 - acc: 0.5996 - val_loss: 0.0588 - val_acc: 0.5436\n",
      "Epoch 84/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0533 - acc: 0.6021 - val_loss: 0.0596 - val_acc: 0.5499\n",
      "Epoch 85/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0537 - acc: 0.6012 - val_loss: 0.0598 - val_acc: 0.5330\n",
      "Epoch 86/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0537 - acc: 0.6046 - val_loss: 0.0592 - val_acc: 0.5393\n",
      "Epoch 87/100\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0534 - acc: 0.6046 - val_loss: 0.0595 - val_acc: 0.5368\n",
      "Epoch 88/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0534 - acc: 0.6021 - val_loss: 0.0590 - val_acc: 0.5374\n",
      "Epoch 89/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0533 - acc: 0.6049 - val_loss: 0.0598 - val_acc: 0.5324\n",
      "Epoch 90/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0533 - acc: 0.6040 - val_loss: 0.0597 - val_acc: 0.5405\n",
      "Epoch 91/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0531 - acc: 0.6074 - val_loss: 0.0596 - val_acc: 0.5474\n",
      "Epoch 92/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0531 - acc: 0.6109 - val_loss: 0.0603 - val_acc: 0.5380\n",
      "Epoch 93/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0531 - acc: 0.6080 - val_loss: 0.0606 - val_acc: 0.5355\n",
      "Epoch 94/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0536 - acc: 0.6034 - val_loss: 0.0594 - val_acc: 0.5443\n",
      "Epoch 95/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0530 - acc: 0.6102 - val_loss: 0.0606 - val_acc: 0.5418\n",
      "Epoch 96/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0531 - acc: 0.6056 - val_loss: 0.0605 - val_acc: 0.5405\n",
      "Epoch 97/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0532 - acc: 0.6043 - val_loss: 0.0594 - val_acc: 0.5399\n",
      "Epoch 98/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0530 - acc: 0.6121 - val_loss: 0.0612 - val_acc: 0.5343\n",
      "Epoch 99/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0531 - acc: 0.6115 - val_loss: 0.0596 - val_acc: 0.5387\n",
      "Epoch 100/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0528 - acc: 0.6102 - val_loss: 0.0601 - val_acc: 0.5380\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/100\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0556 - acc: 0.5800 - val_loss: 0.0557 - val_acc: 0.5736\n",
      "Epoch 2/100\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0550 - acc: 0.5862 - val_loss: 0.0552 - val_acc: 0.5904\n",
      "Epoch 3/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0543 - acc: 0.5906 - val_loss: 0.0554 - val_acc: 0.5804\n",
      "Epoch 4/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0540 - acc: 0.5978 - val_loss: 0.0559 - val_acc: 0.5786\n",
      "Epoch 5/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0543 - acc: 0.5943 - val_loss: 0.0559 - val_acc: 0.5698\n",
      "Epoch 6/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0538 - acc: 0.5956 - val_loss: 0.0567 - val_acc: 0.5661\n",
      "Epoch 7/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0535 - acc: 0.6043 - val_loss: 0.0566 - val_acc: 0.5655\n",
      "Epoch 8/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0538 - acc: 0.5978 - val_loss: 0.0567 - val_acc: 0.5692\n",
      "Epoch 9/100\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0536 - acc: 0.5999 - val_loss: 0.0569 - val_acc: 0.5611\n",
      "Epoch 10/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0533 - acc: 0.6006 - val_loss: 0.0571 - val_acc: 0.5655\n",
      "Epoch 11/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0535 - acc: 0.6021 - val_loss: 0.0565 - val_acc: 0.5686\n",
      "Epoch 12/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0531 - acc: 0.6068 - val_loss: 0.0568 - val_acc: 0.5661\n",
      "Epoch 13/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0534 - acc: 0.6012 - val_loss: 0.0569 - val_acc: 0.5623\n",
      "Epoch 14/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0531 - acc: 0.6074 - val_loss: 0.0572 - val_acc: 0.5623\n",
      "Epoch 15/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0529 - acc: 0.6040 - val_loss: 0.0569 - val_acc: 0.5648\n",
      "Epoch 16/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0529 - acc: 0.6071 - val_loss: 0.0567 - val_acc: 0.5667\n",
      "Epoch 17/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0528 - acc: 0.6080 - val_loss: 0.0584 - val_acc: 0.5424\n",
      "Epoch 18/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0526 - acc: 0.6105 - val_loss: 0.0574 - val_acc: 0.5580\n",
      "Epoch 19/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0526 - acc: 0.6118 - val_loss: 0.0571 - val_acc: 0.5567\n",
      "Epoch 20/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0526 - acc: 0.6112 - val_loss: 0.0576 - val_acc: 0.5499\n",
      "Epoch 21/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0524 - acc: 0.6124 - val_loss: 0.0583 - val_acc: 0.5574\n",
      "Epoch 22/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0532 - acc: 0.5968 - val_loss: 0.0579 - val_acc: 0.5549\n",
      "Epoch 23/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0524 - acc: 0.6158 - val_loss: 0.0582 - val_acc: 0.5511\n",
      "Epoch 24/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0526 - acc: 0.6152 - val_loss: 0.0582 - val_acc: 0.5505\n",
      "Epoch 25/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0526 - acc: 0.6109 - val_loss: 0.0587 - val_acc: 0.5430\n",
      "Epoch 26/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0523 - acc: 0.6112 - val_loss: 0.0580 - val_acc: 0.5536\n",
      "Epoch 27/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0523 - acc: 0.6115 - val_loss: 0.0578 - val_acc: 0.5524\n",
      "Epoch 28/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0521 - acc: 0.6155 - val_loss: 0.0577 - val_acc: 0.5592\n",
      "Epoch 29/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0525 - acc: 0.6115 - val_loss: 0.0600 - val_acc: 0.5287\n",
      "Epoch 30/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0529 - acc: 0.6065 - val_loss: 0.0580 - val_acc: 0.5517\n",
      "Epoch 31/100\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0519 - acc: 0.6196 - val_loss: 0.0583 - val_acc: 0.5549\n",
      "Epoch 32/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0521 - acc: 0.6115 - val_loss: 0.0582 - val_acc: 0.5536\n",
      "Epoch 33/100\n",
      "3207/3207 [==============================] - 1s 207us/step - loss: 0.0517 - acc: 0.6224 - val_loss: 0.0586 - val_acc: 0.5499\n",
      "Epoch 34/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0520 - acc: 0.6193 - val_loss: 0.0588 - val_acc: 0.5511\n",
      "Epoch 35/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0518 - acc: 0.6227 - val_loss: 0.0609 - val_acc: 0.5156\n",
      "Epoch 36/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0520 - acc: 0.6215 - val_loss: 0.0600 - val_acc: 0.5393\n",
      "Epoch 37/100\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0519 - acc: 0.6140 - val_loss: 0.0599 - val_acc: 0.5387\n",
      "Epoch 38/100\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0518 - acc: 0.6190 - val_loss: 0.0594 - val_acc: 0.5461\n",
      "Epoch 39/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0517 - acc: 0.6227 - val_loss: 0.0591 - val_acc: 0.5480\n",
      "Epoch 40/100\n",
      "3207/3207 [==============================] - 1s 196us/step - loss: 0.0517 - acc: 0.6180 - val_loss: 0.0588 - val_acc: 0.5380\n",
      "Epoch 41/100\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0519 - acc: 0.6186 - val_loss: 0.0590 - val_acc: 0.5499\n",
      "Epoch 42/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0515 - acc: 0.6252 - val_loss: 0.0593 - val_acc: 0.5474\n",
      "Epoch 43/100\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0517 - acc: 0.6196 - val_loss: 0.0593 - val_acc: 0.5511\n",
      "Epoch 44/100\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0516 - acc: 0.6268 - val_loss: 0.0593 - val_acc: 0.5399\n",
      "Epoch 45/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0516 - acc: 0.6196 - val_loss: 0.0589 - val_acc: 0.5480\n",
      "Epoch 46/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0517 - acc: 0.6230 - val_loss: 0.0595 - val_acc: 0.5455\n",
      "Epoch 47/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0515 - acc: 0.6268 - val_loss: 0.0600 - val_acc: 0.5374\n",
      "Epoch 48/100\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0516 - acc: 0.6274 - val_loss: 0.0592 - val_acc: 0.5474\n",
      "Epoch 49/100\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0514 - acc: 0.6236 - val_loss: 0.0593 - val_acc: 0.5449\n",
      "Epoch 50/100\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0516 - acc: 0.6227 - val_loss: 0.0596 - val_acc: 0.5461\n",
      "Epoch 51/100\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0516 - acc: 0.6227 - val_loss: 0.0601 - val_acc: 0.5455\n",
      "Epoch 52/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0513 - acc: 0.6243 - val_loss: 0.0597 - val_acc: 0.5505\n",
      "Epoch 53/100\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0514 - acc: 0.6271 - val_loss: 0.0595 - val_acc: 0.5493\n",
      "Epoch 54/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0512 - acc: 0.6233 - val_loss: 0.0597 - val_acc: 0.5436\n",
      "Epoch 55/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0512 - acc: 0.6261 - val_loss: 0.0598 - val_acc: 0.5405\n",
      "Epoch 56/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0514 - acc: 0.6199 - val_loss: 0.0594 - val_acc: 0.5443\n",
      "Epoch 57/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0511 - acc: 0.6289 - val_loss: 0.0596 - val_acc: 0.5443\n",
      "Epoch 58/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0511 - acc: 0.6230 - val_loss: 0.0600 - val_acc: 0.5387\n",
      "Epoch 59/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0508 - acc: 0.6342 - val_loss: 0.0603 - val_acc: 0.5387\n",
      "Epoch 60/100\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0509 - acc: 0.6286 - val_loss: 0.0598 - val_acc: 0.5393\n",
      "Epoch 61/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0510 - acc: 0.6286 - val_loss: 0.0602 - val_acc: 0.5387\n",
      "Epoch 62/100\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0509 - acc: 0.6264 - val_loss: 0.0601 - val_acc: 0.5393\n",
      "Epoch 63/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0508 - acc: 0.6324 - val_loss: 0.0605 - val_acc: 0.5355\n",
      "Epoch 64/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0510 - acc: 0.6283 - val_loss: 0.0607 - val_acc: 0.5312\n",
      "Epoch 65/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0510 - acc: 0.6261 - val_loss: 0.0603 - val_acc: 0.5362\n",
      "Epoch 66/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0508 - acc: 0.6268 - val_loss: 0.0607 - val_acc: 0.5411\n",
      "Epoch 67/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0508 - acc: 0.6289 - val_loss: 0.0604 - val_acc: 0.5387\n",
      "Epoch 68/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0506 - acc: 0.6308 - val_loss: 0.0617 - val_acc: 0.5324\n",
      "Epoch 69/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0508 - acc: 0.6302 - val_loss: 0.0608 - val_acc: 0.5324\n",
      "Epoch 70/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0507 - acc: 0.6305 - val_loss: 0.0605 - val_acc: 0.5355\n",
      "Epoch 71/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0506 - acc: 0.6333 - val_loss: 0.0604 - val_acc: 0.5405\n",
      "Epoch 72/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0505 - acc: 0.6268 - val_loss: 0.0607 - val_acc: 0.5349\n",
      "Epoch 73/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0507 - acc: 0.6317 - val_loss: 0.0611 - val_acc: 0.5337\n",
      "Epoch 74/100\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0504 - acc: 0.6339 - val_loss: 0.0608 - val_acc: 0.5318\n",
      "Epoch 75/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0502 - acc: 0.6361 - val_loss: 0.0604 - val_acc: 0.5393\n",
      "Epoch 76/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0505 - acc: 0.6289 - val_loss: 0.0617 - val_acc: 0.5305\n",
      "Epoch 77/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0506 - acc: 0.6302 - val_loss: 0.0607 - val_acc: 0.5368\n",
      "Epoch 78/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0503 - acc: 0.6333 - val_loss: 0.0614 - val_acc: 0.5318\n",
      "Epoch 79/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0506 - acc: 0.6277 - val_loss: 0.0610 - val_acc: 0.5299\n",
      "Epoch 80/100\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0501 - acc: 0.6339 - val_loss: 0.0617 - val_acc: 0.5343\n",
      "Epoch 81/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0501 - acc: 0.6380 - val_loss: 0.0606 - val_acc: 0.5411\n",
      "Epoch 82/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0500 - acc: 0.6367 - val_loss: 0.0614 - val_acc: 0.5393\n",
      "Epoch 83/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0501 - acc: 0.6395 - val_loss: 0.0614 - val_acc: 0.5299\n",
      "Epoch 84/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0500 - acc: 0.6367 - val_loss: 0.0623 - val_acc: 0.5231\n",
      "Epoch 85/100\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0501 - acc: 0.6383 - val_loss: 0.0608 - val_acc: 0.5380\n",
      "Epoch 86/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0501 - acc: 0.6405 - val_loss: 0.0610 - val_acc: 0.5374\n",
      "Epoch 87/100\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0501 - acc: 0.6355 - val_loss: 0.0609 - val_acc: 0.5368\n",
      "Epoch 88/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0498 - acc: 0.6367 - val_loss: 0.0619 - val_acc: 0.5380\n",
      "Epoch 89/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0500 - acc: 0.6330 - val_loss: 0.0610 - val_acc: 0.5355\n",
      "Epoch 90/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0498 - acc: 0.6370 - val_loss: 0.0613 - val_acc: 0.5293\n",
      "Epoch 91/100\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0501 - acc: 0.6321 - val_loss: 0.0618 - val_acc: 0.5287\n",
      "Epoch 92/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0500 - acc: 0.6361 - val_loss: 0.0618 - val_acc: 0.5368\n",
      "Epoch 93/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0500 - acc: 0.6327 - val_loss: 0.0615 - val_acc: 0.5393\n",
      "Epoch 94/100\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0497 - acc: 0.6364 - val_loss: 0.0619 - val_acc: 0.5362\n",
      "Epoch 95/100\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0501 - acc: 0.6349 - val_loss: 0.0612 - val_acc: 0.5349\n",
      "Epoch 96/100\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0497 - acc: 0.6336 - val_loss: 0.0614 - val_acc: 0.5337\n",
      "Epoch 97/100\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0496 - acc: 0.6370 - val_loss: 0.0615 - val_acc: 0.5330\n",
      "Epoch 98/100\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0498 - acc: 0.6411 - val_loss: 0.0623 - val_acc: 0.5349\n",
      "Epoch 99/100\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0498 - acc: 0.6361 - val_loss: 0.0618 - val_acc: 0.5355\n",
      "Epoch 100/100\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0499 - acc: 0.6377 - val_loss: 0.0617 - val_acc: 0.5368\n",
      "Train on 3208 samples, validate on 1603 samples\n",
      "Epoch 1/100\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0561 - acc: 0.5798 - val_loss: 0.0491 - val_acc: 0.6519\n",
      "Epoch 2/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0548 - acc: 0.5916 - val_loss: 0.0512 - val_acc: 0.6338\n",
      "Epoch 3/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0550 - acc: 0.5876 - val_loss: 0.0507 - val_acc: 0.6357\n",
      "Epoch 4/100\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0544 - acc: 0.5910 - val_loss: 0.0500 - val_acc: 0.6344\n",
      "Epoch 5/100\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0542 - acc: 0.5907 - val_loss: 0.0510 - val_acc: 0.6332\n",
      "Epoch 6/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0540 - acc: 0.5985 - val_loss: 0.0507 - val_acc: 0.6388\n",
      "Epoch 7/100\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0538 - acc: 0.5991 - val_loss: 0.0521 - val_acc: 0.6095\n",
      "Epoch 8/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0535 - acc: 0.5994 - val_loss: 0.0513 - val_acc: 0.6269\n",
      "Epoch 9/100\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0540 - acc: 0.6013 - val_loss: 0.0517 - val_acc: 0.6195\n",
      "Epoch 10/100\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0534 - acc: 0.5954 - val_loss: 0.0526 - val_acc: 0.6207\n",
      "Epoch 11/100\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0534 - acc: 0.5973 - val_loss: 0.0515 - val_acc: 0.6288\n",
      "Epoch 12/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0532 - acc: 0.6060 - val_loss: 0.0515 - val_acc: 0.6301\n",
      "Epoch 13/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0532 - acc: 0.5969 - val_loss: 0.0516 - val_acc: 0.6269\n",
      "Epoch 14/100\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0534 - acc: 0.6022 - val_loss: 0.0518 - val_acc: 0.6251\n",
      "Epoch 15/100\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0529 - acc: 0.6091 - val_loss: 0.0528 - val_acc: 0.6157\n",
      "Epoch 16/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0532 - acc: 0.6038 - val_loss: 0.0527 - val_acc: 0.6170\n",
      "Epoch 17/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0527 - acc: 0.6060 - val_loss: 0.0531 - val_acc: 0.6145\n",
      "Epoch 18/100\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0526 - acc: 0.6091 - val_loss: 0.0524 - val_acc: 0.6207\n",
      "Epoch 19/100\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0525 - acc: 0.6075 - val_loss: 0.0529 - val_acc: 0.6120\n",
      "Epoch 20/100\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0524 - acc: 0.6119 - val_loss: 0.0523 - val_acc: 0.6095\n",
      "Epoch 21/100\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0524 - acc: 0.6044 - val_loss: 0.0532 - val_acc: 0.6120\n",
      "Epoch 22/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0525 - acc: 0.6097 - val_loss: 0.0535 - val_acc: 0.6032\n",
      "Epoch 23/100\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0524 - acc: 0.6072 - val_loss: 0.0546 - val_acc: 0.5933\n",
      "Epoch 24/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0520 - acc: 0.6128 - val_loss: 0.0535 - val_acc: 0.6057\n",
      "Epoch 25/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0525 - acc: 0.6097 - val_loss: 0.0529 - val_acc: 0.6095\n",
      "Epoch 26/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0523 - acc: 0.6085 - val_loss: 0.0535 - val_acc: 0.6101\n",
      "Epoch 27/100\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0523 - acc: 0.6153 - val_loss: 0.0537 - val_acc: 0.6057\n",
      "Epoch 28/100\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0518 - acc: 0.6169 - val_loss: 0.0540 - val_acc: 0.6020\n",
      "Epoch 29/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0522 - acc: 0.6125 - val_loss: 0.0538 - val_acc: 0.6032\n",
      "Epoch 30/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0518 - acc: 0.6175 - val_loss: 0.0534 - val_acc: 0.6070\n",
      "Epoch 31/100\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0521 - acc: 0.6072 - val_loss: 0.0541 - val_acc: 0.6082\n",
      "Epoch 32/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0520 - acc: 0.6135 - val_loss: 0.0540 - val_acc: 0.6020\n",
      "Epoch 33/100\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0517 - acc: 0.6160 - val_loss: 0.0544 - val_acc: 0.5958\n",
      "Epoch 34/100\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0518 - acc: 0.6153 - val_loss: 0.0544 - val_acc: 0.5970\n",
      "Epoch 35/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0515 - acc: 0.6194 - val_loss: 0.0549 - val_acc: 0.5845\n",
      "Epoch 36/100\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0518 - acc: 0.6119 - val_loss: 0.0578 - val_acc: 0.5596\n",
      "Epoch 37/100\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0515 - acc: 0.6156 - val_loss: 0.0549 - val_acc: 0.5914\n",
      "Epoch 38/100\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0514 - acc: 0.6185 - val_loss: 0.0544 - val_acc: 0.5964\n",
      "Epoch 39/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0515 - acc: 0.6219 - val_loss: 0.0542 - val_acc: 0.5995\n",
      "Epoch 40/100\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0514 - acc: 0.6156 - val_loss: 0.0541 - val_acc: 0.5939\n",
      "Epoch 41/100\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0514 - acc: 0.6213 - val_loss: 0.0546 - val_acc: 0.5939\n",
      "Epoch 42/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0515 - acc: 0.6160 - val_loss: 0.0550 - val_acc: 0.5951\n",
      "Epoch 43/100\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0514 - acc: 0.6178 - val_loss: 0.0545 - val_acc: 0.5939\n",
      "Epoch 44/100\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0511 - acc: 0.6200 - val_loss: 0.0550 - val_acc: 0.5976\n",
      "Epoch 45/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0511 - acc: 0.6191 - val_loss: 0.0546 - val_acc: 0.5964\n",
      "Epoch 46/100\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0509 - acc: 0.6219 - val_loss: 0.0551 - val_acc: 0.5939\n",
      "Epoch 47/100\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0511 - acc: 0.6206 - val_loss: 0.0548 - val_acc: 0.5964\n",
      "Epoch 48/100\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0509 - acc: 0.6266 - val_loss: 0.0550 - val_acc: 0.5964\n",
      "Epoch 49/100\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0511 - acc: 0.6203 - val_loss: 0.0553 - val_acc: 0.5926\n",
      "Epoch 50/100\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0508 - acc: 0.6225 - val_loss: 0.0552 - val_acc: 0.5964\n",
      "Epoch 51/100\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0508 - acc: 0.6241 - val_loss: 0.0556 - val_acc: 0.5839\n",
      "Epoch 52/100\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0509 - acc: 0.6228 - val_loss: 0.0557 - val_acc: 0.5945\n",
      "Epoch 53/100\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0510 - acc: 0.6166 - val_loss: 0.0550 - val_acc: 0.5976\n",
      "Epoch 54/100\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0509 - acc: 0.6250 - val_loss: 0.0553 - val_acc: 0.5870\n",
      "Epoch 55/100\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0506 - acc: 0.6244 - val_loss: 0.0555 - val_acc: 0.5883\n",
      "Epoch 56/100\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0508 - acc: 0.6269 - val_loss: 0.0554 - val_acc: 0.5914\n",
      "Epoch 57/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0506 - acc: 0.6275 - val_loss: 0.0567 - val_acc: 0.5827\n",
      "Epoch 58/100\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0505 - acc: 0.6244 - val_loss: 0.0557 - val_acc: 0.5926\n",
      "Epoch 59/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0507 - acc: 0.6262 - val_loss: 0.0571 - val_acc: 0.5827\n",
      "Epoch 60/100\n",
      "3208/3208 [==============================] - 1s 190us/step - loss: 0.0505 - acc: 0.6234 - val_loss: 0.0569 - val_acc: 0.5783\n",
      "Epoch 61/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0508 - acc: 0.6244 - val_loss: 0.0560 - val_acc: 0.5908\n",
      "Epoch 62/100\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0506 - acc: 0.6231 - val_loss: 0.0559 - val_acc: 0.5852\n",
      "Epoch 63/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0504 - acc: 0.6284 - val_loss: 0.0560 - val_acc: 0.5939\n",
      "Epoch 64/100\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0505 - acc: 0.6287 - val_loss: 0.0556 - val_acc: 0.5908\n",
      "Epoch 65/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0501 - acc: 0.6337 - val_loss: 0.0557 - val_acc: 0.5908\n",
      "Epoch 66/100\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0503 - acc: 0.6259 - val_loss: 0.0561 - val_acc: 0.5827\n",
      "Epoch 67/100\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0508 - acc: 0.6238 - val_loss: 0.0567 - val_acc: 0.5752\n",
      "Epoch 68/100\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0506 - acc: 0.6247 - val_loss: 0.0565 - val_acc: 0.5895\n",
      "Epoch 69/100\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0501 - acc: 0.6315 - val_loss: 0.0561 - val_acc: 0.5908\n",
      "Epoch 70/100\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0502 - acc: 0.6315 - val_loss: 0.0560 - val_acc: 0.5883\n",
      "Epoch 71/100\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0502 - acc: 0.6284 - val_loss: 0.0580 - val_acc: 0.5671\n",
      "Epoch 72/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0508 - acc: 0.6213 - val_loss: 0.0556 - val_acc: 0.5983\n",
      "Epoch 73/100\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0503 - acc: 0.6294 - val_loss: 0.0561 - val_acc: 0.5933\n",
      "Epoch 74/100\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0501 - acc: 0.6297 - val_loss: 0.0568 - val_acc: 0.5808\n",
      "Epoch 75/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0500 - acc: 0.6294 - val_loss: 0.0565 - val_acc: 0.5951\n",
      "Epoch 76/100\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0500 - acc: 0.6319 - val_loss: 0.0567 - val_acc: 0.5852\n",
      "Epoch 77/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0498 - acc: 0.6319 - val_loss: 0.0567 - val_acc: 0.5820\n",
      "Epoch 78/100\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0499 - acc: 0.6325 - val_loss: 0.0560 - val_acc: 0.5883\n",
      "Epoch 79/100\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0497 - acc: 0.6331 - val_loss: 0.0569 - val_acc: 0.5833\n",
      "Epoch 80/100\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0501 - acc: 0.6281 - val_loss: 0.0567 - val_acc: 0.5827\n",
      "Epoch 81/100\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0504 - acc: 0.6262 - val_loss: 0.0575 - val_acc: 0.5733\n",
      "Epoch 82/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0501 - acc: 0.6281 - val_loss: 0.0567 - val_acc: 0.5852\n",
      "Epoch 83/100\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0498 - acc: 0.6309 - val_loss: 0.0572 - val_acc: 0.5820\n",
      "Epoch 84/100\n",
      "3208/3208 [==============================] - 1s 183us/step - loss: 0.0496 - acc: 0.6328 - val_loss: 0.0567 - val_acc: 0.5852\n",
      "Epoch 85/100\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0498 - acc: 0.6344 - val_loss: 0.0569 - val_acc: 0.5858\n",
      "Epoch 86/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0498 - acc: 0.6368 - val_loss: 0.0569 - val_acc: 0.5876\n",
      "Epoch 87/100\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0498 - acc: 0.6319 - val_loss: 0.0567 - val_acc: 0.5895\n",
      "Epoch 88/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0500 - acc: 0.6291 - val_loss: 0.0579 - val_acc: 0.5721\n",
      "Epoch 89/100\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0497 - acc: 0.6350 - val_loss: 0.0576 - val_acc: 0.5827\n",
      "Epoch 90/100\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0496 - acc: 0.6272 - val_loss: 0.0567 - val_acc: 0.5908\n",
      "Epoch 91/100\n",
      "3208/3208 [==============================] - 1s 187us/step - loss: 0.0494 - acc: 0.6372 - val_loss: 0.0573 - val_acc: 0.5820\n",
      "Epoch 92/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0496 - acc: 0.6337 - val_loss: 0.0568 - val_acc: 0.5914\n",
      "Epoch 93/100\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0495 - acc: 0.6353 - val_loss: 0.0575 - val_acc: 0.5845\n",
      "Epoch 94/100\n",
      "3208/3208 [==============================] - 1s 191us/step - loss: 0.0494 - acc: 0.6384 - val_loss: 0.0572 - val_acc: 0.5876\n",
      "Epoch 95/100\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0496 - acc: 0.6347 - val_loss: 0.0573 - val_acc: 0.5876\n",
      "Epoch 96/100\n",
      "3208/3208 [==============================] - 1s 193us/step - loss: 0.0494 - acc: 0.6378 - val_loss: 0.0587 - val_acc: 0.5696\n",
      "Epoch 97/100\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0495 - acc: 0.6350 - val_loss: 0.0571 - val_acc: 0.5845\n",
      "Epoch 98/100\n",
      "3208/3208 [==============================] - 1s 189us/step - loss: 0.0495 - acc: 0.6309 - val_loss: 0.0578 - val_acc: 0.5770\n",
      "Epoch 99/100\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0495 - acc: 0.6322 - val_loss: 0.0580 - val_acc: 0.5864\n",
      "Epoch 100/100\n",
      "3208/3208 [==============================] - 1s 194us/step - loss: 0.0494 - acc: 0.6315 - val_loss: 0.0581 - val_acc: 0.5745\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=16, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=100, validation_data=(dev_valid_x, dev_valid_y))\n",
    "  \n",
    "pred_y = nn_model.predict(valid_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result_cv = result_cv.append({'detail':'kfold=3 short model less epoch', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 15555
    },
    "colab_type": "code",
    "id": "sN36KDVUjIK-",
    "outputId": "25191130-5c08-4f61-8736-13315d912086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_88 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,386\n",
      "Trainable params: 1,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/150\n",
      "3207/3207 [==============================] - 2s 480us/step - loss: 0.0721 - acc: 0.4178 - val_loss: 0.0674 - val_acc: 0.4570\n",
      "Epoch 2/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0672 - acc: 0.4524 - val_loss: 0.0664 - val_acc: 0.4589\n",
      "Epoch 3/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0664 - acc: 0.4459 - val_loss: 0.0659 - val_acc: 0.4595\n",
      "Epoch 4/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0657 - acc: 0.4602 - val_loss: 0.0651 - val_acc: 0.4589\n",
      "Epoch 5/150\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0649 - acc: 0.4596 - val_loss: 0.0645 - val_acc: 0.4557\n",
      "Epoch 6/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0642 - acc: 0.4730 - val_loss: 0.0641 - val_acc: 0.4732\n",
      "Epoch 7/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0638 - acc: 0.4805 - val_loss: 0.0635 - val_acc: 0.4657\n",
      "Epoch 8/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0631 - acc: 0.4768 - val_loss: 0.0632 - val_acc: 0.4832\n",
      "Epoch 9/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0621 - acc: 0.4952 - val_loss: 0.0621 - val_acc: 0.4825\n",
      "Epoch 10/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0616 - acc: 0.5055 - val_loss: 0.0633 - val_acc: 0.4956\n",
      "Epoch 11/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0613 - acc: 0.5129 - val_loss: 0.0614 - val_acc: 0.5037\n",
      "Epoch 12/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0607 - acc: 0.5176 - val_loss: 0.0606 - val_acc: 0.5037\n",
      "Epoch 13/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0599 - acc: 0.5276 - val_loss: 0.0631 - val_acc: 0.4944\n",
      "Epoch 14/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0596 - acc: 0.5316 - val_loss: 0.0607 - val_acc: 0.5200\n",
      "Epoch 15/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0593 - acc: 0.5366 - val_loss: 0.0599 - val_acc: 0.5200\n",
      "Epoch 16/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0589 - acc: 0.5407 - val_loss: 0.0603 - val_acc: 0.5387\n",
      "Epoch 17/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0589 - acc: 0.5460 - val_loss: 0.0605 - val_acc: 0.5118\n",
      "Epoch 18/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0585 - acc: 0.5538 - val_loss: 0.0596 - val_acc: 0.5461\n",
      "Epoch 19/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0582 - acc: 0.5603 - val_loss: 0.0600 - val_acc: 0.5380\n",
      "Epoch 20/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0579 - acc: 0.5557 - val_loss: 0.0606 - val_acc: 0.5399\n",
      "Epoch 21/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0580 - acc: 0.5572 - val_loss: 0.0586 - val_acc: 0.5567\n",
      "Epoch 22/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0580 - acc: 0.5656 - val_loss: 0.0602 - val_acc: 0.5349\n",
      "Epoch 23/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0577 - acc: 0.5647 - val_loss: 0.0589 - val_acc: 0.5474\n",
      "Epoch 24/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0571 - acc: 0.5766 - val_loss: 0.0616 - val_acc: 0.5324\n",
      "Epoch 25/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0575 - acc: 0.5722 - val_loss: 0.0588 - val_acc: 0.5430\n",
      "Epoch 26/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0573 - acc: 0.5681 - val_loss: 0.0591 - val_acc: 0.5517\n",
      "Epoch 27/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0571 - acc: 0.5747 - val_loss: 0.0590 - val_acc: 0.5586\n",
      "Epoch 28/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0569 - acc: 0.5722 - val_loss: 0.0596 - val_acc: 0.5436\n",
      "Epoch 29/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0568 - acc: 0.5787 - val_loss: 0.0591 - val_acc: 0.5524\n",
      "Epoch 30/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0569 - acc: 0.5700 - val_loss: 0.0584 - val_acc: 0.5474\n",
      "Epoch 31/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0566 - acc: 0.5769 - val_loss: 0.0588 - val_acc: 0.5499\n",
      "Epoch 32/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0564 - acc: 0.5759 - val_loss: 0.0585 - val_acc: 0.5549\n",
      "Epoch 33/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0561 - acc: 0.5831 - val_loss: 0.0585 - val_acc: 0.5599\n",
      "Epoch 34/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0561 - acc: 0.5809 - val_loss: 0.0587 - val_acc: 0.5586\n",
      "Epoch 35/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0561 - acc: 0.5800 - val_loss: 0.0592 - val_acc: 0.5555\n",
      "Epoch 36/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0559 - acc: 0.5875 - val_loss: 0.0596 - val_acc: 0.5474\n",
      "Epoch 37/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0560 - acc: 0.5803 - val_loss: 0.0592 - val_acc: 0.5530\n",
      "Epoch 38/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0558 - acc: 0.5872 - val_loss: 0.0589 - val_acc: 0.5555\n",
      "Epoch 39/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0558 - acc: 0.5825 - val_loss: 0.0588 - val_acc: 0.5536\n",
      "Epoch 40/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0557 - acc: 0.5865 - val_loss: 0.0590 - val_acc: 0.5549\n",
      "Epoch 41/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0556 - acc: 0.5906 - val_loss: 0.0589 - val_acc: 0.5536\n",
      "Epoch 42/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0554 - acc: 0.5884 - val_loss: 0.0591 - val_acc: 0.5455\n",
      "Epoch 43/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0556 - acc: 0.5856 - val_loss: 0.0603 - val_acc: 0.5399\n",
      "Epoch 44/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0554 - acc: 0.5893 - val_loss: 0.0586 - val_acc: 0.5474\n",
      "Epoch 45/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0557 - acc: 0.5878 - val_loss: 0.0587 - val_acc: 0.5599\n",
      "Epoch 46/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0553 - acc: 0.5868 - val_loss: 0.0590 - val_acc: 0.5542\n",
      "Epoch 47/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0553 - acc: 0.5900 - val_loss: 0.0588 - val_acc: 0.5555\n",
      "Epoch 48/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0553 - acc: 0.5878 - val_loss: 0.0595 - val_acc: 0.5524\n",
      "Epoch 49/150\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0554 - acc: 0.5943 - val_loss: 0.0588 - val_acc: 0.5611\n",
      "Epoch 50/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0554 - acc: 0.5887 - val_loss: 0.0587 - val_acc: 0.5505\n",
      "Epoch 51/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0550 - acc: 0.5937 - val_loss: 0.0588 - val_acc: 0.5617\n",
      "Epoch 52/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0551 - acc: 0.5946 - val_loss: 0.0591 - val_acc: 0.5499\n",
      "Epoch 53/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0555 - acc: 0.5878 - val_loss: 0.0588 - val_acc: 0.5623\n",
      "Epoch 54/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0550 - acc: 0.5890 - val_loss: 0.0597 - val_acc: 0.5461\n",
      "Epoch 55/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0551 - acc: 0.5956 - val_loss: 0.0591 - val_acc: 0.5536\n",
      "Epoch 56/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0547 - acc: 0.5965 - val_loss: 0.0595 - val_acc: 0.5555\n",
      "Epoch 57/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0548 - acc: 0.5965 - val_loss: 0.0589 - val_acc: 0.5474\n",
      "Epoch 58/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0550 - acc: 0.5946 - val_loss: 0.0600 - val_acc: 0.5443\n",
      "Epoch 59/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0548 - acc: 0.5959 - val_loss: 0.0590 - val_acc: 0.5549\n",
      "Epoch 60/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0550 - acc: 0.5931 - val_loss: 0.0587 - val_acc: 0.5648\n",
      "Epoch 61/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0548 - acc: 0.5940 - val_loss: 0.0589 - val_acc: 0.5574\n",
      "Epoch 62/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0546 - acc: 0.6015 - val_loss: 0.0590 - val_acc: 0.5530\n",
      "Epoch 63/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0545 - acc: 0.6021 - val_loss: 0.0621 - val_acc: 0.5337\n",
      "Epoch 64/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0544 - acc: 0.6012 - val_loss: 0.0594 - val_acc: 0.5549\n",
      "Epoch 65/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0545 - acc: 0.6046 - val_loss: 0.0588 - val_acc: 0.5530\n",
      "Epoch 66/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0545 - acc: 0.5965 - val_loss: 0.0593 - val_acc: 0.5574\n",
      "Epoch 67/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0545 - acc: 0.6012 - val_loss: 0.0596 - val_acc: 0.5468\n",
      "Epoch 68/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0543 - acc: 0.6015 - val_loss: 0.0591 - val_acc: 0.5536\n",
      "Epoch 69/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0542 - acc: 0.5981 - val_loss: 0.0590 - val_acc: 0.5555\n",
      "Epoch 70/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0541 - acc: 0.5987 - val_loss: 0.0593 - val_acc: 0.5493\n",
      "Epoch 71/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0541 - acc: 0.6009 - val_loss: 0.0593 - val_acc: 0.5549\n",
      "Epoch 72/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0540 - acc: 0.6027 - val_loss: 0.0596 - val_acc: 0.5542\n",
      "Epoch 73/150\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0543 - acc: 0.6031 - val_loss: 0.0589 - val_acc: 0.5524\n",
      "Epoch 74/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0540 - acc: 0.6077 - val_loss: 0.0599 - val_acc: 0.5555\n",
      "Epoch 75/150\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0538 - acc: 0.6043 - val_loss: 0.0593 - val_acc: 0.5480\n",
      "Epoch 76/150\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0537 - acc: 0.6062 - val_loss: 0.0592 - val_acc: 0.5524\n",
      "Epoch 77/150\n",
      "3207/3207 [==============================] - 1s 204us/step - loss: 0.0540 - acc: 0.6018 - val_loss: 0.0592 - val_acc: 0.5605\n",
      "Epoch 78/150\n",
      "3207/3207 [==============================] - 1s 208us/step - loss: 0.0535 - acc: 0.6090 - val_loss: 0.0593 - val_acc: 0.5549\n",
      "Epoch 79/150\n",
      "3207/3207 [==============================] - 1s 211us/step - loss: 0.0539 - acc: 0.6021 - val_loss: 0.0602 - val_acc: 0.5449\n",
      "Epoch 80/150\n",
      "3207/3207 [==============================] - 1s 207us/step - loss: 0.0538 - acc: 0.6043 - val_loss: 0.0593 - val_acc: 0.5611\n",
      "Epoch 81/150\n",
      "3207/3207 [==============================] - 1s 212us/step - loss: 0.0537 - acc: 0.6046 - val_loss: 0.0599 - val_acc: 0.5536\n",
      "Epoch 82/150\n",
      "3207/3207 [==============================] - 1s 207us/step - loss: 0.0537 - acc: 0.6049 - val_loss: 0.0596 - val_acc: 0.5530\n",
      "Epoch 83/150\n",
      "3207/3207 [==============================] - 1s 204us/step - loss: 0.0535 - acc: 0.6046 - val_loss: 0.0598 - val_acc: 0.5549\n",
      "Epoch 84/150\n",
      "3207/3207 [==============================] - 1s 209us/step - loss: 0.0536 - acc: 0.6074 - val_loss: 0.0596 - val_acc: 0.5455\n",
      "Epoch 85/150\n",
      "3207/3207 [==============================] - 1s 207us/step - loss: 0.0534 - acc: 0.6121 - val_loss: 0.0601 - val_acc: 0.5505\n",
      "Epoch 86/150\n",
      "3207/3207 [==============================] - 1s 204us/step - loss: 0.0538 - acc: 0.6052 - val_loss: 0.0592 - val_acc: 0.5480\n",
      "Epoch 87/150\n",
      "3207/3207 [==============================] - 1s 208us/step - loss: 0.0534 - acc: 0.6096 - val_loss: 0.0597 - val_acc: 0.5542\n",
      "Epoch 88/150\n",
      "3207/3207 [==============================] - 1s 208us/step - loss: 0.0533 - acc: 0.6090 - val_loss: 0.0608 - val_acc: 0.5505\n",
      "Epoch 89/150\n",
      "3207/3207 [==============================] - 1s 203us/step - loss: 0.0535 - acc: 0.6062 - val_loss: 0.0598 - val_acc: 0.5493\n",
      "Epoch 90/150\n",
      "3207/3207 [==============================] - 1s 210us/step - loss: 0.0533 - acc: 0.6112 - val_loss: 0.0599 - val_acc: 0.5561\n",
      "Epoch 91/150\n",
      "3207/3207 [==============================] - 1s 206us/step - loss: 0.0533 - acc: 0.6080 - val_loss: 0.0601 - val_acc: 0.5468\n",
      "Epoch 92/150\n",
      "3207/3207 [==============================] - 1s 172us/step - loss: 0.0532 - acc: 0.6084 - val_loss: 0.0593 - val_acc: 0.5530\n",
      "Epoch 93/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0532 - acc: 0.6099 - val_loss: 0.0597 - val_acc: 0.5499\n",
      "Epoch 94/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0533 - acc: 0.6118 - val_loss: 0.0599 - val_acc: 0.5536\n",
      "Epoch 95/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0529 - acc: 0.6174 - val_loss: 0.0602 - val_acc: 0.5555\n",
      "Epoch 96/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0530 - acc: 0.6112 - val_loss: 0.0597 - val_acc: 0.5493\n",
      "Epoch 97/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0532 - acc: 0.6049 - val_loss: 0.0598 - val_acc: 0.5517\n",
      "Epoch 98/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0532 - acc: 0.6121 - val_loss: 0.0593 - val_acc: 0.5499\n",
      "Epoch 99/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0531 - acc: 0.6146 - val_loss: 0.0614 - val_acc: 0.5486\n",
      "Epoch 100/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0530 - acc: 0.6143 - val_loss: 0.0602 - val_acc: 0.5474\n",
      "Epoch 101/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0530 - acc: 0.6146 - val_loss: 0.0595 - val_acc: 0.5542\n",
      "Epoch 102/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0531 - acc: 0.6133 - val_loss: 0.0604 - val_acc: 0.5505\n",
      "Epoch 103/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0529 - acc: 0.6093 - val_loss: 0.0598 - val_acc: 0.5555\n",
      "Epoch 104/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0529 - acc: 0.6133 - val_loss: 0.0596 - val_acc: 0.5605\n",
      "Epoch 105/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0528 - acc: 0.6137 - val_loss: 0.0599 - val_acc: 0.5480\n",
      "Epoch 106/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0531 - acc: 0.6180 - val_loss: 0.0604 - val_acc: 0.5461\n",
      "Epoch 107/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0529 - acc: 0.6174 - val_loss: 0.0601 - val_acc: 0.5505\n",
      "Epoch 108/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0527 - acc: 0.6102 - val_loss: 0.0606 - val_acc: 0.5530\n",
      "Epoch 109/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0529 - acc: 0.6112 - val_loss: 0.0605 - val_acc: 0.5480\n",
      "Epoch 110/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0528 - acc: 0.6162 - val_loss: 0.0615 - val_acc: 0.5505\n",
      "Epoch 111/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0529 - acc: 0.6118 - val_loss: 0.0600 - val_acc: 0.5493\n",
      "Epoch 112/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0525 - acc: 0.6202 - val_loss: 0.0601 - val_acc: 0.5511\n",
      "Epoch 113/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0525 - acc: 0.6165 - val_loss: 0.0609 - val_acc: 0.5443\n",
      "Epoch 114/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0528 - acc: 0.6127 - val_loss: 0.0602 - val_acc: 0.5493\n",
      "Epoch 115/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0527 - acc: 0.6140 - val_loss: 0.0603 - val_acc: 0.5480\n",
      "Epoch 116/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0525 - acc: 0.6205 - val_loss: 0.0605 - val_acc: 0.5436\n",
      "Epoch 117/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0525 - acc: 0.6146 - val_loss: 0.0607 - val_acc: 0.5468\n",
      "Epoch 118/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0523 - acc: 0.6227 - val_loss: 0.0603 - val_acc: 0.5474\n",
      "Epoch 119/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0524 - acc: 0.6171 - val_loss: 0.0603 - val_acc: 0.5480\n",
      "Epoch 120/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0527 - acc: 0.6133 - val_loss: 0.0605 - val_acc: 0.5474\n",
      "Epoch 121/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0526 - acc: 0.6177 - val_loss: 0.0603 - val_acc: 0.5443\n",
      "Epoch 122/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0522 - acc: 0.6255 - val_loss: 0.0616 - val_acc: 0.5461\n",
      "Epoch 123/150\n",
      "3207/3207 [==============================] - 1s 172us/step - loss: 0.0526 - acc: 0.6177 - val_loss: 0.0601 - val_acc: 0.5536\n",
      "Epoch 124/150\n",
      "3207/3207 [==============================] - 1s 210us/step - loss: 0.0523 - acc: 0.6224 - val_loss: 0.0613 - val_acc: 0.5461\n",
      "Epoch 125/150\n",
      "3207/3207 [==============================] - 1s 205us/step - loss: 0.0521 - acc: 0.6224 - val_loss: 0.0611 - val_acc: 0.5449\n",
      "Epoch 126/150\n",
      "3207/3207 [==============================] - 1s 208us/step - loss: 0.0526 - acc: 0.6171 - val_loss: 0.0612 - val_acc: 0.5443\n",
      "Epoch 127/150\n",
      "3207/3207 [==============================] - 1s 206us/step - loss: 0.0525 - acc: 0.6180 - val_loss: 0.0614 - val_acc: 0.5430\n",
      "Epoch 128/150\n",
      "3207/3207 [==============================] - 1s 208us/step - loss: 0.0520 - acc: 0.6227 - val_loss: 0.0607 - val_acc: 0.5499\n",
      "Epoch 129/150\n",
      "3207/3207 [==============================] - 1s 212us/step - loss: 0.0520 - acc: 0.6227 - val_loss: 0.0606 - val_acc: 0.5468\n",
      "Epoch 130/150\n",
      "3207/3207 [==============================] - 1s 204us/step - loss: 0.0521 - acc: 0.6211 - val_loss: 0.0609 - val_acc: 0.5493\n",
      "Epoch 131/150\n",
      "3207/3207 [==============================] - 1s 195us/step - loss: 0.0521 - acc: 0.6246 - val_loss: 0.0606 - val_acc: 0.5474\n",
      "Epoch 132/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0522 - acc: 0.6162 - val_loss: 0.0617 - val_acc: 0.5505\n",
      "Epoch 133/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0520 - acc: 0.6246 - val_loss: 0.0626 - val_acc: 0.5461\n",
      "Epoch 134/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0518 - acc: 0.6255 - val_loss: 0.0609 - val_acc: 0.5430\n",
      "Epoch 135/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0522 - acc: 0.6165 - val_loss: 0.0605 - val_acc: 0.5486\n",
      "Epoch 136/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0521 - acc: 0.6227 - val_loss: 0.0607 - val_acc: 0.5436\n",
      "Epoch 137/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0519 - acc: 0.6180 - val_loss: 0.0608 - val_acc: 0.5530\n",
      "Epoch 138/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0521 - acc: 0.6193 - val_loss: 0.0613 - val_acc: 0.5430\n",
      "Epoch 139/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0517 - acc: 0.6218 - val_loss: 0.0614 - val_acc: 0.5455\n",
      "Epoch 140/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0522 - acc: 0.6199 - val_loss: 0.0609 - val_acc: 0.5449\n",
      "Epoch 141/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0520 - acc: 0.6224 - val_loss: 0.0609 - val_acc: 0.5480\n",
      "Epoch 142/150\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0519 - acc: 0.6243 - val_loss: 0.0621 - val_acc: 0.5424\n",
      "Epoch 143/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0520 - acc: 0.6236 - val_loss: 0.0614 - val_acc: 0.5443\n",
      "Epoch 144/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0518 - acc: 0.6215 - val_loss: 0.0613 - val_acc: 0.5424\n",
      "Epoch 145/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0517 - acc: 0.6199 - val_loss: 0.0612 - val_acc: 0.5480\n",
      "Epoch 146/150\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0517 - acc: 0.6243 - val_loss: 0.0615 - val_acc: 0.5468\n",
      "Epoch 147/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0519 - acc: 0.6199 - val_loss: 0.0614 - val_acc: 0.5480\n",
      "Epoch 148/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0517 - acc: 0.6227 - val_loss: 0.0614 - val_acc: 0.5474\n",
      "Epoch 149/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0518 - acc: 0.6268 - val_loss: 0.0614 - val_acc: 0.5424\n",
      "Epoch 150/150\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0516 - acc: 0.6246 - val_loss: 0.0615 - val_acc: 0.5387\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0555 - acc: 0.5962 - val_loss: 0.0531 - val_acc: 0.6116\n",
      "Epoch 2/150\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0549 - acc: 0.5984 - val_loss: 0.0544 - val_acc: 0.6041\n",
      "Epoch 3/150\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0546 - acc: 0.5987 - val_loss: 0.0552 - val_acc: 0.5898\n",
      "Epoch 4/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0542 - acc: 0.6059 - val_loss: 0.0543 - val_acc: 0.5948\n",
      "Epoch 5/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0540 - acc: 0.5978 - val_loss: 0.0547 - val_acc: 0.5898\n",
      "Epoch 6/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0537 - acc: 0.6046 - val_loss: 0.0549 - val_acc: 0.5916\n",
      "Epoch 7/150\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0536 - acc: 0.6068 - val_loss: 0.0551 - val_acc: 0.5817\n",
      "Epoch 8/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0537 - acc: 0.6021 - val_loss: 0.0556 - val_acc: 0.5804\n",
      "Epoch 9/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0539 - acc: 0.6024 - val_loss: 0.0552 - val_acc: 0.5885\n",
      "Epoch 10/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0535 - acc: 0.6056 - val_loss: 0.0565 - val_acc: 0.5736\n",
      "Epoch 11/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0538 - acc: 0.5999 - val_loss: 0.0563 - val_acc: 0.5723\n",
      "Epoch 12/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0533 - acc: 0.6074 - val_loss: 0.0562 - val_acc: 0.5729\n",
      "Epoch 13/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0531 - acc: 0.6127 - val_loss: 0.0563 - val_acc: 0.5698\n",
      "Epoch 14/150\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0527 - acc: 0.6130 - val_loss: 0.0566 - val_acc: 0.5736\n",
      "Epoch 15/150\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0529 - acc: 0.6143 - val_loss: 0.0563 - val_acc: 0.5736\n",
      "Epoch 16/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0529 - acc: 0.6093 - val_loss: 0.0569 - val_acc: 0.5680\n",
      "Epoch 17/150\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0526 - acc: 0.6146 - val_loss: 0.0571 - val_acc: 0.5617\n",
      "Epoch 18/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0529 - acc: 0.6087 - val_loss: 0.0574 - val_acc: 0.5592\n",
      "Epoch 19/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0529 - acc: 0.6130 - val_loss: 0.0572 - val_acc: 0.5630\n",
      "Epoch 20/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0529 - acc: 0.6133 - val_loss: 0.0570 - val_acc: 0.5711\n",
      "Epoch 21/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0526 - acc: 0.6109 - val_loss: 0.0574 - val_acc: 0.5648\n",
      "Epoch 22/150\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0524 - acc: 0.6177 - val_loss: 0.0575 - val_acc: 0.5667\n",
      "Epoch 23/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0523 - acc: 0.6180 - val_loss: 0.0574 - val_acc: 0.5611\n",
      "Epoch 24/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0524 - acc: 0.6227 - val_loss: 0.0586 - val_acc: 0.5530\n",
      "Epoch 25/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0522 - acc: 0.6177 - val_loss: 0.0576 - val_acc: 0.5630\n",
      "Epoch 26/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0523 - acc: 0.6155 - val_loss: 0.0579 - val_acc: 0.5549\n",
      "Epoch 27/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0525 - acc: 0.6099 - val_loss: 0.0578 - val_acc: 0.5630\n",
      "Epoch 28/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0521 - acc: 0.6165 - val_loss: 0.0582 - val_acc: 0.5561\n",
      "Epoch 29/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0522 - acc: 0.6177 - val_loss: 0.0579 - val_acc: 0.5599\n",
      "Epoch 30/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0522 - acc: 0.6243 - val_loss: 0.0588 - val_acc: 0.5511\n",
      "Epoch 31/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0522 - acc: 0.6193 - val_loss: 0.0578 - val_acc: 0.5561\n",
      "Epoch 32/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0521 - acc: 0.6224 - val_loss: 0.0592 - val_acc: 0.5418\n",
      "Epoch 33/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0519 - acc: 0.6252 - val_loss: 0.0585 - val_acc: 0.5542\n",
      "Epoch 34/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0518 - acc: 0.6199 - val_loss: 0.0586 - val_acc: 0.5474\n",
      "Epoch 35/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0519 - acc: 0.6239 - val_loss: 0.0593 - val_acc: 0.5480\n",
      "Epoch 36/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0517 - acc: 0.6274 - val_loss: 0.0587 - val_acc: 0.5536\n",
      "Epoch 37/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0518 - acc: 0.6211 - val_loss: 0.0587 - val_acc: 0.5524\n",
      "Epoch 38/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0517 - acc: 0.6205 - val_loss: 0.0587 - val_acc: 0.5480\n",
      "Epoch 39/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0517 - acc: 0.6264 - val_loss: 0.0584 - val_acc: 0.5480\n",
      "Epoch 40/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0517 - acc: 0.6149 - val_loss: 0.0587 - val_acc: 0.5505\n",
      "Epoch 41/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0518 - acc: 0.6199 - val_loss: 0.0587 - val_acc: 0.5505\n",
      "Epoch 42/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0515 - acc: 0.6230 - val_loss: 0.0592 - val_acc: 0.5499\n",
      "Epoch 43/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0514 - acc: 0.6252 - val_loss: 0.0601 - val_acc: 0.5474\n",
      "Epoch 44/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0515 - acc: 0.6215 - val_loss: 0.0588 - val_acc: 0.5499\n",
      "Epoch 45/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0516 - acc: 0.6255 - val_loss: 0.0594 - val_acc: 0.5511\n",
      "Epoch 46/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0515 - acc: 0.6252 - val_loss: 0.0590 - val_acc: 0.5493\n",
      "Epoch 47/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0515 - acc: 0.6239 - val_loss: 0.0596 - val_acc: 0.5474\n",
      "Epoch 48/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0516 - acc: 0.6233 - val_loss: 0.0594 - val_acc: 0.5505\n",
      "Epoch 49/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0514 - acc: 0.6274 - val_loss: 0.0594 - val_acc: 0.5436\n",
      "Epoch 50/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0513 - acc: 0.6249 - val_loss: 0.0596 - val_acc: 0.5387\n",
      "Epoch 51/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0512 - acc: 0.6246 - val_loss: 0.0595 - val_acc: 0.5493\n",
      "Epoch 52/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0514 - acc: 0.6230 - val_loss: 0.0601 - val_acc: 0.5399\n",
      "Epoch 53/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0512 - acc: 0.6233 - val_loss: 0.0602 - val_acc: 0.5455\n",
      "Epoch 54/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0513 - acc: 0.6252 - val_loss: 0.0601 - val_acc: 0.5430\n",
      "Epoch 55/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0511 - acc: 0.6258 - val_loss: 0.0598 - val_acc: 0.5393\n",
      "Epoch 56/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0514 - acc: 0.6255 - val_loss: 0.0598 - val_acc: 0.5387\n",
      "Epoch 57/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0509 - acc: 0.6296 - val_loss: 0.0598 - val_acc: 0.5399\n",
      "Epoch 58/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0510 - acc: 0.6274 - val_loss: 0.0599 - val_acc: 0.5380\n",
      "Epoch 59/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0510 - acc: 0.6274 - val_loss: 0.0595 - val_acc: 0.5455\n",
      "Epoch 60/150\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0510 - acc: 0.6321 - val_loss: 0.0601 - val_acc: 0.5368\n",
      "Epoch 61/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0512 - acc: 0.6268 - val_loss: 0.0596 - val_acc: 0.5393\n",
      "Epoch 62/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0509 - acc: 0.6243 - val_loss: 0.0600 - val_acc: 0.5355\n",
      "Epoch 63/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0508 - acc: 0.6321 - val_loss: 0.0594 - val_acc: 0.5449\n",
      "Epoch 64/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0509 - acc: 0.6314 - val_loss: 0.0602 - val_acc: 0.5455\n",
      "Epoch 65/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0511 - acc: 0.6308 - val_loss: 0.0598 - val_acc: 0.5436\n",
      "Epoch 66/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0507 - acc: 0.6314 - val_loss: 0.0602 - val_acc: 0.5480\n",
      "Epoch 67/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0508 - acc: 0.6280 - val_loss: 0.0597 - val_acc: 0.5424\n",
      "Epoch 68/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0506 - acc: 0.6330 - val_loss: 0.0604 - val_acc: 0.5405\n",
      "Epoch 69/150\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0508 - acc: 0.6345 - val_loss: 0.0605 - val_acc: 0.5411\n",
      "Epoch 70/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0509 - acc: 0.6311 - val_loss: 0.0604 - val_acc: 0.5424\n",
      "Epoch 71/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0506 - acc: 0.6302 - val_loss: 0.0608 - val_acc: 0.5305\n",
      "Epoch 72/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0506 - acc: 0.6361 - val_loss: 0.0603 - val_acc: 0.5387\n",
      "Epoch 73/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0508 - acc: 0.6333 - val_loss: 0.0606 - val_acc: 0.5355\n",
      "Epoch 74/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0508 - acc: 0.6330 - val_loss: 0.0610 - val_acc: 0.5287\n",
      "Epoch 75/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0510 - acc: 0.6280 - val_loss: 0.0604 - val_acc: 0.5324\n",
      "Epoch 76/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0506 - acc: 0.6311 - val_loss: 0.0604 - val_acc: 0.5411\n",
      "Epoch 77/150\n",
      "3207/3207 [==============================] - 1s 156us/step - loss: 0.0506 - acc: 0.6361 - val_loss: 0.0603 - val_acc: 0.5418\n",
      "Epoch 78/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0503 - acc: 0.6330 - val_loss: 0.0604 - val_acc: 0.5368\n",
      "Epoch 79/150\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0505 - acc: 0.6392 - val_loss: 0.0606 - val_acc: 0.5424\n",
      "Epoch 80/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0507 - acc: 0.6342 - val_loss: 0.0611 - val_acc: 0.5299\n",
      "Epoch 81/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0506 - acc: 0.6358 - val_loss: 0.0610 - val_acc: 0.5281\n",
      "Epoch 82/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0505 - acc: 0.6333 - val_loss: 0.0608 - val_acc: 0.5374\n",
      "Epoch 83/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0502 - acc: 0.6392 - val_loss: 0.0614 - val_acc: 0.5324\n",
      "Epoch 84/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0503 - acc: 0.6411 - val_loss: 0.0615 - val_acc: 0.5368\n",
      "Epoch 85/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0506 - acc: 0.6380 - val_loss: 0.0615 - val_acc: 0.5305\n",
      "Epoch 86/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0501 - acc: 0.6433 - val_loss: 0.0614 - val_acc: 0.5387\n",
      "Epoch 87/150\n",
      "3207/3207 [==============================] - 1s 156us/step - loss: 0.0501 - acc: 0.6408 - val_loss: 0.0607 - val_acc: 0.5362\n",
      "Epoch 88/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0505 - acc: 0.6345 - val_loss: 0.0614 - val_acc: 0.5312\n",
      "Epoch 89/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0503 - acc: 0.6352 - val_loss: 0.0614 - val_acc: 0.5393\n",
      "Epoch 90/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0503 - acc: 0.6361 - val_loss: 0.0617 - val_acc: 0.5318\n",
      "Epoch 91/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0501 - acc: 0.6355 - val_loss: 0.0609 - val_acc: 0.5330\n",
      "Epoch 92/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0500 - acc: 0.6402 - val_loss: 0.0611 - val_acc: 0.5343\n",
      "Epoch 93/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0502 - acc: 0.6383 - val_loss: 0.0614 - val_acc: 0.5355\n",
      "Epoch 94/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0503 - acc: 0.6389 - val_loss: 0.0610 - val_acc: 0.5324\n",
      "Epoch 95/150\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0500 - acc: 0.6408 - val_loss: 0.0618 - val_acc: 0.5299\n",
      "Epoch 96/150\n",
      "3207/3207 [==============================] - 1s 157us/step - loss: 0.0505 - acc: 0.6361 - val_loss: 0.0617 - val_acc: 0.5418\n",
      "Epoch 97/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0502 - acc: 0.6389 - val_loss: 0.0618 - val_acc: 0.5305\n",
      "Epoch 98/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0502 - acc: 0.6386 - val_loss: 0.0613 - val_acc: 0.5349\n",
      "Epoch 99/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0501 - acc: 0.6402 - val_loss: 0.0614 - val_acc: 0.5343\n",
      "Epoch 100/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0499 - acc: 0.6364 - val_loss: 0.0614 - val_acc: 0.5411\n",
      "Epoch 101/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0498 - acc: 0.6380 - val_loss: 0.0616 - val_acc: 0.5337\n",
      "Epoch 102/150\n",
      "3207/3207 [==============================] - 1s 157us/step - loss: 0.0500 - acc: 0.6392 - val_loss: 0.0617 - val_acc: 0.5330\n",
      "Epoch 103/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0500 - acc: 0.6392 - val_loss: 0.0615 - val_acc: 0.5305\n",
      "Epoch 104/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0499 - acc: 0.6395 - val_loss: 0.0616 - val_acc: 0.5274\n",
      "Epoch 105/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0498 - acc: 0.6377 - val_loss: 0.0614 - val_acc: 0.5349\n",
      "Epoch 106/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0501 - acc: 0.6399 - val_loss: 0.0612 - val_acc: 0.5349\n",
      "Epoch 107/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0500 - acc: 0.6420 - val_loss: 0.0618 - val_acc: 0.5262\n",
      "Epoch 108/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0498 - acc: 0.6405 - val_loss: 0.0619 - val_acc: 0.5249\n",
      "Epoch 109/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0499 - acc: 0.6411 - val_loss: 0.0618 - val_acc: 0.5337\n",
      "Epoch 110/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0497 - acc: 0.6420 - val_loss: 0.0623 - val_acc: 0.5299\n",
      "Epoch 111/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0499 - acc: 0.6430 - val_loss: 0.0617 - val_acc: 0.5387\n",
      "Epoch 112/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0496 - acc: 0.6433 - val_loss: 0.0617 - val_acc: 0.5368\n",
      "Epoch 113/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0498 - acc: 0.6417 - val_loss: 0.0619 - val_acc: 0.5337\n",
      "Epoch 114/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0498 - acc: 0.6389 - val_loss: 0.0614 - val_acc: 0.5268\n",
      "Epoch 115/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0495 - acc: 0.6408 - val_loss: 0.0621 - val_acc: 0.5362\n",
      "Epoch 116/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0497 - acc: 0.6417 - val_loss: 0.0623 - val_acc: 0.5224\n",
      "Epoch 117/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0494 - acc: 0.6427 - val_loss: 0.0628 - val_acc: 0.5231\n",
      "Epoch 118/150\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0497 - acc: 0.6445 - val_loss: 0.0640 - val_acc: 0.5231\n",
      "Epoch 119/150\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0497 - acc: 0.6436 - val_loss: 0.0632 - val_acc: 0.5137\n",
      "Epoch 120/150\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0498 - acc: 0.6430 - val_loss: 0.0619 - val_acc: 0.5355\n",
      "Epoch 121/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0498 - acc: 0.6436 - val_loss: 0.0627 - val_acc: 0.5243\n",
      "Epoch 122/150\n",
      "3207/3207 [==============================] - 1s 156us/step - loss: 0.0496 - acc: 0.6423 - val_loss: 0.0626 - val_acc: 0.5299\n",
      "Epoch 123/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0497 - acc: 0.6452 - val_loss: 0.0628 - val_acc: 0.5218\n",
      "Epoch 124/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0494 - acc: 0.6423 - val_loss: 0.0618 - val_acc: 0.5287\n",
      "Epoch 125/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0496 - acc: 0.6430 - val_loss: 0.0631 - val_acc: 0.5224\n",
      "Epoch 126/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0496 - acc: 0.6492 - val_loss: 0.0626 - val_acc: 0.5343\n",
      "Epoch 127/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0496 - acc: 0.6417 - val_loss: 0.0621 - val_acc: 0.5349\n",
      "Epoch 128/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0493 - acc: 0.6505 - val_loss: 0.0625 - val_acc: 0.5256\n",
      "Epoch 129/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0495 - acc: 0.6433 - val_loss: 0.0626 - val_acc: 0.5268\n",
      "Epoch 130/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0494 - acc: 0.6423 - val_loss: 0.0625 - val_acc: 0.5305\n",
      "Epoch 131/150\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0494 - acc: 0.6470 - val_loss: 0.0620 - val_acc: 0.5312\n",
      "Epoch 132/150\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0492 - acc: 0.6445 - val_loss: 0.0630 - val_acc: 0.5287\n",
      "Epoch 133/150\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0492 - acc: 0.6448 - val_loss: 0.0624 - val_acc: 0.5312\n",
      "Epoch 134/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0492 - acc: 0.6489 - val_loss: 0.0633 - val_acc: 0.5249\n",
      "Epoch 135/150\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0495 - acc: 0.6417 - val_loss: 0.0629 - val_acc: 0.5243\n",
      "Epoch 136/150\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0494 - acc: 0.6467 - val_loss: 0.0635 - val_acc: 0.5224\n",
      "Epoch 137/150\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0494 - acc: 0.6467 - val_loss: 0.0628 - val_acc: 0.5262\n",
      "Epoch 138/150\n",
      "3207/3207 [==============================] - 0s 155us/step - loss: 0.0493 - acc: 0.6476 - val_loss: 0.0631 - val_acc: 0.5312\n",
      "Epoch 139/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0495 - acc: 0.6439 - val_loss: 0.0630 - val_acc: 0.5274\n",
      "Epoch 140/150\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0493 - acc: 0.6505 - val_loss: 0.0626 - val_acc: 0.5330\n",
      "Epoch 141/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0491 - acc: 0.6511 - val_loss: 0.0627 - val_acc: 0.5268\n",
      "Epoch 142/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0491 - acc: 0.6448 - val_loss: 0.0626 - val_acc: 0.5274\n",
      "Epoch 143/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0493 - acc: 0.6505 - val_loss: 0.0626 - val_acc: 0.5337\n",
      "Epoch 144/150\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0490 - acc: 0.6486 - val_loss: 0.0631 - val_acc: 0.5256\n",
      "Epoch 145/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0491 - acc: 0.6498 - val_loss: 0.0630 - val_acc: 0.5218\n",
      "Epoch 146/150\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0490 - acc: 0.6436 - val_loss: 0.0626 - val_acc: 0.5337\n",
      "Epoch 147/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0490 - acc: 0.6523 - val_loss: 0.0636 - val_acc: 0.5281\n",
      "Epoch 148/150\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0493 - acc: 0.6439 - val_loss: 0.0633 - val_acc: 0.5268\n",
      "Epoch 149/150\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0488 - acc: 0.6498 - val_loss: 0.0628 - val_acc: 0.5318\n",
      "Epoch 150/150\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0488 - acc: 0.6533 - val_loss: 0.0634 - val_acc: 0.5243\n",
      "Train on 3208 samples, validate on 1603 samples\n",
      "Epoch 1/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0567 - acc: 0.5823 - val_loss: 0.0486 - val_acc: 0.6600\n",
      "Epoch 2/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0555 - acc: 0.5895 - val_loss: 0.0485 - val_acc: 0.6631\n",
      "Epoch 3/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0552 - acc: 0.5910 - val_loss: 0.0492 - val_acc: 0.6482\n",
      "Epoch 4/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0548 - acc: 0.5888 - val_loss: 0.0507 - val_acc: 0.6363\n",
      "Epoch 5/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0543 - acc: 0.5982 - val_loss: 0.0497 - val_acc: 0.6444\n",
      "Epoch 6/150\n",
      "3208/3208 [==============================] - 0s 154us/step - loss: 0.0541 - acc: 0.6054 - val_loss: 0.0503 - val_acc: 0.6475\n",
      "Epoch 7/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0540 - acc: 0.6026 - val_loss: 0.0503 - val_acc: 0.6457\n",
      "Epoch 8/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0537 - acc: 0.6038 - val_loss: 0.0507 - val_acc: 0.6376\n",
      "Epoch 9/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0537 - acc: 0.5991 - val_loss: 0.0505 - val_acc: 0.6407\n",
      "Epoch 10/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0536 - acc: 0.6057 - val_loss: 0.0512 - val_acc: 0.6369\n",
      "Epoch 11/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0538 - acc: 0.6032 - val_loss: 0.0510 - val_acc: 0.6307\n",
      "Epoch 12/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0534 - acc: 0.6050 - val_loss: 0.0518 - val_acc: 0.6276\n",
      "Epoch 13/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0533 - acc: 0.6088 - val_loss: 0.0516 - val_acc: 0.6282\n",
      "Epoch 14/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0533 - acc: 0.6075 - val_loss: 0.0523 - val_acc: 0.6251\n",
      "Epoch 15/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0532 - acc: 0.6035 - val_loss: 0.0516 - val_acc: 0.6245\n",
      "Epoch 16/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0531 - acc: 0.6103 - val_loss: 0.0519 - val_acc: 0.6213\n",
      "Epoch 17/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0528 - acc: 0.6066 - val_loss: 0.0519 - val_acc: 0.6288\n",
      "Epoch 18/150\n",
      "3208/3208 [==============================] - 1s 157us/step - loss: 0.0530 - acc: 0.6075 - val_loss: 0.0520 - val_acc: 0.6245\n",
      "Epoch 19/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0528 - acc: 0.6132 - val_loss: 0.0524 - val_acc: 0.6232\n",
      "Epoch 20/150\n",
      "3208/3208 [==============================] - 1s 157us/step - loss: 0.0526 - acc: 0.6082 - val_loss: 0.0521 - val_acc: 0.6220\n",
      "Epoch 21/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0528 - acc: 0.6097 - val_loss: 0.0547 - val_acc: 0.5752\n",
      "Epoch 22/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0527 - acc: 0.6132 - val_loss: 0.0522 - val_acc: 0.6220\n",
      "Epoch 23/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0527 - acc: 0.6100 - val_loss: 0.0535 - val_acc: 0.6126\n",
      "Epoch 24/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0528 - acc: 0.6060 - val_loss: 0.0523 - val_acc: 0.6294\n",
      "Epoch 25/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0522 - acc: 0.6175 - val_loss: 0.0529 - val_acc: 0.6182\n",
      "Epoch 26/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0525 - acc: 0.6163 - val_loss: 0.0533 - val_acc: 0.6064\n",
      "Epoch 27/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0527 - acc: 0.6119 - val_loss: 0.0536 - val_acc: 0.6057\n",
      "Epoch 28/150\n",
      "3208/3208 [==============================] - 0s 154us/step - loss: 0.0524 - acc: 0.6132 - val_loss: 0.0543 - val_acc: 0.5945\n",
      "Epoch 29/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0523 - acc: 0.6113 - val_loss: 0.0531 - val_acc: 0.6182\n",
      "Epoch 30/150\n",
      "3208/3208 [==============================] - 0s 155us/step - loss: 0.0523 - acc: 0.6128 - val_loss: 0.0536 - val_acc: 0.6095\n",
      "Epoch 31/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0523 - acc: 0.6079 - val_loss: 0.0539 - val_acc: 0.5995\n",
      "Epoch 32/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0520 - acc: 0.6166 - val_loss: 0.0529 - val_acc: 0.6163\n",
      "Epoch 33/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0520 - acc: 0.6178 - val_loss: 0.0534 - val_acc: 0.6163\n",
      "Epoch 34/150\n",
      "3208/3208 [==============================] - 0s 155us/step - loss: 0.0523 - acc: 0.6113 - val_loss: 0.0534 - val_acc: 0.6207\n",
      "Epoch 35/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0520 - acc: 0.6141 - val_loss: 0.0540 - val_acc: 0.6114\n",
      "Epoch 36/150\n",
      "3208/3208 [==============================] - 0s 155us/step - loss: 0.0518 - acc: 0.6231 - val_loss: 0.0541 - val_acc: 0.6051\n",
      "Epoch 37/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0520 - acc: 0.6203 - val_loss: 0.0549 - val_acc: 0.5920\n",
      "Epoch 38/150\n",
      "3208/3208 [==============================] - 0s 155us/step - loss: 0.0519 - acc: 0.6234 - val_loss: 0.0538 - val_acc: 0.6120\n",
      "Epoch 39/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0518 - acc: 0.6138 - val_loss: 0.0545 - val_acc: 0.6001\n",
      "Epoch 40/150\n",
      "3208/3208 [==============================] - 0s 155us/step - loss: 0.0517 - acc: 0.6219 - val_loss: 0.0543 - val_acc: 0.6051\n",
      "Epoch 41/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0518 - acc: 0.6206 - val_loss: 0.0545 - val_acc: 0.5983\n",
      "Epoch 42/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0515 - acc: 0.6172 - val_loss: 0.0542 - val_acc: 0.6145\n",
      "Epoch 43/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0515 - acc: 0.6194 - val_loss: 0.0546 - val_acc: 0.6039\n",
      "Epoch 44/150\n",
      "3208/3208 [==============================] - 1s 156us/step - loss: 0.0516 - acc: 0.6209 - val_loss: 0.0551 - val_acc: 0.6020\n",
      "Epoch 45/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0515 - acc: 0.6203 - val_loss: 0.0556 - val_acc: 0.5845\n",
      "Epoch 46/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0517 - acc: 0.6181 - val_loss: 0.0547 - val_acc: 0.5976\n",
      "Epoch 47/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0513 - acc: 0.6194 - val_loss: 0.0561 - val_acc: 0.5783\n",
      "Epoch 48/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0514 - acc: 0.6234 - val_loss: 0.0547 - val_acc: 0.5983\n",
      "Epoch 49/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0515 - acc: 0.6197 - val_loss: 0.0548 - val_acc: 0.6089\n",
      "Epoch 50/150\n",
      "3208/3208 [==============================] - 1s 157us/step - loss: 0.0512 - acc: 0.6256 - val_loss: 0.0553 - val_acc: 0.6020\n",
      "Epoch 51/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0511 - acc: 0.6287 - val_loss: 0.0552 - val_acc: 0.6007\n",
      "Epoch 52/150\n",
      "3208/3208 [==============================] - 0s 155us/step - loss: 0.0513 - acc: 0.6262 - val_loss: 0.0548 - val_acc: 0.6032\n",
      "Epoch 53/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0513 - acc: 0.6231 - val_loss: 0.0548 - val_acc: 0.6020\n",
      "Epoch 54/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0511 - acc: 0.6291 - val_loss: 0.0554 - val_acc: 0.5889\n",
      "Epoch 55/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0513 - acc: 0.6259 - val_loss: 0.0553 - val_acc: 0.5970\n",
      "Epoch 56/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0512 - acc: 0.6209 - val_loss: 0.0558 - val_acc: 0.5889\n",
      "Epoch 57/150\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0513 - acc: 0.6259 - val_loss: 0.0554 - val_acc: 0.5951\n",
      "Epoch 58/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0508 - acc: 0.6247 - val_loss: 0.0554 - val_acc: 0.5964\n",
      "Epoch 59/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0511 - acc: 0.6244 - val_loss: 0.0557 - val_acc: 0.5933\n",
      "Epoch 60/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0509 - acc: 0.6269 - val_loss: 0.0558 - val_acc: 0.5983\n",
      "Epoch 61/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0511 - acc: 0.6219 - val_loss: 0.0554 - val_acc: 0.5933\n",
      "Epoch 62/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0507 - acc: 0.6250 - val_loss: 0.0554 - val_acc: 0.6014\n",
      "Epoch 63/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0509 - acc: 0.6253 - val_loss: 0.0553 - val_acc: 0.6020\n",
      "Epoch 64/150\n",
      "3208/3208 [==============================] - 1s 156us/step - loss: 0.0507 - acc: 0.6266 - val_loss: 0.0555 - val_acc: 0.5995\n",
      "Epoch 65/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0507 - acc: 0.6284 - val_loss: 0.0565 - val_acc: 0.5827\n",
      "Epoch 66/150\n",
      "3208/3208 [==============================] - 0s 155us/step - loss: 0.0511 - acc: 0.6238 - val_loss: 0.0554 - val_acc: 0.5958\n",
      "Epoch 67/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0506 - acc: 0.6241 - val_loss: 0.0554 - val_acc: 0.6007\n",
      "Epoch 68/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0510 - acc: 0.6287 - val_loss: 0.0570 - val_acc: 0.5889\n",
      "Epoch 69/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0508 - acc: 0.6281 - val_loss: 0.0566 - val_acc: 0.5945\n",
      "Epoch 70/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0506 - acc: 0.6269 - val_loss: 0.0566 - val_acc: 0.5808\n",
      "Epoch 71/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0505 - acc: 0.6297 - val_loss: 0.0565 - val_acc: 0.5895\n",
      "Epoch 72/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0507 - acc: 0.6275 - val_loss: 0.0571 - val_acc: 0.5671\n",
      "Epoch 73/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0507 - acc: 0.6297 - val_loss: 0.0561 - val_acc: 0.5926\n",
      "Epoch 74/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0506 - acc: 0.6294 - val_loss: 0.0559 - val_acc: 0.6032\n",
      "Epoch 75/150\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0508 - acc: 0.6259 - val_loss: 0.0565 - val_acc: 0.5926\n",
      "Epoch 76/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0505 - acc: 0.6291 - val_loss: 0.0571 - val_acc: 0.5852\n",
      "Epoch 77/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0503 - acc: 0.6225 - val_loss: 0.0574 - val_acc: 0.5752\n",
      "Epoch 78/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0505 - acc: 0.6297 - val_loss: 0.0568 - val_acc: 0.5852\n",
      "Epoch 79/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0505 - acc: 0.6334 - val_loss: 0.0562 - val_acc: 0.5933\n",
      "Epoch 80/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0507 - acc: 0.6272 - val_loss: 0.0564 - val_acc: 0.5908\n",
      "Epoch 81/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0504 - acc: 0.6337 - val_loss: 0.0572 - val_acc: 0.5733\n",
      "Epoch 82/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0505 - acc: 0.6297 - val_loss: 0.0570 - val_acc: 0.5889\n",
      "Epoch 83/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0506 - acc: 0.6294 - val_loss: 0.0566 - val_acc: 0.5945\n",
      "Epoch 84/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0503 - acc: 0.6315 - val_loss: 0.0572 - val_acc: 0.5827\n",
      "Epoch 85/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0505 - acc: 0.6291 - val_loss: 0.0574 - val_acc: 0.5721\n",
      "Epoch 86/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0502 - acc: 0.6306 - val_loss: 0.0563 - val_acc: 0.5951\n",
      "Epoch 87/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0508 - acc: 0.6266 - val_loss: 0.0559 - val_acc: 0.5939\n",
      "Epoch 88/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0506 - acc: 0.6325 - val_loss: 0.0561 - val_acc: 0.5976\n",
      "Epoch 89/150\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0503 - acc: 0.6353 - val_loss: 0.0569 - val_acc: 0.5833\n",
      "Epoch 90/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0501 - acc: 0.6322 - val_loss: 0.0565 - val_acc: 0.5914\n",
      "Epoch 91/150\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0501 - acc: 0.6275 - val_loss: 0.0567 - val_acc: 0.5876\n",
      "Epoch 92/150\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0500 - acc: 0.6353 - val_loss: 0.0580 - val_acc: 0.5708\n",
      "Epoch 93/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0503 - acc: 0.6266 - val_loss: 0.0572 - val_acc: 0.5876\n",
      "Epoch 94/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0503 - acc: 0.6297 - val_loss: 0.0594 - val_acc: 0.5502\n",
      "Epoch 95/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0507 - acc: 0.6315 - val_loss: 0.0569 - val_acc: 0.5914\n",
      "Epoch 96/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0502 - acc: 0.6322 - val_loss: 0.0575 - val_acc: 0.5833\n",
      "Epoch 97/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0503 - acc: 0.6362 - val_loss: 0.0584 - val_acc: 0.5652\n",
      "Epoch 98/150\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0501 - acc: 0.6353 - val_loss: 0.0573 - val_acc: 0.5858\n",
      "Epoch 99/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0500 - acc: 0.6340 - val_loss: 0.0574 - val_acc: 0.5820\n",
      "Epoch 100/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0499 - acc: 0.6400 - val_loss: 0.0567 - val_acc: 0.5876\n",
      "Epoch 101/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0500 - acc: 0.6350 - val_loss: 0.0569 - val_acc: 0.5839\n",
      "Epoch 102/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0503 - acc: 0.6325 - val_loss: 0.0583 - val_acc: 0.5683\n",
      "Epoch 103/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0502 - acc: 0.6325 - val_loss: 0.0575 - val_acc: 0.5802\n",
      "Epoch 104/150\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0499 - acc: 0.6350 - val_loss: 0.0576 - val_acc: 0.5889\n",
      "Epoch 105/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0501 - acc: 0.6337 - val_loss: 0.0570 - val_acc: 0.5864\n",
      "Epoch 106/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0496 - acc: 0.6365 - val_loss: 0.0576 - val_acc: 0.5845\n",
      "Epoch 107/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0499 - acc: 0.6400 - val_loss: 0.0579 - val_acc: 0.5770\n",
      "Epoch 108/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0498 - acc: 0.6390 - val_loss: 0.0581 - val_acc: 0.5721\n",
      "Epoch 109/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0498 - acc: 0.6384 - val_loss: 0.0577 - val_acc: 0.5820\n",
      "Epoch 110/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0499 - acc: 0.6403 - val_loss: 0.0581 - val_acc: 0.5770\n",
      "Epoch 111/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0495 - acc: 0.6372 - val_loss: 0.0584 - val_acc: 0.5827\n",
      "Epoch 112/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0496 - acc: 0.6362 - val_loss: 0.0573 - val_acc: 0.5901\n",
      "Epoch 113/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0498 - acc: 0.6365 - val_loss: 0.0574 - val_acc: 0.5839\n",
      "Epoch 114/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0496 - acc: 0.6387 - val_loss: 0.0574 - val_acc: 0.5839\n",
      "Epoch 115/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0499 - acc: 0.6387 - val_loss: 0.0575 - val_acc: 0.5839\n",
      "Epoch 116/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0497 - acc: 0.6368 - val_loss: 0.0578 - val_acc: 0.5883\n",
      "Epoch 117/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0498 - acc: 0.6362 - val_loss: 0.0583 - val_acc: 0.5808\n",
      "Epoch 118/150\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0496 - acc: 0.6368 - val_loss: 0.0586 - val_acc: 0.5696\n",
      "Epoch 119/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0494 - acc: 0.6390 - val_loss: 0.0575 - val_acc: 0.5852\n",
      "Epoch 120/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0495 - acc: 0.6400 - val_loss: 0.0580 - val_acc: 0.5802\n",
      "Epoch 121/150\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0498 - acc: 0.6368 - val_loss: 0.0575 - val_acc: 0.5883\n",
      "Epoch 122/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0495 - acc: 0.6450 - val_loss: 0.0583 - val_acc: 0.5814\n",
      "Epoch 123/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0496 - acc: 0.6462 - val_loss: 0.0580 - val_acc: 0.5795\n",
      "Epoch 124/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0499 - acc: 0.6359 - val_loss: 0.0583 - val_acc: 0.5839\n",
      "Epoch 125/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0495 - acc: 0.6462 - val_loss: 0.0582 - val_acc: 0.5802\n",
      "Epoch 126/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0494 - acc: 0.6421 - val_loss: 0.0578 - val_acc: 0.5777\n",
      "Epoch 127/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0494 - acc: 0.6409 - val_loss: 0.0587 - val_acc: 0.5739\n",
      "Epoch 128/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0496 - acc: 0.6344 - val_loss: 0.0581 - val_acc: 0.5808\n",
      "Epoch 129/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0495 - acc: 0.6387 - val_loss: 0.0578 - val_acc: 0.5795\n",
      "Epoch 130/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0494 - acc: 0.6400 - val_loss: 0.0577 - val_acc: 0.5758\n",
      "Epoch 131/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0494 - acc: 0.6412 - val_loss: 0.0579 - val_acc: 0.5833\n",
      "Epoch 132/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0493 - acc: 0.6387 - val_loss: 0.0579 - val_acc: 0.5845\n",
      "Epoch 133/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0497 - acc: 0.6425 - val_loss: 0.0580 - val_acc: 0.5852\n",
      "Epoch 134/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0496 - acc: 0.6471 - val_loss: 0.0588 - val_acc: 0.5696\n",
      "Epoch 135/150\n",
      "3208/3208 [==============================] - 1s 156us/step - loss: 0.0495 - acc: 0.6403 - val_loss: 0.0589 - val_acc: 0.5827\n",
      "Epoch 136/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0492 - acc: 0.6481 - val_loss: 0.0590 - val_acc: 0.5708\n",
      "Epoch 137/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0492 - acc: 0.6450 - val_loss: 0.0583 - val_acc: 0.5770\n",
      "Epoch 138/150\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0494 - acc: 0.6403 - val_loss: 0.0590 - val_acc: 0.5689\n",
      "Epoch 139/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0490 - acc: 0.6465 - val_loss: 0.0583 - val_acc: 0.5845\n",
      "Epoch 140/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0493 - acc: 0.6434 - val_loss: 0.0586 - val_acc: 0.5820\n",
      "Epoch 141/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0492 - acc: 0.6406 - val_loss: 0.0587 - val_acc: 0.5714\n",
      "Epoch 142/150\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0496 - acc: 0.6397 - val_loss: 0.0583 - val_acc: 0.5783\n",
      "Epoch 143/150\n",
      "3208/3208 [==============================] - 1s 156us/step - loss: 0.0491 - acc: 0.6434 - val_loss: 0.0584 - val_acc: 0.5839\n",
      "Epoch 144/150\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0490 - acc: 0.6425 - val_loss: 0.0584 - val_acc: 0.5852\n",
      "Epoch 145/150\n",
      "3208/3208 [==============================] - 1s 157us/step - loss: 0.0491 - acc: 0.6450 - val_loss: 0.0590 - val_acc: 0.5714\n",
      "Epoch 146/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0491 - acc: 0.6450 - val_loss: 0.0583 - val_acc: 0.5839\n",
      "Epoch 147/150\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0494 - acc: 0.6499 - val_loss: 0.0588 - val_acc: 0.5758\n",
      "Epoch 148/150\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0492 - acc: 0.6468 - val_loss: 0.0584 - val_acc: 0.5852\n",
      "Epoch 149/150\n",
      "3208/3208 [==============================] - 1s 158us/step - loss: 0.0489 - acc: 0.6443 - val_loss: 0.0592 - val_acc: 0.5721\n",
      "Epoch 150/150\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0494 - acc: 0.6421 - val_loss: 0.0590 - val_acc: 0.5820\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=150, validation_data=(dev_valid_x, dev_valid_y))\n",
    "  \n",
    "pred_y = nn_model.predict(valid_x)  \n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result_cv = result_cv.append({'detail':'kfold=3 single layer', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 20655
    },
    "colab_type": "code",
    "id": "chw_-hdjlvcD",
    "outputId": "840b32ec-985c-42a4-8f0a-c5cc7b05d246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_90 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,386\n",
      "Trainable params: 1,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 2s 487us/step - loss: 0.0725 - acc: 0.4082 - val_loss: 0.0675 - val_acc: 0.4557\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0671 - acc: 0.4503 - val_loss: 0.0677 - val_acc: 0.4732\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0664 - acc: 0.4518 - val_loss: 0.0659 - val_acc: 0.4645\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0656 - acc: 0.4549 - val_loss: 0.0653 - val_acc: 0.4638\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0651 - acc: 0.4624 - val_loss: 0.0648 - val_acc: 0.4607\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0647 - acc: 0.4609 - val_loss: 0.0646 - val_acc: 0.4701\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0645 - acc: 0.4637 - val_loss: 0.0646 - val_acc: 0.4726\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0641 - acc: 0.4674 - val_loss: 0.0643 - val_acc: 0.4744\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0638 - acc: 0.4743 - val_loss: 0.0639 - val_acc: 0.4695\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0636 - acc: 0.4749 - val_loss: 0.0639 - val_acc: 0.4701\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0633 - acc: 0.4746 - val_loss: 0.0635 - val_acc: 0.4701\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0631 - acc: 0.4808 - val_loss: 0.0635 - val_acc: 0.4732\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0628 - acc: 0.4793 - val_loss: 0.0635 - val_acc: 0.4732\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0626 - acc: 0.4864 - val_loss: 0.0633 - val_acc: 0.4763\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0624 - acc: 0.4849 - val_loss: 0.0633 - val_acc: 0.4794\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0622 - acc: 0.4892 - val_loss: 0.0631 - val_acc: 0.4938\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0620 - acc: 0.4992 - val_loss: 0.0628 - val_acc: 0.4857\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0617 - acc: 0.4958 - val_loss: 0.0635 - val_acc: 0.4944\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0616 - acc: 0.4970 - val_loss: 0.0627 - val_acc: 0.4969\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0614 - acc: 0.5058 - val_loss: 0.0624 - val_acc: 0.4913\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0613 - acc: 0.5111 - val_loss: 0.0622 - val_acc: 0.4857\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0611 - acc: 0.5067 - val_loss: 0.0621 - val_acc: 0.4869\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0610 - acc: 0.5104 - val_loss: 0.0621 - val_acc: 0.4850\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0608 - acc: 0.5070 - val_loss: 0.0621 - val_acc: 0.4981\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0607 - acc: 0.5164 - val_loss: 0.0618 - val_acc: 0.4900\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0606 - acc: 0.5170 - val_loss: 0.0619 - val_acc: 0.4988\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0605 - acc: 0.5257 - val_loss: 0.0616 - val_acc: 0.4963\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0604 - acc: 0.5151 - val_loss: 0.0617 - val_acc: 0.4994\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0603 - acc: 0.5223 - val_loss: 0.0614 - val_acc: 0.4906\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0602 - acc: 0.5267 - val_loss: 0.0614 - val_acc: 0.4919\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0601 - acc: 0.5229 - val_loss: 0.0614 - val_acc: 0.5000\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0600 - acc: 0.5295 - val_loss: 0.0612 - val_acc: 0.4882\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0599 - acc: 0.5310 - val_loss: 0.0611 - val_acc: 0.4919\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0599 - acc: 0.5257 - val_loss: 0.0613 - val_acc: 0.5012\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0598 - acc: 0.5385 - val_loss: 0.0611 - val_acc: 0.5006\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0597 - acc: 0.5348 - val_loss: 0.0612 - val_acc: 0.5050\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0597 - acc: 0.5323 - val_loss: 0.0612 - val_acc: 0.5081\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0596 - acc: 0.5376 - val_loss: 0.0610 - val_acc: 0.5106\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0595 - acc: 0.5348 - val_loss: 0.0610 - val_acc: 0.5118\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0595 - acc: 0.5401 - val_loss: 0.0609 - val_acc: 0.5056\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0594 - acc: 0.5407 - val_loss: 0.0611 - val_acc: 0.5112\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0593 - acc: 0.5460 - val_loss: 0.0608 - val_acc: 0.5025\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0593 - acc: 0.5419 - val_loss: 0.0608 - val_acc: 0.5056\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0593 - acc: 0.5482 - val_loss: 0.0607 - val_acc: 0.5112\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0592 - acc: 0.5460 - val_loss: 0.0607 - val_acc: 0.5125\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0592 - acc: 0.5394 - val_loss: 0.0608 - val_acc: 0.5193\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0591 - acc: 0.5419 - val_loss: 0.0611 - val_acc: 0.5143\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0591 - acc: 0.5463 - val_loss: 0.0607 - val_acc: 0.5200\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0590 - acc: 0.5447 - val_loss: 0.0607 - val_acc: 0.5224\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0590 - acc: 0.5444 - val_loss: 0.0607 - val_acc: 0.5181\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0590 - acc: 0.5488 - val_loss: 0.0607 - val_acc: 0.5206\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0589 - acc: 0.5497 - val_loss: 0.0605 - val_acc: 0.5143\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0588 - acc: 0.5491 - val_loss: 0.0607 - val_acc: 0.5237\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0589 - acc: 0.5510 - val_loss: 0.0605 - val_acc: 0.5262\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0588 - acc: 0.5535 - val_loss: 0.0605 - val_acc: 0.5200\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0588 - acc: 0.5488 - val_loss: 0.0604 - val_acc: 0.5299\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0587 - acc: 0.5538 - val_loss: 0.0604 - val_acc: 0.5256\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0587 - acc: 0.5544 - val_loss: 0.0604 - val_acc: 0.5256\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0587 - acc: 0.5557 - val_loss: 0.0604 - val_acc: 0.5293\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0586 - acc: 0.5560 - val_loss: 0.0604 - val_acc: 0.5281\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0586 - acc: 0.5532 - val_loss: 0.0607 - val_acc: 0.5231\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0586 - acc: 0.5563 - val_loss: 0.0603 - val_acc: 0.5287\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0586 - acc: 0.5541 - val_loss: 0.0604 - val_acc: 0.5337\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0585 - acc: 0.5529 - val_loss: 0.0606 - val_acc: 0.5274\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0585 - acc: 0.5594 - val_loss: 0.0603 - val_acc: 0.5293\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0585 - acc: 0.5563 - val_loss: 0.0603 - val_acc: 0.5324\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0584 - acc: 0.5563 - val_loss: 0.0602 - val_acc: 0.5343\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0585 - acc: 0.5578 - val_loss: 0.0604 - val_acc: 0.5343\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0584 - acc: 0.5575 - val_loss: 0.0603 - val_acc: 0.5293\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0584 - acc: 0.5550 - val_loss: 0.0604 - val_acc: 0.5305\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0583 - acc: 0.5591 - val_loss: 0.0601 - val_acc: 0.5330\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0583 - acc: 0.5631 - val_loss: 0.0602 - val_acc: 0.5330\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0583 - acc: 0.5585 - val_loss: 0.0599 - val_acc: 0.5312\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0582 - acc: 0.5619 - val_loss: 0.0600 - val_acc: 0.5330\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0582 - acc: 0.5628 - val_loss: 0.0599 - val_acc: 0.5343\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0582 - acc: 0.5578 - val_loss: 0.0598 - val_acc: 0.5374\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0581 - acc: 0.5622 - val_loss: 0.0601 - val_acc: 0.5343\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0581 - acc: 0.5659 - val_loss: 0.0598 - val_acc: 0.5318\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0581 - acc: 0.5603 - val_loss: 0.0599 - val_acc: 0.5330\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0580 - acc: 0.5635 - val_loss: 0.0598 - val_acc: 0.5374\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0580 - acc: 0.5603 - val_loss: 0.0598 - val_acc: 0.5362\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0580 - acc: 0.5650 - val_loss: 0.0597 - val_acc: 0.5405\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0580 - acc: 0.5641 - val_loss: 0.0598 - val_acc: 0.5374\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0579 - acc: 0.5603 - val_loss: 0.0598 - val_acc: 0.5399\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0579 - acc: 0.5650 - val_loss: 0.0597 - val_acc: 0.5443\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0579 - acc: 0.5635 - val_loss: 0.0597 - val_acc: 0.5362\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0579 - acc: 0.5650 - val_loss: 0.0597 - val_acc: 0.5393\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0579 - acc: 0.5628 - val_loss: 0.0598 - val_acc: 0.5355\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0578 - acc: 0.5635 - val_loss: 0.0599 - val_acc: 0.5399\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0578 - acc: 0.5678 - val_loss: 0.0597 - val_acc: 0.5374\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0578 - acc: 0.5669 - val_loss: 0.0595 - val_acc: 0.5399\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0578 - acc: 0.5688 - val_loss: 0.0597 - val_acc: 0.5387\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0577 - acc: 0.5650 - val_loss: 0.0597 - val_acc: 0.5393\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0577 - acc: 0.5697 - val_loss: 0.0596 - val_acc: 0.5399\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0577 - acc: 0.5678 - val_loss: 0.0595 - val_acc: 0.5411\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0577 - acc: 0.5713 - val_loss: 0.0596 - val_acc: 0.5393\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0577 - acc: 0.5650 - val_loss: 0.0596 - val_acc: 0.5380\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0576 - acc: 0.5684 - val_loss: 0.0595 - val_acc: 0.5411\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0576 - acc: 0.5700 - val_loss: 0.0595 - val_acc: 0.5418\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0576 - acc: 0.5666 - val_loss: 0.0595 - val_acc: 0.5399\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0576 - acc: 0.5697 - val_loss: 0.0595 - val_acc: 0.5399\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0576 - acc: 0.5728 - val_loss: 0.0594 - val_acc: 0.5436\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0576 - acc: 0.5681 - val_loss: 0.0594 - val_acc: 0.5436\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0576 - acc: 0.5688 - val_loss: 0.0595 - val_acc: 0.5405\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0576 - acc: 0.5700 - val_loss: 0.0595 - val_acc: 0.5411\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0575 - acc: 0.5719 - val_loss: 0.0594 - val_acc: 0.5424\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0576 - acc: 0.5694 - val_loss: 0.0594 - val_acc: 0.5436\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0575 - acc: 0.5716 - val_loss: 0.0595 - val_acc: 0.5430\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0575 - acc: 0.5659 - val_loss: 0.0595 - val_acc: 0.5405\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0575 - acc: 0.5762 - val_loss: 0.0594 - val_acc: 0.5424\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0575 - acc: 0.5694 - val_loss: 0.0594 - val_acc: 0.5436\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0575 - acc: 0.5703 - val_loss: 0.0594 - val_acc: 0.5443\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0575 - acc: 0.5719 - val_loss: 0.0595 - val_acc: 0.5436\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0574 - acc: 0.5706 - val_loss: 0.0595 - val_acc: 0.5393\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0574 - acc: 0.5697 - val_loss: 0.0595 - val_acc: 0.5418\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0574 - acc: 0.5741 - val_loss: 0.0593 - val_acc: 0.5455\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0574 - acc: 0.5731 - val_loss: 0.0594 - val_acc: 0.5461\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0574 - acc: 0.5731 - val_loss: 0.0594 - val_acc: 0.5424\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0574 - acc: 0.5734 - val_loss: 0.0593 - val_acc: 0.5436\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0574 - acc: 0.5753 - val_loss: 0.0594 - val_acc: 0.5411\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0574 - acc: 0.5716 - val_loss: 0.0594 - val_acc: 0.5399\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0573 - acc: 0.5731 - val_loss: 0.0594 - val_acc: 0.5411\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0573 - acc: 0.5737 - val_loss: 0.0594 - val_acc: 0.5411\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0573 - acc: 0.5753 - val_loss: 0.0593 - val_acc: 0.5443\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0573 - acc: 0.5731 - val_loss: 0.0594 - val_acc: 0.5443\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0573 - acc: 0.5728 - val_loss: 0.0593 - val_acc: 0.5443\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0573 - acc: 0.5762 - val_loss: 0.0593 - val_acc: 0.5443\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0573 - acc: 0.5737 - val_loss: 0.0593 - val_acc: 0.5424\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0573 - acc: 0.5725 - val_loss: 0.0593 - val_acc: 0.5443\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0573 - acc: 0.5762 - val_loss: 0.0593 - val_acc: 0.5436\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0572 - acc: 0.5737 - val_loss: 0.0592 - val_acc: 0.5449\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0572 - acc: 0.5741 - val_loss: 0.0594 - val_acc: 0.5455\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0572 - acc: 0.5731 - val_loss: 0.0593 - val_acc: 0.5449\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0572 - acc: 0.5766 - val_loss: 0.0593 - val_acc: 0.5449\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0572 - acc: 0.5744 - val_loss: 0.0593 - val_acc: 0.5455\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0572 - acc: 0.5778 - val_loss: 0.0593 - val_acc: 0.5461\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0572 - acc: 0.5753 - val_loss: 0.0592 - val_acc: 0.5461\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0572 - acc: 0.5766 - val_loss: 0.0592 - val_acc: 0.5486\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0572 - acc: 0.5734 - val_loss: 0.0593 - val_acc: 0.5468\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0572 - acc: 0.5759 - val_loss: 0.0592 - val_acc: 0.5474\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0571 - acc: 0.5781 - val_loss: 0.0592 - val_acc: 0.5474\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0572 - acc: 0.5781 - val_loss: 0.0592 - val_acc: 0.5468\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0571 - acc: 0.5762 - val_loss: 0.0592 - val_acc: 0.5443\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0571 - acc: 0.5781 - val_loss: 0.0592 - val_acc: 0.5468\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0571 - acc: 0.5753 - val_loss: 0.0592 - val_acc: 0.5461\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0571 - acc: 0.5772 - val_loss: 0.0591 - val_acc: 0.5474\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0571 - acc: 0.5769 - val_loss: 0.0592 - val_acc: 0.5449\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0571 - acc: 0.5778 - val_loss: 0.0591 - val_acc: 0.5486\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0571 - acc: 0.5775 - val_loss: 0.0592 - val_acc: 0.5461\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0571 - acc: 0.5781 - val_loss: 0.0592 - val_acc: 0.5455\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0571 - acc: 0.5769 - val_loss: 0.0591 - val_acc: 0.5461\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0571 - acc: 0.5766 - val_loss: 0.0592 - val_acc: 0.5449\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0571 - acc: 0.5772 - val_loss: 0.0591 - val_acc: 0.5480\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0571 - acc: 0.5772 - val_loss: 0.0591 - val_acc: 0.5474\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0570 - acc: 0.5769 - val_loss: 0.0591 - val_acc: 0.5455\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0570 - acc: 0.5784 - val_loss: 0.0591 - val_acc: 0.5455\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0570 - acc: 0.5737 - val_loss: 0.0591 - val_acc: 0.5461\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0570 - acc: 0.5756 - val_loss: 0.0591 - val_acc: 0.5480\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0570 - acc: 0.5778 - val_loss: 0.0592 - val_acc: 0.5505\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0570 - acc: 0.5759 - val_loss: 0.0591 - val_acc: 0.5486\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0570 - acc: 0.5781 - val_loss: 0.0591 - val_acc: 0.5486\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0570 - acc: 0.5775 - val_loss: 0.0592 - val_acc: 0.5505\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0570 - acc: 0.5790 - val_loss: 0.0591 - val_acc: 0.5505\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0570 - acc: 0.5781 - val_loss: 0.0591 - val_acc: 0.5505\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0570 - acc: 0.5781 - val_loss: 0.0592 - val_acc: 0.5486\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0570 - acc: 0.5775 - val_loss: 0.0591 - val_acc: 0.5493\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0570 - acc: 0.5769 - val_loss: 0.0591 - val_acc: 0.5486\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0570 - acc: 0.5778 - val_loss: 0.0592 - val_acc: 0.5511\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0569 - acc: 0.5772 - val_loss: 0.0591 - val_acc: 0.5474\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0569 - acc: 0.5784 - val_loss: 0.0591 - val_acc: 0.5486\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5790 - val_loss: 0.0590 - val_acc: 0.5480\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0569 - acc: 0.5787 - val_loss: 0.0590 - val_acc: 0.5493\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5790 - val_loss: 0.0591 - val_acc: 0.5493\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0569 - acc: 0.5778 - val_loss: 0.0590 - val_acc: 0.5461\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5781 - val_loss: 0.0590 - val_acc: 0.5455\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0569 - acc: 0.5797 - val_loss: 0.0590 - val_acc: 0.5493\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5794 - val_loss: 0.0590 - val_acc: 0.5486\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0569 - acc: 0.5787 - val_loss: 0.0590 - val_acc: 0.5493\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0569 - acc: 0.5797 - val_loss: 0.0591 - val_acc: 0.5511\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0569 - acc: 0.5772 - val_loss: 0.0591 - val_acc: 0.5499\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0569 - acc: 0.5784 - val_loss: 0.0591 - val_acc: 0.5517\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0569 - acc: 0.5787 - val_loss: 0.0590 - val_acc: 0.5505\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5790 - val_loss: 0.0590 - val_acc: 0.5480\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 158us/step - loss: 0.0569 - acc: 0.5797 - val_loss: 0.0590 - val_acc: 0.5524\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0568 - acc: 0.5803 - val_loss: 0.0590 - val_acc: 0.5511\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0568 - acc: 0.5790 - val_loss: 0.0590 - val_acc: 0.5493\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0568 - acc: 0.5800 - val_loss: 0.0591 - val_acc: 0.5493\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0568 - acc: 0.5781 - val_loss: 0.0590 - val_acc: 0.5530\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0568 - acc: 0.5790 - val_loss: 0.0590 - val_acc: 0.5480\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0568 - acc: 0.5800 - val_loss: 0.0589 - val_acc: 0.5486\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0568 - acc: 0.5778 - val_loss: 0.0590 - val_acc: 0.5505\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0568 - acc: 0.5797 - val_loss: 0.0590 - val_acc: 0.5499\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0568 - acc: 0.5790 - val_loss: 0.0590 - val_acc: 0.5511\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0568 - acc: 0.5806 - val_loss: 0.0591 - val_acc: 0.5499\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0568 - acc: 0.5800 - val_loss: 0.0590 - val_acc: 0.5480\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0568 - acc: 0.5806 - val_loss: 0.0590 - val_acc: 0.5499\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0568 - acc: 0.5809 - val_loss: 0.0590 - val_acc: 0.5511\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0568 - acc: 0.5803 - val_loss: 0.0590 - val_acc: 0.5511\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0568 - acc: 0.5809 - val_loss: 0.0590 - val_acc: 0.5524\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0568 - acc: 0.5803 - val_loss: 0.0590 - val_acc: 0.5511\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0572 - acc: 0.5719 - val_loss: 0.0581 - val_acc: 0.5630\n",
      "Epoch 2/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0572 - acc: 0.5722 - val_loss: 0.0582 - val_acc: 0.5630\n",
      "Epoch 3/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0571 - acc: 0.5741 - val_loss: 0.0581 - val_acc: 0.5605\n",
      "Epoch 4/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0571 - acc: 0.5713 - val_loss: 0.0582 - val_acc: 0.5617\n",
      "Epoch 5/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0571 - acc: 0.5741 - val_loss: 0.0581 - val_acc: 0.5617\n",
      "Epoch 6/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0571 - acc: 0.5734 - val_loss: 0.0582 - val_acc: 0.5605\n",
      "Epoch 7/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0570 - acc: 0.5741 - val_loss: 0.0582 - val_acc: 0.5592\n",
      "Epoch 8/200\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0570 - acc: 0.5731 - val_loss: 0.0582 - val_acc: 0.5599\n",
      "Epoch 9/200\n",
      "3207/3207 [==============================] - 1s 206us/step - loss: 0.0570 - acc: 0.5734 - val_loss: 0.0582 - val_acc: 0.5580\n",
      "Epoch 10/200\n",
      "3207/3207 [==============================] - 1s 209us/step - loss: 0.0569 - acc: 0.5737 - val_loss: 0.0582 - val_acc: 0.5611\n",
      "Epoch 11/200\n",
      "3207/3207 [==============================] - 1s 216us/step - loss: 0.0569 - acc: 0.5750 - val_loss: 0.0582 - val_acc: 0.5599\n",
      "Epoch 12/200\n",
      "3207/3207 [==============================] - 1s 204us/step - loss: 0.0569 - acc: 0.5741 - val_loss: 0.0582 - val_acc: 0.5592\n",
      "Epoch 13/200\n",
      "3207/3207 [==============================] - 1s 209us/step - loss: 0.0569 - acc: 0.5750 - val_loss: 0.0582 - val_acc: 0.5605\n",
      "Epoch 14/200\n",
      "3207/3207 [==============================] - 1s 213us/step - loss: 0.0569 - acc: 0.5769 - val_loss: 0.0582 - val_acc: 0.5599\n",
      "Epoch 15/200\n",
      "3207/3207 [==============================] - 1s 208us/step - loss: 0.0569 - acc: 0.5769 - val_loss: 0.0582 - val_acc: 0.5586\n",
      "Epoch 16/200\n",
      "3207/3207 [==============================] - 1s 212us/step - loss: 0.0568 - acc: 0.5747 - val_loss: 0.0583 - val_acc: 0.5605\n",
      "Epoch 17/200\n",
      "3207/3207 [==============================] - 1s 210us/step - loss: 0.0568 - acc: 0.5772 - val_loss: 0.0583 - val_acc: 0.5599\n",
      "Epoch 18/200\n",
      "3207/3207 [==============================] - 1s 208us/step - loss: 0.0568 - acc: 0.5762 - val_loss: 0.0583 - val_acc: 0.5580\n",
      "Epoch 19/200\n",
      "3207/3207 [==============================] - 1s 210us/step - loss: 0.0568 - acc: 0.5762 - val_loss: 0.0583 - val_acc: 0.5586\n",
      "Epoch 20/200\n",
      "3207/3207 [==============================] - 1s 212us/step - loss: 0.0568 - acc: 0.5790 - val_loss: 0.0583 - val_acc: 0.5586\n",
      "Epoch 21/200\n",
      "3207/3207 [==============================] - 1s 208us/step - loss: 0.0568 - acc: 0.5778 - val_loss: 0.0583 - val_acc: 0.5586\n",
      "Epoch 22/200\n",
      "3207/3207 [==============================] - 1s 212us/step - loss: 0.0567 - acc: 0.5787 - val_loss: 0.0583 - val_acc: 0.5580\n",
      "Epoch 23/200\n",
      "3207/3207 [==============================] - 1s 194us/step - loss: 0.0567 - acc: 0.5794 - val_loss: 0.0583 - val_acc: 0.5567\n",
      "Epoch 24/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0567 - acc: 0.5790 - val_loss: 0.0583 - val_acc: 0.5567\n",
      "Epoch 25/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0567 - acc: 0.5790 - val_loss: 0.0583 - val_acc: 0.5561\n",
      "Epoch 26/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0567 - acc: 0.5794 - val_loss: 0.0583 - val_acc: 0.5555\n",
      "Epoch 27/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0567 - acc: 0.5806 - val_loss: 0.0583 - val_acc: 0.5574\n",
      "Epoch 28/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0567 - acc: 0.5797 - val_loss: 0.0584 - val_acc: 0.5567\n",
      "Epoch 29/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0567 - acc: 0.5803 - val_loss: 0.0584 - val_acc: 0.5580\n",
      "Epoch 30/200\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0566 - acc: 0.5794 - val_loss: 0.0584 - val_acc: 0.5574\n",
      "Epoch 31/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0566 - acc: 0.5812 - val_loss: 0.0584 - val_acc: 0.5574\n",
      "Epoch 32/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0566 - acc: 0.5797 - val_loss: 0.0584 - val_acc: 0.5561\n",
      "Epoch 33/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0566 - acc: 0.5797 - val_loss: 0.0584 - val_acc: 0.5580\n",
      "Epoch 34/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0566 - acc: 0.5806 - val_loss: 0.0584 - val_acc: 0.5542\n",
      "Epoch 35/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0566 - acc: 0.5812 - val_loss: 0.0584 - val_acc: 0.5574\n",
      "Epoch 36/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0566 - acc: 0.5797 - val_loss: 0.0584 - val_acc: 0.5567\n",
      "Epoch 37/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0566 - acc: 0.5800 - val_loss: 0.0584 - val_acc: 0.5549\n",
      "Epoch 38/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0566 - acc: 0.5812 - val_loss: 0.0584 - val_acc: 0.5549\n",
      "Epoch 39/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0565 - acc: 0.5794 - val_loss: 0.0584 - val_acc: 0.5549\n",
      "Epoch 40/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0565 - acc: 0.5806 - val_loss: 0.0584 - val_acc: 0.5549\n",
      "Epoch 41/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0565 - acc: 0.5787 - val_loss: 0.0584 - val_acc: 0.5536\n",
      "Epoch 42/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0565 - acc: 0.5819 - val_loss: 0.0584 - val_acc: 0.5530\n",
      "Epoch 43/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0565 - acc: 0.5809 - val_loss: 0.0584 - val_acc: 0.5542\n",
      "Epoch 44/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0565 - acc: 0.5828 - val_loss: 0.0584 - val_acc: 0.5542\n",
      "Epoch 45/200\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0565 - acc: 0.5815 - val_loss: 0.0584 - val_acc: 0.5530\n",
      "Epoch 46/200\n",
      "3207/3207 [==============================] - 1s 206us/step - loss: 0.0565 - acc: 0.5825 - val_loss: 0.0584 - val_acc: 0.5530\n",
      "Epoch 47/200\n",
      "3207/3207 [==============================] - 1s 216us/step - loss: 0.0565 - acc: 0.5825 - val_loss: 0.0585 - val_acc: 0.5561\n",
      "Epoch 48/200\n",
      "3207/3207 [==============================] - 1s 211us/step - loss: 0.0565 - acc: 0.5815 - val_loss: 0.0585 - val_acc: 0.5524\n",
      "Epoch 49/200\n",
      "3207/3207 [==============================] - 1s 207us/step - loss: 0.0564 - acc: 0.5806 - val_loss: 0.0585 - val_acc: 0.5549\n",
      "Epoch 50/200\n",
      "3207/3207 [==============================] - 1s 215us/step - loss: 0.0565 - acc: 0.5812 - val_loss: 0.0585 - val_acc: 0.5555\n",
      "Epoch 51/200\n",
      "3207/3207 [==============================] - 1s 213us/step - loss: 0.0564 - acc: 0.5819 - val_loss: 0.0585 - val_acc: 0.5567\n",
      "Epoch 52/200\n",
      "3207/3207 [==============================] - 1s 211us/step - loss: 0.0564 - acc: 0.5803 - val_loss: 0.0585 - val_acc: 0.5549\n",
      "Epoch 53/200\n",
      "3207/3207 [==============================] - 1s 200us/step - loss: 0.0564 - acc: 0.5812 - val_loss: 0.0585 - val_acc: 0.5517\n",
      "Epoch 54/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0564 - acc: 0.5815 - val_loss: 0.0585 - val_acc: 0.5530\n",
      "Epoch 55/200\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0564 - acc: 0.5803 - val_loss: 0.0585 - val_acc: 0.5511\n",
      "Epoch 56/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0564 - acc: 0.5812 - val_loss: 0.0585 - val_acc: 0.5524\n",
      "Epoch 57/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0564 - acc: 0.5812 - val_loss: 0.0585 - val_acc: 0.5517\n",
      "Epoch 58/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0564 - acc: 0.5800 - val_loss: 0.0585 - val_acc: 0.5536\n",
      "Epoch 59/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0564 - acc: 0.5812 - val_loss: 0.0585 - val_acc: 0.5536\n",
      "Epoch 60/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0564 - acc: 0.5803 - val_loss: 0.0585 - val_acc: 0.5524\n",
      "Epoch 61/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0563 - acc: 0.5822 - val_loss: 0.0585 - val_acc: 0.5542\n",
      "Epoch 62/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0563 - acc: 0.5819 - val_loss: 0.0585 - val_acc: 0.5567\n",
      "Epoch 63/200\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0563 - acc: 0.5812 - val_loss: 0.0585 - val_acc: 0.5530\n",
      "Epoch 64/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0563 - acc: 0.5819 - val_loss: 0.0585 - val_acc: 0.5530\n",
      "Epoch 65/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0563 - acc: 0.5822 - val_loss: 0.0585 - val_acc: 0.5542\n",
      "Epoch 66/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0563 - acc: 0.5831 - val_loss: 0.0586 - val_acc: 0.5530\n",
      "Epoch 67/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0563 - acc: 0.5822 - val_loss: 0.0585 - val_acc: 0.5542\n",
      "Epoch 68/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0563 - acc: 0.5822 - val_loss: 0.0585 - val_acc: 0.5536\n",
      "Epoch 69/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0563 - acc: 0.5831 - val_loss: 0.0586 - val_acc: 0.5542\n",
      "Epoch 70/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0563 - acc: 0.5815 - val_loss: 0.0585 - val_acc: 0.5536\n",
      "Epoch 71/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0563 - acc: 0.5834 - val_loss: 0.0586 - val_acc: 0.5542\n",
      "Epoch 72/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0563 - acc: 0.5834 - val_loss: 0.0586 - val_acc: 0.5524\n",
      "Epoch 73/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0563 - acc: 0.5834 - val_loss: 0.0586 - val_acc: 0.5536\n",
      "Epoch 74/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0562 - acc: 0.5847 - val_loss: 0.0586 - val_acc: 0.5530\n",
      "Epoch 75/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0563 - acc: 0.5837 - val_loss: 0.0586 - val_acc: 0.5542\n",
      "Epoch 76/200\n",
      "3207/3207 [==============================] - 1s 172us/step - loss: 0.0562 - acc: 0.5840 - val_loss: 0.0586 - val_acc: 0.5574\n",
      "Epoch 77/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0562 - acc: 0.5834 - val_loss: 0.0586 - val_acc: 0.5536\n",
      "Epoch 78/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0562 - acc: 0.5843 - val_loss: 0.0586 - val_acc: 0.5530\n",
      "Epoch 79/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0562 - acc: 0.5856 - val_loss: 0.0586 - val_acc: 0.5549\n",
      "Epoch 80/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0562 - acc: 0.5815 - val_loss: 0.0586 - val_acc: 0.5524\n",
      "Epoch 81/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0562 - acc: 0.5847 - val_loss: 0.0586 - val_acc: 0.5536\n",
      "Epoch 82/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0562 - acc: 0.5815 - val_loss: 0.0586 - val_acc: 0.5505\n",
      "Epoch 83/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0562 - acc: 0.5834 - val_loss: 0.0586 - val_acc: 0.5511\n",
      "Epoch 84/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0562 - acc: 0.5840 - val_loss: 0.0586 - val_acc: 0.5517\n",
      "Epoch 85/200\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0562 - acc: 0.5840 - val_loss: 0.0586 - val_acc: 0.5536\n",
      "Epoch 86/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0562 - acc: 0.5825 - val_loss: 0.0586 - val_acc: 0.5530\n",
      "Epoch 87/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0562 - acc: 0.5840 - val_loss: 0.0586 - val_acc: 0.5499\n",
      "Epoch 88/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0562 - acc: 0.5847 - val_loss: 0.0586 - val_acc: 0.5517\n",
      "Epoch 89/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0562 - acc: 0.5828 - val_loss: 0.0586 - val_acc: 0.5530\n",
      "Epoch 90/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0562 - acc: 0.5837 - val_loss: 0.0586 - val_acc: 0.5524\n",
      "Epoch 91/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0562 - acc: 0.5843 - val_loss: 0.0586 - val_acc: 0.5505\n",
      "Epoch 92/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0562 - acc: 0.5843 - val_loss: 0.0586 - val_acc: 0.5505\n",
      "Epoch 93/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0561 - acc: 0.5843 - val_loss: 0.0586 - val_acc: 0.5517\n",
      "Epoch 94/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0561 - acc: 0.5837 - val_loss: 0.0586 - val_acc: 0.5511\n",
      "Epoch 95/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0561 - acc: 0.5850 - val_loss: 0.0586 - val_acc: 0.5505\n",
      "Epoch 96/200\n",
      "3207/3207 [==============================] - 1s 159us/step - loss: 0.0561 - acc: 0.5840 - val_loss: 0.0586 - val_acc: 0.5499\n",
      "Epoch 97/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0561 - acc: 0.5850 - val_loss: 0.0586 - val_acc: 0.5493\n",
      "Epoch 98/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0561 - acc: 0.5840 - val_loss: 0.0586 - val_acc: 0.5499\n",
      "Epoch 99/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0561 - acc: 0.5853 - val_loss: 0.0587 - val_acc: 0.5517\n",
      "Epoch 100/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0561 - acc: 0.5834 - val_loss: 0.0586 - val_acc: 0.5505\n",
      "Epoch 101/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0561 - acc: 0.5843 - val_loss: 0.0587 - val_acc: 0.5499\n",
      "Epoch 102/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0561 - acc: 0.5834 - val_loss: 0.0587 - val_acc: 0.5493\n",
      "Epoch 103/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0561 - acc: 0.5834 - val_loss: 0.0587 - val_acc: 0.5493\n",
      "Epoch 104/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0561 - acc: 0.5831 - val_loss: 0.0587 - val_acc: 0.5499\n",
      "Epoch 105/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0561 - acc: 0.5843 - val_loss: 0.0587 - val_acc: 0.5505\n",
      "Epoch 106/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0561 - acc: 0.5847 - val_loss: 0.0587 - val_acc: 0.5511\n",
      "Epoch 107/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0561 - acc: 0.5853 - val_loss: 0.0587 - val_acc: 0.5517\n",
      "Epoch 108/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0561 - acc: 0.5837 - val_loss: 0.0587 - val_acc: 0.5499\n",
      "Epoch 109/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0561 - acc: 0.5843 - val_loss: 0.0587 - val_acc: 0.5493\n",
      "Epoch 110/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0561 - acc: 0.5850 - val_loss: 0.0587 - val_acc: 0.5493\n",
      "Epoch 111/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0561 - acc: 0.5868 - val_loss: 0.0587 - val_acc: 0.5486\n",
      "Epoch 112/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0561 - acc: 0.5859 - val_loss: 0.0587 - val_acc: 0.5499\n",
      "Epoch 113/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0560 - acc: 0.5847 - val_loss: 0.0587 - val_acc: 0.5493\n",
      "Epoch 114/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0560 - acc: 0.5853 - val_loss: 0.0587 - val_acc: 0.5486\n",
      "Epoch 115/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0560 - acc: 0.5859 - val_loss: 0.0587 - val_acc: 0.5493\n",
      "Epoch 116/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0560 - acc: 0.5850 - val_loss: 0.0587 - val_acc: 0.5505\n",
      "Epoch 117/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0560 - acc: 0.5843 - val_loss: 0.0587 - val_acc: 0.5474\n",
      "Epoch 118/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0560 - acc: 0.5840 - val_loss: 0.0587 - val_acc: 0.5474\n",
      "Epoch 119/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0560 - acc: 0.5859 - val_loss: 0.0587 - val_acc: 0.5486\n",
      "Epoch 120/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0560 - acc: 0.5853 - val_loss: 0.0587 - val_acc: 0.5486\n",
      "Epoch 121/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0560 - acc: 0.5843 - val_loss: 0.0587 - val_acc: 0.5493\n",
      "Epoch 122/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0560 - acc: 0.5834 - val_loss: 0.0587 - val_acc: 0.5480\n",
      "Epoch 123/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0560 - acc: 0.5843 - val_loss: 0.0587 - val_acc: 0.5480\n",
      "Epoch 124/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0560 - acc: 0.5847 - val_loss: 0.0587 - val_acc: 0.5480\n",
      "Epoch 125/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0560 - acc: 0.5859 - val_loss: 0.0587 - val_acc: 0.5480\n",
      "Epoch 126/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0560 - acc: 0.5850 - val_loss: 0.0587 - val_acc: 0.5480\n",
      "Epoch 127/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0560 - acc: 0.5837 - val_loss: 0.0587 - val_acc: 0.5486\n",
      "Epoch 128/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0560 - acc: 0.5856 - val_loss: 0.0587 - val_acc: 0.5486\n",
      "Epoch 129/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0560 - acc: 0.5856 - val_loss: 0.0587 - val_acc: 0.5468\n",
      "Epoch 130/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0560 - acc: 0.5872 - val_loss: 0.0587 - val_acc: 0.5480\n",
      "Epoch 131/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0560 - acc: 0.5843 - val_loss: 0.0587 - val_acc: 0.5480\n",
      "Epoch 132/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0560 - acc: 0.5840 - val_loss: 0.0587 - val_acc: 0.5486\n",
      "Epoch 133/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0560 - acc: 0.5862 - val_loss: 0.0587 - val_acc: 0.5474\n",
      "Epoch 134/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0560 - acc: 0.5853 - val_loss: 0.0587 - val_acc: 0.5474\n",
      "Epoch 135/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0560 - acc: 0.5856 - val_loss: 0.0587 - val_acc: 0.5474\n",
      "Epoch 136/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0560 - acc: 0.5853 - val_loss: 0.0587 - val_acc: 0.5468\n",
      "Epoch 137/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0559 - acc: 0.5850 - val_loss: 0.0587 - val_acc: 0.5468\n",
      "Epoch 138/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0559 - acc: 0.5850 - val_loss: 0.0587 - val_acc: 0.5480\n",
      "Epoch 139/200\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0559 - acc: 0.5850 - val_loss: 0.0587 - val_acc: 0.5474\n",
      "Epoch 140/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0559 - acc: 0.5853 - val_loss: 0.0587 - val_acc: 0.5468\n",
      "Epoch 141/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0559 - acc: 0.5856 - val_loss: 0.0587 - val_acc: 0.5474\n",
      "Epoch 142/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0559 - acc: 0.5840 - val_loss: 0.0587 - val_acc: 0.5468\n",
      "Epoch 143/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0559 - acc: 0.5856 - val_loss: 0.0588 - val_acc: 0.5480\n",
      "Epoch 144/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0559 - acc: 0.5847 - val_loss: 0.0587 - val_acc: 0.5486\n",
      "Epoch 145/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0559 - acc: 0.5853 - val_loss: 0.0587 - val_acc: 0.5474\n",
      "Epoch 146/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0559 - acc: 0.5850 - val_loss: 0.0587 - val_acc: 0.5474\n",
      "Epoch 147/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0559 - acc: 0.5850 - val_loss: 0.0587 - val_acc: 0.5474\n",
      "Epoch 148/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0559 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5480\n",
      "Epoch 149/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0559 - acc: 0.5859 - val_loss: 0.0588 - val_acc: 0.5480\n",
      "Epoch 150/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0559 - acc: 0.5856 - val_loss: 0.0588 - val_acc: 0.5480\n",
      "Epoch 151/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0559 - acc: 0.5850 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 152/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0559 - acc: 0.5859 - val_loss: 0.0588 - val_acc: 0.5461\n",
      "Epoch 153/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0559 - acc: 0.5859 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 154/200\n",
      "3207/3207 [==============================] - 1s 160us/step - loss: 0.0559 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5461\n",
      "Epoch 155/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0559 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 156/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0559 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5461\n",
      "Epoch 157/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0559 - acc: 0.5850 - val_loss: 0.0588 - val_acc: 0.5461\n",
      "Epoch 158/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0559 - acc: 0.5859 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 159/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0559 - acc: 0.5840 - val_loss: 0.0588 - val_acc: 0.5480\n",
      "Epoch 160/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0559 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 161/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0559 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5493\n",
      "Epoch 162/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0559 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5486\n",
      "Epoch 163/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0559 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 164/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0559 - acc: 0.5865 - val_loss: 0.0588 - val_acc: 0.5486\n",
      "Epoch 165/200\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0558 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 166/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0558 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5455\n",
      "Epoch 167/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0558 - acc: 0.5856 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 168/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0558 - acc: 0.5837 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 169/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0558 - acc: 0.5856 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 170/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0558 - acc: 0.5859 - val_loss: 0.0588 - val_acc: 0.5480\n",
      "Epoch 171/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0558 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5480\n",
      "Epoch 172/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0558 - acc: 0.5843 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 173/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0558 - acc: 0.5862 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 174/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0558 - acc: 0.5847 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 175/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0558 - acc: 0.5856 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 176/200\n",
      "3207/3207 [==============================] - 1s 172us/step - loss: 0.0558 - acc: 0.5840 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 177/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0558 - acc: 0.5859 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 178/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0558 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 179/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0558 - acc: 0.5856 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 180/200\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0558 - acc: 0.5859 - val_loss: 0.0588 - val_acc: 0.5480\n",
      "Epoch 181/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0558 - acc: 0.5856 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 182/200\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0558 - acc: 0.5859 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 183/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0558 - acc: 0.5862 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 184/200\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0558 - acc: 0.5868 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 185/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0558 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 186/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0558 - acc: 0.5865 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 187/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0558 - acc: 0.5850 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 188/200\n",
      "3207/3207 [==============================] - 1s 169us/step - loss: 0.0558 - acc: 0.5859 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 189/200\n",
      "3207/3207 [==============================] - 1s 164us/step - loss: 0.0558 - acc: 0.5865 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 190/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0558 - acc: 0.5872 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 191/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0558 - acc: 0.5856 - val_loss: 0.0588 - val_acc: 0.5468\n",
      "Epoch 192/200\n",
      "3207/3207 [==============================] - 1s 165us/step - loss: 0.0558 - acc: 0.5862 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 193/200\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0558 - acc: 0.5853 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 194/200\n",
      "3207/3207 [==============================] - 1s 162us/step - loss: 0.0558 - acc: 0.5847 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 195/200\n",
      "3207/3207 [==============================] - 1s 166us/step - loss: 0.0558 - acc: 0.5856 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 196/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0558 - acc: 0.5850 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 197/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0558 - acc: 0.5862 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 198/200\n",
      "3207/3207 [==============================] - 1s 163us/step - loss: 0.0558 - acc: 0.5859 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 199/200\n",
      "3207/3207 [==============================] - 1s 167us/step - loss: 0.0557 - acc: 0.5856 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Epoch 200/200\n",
      "3207/3207 [==============================] - 1s 161us/step - loss: 0.0557 - acc: 0.5865 - val_loss: 0.0588 - val_acc: 0.5474\n",
      "Train on 3208 samples, validate on 1603 samples\n",
      "Epoch 1/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0577 - acc: 0.5602 - val_loss: 0.0549 - val_acc: 0.5995\n",
      "Epoch 2/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0577 - acc: 0.5605 - val_loss: 0.0549 - val_acc: 0.5970\n",
      "Epoch 3/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0577 - acc: 0.5608 - val_loss: 0.0550 - val_acc: 0.5970\n",
      "Epoch 4/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0577 - acc: 0.5605 - val_loss: 0.0550 - val_acc: 0.5964\n",
      "Epoch 5/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0576 - acc: 0.5608 - val_loss: 0.0550 - val_acc: 0.5958\n",
      "Epoch 6/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0576 - acc: 0.5617 - val_loss: 0.0550 - val_acc: 0.5964\n",
      "Epoch 7/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0576 - acc: 0.5611 - val_loss: 0.0550 - val_acc: 0.5951\n",
      "Epoch 8/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0576 - acc: 0.5614 - val_loss: 0.0551 - val_acc: 0.5964\n",
      "Epoch 9/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0576 - acc: 0.5617 - val_loss: 0.0551 - val_acc: 0.5951\n",
      "Epoch 10/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0576 - acc: 0.5627 - val_loss: 0.0551 - val_acc: 0.5964\n",
      "Epoch 11/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0576 - acc: 0.5617 - val_loss: 0.0551 - val_acc: 0.5945\n",
      "Epoch 12/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0576 - acc: 0.5633 - val_loss: 0.0551 - val_acc: 0.5951\n",
      "Epoch 13/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0575 - acc: 0.5623 - val_loss: 0.0551 - val_acc: 0.5951\n",
      "Epoch 14/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0575 - acc: 0.5627 - val_loss: 0.0551 - val_acc: 0.5958\n",
      "Epoch 15/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0575 - acc: 0.5627 - val_loss: 0.0551 - val_acc: 0.5951\n",
      "Epoch 16/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0575 - acc: 0.5620 - val_loss: 0.0551 - val_acc: 0.5964\n",
      "Epoch 17/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0575 - acc: 0.5627 - val_loss: 0.0551 - val_acc: 0.5945\n",
      "Epoch 18/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0575 - acc: 0.5623 - val_loss: 0.0551 - val_acc: 0.5945\n",
      "Epoch 19/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0575 - acc: 0.5630 - val_loss: 0.0551 - val_acc: 0.5914\n",
      "Epoch 20/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0575 - acc: 0.5636 - val_loss: 0.0551 - val_acc: 0.5933\n",
      "Epoch 21/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0575 - acc: 0.5623 - val_loss: 0.0551 - val_acc: 0.5926\n",
      "Epoch 22/200\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0575 - acc: 0.5636 - val_loss: 0.0551 - val_acc: 0.5926\n",
      "Epoch 23/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0575 - acc: 0.5617 - val_loss: 0.0552 - val_acc: 0.5920\n",
      "Epoch 24/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0575 - acc: 0.5636 - val_loss: 0.0552 - val_acc: 0.5926\n",
      "Epoch 25/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0575 - acc: 0.5627 - val_loss: 0.0552 - val_acc: 0.5920\n",
      "Epoch 26/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0574 - acc: 0.5630 - val_loss: 0.0552 - val_acc: 0.5933\n",
      "Epoch 27/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0574 - acc: 0.5633 - val_loss: 0.0552 - val_acc: 0.5908\n",
      "Epoch 28/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0574 - acc: 0.5617 - val_loss: 0.0552 - val_acc: 0.5920\n",
      "Epoch 29/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0574 - acc: 0.5623 - val_loss: 0.0552 - val_acc: 0.5939\n",
      "Epoch 30/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0574 - acc: 0.5645 - val_loss: 0.0552 - val_acc: 0.5914\n",
      "Epoch 31/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0574 - acc: 0.5655 - val_loss: 0.0552 - val_acc: 0.5933\n",
      "Epoch 32/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0574 - acc: 0.5627 - val_loss: 0.0552 - val_acc: 0.5908\n",
      "Epoch 33/200\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0574 - acc: 0.5627 - val_loss: 0.0552 - val_acc: 0.5920\n",
      "Epoch 34/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0574 - acc: 0.5642 - val_loss: 0.0552 - val_acc: 0.5926\n",
      "Epoch 35/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0574 - acc: 0.5642 - val_loss: 0.0552 - val_acc: 0.5920\n",
      "Epoch 36/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0574 - acc: 0.5642 - val_loss: 0.0552 - val_acc: 0.5914\n",
      "Epoch 37/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0574 - acc: 0.5639 - val_loss: 0.0552 - val_acc: 0.5920\n",
      "Epoch 38/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0574 - acc: 0.5639 - val_loss: 0.0552 - val_acc: 0.5914\n",
      "Epoch 39/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0574 - acc: 0.5645 - val_loss: 0.0552 - val_acc: 0.5933\n",
      "Epoch 40/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0574 - acc: 0.5642 - val_loss: 0.0552 - val_acc: 0.5933\n",
      "Epoch 41/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0573 - acc: 0.5645 - val_loss: 0.0553 - val_acc: 0.5920\n",
      "Epoch 42/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0574 - acc: 0.5633 - val_loss: 0.0553 - val_acc: 0.5933\n",
      "Epoch 43/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0573 - acc: 0.5630 - val_loss: 0.0553 - val_acc: 0.5951\n",
      "Epoch 44/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0573 - acc: 0.5645 - val_loss: 0.0553 - val_acc: 0.5926\n",
      "Epoch 45/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0573 - acc: 0.5639 - val_loss: 0.0553 - val_acc: 0.5933\n",
      "Epoch 46/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0573 - acc: 0.5636 - val_loss: 0.0553 - val_acc: 0.5920\n",
      "Epoch 47/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0573 - acc: 0.5636 - val_loss: 0.0553 - val_acc: 0.5926\n",
      "Epoch 48/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0573 - acc: 0.5630 - val_loss: 0.0553 - val_acc: 0.5926\n",
      "Epoch 49/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0573 - acc: 0.5648 - val_loss: 0.0553 - val_acc: 0.5908\n",
      "Epoch 50/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0573 - acc: 0.5639 - val_loss: 0.0553 - val_acc: 0.5939\n",
      "Epoch 51/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0573 - acc: 0.5642 - val_loss: 0.0553 - val_acc: 0.5901\n",
      "Epoch 52/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0573 - acc: 0.5639 - val_loss: 0.0553 - val_acc: 0.5914\n",
      "Epoch 53/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0573 - acc: 0.5636 - val_loss: 0.0553 - val_acc: 0.5914\n",
      "Epoch 54/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0573 - acc: 0.5642 - val_loss: 0.0553 - val_acc: 0.5920\n",
      "Epoch 55/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0573 - acc: 0.5642 - val_loss: 0.0553 - val_acc: 0.5914\n",
      "Epoch 56/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0573 - acc: 0.5639 - val_loss: 0.0553 - val_acc: 0.5939\n",
      "Epoch 57/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0573 - acc: 0.5648 - val_loss: 0.0553 - val_acc: 0.5920\n",
      "Epoch 58/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0573 - acc: 0.5642 - val_loss: 0.0553 - val_acc: 0.5939\n",
      "Epoch 59/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0573 - acc: 0.5645 - val_loss: 0.0553 - val_acc: 0.5951\n",
      "Epoch 60/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0573 - acc: 0.5651 - val_loss: 0.0553 - val_acc: 0.5933\n",
      "Epoch 61/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0573 - acc: 0.5645 - val_loss: 0.0553 - val_acc: 0.5920\n",
      "Epoch 62/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0572 - acc: 0.5658 - val_loss: 0.0553 - val_acc: 0.5920\n",
      "Epoch 63/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0572 - acc: 0.5645 - val_loss: 0.0553 - val_acc: 0.5926\n",
      "Epoch 64/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0572 - acc: 0.5639 - val_loss: 0.0554 - val_acc: 0.5945\n",
      "Epoch 65/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0572 - acc: 0.5639 - val_loss: 0.0554 - val_acc: 0.5914\n",
      "Epoch 66/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0572 - acc: 0.5651 - val_loss: 0.0554 - val_acc: 0.5920\n",
      "Epoch 67/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0572 - acc: 0.5645 - val_loss: 0.0554 - val_acc: 0.5926\n",
      "Epoch 68/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0572 - acc: 0.5658 - val_loss: 0.0554 - val_acc: 0.5933\n",
      "Epoch 69/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0572 - acc: 0.5651 - val_loss: 0.0554 - val_acc: 0.5939\n",
      "Epoch 70/200\n",
      "3208/3208 [==============================] - 1s 159us/step - loss: 0.0572 - acc: 0.5664 - val_loss: 0.0554 - val_acc: 0.5926\n",
      "Epoch 71/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0572 - acc: 0.5658 - val_loss: 0.0554 - val_acc: 0.5895\n",
      "Epoch 72/200\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0572 - acc: 0.5655 - val_loss: 0.0554 - val_acc: 0.5933\n",
      "Epoch 73/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0572 - acc: 0.5651 - val_loss: 0.0554 - val_acc: 0.5958\n",
      "Epoch 74/200\n",
      "3208/3208 [==============================] - 1s 161us/step - loss: 0.0572 - acc: 0.5658 - val_loss: 0.0554 - val_acc: 0.5920\n",
      "Epoch 75/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0572 - acc: 0.5658 - val_loss: 0.0554 - val_acc: 0.5945\n",
      "Epoch 76/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0572 - acc: 0.5670 - val_loss: 0.0554 - val_acc: 0.5926\n",
      "Epoch 77/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0572 - acc: 0.5664 - val_loss: 0.0554 - val_acc: 0.5914\n",
      "Epoch 78/200\n",
      "3208/3208 [==============================] - 1s 160us/step - loss: 0.0572 - acc: 0.5670 - val_loss: 0.0554 - val_acc: 0.5951\n",
      "Epoch 79/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0572 - acc: 0.5670 - val_loss: 0.0554 - val_acc: 0.5945\n",
      "Epoch 80/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0572 - acc: 0.5667 - val_loss: 0.0554 - val_acc: 0.5926\n",
      "Epoch 81/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0572 - acc: 0.5676 - val_loss: 0.0554 - val_acc: 0.5933\n",
      "Epoch 82/200\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0572 - acc: 0.5667 - val_loss: 0.0554 - val_acc: 0.5926\n",
      "Epoch 83/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0572 - acc: 0.5676 - val_loss: 0.0554 - val_acc: 0.5914\n",
      "Epoch 84/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0572 - acc: 0.5683 - val_loss: 0.0554 - val_acc: 0.5920\n",
      "Epoch 85/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0571 - acc: 0.5680 - val_loss: 0.0554 - val_acc: 0.5914\n",
      "Epoch 86/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0572 - acc: 0.5667 - val_loss: 0.0554 - val_acc: 0.5914\n",
      "Epoch 87/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0571 - acc: 0.5686 - val_loss: 0.0554 - val_acc: 0.5908\n",
      "Epoch 88/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0571 - acc: 0.5673 - val_loss: 0.0554 - val_acc: 0.5926\n",
      "Epoch 89/200\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0571 - acc: 0.5673 - val_loss: 0.0554 - val_acc: 0.5914\n",
      "Epoch 90/200\n",
      "3208/3208 [==============================] - 1s 385us/step - loss: 0.0571 - acc: 0.5683 - val_loss: 0.0554 - val_acc: 0.5926\n",
      "Epoch 91/200\n",
      "3208/3208 [==============================] - 2s 472us/step - loss: 0.0571 - acc: 0.5680 - val_loss: 0.0555 - val_acc: 0.5926\n",
      "Epoch 92/200\n",
      "3208/3208 [==============================] - 2s 487us/step - loss: 0.0571 - acc: 0.5673 - val_loss: 0.0555 - val_acc: 0.5920\n",
      "Epoch 93/200\n",
      "3208/3208 [==============================] - 2s 559us/step - loss: 0.0571 - acc: 0.5692 - val_loss: 0.0555 - val_acc: 0.5920\n",
      "Epoch 94/200\n",
      "3208/3208 [==============================] - 2s 521us/step - loss: 0.0571 - acc: 0.5683 - val_loss: 0.0555 - val_acc: 0.5920\n",
      "Epoch 95/200\n",
      "3208/3208 [==============================] - 2s 530us/step - loss: 0.0571 - acc: 0.5683 - val_loss: 0.0555 - val_acc: 0.5926\n",
      "Epoch 96/200\n",
      "3208/3208 [==============================] - 1s 361us/step - loss: 0.0571 - acc: 0.5695 - val_loss: 0.0555 - val_acc: 0.5933\n",
      "Epoch 97/200\n",
      "3208/3208 [==============================] - 1s 414us/step - loss: 0.0571 - acc: 0.5680 - val_loss: 0.0555 - val_acc: 0.5920\n",
      "Epoch 98/200\n",
      "3208/3208 [==============================] - 1s 422us/step - loss: 0.0571 - acc: 0.5701 - val_loss: 0.0555 - val_acc: 0.5926\n",
      "Epoch 99/200\n",
      "3208/3208 [==============================] - 1s 343us/step - loss: 0.0571 - acc: 0.5680 - val_loss: 0.0555 - val_acc: 0.5933\n",
      "Epoch 100/200\n",
      "3208/3208 [==============================] - 1s 307us/step - loss: 0.0571 - acc: 0.5689 - val_loss: 0.0555 - val_acc: 0.5933\n",
      "Epoch 101/200\n",
      "3208/3208 [==============================] - 1s 365us/step - loss: 0.0571 - acc: 0.5686 - val_loss: 0.0555 - val_acc: 0.5926\n",
      "Epoch 102/200\n",
      "3208/3208 [==============================] - 1s 375us/step - loss: 0.0571 - acc: 0.5686 - val_loss: 0.0555 - val_acc: 0.5926\n",
      "Epoch 103/200\n",
      "3208/3208 [==============================] - 1s 368us/step - loss: 0.0571 - acc: 0.5686 - val_loss: 0.0555 - val_acc: 0.5920\n",
      "Epoch 104/200\n",
      "3208/3208 [==============================] - 1s 437us/step - loss: 0.0571 - acc: 0.5692 - val_loss: 0.0555 - val_acc: 0.5926\n",
      "Epoch 105/200\n",
      "3208/3208 [==============================] - 1s 343us/step - loss: 0.0571 - acc: 0.5695 - val_loss: 0.0555 - val_acc: 0.5908\n",
      "Epoch 106/200\n",
      "3208/3208 [==============================] - 1s 356us/step - loss: 0.0571 - acc: 0.5686 - val_loss: 0.0555 - val_acc: 0.5914\n",
      "Epoch 107/200\n",
      "3208/3208 [==============================] - 1s 188us/step - loss: 0.0571 - acc: 0.5686 - val_loss: 0.0555 - val_acc: 0.5933\n",
      "Epoch 108/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0571 - acc: 0.5695 - val_loss: 0.0555 - val_acc: 0.5901\n",
      "Epoch 109/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0571 - acc: 0.5698 - val_loss: 0.0555 - val_acc: 0.5908\n",
      "Epoch 110/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0571 - acc: 0.5692 - val_loss: 0.0555 - val_acc: 0.5933\n",
      "Epoch 111/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0571 - acc: 0.5692 - val_loss: 0.0555 - val_acc: 0.5926\n",
      "Epoch 112/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0571 - acc: 0.5701 - val_loss: 0.0555 - val_acc: 0.5939\n",
      "Epoch 113/200\n",
      "3208/3208 [==============================] - 1s 209us/step - loss: 0.0571 - acc: 0.5711 - val_loss: 0.0555 - val_acc: 0.5920\n",
      "Epoch 114/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0571 - acc: 0.5689 - val_loss: 0.0555 - val_acc: 0.5908\n",
      "Epoch 115/200\n",
      "3208/3208 [==============================] - 1s 184us/step - loss: 0.0570 - acc: 0.5704 - val_loss: 0.0555 - val_acc: 0.5908\n",
      "Epoch 116/200\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0570 - acc: 0.5711 - val_loss: 0.0555 - val_acc: 0.5901\n",
      "Epoch 117/200\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0570 - acc: 0.5695 - val_loss: 0.0555 - val_acc: 0.5895\n",
      "Epoch 118/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0570 - acc: 0.5692 - val_loss: 0.0555 - val_acc: 0.5895\n",
      "Epoch 119/200\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0570 - acc: 0.5686 - val_loss: 0.0555 - val_acc: 0.5901\n",
      "Epoch 120/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0570 - acc: 0.5701 - val_loss: 0.0556 - val_acc: 0.5901\n",
      "Epoch 121/200\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0570 - acc: 0.5689 - val_loss: 0.0555 - val_acc: 0.5914\n",
      "Epoch 122/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0570 - acc: 0.5686 - val_loss: 0.0556 - val_acc: 0.5914\n",
      "Epoch 123/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0570 - acc: 0.5704 - val_loss: 0.0555 - val_acc: 0.5914\n",
      "Epoch 124/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0570 - acc: 0.5698 - val_loss: 0.0556 - val_acc: 0.5908\n",
      "Epoch 125/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0570 - acc: 0.5704 - val_loss: 0.0556 - val_acc: 0.5908\n",
      "Epoch 126/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0570 - acc: 0.5701 - val_loss: 0.0556 - val_acc: 0.5901\n",
      "Epoch 127/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0570 - acc: 0.5695 - val_loss: 0.0556 - val_acc: 0.5895\n",
      "Epoch 128/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0570 - acc: 0.5701 - val_loss: 0.0556 - val_acc: 0.5895\n",
      "Epoch 129/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0570 - acc: 0.5701 - val_loss: 0.0556 - val_acc: 0.5895\n",
      "Epoch 130/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0570 - acc: 0.5708 - val_loss: 0.0556 - val_acc: 0.5889\n",
      "Epoch 131/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0570 - acc: 0.5698 - val_loss: 0.0556 - val_acc: 0.5889\n",
      "Epoch 132/200\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0570 - acc: 0.5698 - val_loss: 0.0556 - val_acc: 0.5889\n",
      "Epoch 133/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0570 - acc: 0.5711 - val_loss: 0.0556 - val_acc: 0.5895\n",
      "Epoch 134/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0570 - acc: 0.5692 - val_loss: 0.0556 - val_acc: 0.5889\n",
      "Epoch 135/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0570 - acc: 0.5711 - val_loss: 0.0556 - val_acc: 0.5883\n",
      "Epoch 136/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0570 - acc: 0.5698 - val_loss: 0.0556 - val_acc: 0.5901\n",
      "Epoch 137/200\n",
      "3208/3208 [==============================] - 1s 162us/step - loss: 0.0570 - acc: 0.5708 - val_loss: 0.0556 - val_acc: 0.5895\n",
      "Epoch 138/200\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0570 - acc: 0.5711 - val_loss: 0.0556 - val_acc: 0.5895\n",
      "Epoch 139/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0570 - acc: 0.5701 - val_loss: 0.0556 - val_acc: 0.5889\n",
      "Epoch 140/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0570 - acc: 0.5708 - val_loss: 0.0556 - val_acc: 0.5883\n",
      "Epoch 141/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0570 - acc: 0.5711 - val_loss: 0.0556 - val_acc: 0.5883\n",
      "Epoch 142/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0570 - acc: 0.5711 - val_loss: 0.0556 - val_acc: 0.5889\n",
      "Epoch 143/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0570 - acc: 0.5720 - val_loss: 0.0556 - val_acc: 0.5908\n",
      "Epoch 144/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0570 - acc: 0.5726 - val_loss: 0.0556 - val_acc: 0.5895\n",
      "Epoch 145/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0570 - acc: 0.5708 - val_loss: 0.0556 - val_acc: 0.5914\n",
      "Epoch 146/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0570 - acc: 0.5714 - val_loss: 0.0556 - val_acc: 0.5895\n",
      "Epoch 147/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0570 - acc: 0.5714 - val_loss: 0.0556 - val_acc: 0.5895\n",
      "Epoch 148/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0570 - acc: 0.5723 - val_loss: 0.0556 - val_acc: 0.5908\n",
      "Epoch 149/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0570 - acc: 0.5723 - val_loss: 0.0556 - val_acc: 0.5883\n",
      "Epoch 150/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0569 - acc: 0.5723 - val_loss: 0.0556 - val_acc: 0.5883\n",
      "Epoch 151/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0569 - acc: 0.5714 - val_loss: 0.0556 - val_acc: 0.5889\n",
      "Epoch 152/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0569 - acc: 0.5726 - val_loss: 0.0556 - val_acc: 0.5895\n",
      "Epoch 153/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0569 - acc: 0.5717 - val_loss: 0.0556 - val_acc: 0.5908\n",
      "Epoch 154/200\n",
      "3208/3208 [==============================] - 1s 163us/step - loss: 0.0569 - acc: 0.5723 - val_loss: 0.0557 - val_acc: 0.5889\n",
      "Epoch 155/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0569 - acc: 0.5723 - val_loss: 0.0556 - val_acc: 0.5883\n",
      "Epoch 156/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0569 - acc: 0.5733 - val_loss: 0.0556 - val_acc: 0.5883\n",
      "Epoch 157/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0569 - acc: 0.5714 - val_loss: 0.0557 - val_acc: 0.5883\n",
      "Epoch 158/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0569 - acc: 0.5723 - val_loss: 0.0557 - val_acc: 0.5889\n",
      "Epoch 159/200\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0569 - acc: 0.5720 - val_loss: 0.0556 - val_acc: 0.5889\n",
      "Epoch 160/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0569 - acc: 0.5729 - val_loss: 0.0557 - val_acc: 0.5895\n",
      "Epoch 161/200\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0569 - acc: 0.5717 - val_loss: 0.0557 - val_acc: 0.5883\n",
      "Epoch 162/200\n",
      "3208/3208 [==============================] - 1s 165us/step - loss: 0.0569 - acc: 0.5733 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 163/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0569 - acc: 0.5729 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 164/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0569 - acc: 0.5726 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 165/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0569 - acc: 0.5720 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 166/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0569 - acc: 0.5723 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 167/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0569 - acc: 0.5729 - val_loss: 0.0557 - val_acc: 0.5883\n",
      "Epoch 168/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0569 - acc: 0.5733 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 169/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0569 - acc: 0.5714 - val_loss: 0.0557 - val_acc: 0.5883\n",
      "Epoch 170/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0569 - acc: 0.5726 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 171/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0569 - acc: 0.5733 - val_loss: 0.0557 - val_acc: 0.5870\n",
      "Epoch 172/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0569 - acc: 0.5726 - val_loss: 0.0557 - val_acc: 0.5883\n",
      "Epoch 173/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0569 - acc: 0.5717 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 174/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5736 - val_loss: 0.0557 - val_acc: 0.5883\n",
      "Epoch 175/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5729 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 176/200\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0569 - acc: 0.5723 - val_loss: 0.0557 - val_acc: 0.5889\n",
      "Epoch 177/200\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0569 - acc: 0.5723 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 178/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0569 - acc: 0.5720 - val_loss: 0.0557 - val_acc: 0.5864\n",
      "Epoch 179/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5733 - val_loss: 0.0557 - val_acc: 0.5901\n",
      "Epoch 180/200\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0569 - acc: 0.5729 - val_loss: 0.0557 - val_acc: 0.5864\n",
      "Epoch 181/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0569 - acc: 0.5723 - val_loss: 0.0557 - val_acc: 0.5858\n",
      "Epoch 182/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0569 - acc: 0.5733 - val_loss: 0.0557 - val_acc: 0.5895\n",
      "Epoch 183/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0569 - acc: 0.5733 - val_loss: 0.0557 - val_acc: 0.5870\n",
      "Epoch 184/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0569 - acc: 0.5720 - val_loss: 0.0557 - val_acc: 0.5864\n",
      "Epoch 185/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5733 - val_loss: 0.0557 - val_acc: 0.5864\n",
      "Epoch 186/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0569 - acc: 0.5729 - val_loss: 0.0557 - val_acc: 0.5870\n",
      "Epoch 187/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5729 - val_loss: 0.0557 - val_acc: 0.5864\n",
      "Epoch 188/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5736 - val_loss: 0.0557 - val_acc: 0.5870\n",
      "Epoch 189/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5723 - val_loss: 0.0557 - val_acc: 0.5864\n",
      "Epoch 190/200\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0569 - acc: 0.5733 - val_loss: 0.0557 - val_acc: 0.5870\n",
      "Epoch 191/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0569 - acc: 0.5717 - val_loss: 0.0557 - val_acc: 0.5864\n",
      "Epoch 192/200\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0569 - acc: 0.5736 - val_loss: 0.0557 - val_acc: 0.5870\n",
      "Epoch 193/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0569 - acc: 0.5742 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 194/200\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0568 - acc: 0.5739 - val_loss: 0.0557 - val_acc: 0.5864\n",
      "Epoch 195/200\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0568 - acc: 0.5736 - val_loss: 0.0557 - val_acc: 0.5870\n",
      "Epoch 196/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0568 - acc: 0.5723 - val_loss: 0.0557 - val_acc: 0.5864\n",
      "Epoch 197/200\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0568 - acc: 0.5714 - val_loss: 0.0557 - val_acc: 0.5876\n",
      "Epoch 198/200\n",
      "3208/3208 [==============================] - 1s 166us/step - loss: 0.0568 - acc: 0.5739 - val_loss: 0.0557 - val_acc: 0.5870\n",
      "Epoch 199/200\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0568 - acc: 0.5726 - val_loss: 0.0557 - val_acc: 0.5870\n",
      "Epoch 200/200\n",
      "3208/3208 [==============================] - 1s 164us/step - loss: 0.0568 - acc: 0.5736 - val_loss: 0.0558 - val_acc: 0.5870\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='sigmoid'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01, decay=0.002)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=200, validation_data=(dev_valid_x, dev_valid_y))\n",
    "\n",
    "pred_y = nn_model.predict(valid_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(valid_ordinal_y), pred_y)\n",
    "result_cv = result_cv.append({'detail':'kfold=3 single layer decay=0.002', 'accuracy':accuracy_score(np.array(valid_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoHOTiSBrYee"
   },
   "source": [
    "By experimenting different models, I found out the the best model is with parameters, kfold=3 single layer decay=0.002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "88TGqOHpOmzb",
    "outputId": "28deb156-4516-425e-a34a-5671122331c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detail</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>0.431421</td>\n",
       "      <td>0.212384</td>\n",
       "      <td>0.173564</td>\n",
       "      <td>0.152795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dropout 0.2</td>\n",
       "      <td>0.453865</td>\n",
       "      <td>0.092566</td>\n",
       "      <td>0.168310</td>\n",
       "      <td>0.118978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kfold=3</td>\n",
       "      <td>0.588529</td>\n",
       "      <td>0.457233</td>\n",
       "      <td>0.393595</td>\n",
       "      <td>0.408977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sgd</td>\n",
       "      <td>0.307564</td>\n",
       "      <td>0.030756</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.047044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.584372</td>\n",
       "      <td>0.491248</td>\n",
       "      <td>0.379481</td>\n",
       "      <td>0.388334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.532003</td>\n",
       "      <td>0.191954</td>\n",
       "      <td>0.242110</td>\n",
       "      <td>0.211166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.564422</td>\n",
       "      <td>0.311657</td>\n",
       "      <td>0.299206</td>\n",
       "      <td>0.280823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kfold=3 short model</td>\n",
       "      <td>0.553616</td>\n",
       "      <td>0.397617</td>\n",
       "      <td>0.368348</td>\n",
       "      <td>0.374616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kfold=3 short model less epoch</td>\n",
       "      <td>0.576891</td>\n",
       "      <td>0.392635</td>\n",
       "      <td>0.369482</td>\n",
       "      <td>0.371070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kfold=3 single layer</td>\n",
       "      <td>0.584372</td>\n",
       "      <td>0.441956</td>\n",
       "      <td>0.411238</td>\n",
       "      <td>0.419377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kfold=3 single layer decay=0.002</td>\n",
       "      <td>0.589360</td>\n",
       "      <td>0.478010</td>\n",
       "      <td>0.390414</td>\n",
       "      <td>0.405967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              detail  accuracy  precision    recall        f1\n",
       "0                            default  0.431421   0.212384  0.173564  0.152795\n",
       "1                        dropout 0.2  0.453865   0.092566  0.168310  0.118978\n",
       "2                            kfold=3  0.588529   0.457233  0.393595  0.408977\n",
       "3                                sgd  0.307564   0.030756  0.100000  0.047044\n",
       "4                            RMSprop  0.584372   0.491248  0.379481  0.388334\n",
       "5                            Adagrad  0.532003   0.191954  0.242110  0.211166\n",
       "6                           Adadelta  0.564422   0.311657  0.299206  0.280823\n",
       "7                kfold=3 short model  0.553616   0.397617  0.368348  0.374616\n",
       "8     kfold=3 short model less epoch  0.576891   0.392635  0.369482  0.371070\n",
       "9               kfold=3 single layer  0.584372   0.441956  0.411238  0.419377\n",
       "10  kfold=3 single layer decay=0.002  0.589360   0.478010  0.390414  0.405967"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d02l0bYCde5V"
   },
   "outputs": [],
   "source": [
    "result_cv_dt = pd.DataFrame(columns=['detail', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "dt = DecisionTreeClassifier()\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  dt.fit(dev_train_x, dev_train_y)\n",
    "  pred_y = dt.predict(dev_valid_x)\n",
    "  precision, recall, fscore, support = score(dev_valid_y, pred_y)\n",
    "  result_cv_dt = result_cv_dt.append({'detail':'default', 'accuracy':accuracy_score(dev_valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "LLXuzgMpOtbe",
    "outputId": "e7c2ec86-4981-4553-a6cd-1daf78674aa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy     0.572229\n",
       "precision    0.403081\n",
       "recall       0.406789\n",
       "f1           0.402817\n",
       "dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cv_dt.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrDbpifDelZw"
   },
   "outputs": [],
   "source": [
    "result_cv_knn = pd.DataFrame(columns=['detail', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "knn = KNeighborsClassifier()\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  knn.fit(dev_train_x, dev_train_y)\n",
    "  pred_y = knn.predict(dev_valid_x)\n",
    "  precision, recall, fscore, support = score(dev_valid_y, pred_y)\n",
    "  result_cv_knn = result_cv_knn.append({'detail':'default', 'accuracy':accuracy_score(dev_valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "qRIH82HkfKrk",
    "outputId": "ad9b0d41-88da-4ced-de21-9f774fd1bc8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy     0.406152\n",
       "precision    0.577222\n",
       "recall       0.225971\n",
       "f1           0.286922\n",
       "dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cv_knn.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "9Gn93FZzfMpO",
    "outputId": "fd70ec27-fff6-4904-930b-bf839fcaa81f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "result_cv_rf = pd.DataFrame(columns=['detail', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "rf = RandomForestClassifier(n_estimators=60)\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  rf.fit(dev_train_x, dev_train_y)\n",
    "  pred_y = rf.predict(dev_valid_x)\n",
    "  precision, recall, fscore, support = score(dev_valid_y, pred_y)\n",
    "  result_cv_rf = result_cv_rf.append({'detail':'default', 'accuracy':accuracy_score(dev_valid_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "FTHvIUyDfWxn",
    "outputId": "c11df46d-017c-4e27-9b49-5e1317c359c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy     0.481189\n",
       "precision    0.394476\n",
       "recall       0.216149\n",
       "f1           0.248381\n",
       "dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cv_rf.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Naoj699f2Z-"
   },
   "source": [
    "## 5. Test set result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "038GES_qf6i8"
   },
   "outputs": [],
   "source": [
    "result_final = pd.DataFrame(columns=['model', 'accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfbctepKoU06"
   },
   "outputs": [],
   "source": [
    "pred_y = dt.predict(test_x)\n",
    "precision, recall, fscore, support = score(test_y, pred_y)\n",
    "result_final = result_final.append({'model':'dt', 'accuracy':accuracy_score(test_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "iDWPtC3qpTIw",
    "outputId": "8e2509cf-2bb9-4346-b925-1451559ccbd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_y = knn.predict(test_x)\n",
    "precision, recall, fscore, support = score(test_y, pred_y)\n",
    "result_final = result_final.append({'model':'knn', 'accuracy':accuracy_score(test_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "xaX9Z5idpVE5",
    "outputId": "cc3b51e1-8e27-4e73-8c13-07318082ef49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_y = rf.predict(test_x)\n",
    "precision, recall, fscore, support = score(test_y, pred_y)\n",
    "result_final = result_final.append({'model':'rf', 'accuracy':accuracy_score(test_y,pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 15555
    },
    "colab_type": "code",
    "id": "GU5uQZx7t0fc",
    "outputId": "cfee49b9-7ecd-44f8-d098-43b83257055f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_95 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,386\n",
      "Trainable params: 1,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/150\n",
      "3207/3207 [==============================] - 2s 555us/step - loss: 0.0730 - acc: 0.3923 - val_loss: 0.0658 - val_acc: 0.4545\n",
      "Epoch 2/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0651 - acc: 0.4553 - val_loss: 0.0652 - val_acc: 0.4626\n",
      "Epoch 3/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0640 - acc: 0.4606 - val_loss: 0.0638 - val_acc: 0.4782\n",
      "Epoch 4/150\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0628 - acc: 0.4752 - val_loss: 0.0630 - val_acc: 0.4701\n",
      "Epoch 5/150\n",
      "3207/3207 [==============================] - 1s 192us/step - loss: 0.0619 - acc: 0.4920 - val_loss: 0.0638 - val_acc: 0.4894\n",
      "Epoch 6/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0615 - acc: 0.4939 - val_loss: 0.0617 - val_acc: 0.4975\n",
      "Epoch 7/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0605 - acc: 0.5123 - val_loss: 0.0611 - val_acc: 0.4938\n",
      "Epoch 8/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0599 - acc: 0.5136 - val_loss: 0.0607 - val_acc: 0.5062\n",
      "Epoch 9/150\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0595 - acc: 0.5260 - val_loss: 0.0608 - val_acc: 0.5000\n",
      "Epoch 10/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0591 - acc: 0.5313 - val_loss: 0.0602 - val_acc: 0.5118\n",
      "Epoch 11/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0589 - acc: 0.5326 - val_loss: 0.0603 - val_acc: 0.5112\n",
      "Epoch 12/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0584 - acc: 0.5410 - val_loss: 0.0601 - val_acc: 0.5118\n",
      "Epoch 13/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0582 - acc: 0.5479 - val_loss: 0.0601 - val_acc: 0.5137\n",
      "Epoch 14/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0579 - acc: 0.5447 - val_loss: 0.0598 - val_acc: 0.5237\n",
      "Epoch 15/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0577 - acc: 0.5566 - val_loss: 0.0595 - val_acc: 0.5175\n",
      "Epoch 16/150\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0574 - acc: 0.5566 - val_loss: 0.0603 - val_acc: 0.5218\n",
      "Epoch 17/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0573 - acc: 0.5628 - val_loss: 0.0599 - val_acc: 0.5299\n",
      "Epoch 18/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0572 - acc: 0.5616 - val_loss: 0.0595 - val_acc: 0.5281\n",
      "Epoch 19/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0571 - acc: 0.5619 - val_loss: 0.0596 - val_acc: 0.5343\n",
      "Epoch 20/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0569 - acc: 0.5697 - val_loss: 0.0593 - val_acc: 0.5337\n",
      "Epoch 21/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0567 - acc: 0.5728 - val_loss: 0.0594 - val_acc: 0.5355\n",
      "Epoch 22/150\n",
      "3207/3207 [==============================] - 1s 175us/step - loss: 0.0567 - acc: 0.5731 - val_loss: 0.0596 - val_acc: 0.5362\n",
      "Epoch 23/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0565 - acc: 0.5756 - val_loss: 0.0594 - val_acc: 0.5411\n",
      "Epoch 24/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0563 - acc: 0.5744 - val_loss: 0.0599 - val_acc: 0.5293\n",
      "Epoch 25/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0564 - acc: 0.5750 - val_loss: 0.0593 - val_acc: 0.5293\n",
      "Epoch 26/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0564 - acc: 0.5722 - val_loss: 0.0593 - val_acc: 0.5324\n",
      "Epoch 27/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0562 - acc: 0.5775 - val_loss: 0.0596 - val_acc: 0.5455\n",
      "Epoch 28/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0561 - acc: 0.5806 - val_loss: 0.0594 - val_acc: 0.5399\n",
      "Epoch 29/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0561 - acc: 0.5809 - val_loss: 0.0593 - val_acc: 0.5424\n",
      "Epoch 30/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0559 - acc: 0.5803 - val_loss: 0.0594 - val_acc: 0.5480\n",
      "Epoch 31/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0559 - acc: 0.5872 - val_loss: 0.0595 - val_acc: 0.5405\n",
      "Epoch 32/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0559 - acc: 0.5812 - val_loss: 0.0590 - val_acc: 0.5330\n",
      "Epoch 33/150\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0558 - acc: 0.5906 - val_loss: 0.0591 - val_acc: 0.5461\n",
      "Epoch 34/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0556 - acc: 0.5865 - val_loss: 0.0593 - val_acc: 0.5474\n",
      "Epoch 35/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0558 - acc: 0.5881 - val_loss: 0.0591 - val_acc: 0.5493\n",
      "Epoch 36/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0556 - acc: 0.5896 - val_loss: 0.0591 - val_acc: 0.5449\n",
      "Epoch 37/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0555 - acc: 0.5915 - val_loss: 0.0589 - val_acc: 0.5524\n",
      "Epoch 38/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0555 - acc: 0.5959 - val_loss: 0.0591 - val_acc: 0.5449\n",
      "Epoch 39/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0554 - acc: 0.5921 - val_loss: 0.0590 - val_acc: 0.5511\n",
      "Epoch 40/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0554 - acc: 0.5934 - val_loss: 0.0592 - val_acc: 0.5418\n",
      "Epoch 41/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0554 - acc: 0.5940 - val_loss: 0.0591 - val_acc: 0.5493\n",
      "Epoch 42/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0553 - acc: 0.5928 - val_loss: 0.0590 - val_acc: 0.5480\n",
      "Epoch 43/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0554 - acc: 0.5931 - val_loss: 0.0591 - val_acc: 0.5486\n",
      "Epoch 44/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0553 - acc: 0.5968 - val_loss: 0.0590 - val_acc: 0.5493\n",
      "Epoch 45/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0551 - acc: 0.5943 - val_loss: 0.0589 - val_acc: 0.5480\n",
      "Epoch 46/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0551 - acc: 0.5981 - val_loss: 0.0590 - val_acc: 0.5499\n",
      "Epoch 47/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0552 - acc: 0.5962 - val_loss: 0.0591 - val_acc: 0.5443\n",
      "Epoch 48/150\n",
      "3207/3207 [==============================] - 1s 198us/step - loss: 0.0552 - acc: 0.5974 - val_loss: 0.0590 - val_acc: 0.5505\n",
      "Epoch 49/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0551 - acc: 0.5971 - val_loss: 0.0593 - val_acc: 0.5493\n",
      "Epoch 50/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0550 - acc: 0.5949 - val_loss: 0.0589 - val_acc: 0.5511\n",
      "Epoch 51/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0550 - acc: 0.5978 - val_loss: 0.0590 - val_acc: 0.5468\n",
      "Epoch 52/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0550 - acc: 0.5987 - val_loss: 0.0590 - val_acc: 0.5486\n",
      "Epoch 53/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0550 - acc: 0.5984 - val_loss: 0.0589 - val_acc: 0.5499\n",
      "Epoch 54/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0549 - acc: 0.5999 - val_loss: 0.0590 - val_acc: 0.5499\n",
      "Epoch 55/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0549 - acc: 0.5965 - val_loss: 0.0589 - val_acc: 0.5493\n",
      "Epoch 56/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0548 - acc: 0.6034 - val_loss: 0.0592 - val_acc: 0.5480\n",
      "Epoch 57/150\n",
      "3207/3207 [==============================] - 1s 191us/step - loss: 0.0549 - acc: 0.5999 - val_loss: 0.0591 - val_acc: 0.5517\n",
      "Epoch 58/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0548 - acc: 0.5996 - val_loss: 0.0591 - val_acc: 0.5486\n",
      "Epoch 59/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0548 - acc: 0.6009 - val_loss: 0.0590 - val_acc: 0.5505\n",
      "Epoch 60/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0548 - acc: 0.5956 - val_loss: 0.0590 - val_acc: 0.5493\n",
      "Epoch 61/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0548 - acc: 0.5996 - val_loss: 0.0592 - val_acc: 0.5486\n",
      "Epoch 62/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0548 - acc: 0.6006 - val_loss: 0.0592 - val_acc: 0.5493\n",
      "Epoch 63/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0547 - acc: 0.5962 - val_loss: 0.0589 - val_acc: 0.5499\n",
      "Epoch 64/150\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0546 - acc: 0.6009 - val_loss: 0.0593 - val_acc: 0.5493\n",
      "Epoch 65/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0547 - acc: 0.6024 - val_loss: 0.0590 - val_acc: 0.5486\n",
      "Epoch 66/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0546 - acc: 0.6024 - val_loss: 0.0591 - val_acc: 0.5493\n",
      "Epoch 67/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0547 - acc: 0.5996 - val_loss: 0.0592 - val_acc: 0.5505\n",
      "Epoch 68/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0547 - acc: 0.6006 - val_loss: 0.0590 - val_acc: 0.5486\n",
      "Epoch 69/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0546 - acc: 0.6012 - val_loss: 0.0592 - val_acc: 0.5486\n",
      "Epoch 70/150\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0546 - acc: 0.5984 - val_loss: 0.0590 - val_acc: 0.5493\n",
      "Epoch 71/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0546 - acc: 0.6027 - val_loss: 0.0591 - val_acc: 0.5499\n",
      "Epoch 72/150\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0546 - acc: 0.5993 - val_loss: 0.0591 - val_acc: 0.5499\n",
      "Epoch 73/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0545 - acc: 0.5987 - val_loss: 0.0591 - val_acc: 0.5474\n",
      "Epoch 74/150\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0545 - acc: 0.6031 - val_loss: 0.0590 - val_acc: 0.5474\n",
      "Epoch 75/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0545 - acc: 0.6015 - val_loss: 0.0589 - val_acc: 0.5505\n",
      "Epoch 76/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0545 - acc: 0.6018 - val_loss: 0.0589 - val_acc: 0.5499\n",
      "Epoch 77/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0545 - acc: 0.6034 - val_loss: 0.0592 - val_acc: 0.5505\n",
      "Epoch 78/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0545 - acc: 0.6009 - val_loss: 0.0592 - val_acc: 0.5474\n",
      "Epoch 79/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0545 - acc: 0.6006 - val_loss: 0.0590 - val_acc: 0.5524\n",
      "Epoch 80/150\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0545 - acc: 0.6046 - val_loss: 0.0589 - val_acc: 0.5505\n",
      "Epoch 81/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0544 - acc: 0.6018 - val_loss: 0.0590 - val_acc: 0.5493\n",
      "Epoch 82/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0544 - acc: 0.6027 - val_loss: 0.0592 - val_acc: 0.5505\n",
      "Epoch 83/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0544 - acc: 0.5999 - val_loss: 0.0591 - val_acc: 0.5511\n",
      "Epoch 84/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0544 - acc: 0.6034 - val_loss: 0.0589 - val_acc: 0.5511\n",
      "Epoch 85/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0544 - acc: 0.6034 - val_loss: 0.0591 - val_acc: 0.5493\n",
      "Epoch 86/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0544 - acc: 0.6018 - val_loss: 0.0590 - val_acc: 0.5511\n",
      "Epoch 87/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0543 - acc: 0.6024 - val_loss: 0.0592 - val_acc: 0.5517\n",
      "Epoch 88/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0544 - acc: 0.6027 - val_loss: 0.0591 - val_acc: 0.5517\n",
      "Epoch 89/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0543 - acc: 0.6018 - val_loss: 0.0591 - val_acc: 0.5505\n",
      "Epoch 90/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0543 - acc: 0.6056 - val_loss: 0.0592 - val_acc: 0.5524\n",
      "Epoch 91/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0543 - acc: 0.6052 - val_loss: 0.0591 - val_acc: 0.5486\n",
      "Epoch 92/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0543 - acc: 0.6021 - val_loss: 0.0590 - val_acc: 0.5511\n",
      "Epoch 93/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0543 - acc: 0.6049 - val_loss: 0.0591 - val_acc: 0.5505\n",
      "Epoch 94/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0543 - acc: 0.6052 - val_loss: 0.0590 - val_acc: 0.5493\n",
      "Epoch 95/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0543 - acc: 0.6046 - val_loss: 0.0590 - val_acc: 0.5517\n",
      "Epoch 96/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0543 - acc: 0.6027 - val_loss: 0.0589 - val_acc: 0.5517\n",
      "Epoch 97/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0542 - acc: 0.6049 - val_loss: 0.0590 - val_acc: 0.5499\n",
      "Epoch 98/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0542 - acc: 0.6049 - val_loss: 0.0591 - val_acc: 0.5511\n",
      "Epoch 99/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0542 - acc: 0.6062 - val_loss: 0.0590 - val_acc: 0.5530\n",
      "Epoch 100/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0542 - acc: 0.6046 - val_loss: 0.0591 - val_acc: 0.5511\n",
      "Epoch 101/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0542 - acc: 0.6049 - val_loss: 0.0591 - val_acc: 0.5499\n",
      "Epoch 102/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0542 - acc: 0.6049 - val_loss: 0.0591 - val_acc: 0.5511\n",
      "Epoch 103/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0541 - acc: 0.6071 - val_loss: 0.0593 - val_acc: 0.5517\n",
      "Epoch 104/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0542 - acc: 0.6034 - val_loss: 0.0592 - val_acc: 0.5530\n",
      "Epoch 105/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0541 - acc: 0.6065 - val_loss: 0.0590 - val_acc: 0.5486\n",
      "Epoch 106/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0542 - acc: 0.6059 - val_loss: 0.0591 - val_acc: 0.5493\n",
      "Epoch 107/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0541 - acc: 0.6062 - val_loss: 0.0592 - val_acc: 0.5493\n",
      "Epoch 108/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0541 - acc: 0.6056 - val_loss: 0.0590 - val_acc: 0.5530\n",
      "Epoch 109/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0541 - acc: 0.6040 - val_loss: 0.0591 - val_acc: 0.5524\n",
      "Epoch 110/150\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0541 - acc: 0.6059 - val_loss: 0.0590 - val_acc: 0.5524\n",
      "Epoch 111/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0541 - acc: 0.6034 - val_loss: 0.0590 - val_acc: 0.5511\n",
      "Epoch 112/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0541 - acc: 0.6105 - val_loss: 0.0590 - val_acc: 0.5517\n",
      "Epoch 113/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0541 - acc: 0.6084 - val_loss: 0.0590 - val_acc: 0.5542\n",
      "Epoch 114/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0541 - acc: 0.6059 - val_loss: 0.0590 - val_acc: 0.5517\n",
      "Epoch 115/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0541 - acc: 0.6074 - val_loss: 0.0590 - val_acc: 0.5517\n",
      "Epoch 116/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0541 - acc: 0.6071 - val_loss: 0.0591 - val_acc: 0.5536\n",
      "Epoch 117/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0541 - acc: 0.6065 - val_loss: 0.0590 - val_acc: 0.5517\n",
      "Epoch 118/150\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0541 - acc: 0.6065 - val_loss: 0.0591 - val_acc: 0.5536\n",
      "Epoch 119/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0540 - acc: 0.6056 - val_loss: 0.0591 - val_acc: 0.5536\n",
      "Epoch 120/150\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0540 - acc: 0.6087 - val_loss: 0.0590 - val_acc: 0.5542\n",
      "Epoch 121/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0540 - acc: 0.6102 - val_loss: 0.0591 - val_acc: 0.5542\n",
      "Epoch 122/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0540 - acc: 0.6068 - val_loss: 0.0590 - val_acc: 0.5542\n",
      "Epoch 123/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0540 - acc: 0.6077 - val_loss: 0.0590 - val_acc: 0.5536\n",
      "Epoch 124/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0540 - acc: 0.6074 - val_loss: 0.0590 - val_acc: 0.5530\n",
      "Epoch 125/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0540 - acc: 0.6105 - val_loss: 0.0591 - val_acc: 0.5542\n",
      "Epoch 126/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0540 - acc: 0.6102 - val_loss: 0.0590 - val_acc: 0.5567\n",
      "Epoch 127/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0540 - acc: 0.6090 - val_loss: 0.0590 - val_acc: 0.5561\n",
      "Epoch 128/150\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0540 - acc: 0.6071 - val_loss: 0.0590 - val_acc: 0.5530\n",
      "Epoch 129/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0540 - acc: 0.6074 - val_loss: 0.0591 - val_acc: 0.5542\n",
      "Epoch 130/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0540 - acc: 0.6087 - val_loss: 0.0591 - val_acc: 0.5555\n",
      "Epoch 131/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0539 - acc: 0.6077 - val_loss: 0.0592 - val_acc: 0.5542\n",
      "Epoch 132/150\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0539 - acc: 0.6074 - val_loss: 0.0591 - val_acc: 0.5549\n",
      "Epoch 133/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0539 - acc: 0.6099 - val_loss: 0.0590 - val_acc: 0.5536\n",
      "Epoch 134/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0539 - acc: 0.6112 - val_loss: 0.0590 - val_acc: 0.5536\n",
      "Epoch 135/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0539 - acc: 0.6099 - val_loss: 0.0590 - val_acc: 0.5542\n",
      "Epoch 136/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0539 - acc: 0.6087 - val_loss: 0.0591 - val_acc: 0.5561\n",
      "Epoch 137/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0539 - acc: 0.6074 - val_loss: 0.0592 - val_acc: 0.5567\n",
      "Epoch 138/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0539 - acc: 0.6102 - val_loss: 0.0591 - val_acc: 0.5542\n",
      "Epoch 139/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0539 - acc: 0.6077 - val_loss: 0.0591 - val_acc: 0.5549\n",
      "Epoch 140/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0539 - acc: 0.6102 - val_loss: 0.0591 - val_acc: 0.5574\n",
      "Epoch 141/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0539 - acc: 0.6109 - val_loss: 0.0591 - val_acc: 0.5574\n",
      "Epoch 142/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0539 - acc: 0.6099 - val_loss: 0.0590 - val_acc: 0.5555\n",
      "Epoch 143/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0539 - acc: 0.6080 - val_loss: 0.0591 - val_acc: 0.5561\n",
      "Epoch 144/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0539 - acc: 0.6109 - val_loss: 0.0590 - val_acc: 0.5567\n",
      "Epoch 145/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0539 - acc: 0.6109 - val_loss: 0.0591 - val_acc: 0.5561\n",
      "Epoch 146/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0539 - acc: 0.6084 - val_loss: 0.0590 - val_acc: 0.5542\n",
      "Epoch 147/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0539 - acc: 0.6102 - val_loss: 0.0590 - val_acc: 0.5567\n",
      "Epoch 148/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0539 - acc: 0.6090 - val_loss: 0.0591 - val_acc: 0.5524\n",
      "Epoch 149/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0538 - acc: 0.6093 - val_loss: 0.0590 - val_acc: 0.5561\n",
      "Epoch 150/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0538 - acc: 0.6109 - val_loss: 0.0590 - val_acc: 0.5561\n",
      "Train on 3207 samples, validate on 1604 samples\n",
      "Epoch 1/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0558 - acc: 0.5928 - val_loss: 0.0551 - val_acc: 0.5904\n",
      "Epoch 2/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0557 - acc: 0.5925 - val_loss: 0.0551 - val_acc: 0.5879\n",
      "Epoch 3/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0555 - acc: 0.5934 - val_loss: 0.0552 - val_acc: 0.5885\n",
      "Epoch 4/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0555 - acc: 0.5912 - val_loss: 0.0553 - val_acc: 0.5867\n",
      "Epoch 5/150\n",
      "3207/3207 [==============================] - 1s 190us/step - loss: 0.0554 - acc: 0.5931 - val_loss: 0.0553 - val_acc: 0.5892\n",
      "Epoch 6/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0553 - acc: 0.5915 - val_loss: 0.0554 - val_acc: 0.5873\n",
      "Epoch 7/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0553 - acc: 0.5934 - val_loss: 0.0554 - val_acc: 0.5885\n",
      "Epoch 8/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0552 - acc: 0.5946 - val_loss: 0.0555 - val_acc: 0.5879\n",
      "Epoch 9/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0551 - acc: 0.5925 - val_loss: 0.0556 - val_acc: 0.5860\n",
      "Epoch 10/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0551 - acc: 0.5953 - val_loss: 0.0556 - val_acc: 0.5848\n",
      "Epoch 11/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0550 - acc: 0.5959 - val_loss: 0.0557 - val_acc: 0.5848\n",
      "Epoch 12/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0550 - acc: 0.5943 - val_loss: 0.0557 - val_acc: 0.5867\n",
      "Epoch 13/150\n",
      "3207/3207 [==============================] - 1s 189us/step - loss: 0.0550 - acc: 0.5934 - val_loss: 0.0558 - val_acc: 0.5848\n",
      "Epoch 14/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0549 - acc: 0.5931 - val_loss: 0.0558 - val_acc: 0.5842\n",
      "Epoch 15/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0549 - acc: 0.5940 - val_loss: 0.0559 - val_acc: 0.5835\n",
      "Epoch 16/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0548 - acc: 0.5953 - val_loss: 0.0559 - val_acc: 0.5842\n",
      "Epoch 17/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0548 - acc: 0.5953 - val_loss: 0.0559 - val_acc: 0.5817\n",
      "Epoch 18/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0548 - acc: 0.5971 - val_loss: 0.0559 - val_acc: 0.5829\n",
      "Epoch 19/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0548 - acc: 0.5956 - val_loss: 0.0560 - val_acc: 0.5810\n",
      "Epoch 20/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0547 - acc: 0.5949 - val_loss: 0.0560 - val_acc: 0.5810\n",
      "Epoch 21/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0547 - acc: 0.5959 - val_loss: 0.0560 - val_acc: 0.5810\n",
      "Epoch 22/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0547 - acc: 0.5974 - val_loss: 0.0561 - val_acc: 0.5804\n",
      "Epoch 23/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0547 - acc: 0.5962 - val_loss: 0.0561 - val_acc: 0.5810\n",
      "Epoch 24/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0546 - acc: 0.5971 - val_loss: 0.0561 - val_acc: 0.5810\n",
      "Epoch 25/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0546 - acc: 0.5965 - val_loss: 0.0561 - val_acc: 0.5798\n",
      "Epoch 26/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0546 - acc: 0.5981 - val_loss: 0.0562 - val_acc: 0.5804\n",
      "Epoch 27/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0546 - acc: 0.5981 - val_loss: 0.0562 - val_acc: 0.5786\n",
      "Epoch 28/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0545 - acc: 0.5987 - val_loss: 0.0562 - val_acc: 0.5786\n",
      "Epoch 29/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0545 - acc: 0.5965 - val_loss: 0.0563 - val_acc: 0.5779\n",
      "Epoch 30/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0545 - acc: 0.5984 - val_loss: 0.0563 - val_acc: 0.5786\n",
      "Epoch 31/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0545 - acc: 0.6006 - val_loss: 0.0563 - val_acc: 0.5773\n",
      "Epoch 32/150\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0545 - acc: 0.5978 - val_loss: 0.0563 - val_acc: 0.5786\n",
      "Epoch 33/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0544 - acc: 0.5996 - val_loss: 0.0563 - val_acc: 0.5761\n",
      "Epoch 34/150\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0544 - acc: 0.6002 - val_loss: 0.0563 - val_acc: 0.5773\n",
      "Epoch 35/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0544 - acc: 0.5993 - val_loss: 0.0564 - val_acc: 0.5767\n",
      "Epoch 36/150\n",
      "3207/3207 [==============================] - 1s 172us/step - loss: 0.0544 - acc: 0.6018 - val_loss: 0.0564 - val_acc: 0.5773\n",
      "Epoch 37/150\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0544 - acc: 0.5993 - val_loss: 0.0564 - val_acc: 0.5754\n",
      "Epoch 38/150\n",
      "3207/3207 [==============================] - 1s 175us/step - loss: 0.0543 - acc: 0.5996 - val_loss: 0.0564 - val_acc: 0.5736\n",
      "Epoch 39/150\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0543 - acc: 0.5987 - val_loss: 0.0565 - val_acc: 0.5767\n",
      "Epoch 40/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0543 - acc: 0.6002 - val_loss: 0.0565 - val_acc: 0.5748\n",
      "Epoch 41/150\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0543 - acc: 0.5996 - val_loss: 0.0565 - val_acc: 0.5754\n",
      "Epoch 42/150\n",
      "3207/3207 [==============================] - 1s 175us/step - loss: 0.0543 - acc: 0.5999 - val_loss: 0.0565 - val_acc: 0.5742\n",
      "Epoch 43/150\n",
      "3207/3207 [==============================] - 1s 172us/step - loss: 0.0543 - acc: 0.6012 - val_loss: 0.0565 - val_acc: 0.5754\n",
      "Epoch 44/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0543 - acc: 0.6018 - val_loss: 0.0565 - val_acc: 0.5748\n",
      "Epoch 45/150\n",
      "3207/3207 [==============================] - 1s 168us/step - loss: 0.0543 - acc: 0.6009 - val_loss: 0.0565 - val_acc: 0.5736\n",
      "Epoch 46/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0543 - acc: 0.6002 - val_loss: 0.0566 - val_acc: 0.5736\n",
      "Epoch 47/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0542 - acc: 0.5996 - val_loss: 0.0566 - val_acc: 0.5736\n",
      "Epoch 48/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0542 - acc: 0.5999 - val_loss: 0.0567 - val_acc: 0.5717\n",
      "Epoch 49/150\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0542 - acc: 0.6037 - val_loss: 0.0566 - val_acc: 0.5754\n",
      "Epoch 50/150\n",
      "3207/3207 [==============================] - 1s 172us/step - loss: 0.0542 - acc: 0.6012 - val_loss: 0.0566 - val_acc: 0.5736\n",
      "Epoch 51/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0542 - acc: 0.6024 - val_loss: 0.0567 - val_acc: 0.5748\n",
      "Epoch 52/150\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0542 - acc: 0.6021 - val_loss: 0.0566 - val_acc: 0.5729\n",
      "Epoch 53/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0542 - acc: 0.6012 - val_loss: 0.0566 - val_acc: 0.5729\n",
      "Epoch 54/150\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0542 - acc: 0.6015 - val_loss: 0.0566 - val_acc: 0.5742\n",
      "Epoch 55/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0541 - acc: 0.6015 - val_loss: 0.0567 - val_acc: 0.5742\n",
      "Epoch 56/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0542 - acc: 0.6024 - val_loss: 0.0567 - val_acc: 0.5729\n",
      "Epoch 57/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0541 - acc: 0.6043 - val_loss: 0.0567 - val_acc: 0.5748\n",
      "Epoch 58/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0541 - acc: 0.6037 - val_loss: 0.0567 - val_acc: 0.5742\n",
      "Epoch 59/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0541 - acc: 0.6012 - val_loss: 0.0567 - val_acc: 0.5754\n",
      "Epoch 60/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0541 - acc: 0.6031 - val_loss: 0.0568 - val_acc: 0.5748\n",
      "Epoch 61/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0541 - acc: 0.6043 - val_loss: 0.0568 - val_acc: 0.5729\n",
      "Epoch 62/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0541 - acc: 0.6043 - val_loss: 0.0568 - val_acc: 0.5723\n",
      "Epoch 63/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0541 - acc: 0.6031 - val_loss: 0.0568 - val_acc: 0.5729\n",
      "Epoch 64/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0541 - acc: 0.6031 - val_loss: 0.0568 - val_acc: 0.5723\n",
      "Epoch 65/150\n",
      "3207/3207 [==============================] - 1s 188us/step - loss: 0.0541 - acc: 0.6040 - val_loss: 0.0568 - val_acc: 0.5717\n",
      "Epoch 66/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0540 - acc: 0.6040 - val_loss: 0.0568 - val_acc: 0.5717\n",
      "Epoch 67/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0540 - acc: 0.6043 - val_loss: 0.0568 - val_acc: 0.5729\n",
      "Epoch 68/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0540 - acc: 0.6043 - val_loss: 0.0568 - val_acc: 0.5723\n",
      "Epoch 69/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0540 - acc: 0.6012 - val_loss: 0.0568 - val_acc: 0.5723\n",
      "Epoch 70/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0540 - acc: 0.6021 - val_loss: 0.0569 - val_acc: 0.5711\n",
      "Epoch 71/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0540 - acc: 0.6049 - val_loss: 0.0569 - val_acc: 0.5717\n",
      "Epoch 72/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0540 - acc: 0.6040 - val_loss: 0.0569 - val_acc: 0.5704\n",
      "Epoch 73/150\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0540 - acc: 0.6024 - val_loss: 0.0569 - val_acc: 0.5717\n",
      "Epoch 74/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0540 - acc: 0.6059 - val_loss: 0.0569 - val_acc: 0.5717\n",
      "Epoch 75/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0539 - acc: 0.6040 - val_loss: 0.0569 - val_acc: 0.5704\n",
      "Epoch 76/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0540 - acc: 0.6049 - val_loss: 0.0569 - val_acc: 0.5711\n",
      "Epoch 77/150\n",
      "3207/3207 [==============================] - 1s 172us/step - loss: 0.0539 - acc: 0.6031 - val_loss: 0.0569 - val_acc: 0.5686\n",
      "Epoch 78/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0539 - acc: 0.6024 - val_loss: 0.0569 - val_acc: 0.5692\n",
      "Epoch 79/150\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0539 - acc: 0.6021 - val_loss: 0.0569 - val_acc: 0.5680\n",
      "Epoch 80/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0539 - acc: 0.6037 - val_loss: 0.0570 - val_acc: 0.5680\n",
      "Epoch 81/150\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0539 - acc: 0.6018 - val_loss: 0.0570 - val_acc: 0.5686\n",
      "Epoch 82/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0539 - acc: 0.6043 - val_loss: 0.0570 - val_acc: 0.5692\n",
      "Epoch 83/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0539 - acc: 0.6049 - val_loss: 0.0570 - val_acc: 0.5698\n",
      "Epoch 84/150\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0539 - acc: 0.6034 - val_loss: 0.0570 - val_acc: 0.5698\n",
      "Epoch 85/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0539 - acc: 0.6021 - val_loss: 0.0570 - val_acc: 0.5698\n",
      "Epoch 86/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0539 - acc: 0.6027 - val_loss: 0.0570 - val_acc: 0.5692\n",
      "Epoch 87/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0539 - acc: 0.6018 - val_loss: 0.0570 - val_acc: 0.5680\n",
      "Epoch 88/150\n",
      "3207/3207 [==============================] - 1s 170us/step - loss: 0.0539 - acc: 0.6040 - val_loss: 0.0570 - val_acc: 0.5667\n",
      "Epoch 89/150\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0539 - acc: 0.6040 - val_loss: 0.0570 - val_acc: 0.5692\n",
      "Epoch 90/150\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0538 - acc: 0.6031 - val_loss: 0.0570 - val_acc: 0.5680\n",
      "Epoch 91/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0538 - acc: 0.6021 - val_loss: 0.0570 - val_acc: 0.5692\n",
      "Epoch 92/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0538 - acc: 0.6046 - val_loss: 0.0570 - val_acc: 0.5692\n",
      "Epoch 93/150\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0538 - acc: 0.6037 - val_loss: 0.0570 - val_acc: 0.5698\n",
      "Epoch 94/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0538 - acc: 0.6052 - val_loss: 0.0571 - val_acc: 0.5667\n",
      "Epoch 95/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0538 - acc: 0.6040 - val_loss: 0.0571 - val_acc: 0.5655\n",
      "Epoch 96/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0538 - acc: 0.6031 - val_loss: 0.0571 - val_acc: 0.5673\n",
      "Epoch 97/150\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0538 - acc: 0.6040 - val_loss: 0.0571 - val_acc: 0.5661\n",
      "Epoch 98/150\n",
      "3207/3207 [==============================] - 1s 175us/step - loss: 0.0538 - acc: 0.6031 - val_loss: 0.0571 - val_acc: 0.5661\n",
      "Epoch 99/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0538 - acc: 0.6046 - val_loss: 0.0571 - val_acc: 0.5667\n",
      "Epoch 100/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0538 - acc: 0.6027 - val_loss: 0.0571 - val_acc: 0.5673\n",
      "Epoch 101/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0538 - acc: 0.6034 - val_loss: 0.0571 - val_acc: 0.5655\n",
      "Epoch 102/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0538 - acc: 0.6046 - val_loss: 0.0571 - val_acc: 0.5648\n",
      "Epoch 103/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0538 - acc: 0.6037 - val_loss: 0.0571 - val_acc: 0.5661\n",
      "Epoch 104/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0538 - acc: 0.6031 - val_loss: 0.0571 - val_acc: 0.5673\n",
      "Epoch 105/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0538 - acc: 0.6049 - val_loss: 0.0571 - val_acc: 0.5680\n",
      "Epoch 106/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0538 - acc: 0.6031 - val_loss: 0.0571 - val_acc: 0.5642\n",
      "Epoch 107/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0537 - acc: 0.6046 - val_loss: 0.0571 - val_acc: 0.5648\n",
      "Epoch 108/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0538 - acc: 0.6062 - val_loss: 0.0571 - val_acc: 0.5636\n",
      "Epoch 109/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0537 - acc: 0.6052 - val_loss: 0.0572 - val_acc: 0.5661\n",
      "Epoch 110/150\n",
      "3207/3207 [==============================] - 1s 184us/step - loss: 0.0537 - acc: 0.6065 - val_loss: 0.0572 - val_acc: 0.5642\n",
      "Epoch 111/150\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0537 - acc: 0.6049 - val_loss: 0.0572 - val_acc: 0.5636\n",
      "Epoch 112/150\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0537 - acc: 0.6040 - val_loss: 0.0572 - val_acc: 0.5648\n",
      "Epoch 113/150\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0537 - acc: 0.6059 - val_loss: 0.0572 - val_acc: 0.5630\n",
      "Epoch 114/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0537 - acc: 0.6049 - val_loss: 0.0572 - val_acc: 0.5636\n",
      "Epoch 115/150\n",
      "3207/3207 [==============================] - 1s 171us/step - loss: 0.0537 - acc: 0.6059 - val_loss: 0.0572 - val_acc: 0.5648\n",
      "Epoch 116/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0537 - acc: 0.6040 - val_loss: 0.0572 - val_acc: 0.5636\n",
      "Epoch 117/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0537 - acc: 0.6043 - val_loss: 0.0572 - val_acc: 0.5642\n",
      "Epoch 118/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0537 - acc: 0.6062 - val_loss: 0.0572 - val_acc: 0.5617\n",
      "Epoch 119/150\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0537 - acc: 0.6062 - val_loss: 0.0572 - val_acc: 0.5636\n",
      "Epoch 120/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0537 - acc: 0.6056 - val_loss: 0.0572 - val_acc: 0.5636\n",
      "Epoch 121/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0537 - acc: 0.6074 - val_loss: 0.0572 - val_acc: 0.5623\n",
      "Epoch 122/150\n",
      "3207/3207 [==============================] - 1s 175us/step - loss: 0.0537 - acc: 0.6068 - val_loss: 0.0572 - val_acc: 0.5623\n",
      "Epoch 123/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0537 - acc: 0.6065 - val_loss: 0.0572 - val_acc: 0.5630\n",
      "Epoch 124/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0537 - acc: 0.6068 - val_loss: 0.0572 - val_acc: 0.5642\n",
      "Epoch 125/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0537 - acc: 0.6065 - val_loss: 0.0572 - val_acc: 0.5630\n",
      "Epoch 126/150\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0536 - acc: 0.6059 - val_loss: 0.0572 - val_acc: 0.5636\n",
      "Epoch 127/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0536 - acc: 0.6059 - val_loss: 0.0573 - val_acc: 0.5623\n",
      "Epoch 128/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0536 - acc: 0.6087 - val_loss: 0.0573 - val_acc: 0.5617\n",
      "Epoch 129/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0536 - acc: 0.6059 - val_loss: 0.0573 - val_acc: 0.5623\n",
      "Epoch 130/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0536 - acc: 0.6068 - val_loss: 0.0573 - val_acc: 0.5623\n",
      "Epoch 131/150\n",
      "3207/3207 [==============================] - 1s 178us/step - loss: 0.0536 - acc: 0.6074 - val_loss: 0.0573 - val_acc: 0.5611\n",
      "Epoch 132/150\n",
      "3207/3207 [==============================] - 1s 176us/step - loss: 0.0536 - acc: 0.6077 - val_loss: 0.0573 - val_acc: 0.5611\n",
      "Epoch 133/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0536 - acc: 0.6087 - val_loss: 0.0573 - val_acc: 0.5630\n",
      "Epoch 134/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0536 - acc: 0.6068 - val_loss: 0.0573 - val_acc: 0.5617\n",
      "Epoch 135/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0536 - acc: 0.6087 - val_loss: 0.0573 - val_acc: 0.5623\n",
      "Epoch 136/150\n",
      "3207/3207 [==============================] - 1s 186us/step - loss: 0.0536 - acc: 0.6077 - val_loss: 0.0573 - val_acc: 0.5630\n",
      "Epoch 137/150\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0536 - acc: 0.6084 - val_loss: 0.0573 - val_acc: 0.5623\n",
      "Epoch 138/150\n",
      "3207/3207 [==============================] - 1s 183us/step - loss: 0.0536 - acc: 0.6074 - val_loss: 0.0573 - val_acc: 0.5592\n",
      "Epoch 139/150\n",
      "3207/3207 [==============================] - 1s 187us/step - loss: 0.0536 - acc: 0.6074 - val_loss: 0.0573 - val_acc: 0.5592\n",
      "Epoch 140/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0536 - acc: 0.6084 - val_loss: 0.0573 - val_acc: 0.5605\n",
      "Epoch 141/150\n",
      "3207/3207 [==============================] - 1s 182us/step - loss: 0.0536 - acc: 0.6062 - val_loss: 0.0573 - val_acc: 0.5611\n",
      "Epoch 142/150\n",
      "3207/3207 [==============================] - 1s 177us/step - loss: 0.0536 - acc: 0.6077 - val_loss: 0.0573 - val_acc: 0.5611\n",
      "Epoch 143/150\n",
      "3207/3207 [==============================] - 1s 185us/step - loss: 0.0536 - acc: 0.6080 - val_loss: 0.0573 - val_acc: 0.5611\n",
      "Epoch 144/150\n",
      "3207/3207 [==============================] - 1s 181us/step - loss: 0.0536 - acc: 0.6080 - val_loss: 0.0573 - val_acc: 0.5605\n",
      "Epoch 145/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0536 - acc: 0.6084 - val_loss: 0.0574 - val_acc: 0.5605\n",
      "Epoch 146/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0536 - acc: 0.6074 - val_loss: 0.0574 - val_acc: 0.5605\n",
      "Epoch 147/150\n",
      "3207/3207 [==============================] - 1s 173us/step - loss: 0.0536 - acc: 0.6084 - val_loss: 0.0573 - val_acc: 0.5611\n",
      "Epoch 148/150\n",
      "3207/3207 [==============================] - 1s 180us/step - loss: 0.0535 - acc: 0.6077 - val_loss: 0.0574 - val_acc: 0.5605\n",
      "Epoch 149/150\n",
      "3207/3207 [==============================] - 1s 174us/step - loss: 0.0535 - acc: 0.6087 - val_loss: 0.0574 - val_acc: 0.5605\n",
      "Epoch 150/150\n",
      "3207/3207 [==============================] - 1s 179us/step - loss: 0.0535 - acc: 0.6093 - val_loss: 0.0574 - val_acc: 0.5605\n",
      "Train on 3208 samples, validate on 1603 samples\n",
      "Epoch 1/150\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0562 - acc: 0.5751 - val_loss: 0.0520 - val_acc: 0.6351\n",
      "Epoch 2/150\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0562 - acc: 0.5757 - val_loss: 0.0521 - val_acc: 0.6338\n",
      "Epoch 3/150\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0561 - acc: 0.5773 - val_loss: 0.0522 - val_acc: 0.6313\n",
      "Epoch 4/150\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0561 - acc: 0.5789 - val_loss: 0.0522 - val_acc: 0.6326\n",
      "Epoch 5/150\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0560 - acc: 0.5779 - val_loss: 0.0523 - val_acc: 0.6276\n",
      "Epoch 6/150\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0560 - acc: 0.5786 - val_loss: 0.0523 - val_acc: 0.6301\n",
      "Epoch 7/150\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0559 - acc: 0.5795 - val_loss: 0.0523 - val_acc: 0.6282\n",
      "Epoch 8/150\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0559 - acc: 0.5795 - val_loss: 0.0524 - val_acc: 0.6276\n",
      "Epoch 9/150\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0559 - acc: 0.5798 - val_loss: 0.0524 - val_acc: 0.6294\n",
      "Epoch 10/150\n",
      "3208/3208 [==============================] - 1s 205us/step - loss: 0.0558 - acc: 0.5814 - val_loss: 0.0524 - val_acc: 0.6294\n",
      "Epoch 11/150\n",
      "3208/3208 [==============================] - 1s 220us/step - loss: 0.0558 - acc: 0.5807 - val_loss: 0.0524 - val_acc: 0.6294\n",
      "Epoch 12/150\n",
      "3208/3208 [==============================] - 1s 221us/step - loss: 0.0558 - acc: 0.5820 - val_loss: 0.0524 - val_acc: 0.6276\n",
      "Epoch 13/150\n",
      "3208/3208 [==============================] - 1s 221us/step - loss: 0.0558 - acc: 0.5820 - val_loss: 0.0525 - val_acc: 0.6269\n",
      "Epoch 14/150\n",
      "3208/3208 [==============================] - 1s 219us/step - loss: 0.0558 - acc: 0.5820 - val_loss: 0.0525 - val_acc: 0.6276\n",
      "Epoch 15/150\n",
      "3208/3208 [==============================] - 1s 224us/step - loss: 0.0557 - acc: 0.5814 - val_loss: 0.0525 - val_acc: 0.6276\n",
      "Epoch 16/150\n",
      "3208/3208 [==============================] - 1s 220us/step - loss: 0.0557 - acc: 0.5826 - val_loss: 0.0525 - val_acc: 0.6276\n",
      "Epoch 17/150\n",
      "3208/3208 [==============================] - 1s 209us/step - loss: 0.0557 - acc: 0.5832 - val_loss: 0.0526 - val_acc: 0.6276\n",
      "Epoch 18/150\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0557 - acc: 0.5823 - val_loss: 0.0526 - val_acc: 0.6276\n",
      "Epoch 19/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0556 - acc: 0.5826 - val_loss: 0.0526 - val_acc: 0.6276\n",
      "Epoch 20/150\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0556 - acc: 0.5839 - val_loss: 0.0527 - val_acc: 0.6263\n",
      "Epoch 21/150\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0556 - acc: 0.5839 - val_loss: 0.0527 - val_acc: 0.6263\n",
      "Epoch 22/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0556 - acc: 0.5835 - val_loss: 0.0527 - val_acc: 0.6257\n",
      "Epoch 23/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0556 - acc: 0.5845 - val_loss: 0.0527 - val_acc: 0.6245\n",
      "Epoch 24/150\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0556 - acc: 0.5845 - val_loss: 0.0527 - val_acc: 0.6251\n",
      "Epoch 25/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0555 - acc: 0.5854 - val_loss: 0.0527 - val_acc: 0.6232\n",
      "Epoch 26/150\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0555 - acc: 0.5857 - val_loss: 0.0528 - val_acc: 0.6232\n",
      "Epoch 27/150\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0555 - acc: 0.5857 - val_loss: 0.0528 - val_acc: 0.6238\n",
      "Epoch 28/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0555 - acc: 0.5870 - val_loss: 0.0528 - val_acc: 0.6232\n",
      "Epoch 29/150\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0555 - acc: 0.5870 - val_loss: 0.0528 - val_acc: 0.6226\n",
      "Epoch 30/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0555 - acc: 0.5857 - val_loss: 0.0528 - val_acc: 0.6245\n",
      "Epoch 31/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0554 - acc: 0.5879 - val_loss: 0.0528 - val_acc: 0.6238\n",
      "Epoch 32/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0554 - acc: 0.5857 - val_loss: 0.0528 - val_acc: 0.6245\n",
      "Epoch 33/150\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0554 - acc: 0.5876 - val_loss: 0.0529 - val_acc: 0.6220\n",
      "Epoch 34/150\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0554 - acc: 0.5879 - val_loss: 0.0529 - val_acc: 0.6226\n",
      "Epoch 35/150\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0554 - acc: 0.5863 - val_loss: 0.0529 - val_acc: 0.6213\n",
      "Epoch 36/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0554 - acc: 0.5870 - val_loss: 0.0529 - val_acc: 0.6207\n",
      "Epoch 37/150\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0554 - acc: 0.5873 - val_loss: 0.0529 - val_acc: 0.6207\n",
      "Epoch 38/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0554 - acc: 0.5867 - val_loss: 0.0529 - val_acc: 0.6195\n",
      "Epoch 39/150\n",
      "3208/3208 [==============================] - 1s 197us/step - loss: 0.0554 - acc: 0.5873 - val_loss: 0.0529 - val_acc: 0.6188\n",
      "Epoch 40/150\n",
      "3208/3208 [==============================] - 1s 218us/step - loss: 0.0553 - acc: 0.5860 - val_loss: 0.0530 - val_acc: 0.6201\n",
      "Epoch 41/150\n",
      "3208/3208 [==============================] - 1s 218us/step - loss: 0.0553 - acc: 0.5867 - val_loss: 0.0530 - val_acc: 0.6195\n",
      "Epoch 42/150\n",
      "3208/3208 [==============================] - 1s 214us/step - loss: 0.0553 - acc: 0.5873 - val_loss: 0.0530 - val_acc: 0.6201\n",
      "Epoch 43/150\n",
      "3208/3208 [==============================] - 1s 219us/step - loss: 0.0553 - acc: 0.5870 - val_loss: 0.0530 - val_acc: 0.6176\n",
      "Epoch 44/150\n",
      "3208/3208 [==============================] - 1s 220us/step - loss: 0.0553 - acc: 0.5876 - val_loss: 0.0530 - val_acc: 0.6188\n",
      "Epoch 45/150\n",
      "3208/3208 [==============================] - 1s 218us/step - loss: 0.0553 - acc: 0.5882 - val_loss: 0.0531 - val_acc: 0.6176\n",
      "Epoch 46/150\n",
      "3208/3208 [==============================] - 1s 212us/step - loss: 0.0553 - acc: 0.5901 - val_loss: 0.0531 - val_acc: 0.6182\n",
      "Epoch 47/150\n",
      "3208/3208 [==============================] - 1s 216us/step - loss: 0.0553 - acc: 0.5879 - val_loss: 0.0531 - val_acc: 0.6176\n",
      "Epoch 48/150\n",
      "3208/3208 [==============================] - 1s 216us/step - loss: 0.0553 - acc: 0.5876 - val_loss: 0.0531 - val_acc: 0.6188\n",
      "Epoch 49/150\n",
      "3208/3208 [==============================] - 1s 211us/step - loss: 0.0552 - acc: 0.5892 - val_loss: 0.0531 - val_acc: 0.6182\n",
      "Epoch 50/150\n",
      "3208/3208 [==============================] - 1s 220us/step - loss: 0.0552 - acc: 0.5895 - val_loss: 0.0531 - val_acc: 0.6182\n",
      "Epoch 51/150\n",
      "3208/3208 [==============================] - 1s 221us/step - loss: 0.0552 - acc: 0.5901 - val_loss: 0.0531 - val_acc: 0.6182\n",
      "Epoch 52/150\n",
      "3208/3208 [==============================] - 1s 217us/step - loss: 0.0552 - acc: 0.5895 - val_loss: 0.0532 - val_acc: 0.6163\n",
      "Epoch 53/150\n",
      "3208/3208 [==============================] - 1s 218us/step - loss: 0.0552 - acc: 0.5901 - val_loss: 0.0532 - val_acc: 0.6163\n",
      "Epoch 54/150\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0552 - acc: 0.5904 - val_loss: 0.0532 - val_acc: 0.6163\n",
      "Epoch 55/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0552 - acc: 0.5895 - val_loss: 0.0532 - val_acc: 0.6163\n",
      "Epoch 56/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0552 - acc: 0.5895 - val_loss: 0.0532 - val_acc: 0.6138\n",
      "Epoch 57/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0552 - acc: 0.5882 - val_loss: 0.0532 - val_acc: 0.6151\n",
      "Epoch 58/150\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0552 - acc: 0.5888 - val_loss: 0.0533 - val_acc: 0.6145\n",
      "Epoch 59/150\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0552 - acc: 0.5907 - val_loss: 0.0533 - val_acc: 0.6145\n",
      "Epoch 60/150\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0552 - acc: 0.5885 - val_loss: 0.0533 - val_acc: 0.6163\n",
      "Epoch 61/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0552 - acc: 0.5901 - val_loss: 0.0533 - val_acc: 0.6163\n",
      "Epoch 62/150\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0552 - acc: 0.5901 - val_loss: 0.0533 - val_acc: 0.6132\n",
      "Epoch 63/150\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0551 - acc: 0.5895 - val_loss: 0.0533 - val_acc: 0.6138\n",
      "Epoch 64/150\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0551 - acc: 0.5895 - val_loss: 0.0533 - val_acc: 0.6138\n",
      "Epoch 65/150\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0551 - acc: 0.5901 - val_loss: 0.0534 - val_acc: 0.6145\n",
      "Epoch 66/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0551 - acc: 0.5904 - val_loss: 0.0533 - val_acc: 0.6145\n",
      "Epoch 67/150\n",
      "3208/3208 [==============================] - 1s 185us/step - loss: 0.0551 - acc: 0.5904 - val_loss: 0.0533 - val_acc: 0.6145\n",
      "Epoch 68/150\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0551 - acc: 0.5910 - val_loss: 0.0534 - val_acc: 0.6145\n",
      "Epoch 69/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0551 - acc: 0.5901 - val_loss: 0.0534 - val_acc: 0.6138\n",
      "Epoch 70/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0551 - acc: 0.5907 - val_loss: 0.0534 - val_acc: 0.6132\n",
      "Epoch 71/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0551 - acc: 0.5907 - val_loss: 0.0534 - val_acc: 0.6120\n",
      "Epoch 72/150\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0551 - acc: 0.5923 - val_loss: 0.0534 - val_acc: 0.6126\n",
      "Epoch 73/150\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0551 - acc: 0.5904 - val_loss: 0.0534 - val_acc: 0.6120\n",
      "Epoch 74/150\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0551 - acc: 0.5923 - val_loss: 0.0534 - val_acc: 0.6114\n",
      "Epoch 75/150\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0550 - acc: 0.5913 - val_loss: 0.0535 - val_acc: 0.6114\n",
      "Epoch 76/150\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0550 - acc: 0.5913 - val_loss: 0.0535 - val_acc: 0.6114\n",
      "Epoch 77/150\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0550 - acc: 0.5907 - val_loss: 0.0535 - val_acc: 0.6101\n",
      "Epoch 78/150\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0550 - acc: 0.5913 - val_loss: 0.0535 - val_acc: 0.6126\n",
      "Epoch 79/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0550 - acc: 0.5926 - val_loss: 0.0535 - val_acc: 0.6126\n",
      "Epoch 80/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0550 - acc: 0.5913 - val_loss: 0.0535 - val_acc: 0.6107\n",
      "Epoch 81/150\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0550 - acc: 0.5923 - val_loss: 0.0535 - val_acc: 0.6107\n",
      "Epoch 82/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0550 - acc: 0.5923 - val_loss: 0.0535 - val_acc: 0.6114\n",
      "Epoch 83/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0550 - acc: 0.5916 - val_loss: 0.0535 - val_acc: 0.6101\n",
      "Epoch 84/150\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0550 - acc: 0.5920 - val_loss: 0.0536 - val_acc: 0.6114\n",
      "Epoch 85/150\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0550 - acc: 0.5926 - val_loss: 0.0536 - val_acc: 0.6114\n",
      "Epoch 86/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0550 - acc: 0.5926 - val_loss: 0.0536 - val_acc: 0.6120\n",
      "Epoch 87/150\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0550 - acc: 0.5935 - val_loss: 0.0536 - val_acc: 0.6101\n",
      "Epoch 88/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0550 - acc: 0.5932 - val_loss: 0.0536 - val_acc: 0.6095\n",
      "Epoch 89/150\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0550 - acc: 0.5920 - val_loss: 0.0536 - val_acc: 0.6107\n",
      "Epoch 90/150\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0550 - acc: 0.5926 - val_loss: 0.0536 - val_acc: 0.6095\n",
      "Epoch 91/150\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0550 - acc: 0.5932 - val_loss: 0.0536 - val_acc: 0.6114\n",
      "Epoch 92/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0549 - acc: 0.5929 - val_loss: 0.0536 - val_acc: 0.6095\n",
      "Epoch 93/150\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0550 - acc: 0.5926 - val_loss: 0.0536 - val_acc: 0.6114\n",
      "Epoch 94/150\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0549 - acc: 0.5920 - val_loss: 0.0537 - val_acc: 0.6089\n",
      "Epoch 95/150\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0550 - acc: 0.5923 - val_loss: 0.0536 - val_acc: 0.6101\n",
      "Epoch 96/150\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0549 - acc: 0.5916 - val_loss: 0.0537 - val_acc: 0.6082\n",
      "Epoch 97/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0549 - acc: 0.5923 - val_loss: 0.0537 - val_acc: 0.6101\n",
      "Epoch 98/150\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0549 - acc: 0.5935 - val_loss: 0.0537 - val_acc: 0.6082\n",
      "Epoch 99/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0549 - acc: 0.5926 - val_loss: 0.0537 - val_acc: 0.6107\n",
      "Epoch 100/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0549 - acc: 0.5916 - val_loss: 0.0538 - val_acc: 0.6076\n",
      "Epoch 101/150\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0549 - acc: 0.5932 - val_loss: 0.0537 - val_acc: 0.6076\n",
      "Epoch 102/150\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0549 - acc: 0.5938 - val_loss: 0.0537 - val_acc: 0.6095\n",
      "Epoch 103/150\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0549 - acc: 0.5941 - val_loss: 0.0537 - val_acc: 0.6082\n",
      "Epoch 104/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0549 - acc: 0.5929 - val_loss: 0.0538 - val_acc: 0.6076\n",
      "Epoch 105/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0549 - acc: 0.5932 - val_loss: 0.0537 - val_acc: 0.6089\n",
      "Epoch 106/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0549 - acc: 0.5945 - val_loss: 0.0537 - val_acc: 0.6089\n",
      "Epoch 107/150\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0549 - acc: 0.5932 - val_loss: 0.0537 - val_acc: 0.6107\n",
      "Epoch 108/150\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0549 - acc: 0.5935 - val_loss: 0.0538 - val_acc: 0.6089\n",
      "Epoch 109/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0549 - acc: 0.5938 - val_loss: 0.0538 - val_acc: 0.6095\n",
      "Epoch 110/150\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0549 - acc: 0.5938 - val_loss: 0.0538 - val_acc: 0.6095\n",
      "Epoch 111/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0549 - acc: 0.5932 - val_loss: 0.0538 - val_acc: 0.6101\n",
      "Epoch 112/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0549 - acc: 0.5923 - val_loss: 0.0539 - val_acc: 0.6070\n",
      "Epoch 113/150\n",
      "3208/3208 [==============================] - 1s 182us/step - loss: 0.0549 - acc: 0.5932 - val_loss: 0.0538 - val_acc: 0.6089\n",
      "Epoch 114/150\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0548 - acc: 0.5941 - val_loss: 0.0539 - val_acc: 0.6070\n",
      "Epoch 115/150\n",
      "3208/3208 [==============================] - 1s 186us/step - loss: 0.0548 - acc: 0.5941 - val_loss: 0.0538 - val_acc: 0.6082\n",
      "Epoch 116/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0548 - acc: 0.5935 - val_loss: 0.0538 - val_acc: 0.6095\n",
      "Epoch 117/150\n",
      "3208/3208 [==============================] - 1s 180us/step - loss: 0.0548 - acc: 0.5926 - val_loss: 0.0539 - val_acc: 0.6082\n",
      "Epoch 118/150\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0548 - acc: 0.5938 - val_loss: 0.0539 - val_acc: 0.6107\n",
      "Epoch 119/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0548 - acc: 0.5923 - val_loss: 0.0539 - val_acc: 0.6101\n",
      "Epoch 120/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0548 - acc: 0.5941 - val_loss: 0.0539 - val_acc: 0.6076\n",
      "Epoch 121/150\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0548 - acc: 0.5929 - val_loss: 0.0539 - val_acc: 0.6076\n",
      "Epoch 122/150\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0548 - acc: 0.5954 - val_loss: 0.0539 - val_acc: 0.6082\n",
      "Epoch 123/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0548 - acc: 0.5945 - val_loss: 0.0539 - val_acc: 0.6076\n",
      "Epoch 124/150\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0548 - acc: 0.5948 - val_loss: 0.0539 - val_acc: 0.6076\n",
      "Epoch 125/150\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0548 - acc: 0.5941 - val_loss: 0.0539 - val_acc: 0.6076\n",
      "Epoch 126/150\n",
      "3208/3208 [==============================] - 1s 174us/step - loss: 0.0548 - acc: 0.5951 - val_loss: 0.0539 - val_acc: 0.6082\n",
      "Epoch 127/150\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0548 - acc: 0.5941 - val_loss: 0.0539 - val_acc: 0.6070\n",
      "Epoch 128/150\n",
      "3208/3208 [==============================] - 1s 176us/step - loss: 0.0548 - acc: 0.5954 - val_loss: 0.0539 - val_acc: 0.6070\n",
      "Epoch 129/150\n",
      "3208/3208 [==============================] - 1s 179us/step - loss: 0.0548 - acc: 0.5954 - val_loss: 0.0539 - val_acc: 0.6070\n",
      "Epoch 130/150\n",
      "3208/3208 [==============================] - 1s 177us/step - loss: 0.0548 - acc: 0.5957 - val_loss: 0.0539 - val_acc: 0.6070\n",
      "Epoch 131/150\n",
      "3208/3208 [==============================] - 1s 181us/step - loss: 0.0548 - acc: 0.5951 - val_loss: 0.0540 - val_acc: 0.6070\n",
      "Epoch 132/150\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0548 - acc: 0.5957 - val_loss: 0.0540 - val_acc: 0.6070\n",
      "Epoch 133/150\n",
      "3208/3208 [==============================] - 1s 175us/step - loss: 0.0548 - acc: 0.5960 - val_loss: 0.0540 - val_acc: 0.6032\n",
      "Epoch 134/150\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0548 - acc: 0.5963 - val_loss: 0.0540 - val_acc: 0.6070\n",
      "Epoch 135/150\n",
      "3208/3208 [==============================] - 1s 173us/step - loss: 0.0548 - acc: 0.5948 - val_loss: 0.0540 - val_acc: 0.6051\n",
      "Epoch 136/150\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0548 - acc: 0.5960 - val_loss: 0.0540 - val_acc: 0.6076\n",
      "Epoch 137/150\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0548 - acc: 0.5941 - val_loss: 0.0540 - val_acc: 0.6064\n",
      "Epoch 138/150\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0547 - acc: 0.5945 - val_loss: 0.0540 - val_acc: 0.6070\n",
      "Epoch 139/150\n",
      "3208/3208 [==============================] - 1s 178us/step - loss: 0.0547 - acc: 0.5945 - val_loss: 0.0540 - val_acc: 0.6057\n",
      "Epoch 140/150\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0547 - acc: 0.5954 - val_loss: 0.0540 - val_acc: 0.6070\n",
      "Epoch 141/150\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0547 - acc: 0.5954 - val_loss: 0.0540 - val_acc: 0.6070\n",
      "Epoch 142/150\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0547 - acc: 0.5951 - val_loss: 0.0540 - val_acc: 0.6057\n",
      "Epoch 143/150\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0547 - acc: 0.5957 - val_loss: 0.0540 - val_acc: 0.6057\n",
      "Epoch 144/150\n",
      "3208/3208 [==============================] - 1s 170us/step - loss: 0.0547 - acc: 0.5954 - val_loss: 0.0540 - val_acc: 0.6057\n",
      "Epoch 145/150\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0547 - acc: 0.5976 - val_loss: 0.0540 - val_acc: 0.6057\n",
      "Epoch 146/150\n",
      "3208/3208 [==============================] - 1s 171us/step - loss: 0.0547 - acc: 0.5973 - val_loss: 0.0540 - val_acc: 0.6064\n",
      "Epoch 147/150\n",
      "3208/3208 [==============================] - 1s 167us/step - loss: 0.0547 - acc: 0.5966 - val_loss: 0.0540 - val_acc: 0.6057\n",
      "Epoch 148/150\n",
      "3208/3208 [==============================] - 1s 169us/step - loss: 0.0547 - acc: 0.5951 - val_loss: 0.0541 - val_acc: 0.6051\n",
      "Epoch 149/150\n",
      "3208/3208 [==============================] - 1s 168us/step - loss: 0.0547 - acc: 0.5954 - val_loss: 0.0541 - val_acc: 0.6039\n",
      "Epoch 150/150\n",
      "3208/3208 [==============================] - 1s 172us/step - loss: 0.0547 - acc: 0.5963 - val_loss: 0.0541 - val_acc: 0.6064\n"
     ]
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(InputLayer(input_shape=(32,)))\n",
    "nn_model.add(Dense(units=32, activation='relu'))\n",
    "nn_model.add(Dense(units=10, activation='softmax'))\n",
    "nn_model.summary()\n",
    "adam = keras.optimizers.Adam(lr=0.01, decay=0.002)\n",
    "nn_model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "for i in range(0, 3):\n",
    "  dev_valid_x = x_fold[i]\n",
    "  dev_valid_y = y_fold[i]\n",
    "  dev_train_x = dev_x.drop(index=dev_valid_x.index)\n",
    "  dev_train_y = dev_y.drop(index=dev_valid_y.index)\n",
    "  nn_model.fit(x=dev_train_x, y=dev_train_y, epochs=150, validation_data=(dev_valid_x, dev_valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyuqCeLKxTsy"
   },
   "outputs": [],
   "source": [
    "pred_y = nn_model.predict(test_x)  \n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "precision, recall, fscore, support = score(np.array(test_ordinal_y), pred_y)\n",
    "result_final = result_final.append({'model':'dnn', 'accuracy':accuracy_score(np.array(test_ordinal_y),pred_y), 'precision':np.mean(precision), 'recall':np.mean(recall), 'f1':np.mean(fscore)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "6wL5ymd3ouHP",
    "outputId": "ee400372-4a84-4f26-9a5b-87249a011298"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.591022</td>\n",
       "      <td>0.425807</td>\n",
       "      <td>0.428768</td>\n",
       "      <td>0.426074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.425603</td>\n",
       "      <td>0.512865</td>\n",
       "      <td>0.255918</td>\n",
       "      <td>0.321569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.472153</td>\n",
       "      <td>0.386389</td>\n",
       "      <td>0.217117</td>\n",
       "      <td>0.247942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dnn</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.486979</td>\n",
       "      <td>0.415067</td>\n",
       "      <td>0.432074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  accuracy  precision    recall        f1\n",
       "0    dt  0.591022   0.425807  0.428768  0.426074\n",
       "1   knn  0.425603   0.512865  0.255918  0.321569\n",
       "2    rf  0.472153   0.386389  0.217117  0.247942\n",
       "3   dnn  0.583541   0.486979  0.415067  0.432074"
      ]
     },
     "execution_count": 132,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQ5oFHoAuWCk"
   },
   "source": [
    "## 6. Result Interpretation\n",
    "### 1. Performance\n",
    "All models can't overcome underfitting issue. I tried a lot of differenct models it doesn't go up. For the test set about 60% was the best and it is same with validation and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFBMfu8YuXoz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "appstore.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
